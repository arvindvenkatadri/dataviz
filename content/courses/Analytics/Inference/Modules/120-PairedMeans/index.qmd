---
title: "\U0001F0CF Inference for Comparing Two Paired Means"
date: 10/Nov/2022
subtitle: ""
date-modified: "`r Sys.Date()`"
order: 120
tags:
- Permutation
- Monte Carlo Simulation
- Random Number Generation
- Distributions
- Generating Parallel Worlds
editor: 
  markdown: 
    wrap: 72
---

## {{< iconify noto-v1 package >}} Setting up R Packages

```{r}
#| label: setup
#| include: true

library(tidyverse)
library(mosaic)
library(broom) # Tidy Test data
library(resampledata3) # Datasets from Chihara and Hesterberg's book 
library(gt) # for tables

```

```{r}
#| label: Extra-Pedagogical-Packages
#| echo: false
#| message: false

library(checkdown)
library(epoxy)
library(TeachHist)
library(TeachingDemos)
library(visualize) # Plot Densities, Histograms and Probabilities as areas under the curve
library(grateful)
library(MKdescr)

```

#### Plot Fonts and Theme

```{r}
#| label: plot-theme
#| echo: true
#| cache: false
#| code-fold: true
#| messages: false
#| warning: false

library(systemfonts)
library(showtext)
## Clean the slate
systemfonts::clear_local_fonts()
systemfonts::clear_registry()
##
showtext_opts(dpi = 96) #set DPI for showtext
sysfonts::font_add(family = "Alegreya",
  regular = "../../../../../../fonts/Alegreya-Regular.ttf",
  bold = "../../../../../../fonts/Alegreya-Bold.ttf",
  italic = "../../../../../../fonts/Alegreya-Italic.ttf",
  bolditalic = "../../../../../../fonts/Alegreya-BoldItalic.ttf")

sysfonts::font_add(family = "Roboto Condensed", 
  regular = "../../../../../../fonts/RobotoCondensed-Regular.ttf",
  bold = "../../../../../../fonts/RobotoCondensed-Bold.ttf",
  italic = "../../../../../../fonts/RobotoCondensed-Italic.ttf",
  bolditalic = "../../../../../../fonts/RobotoCondensed-BoldItalic.ttf")
showtext_auto(enable = TRUE) #enable showtext
##
theme_custom <- function(){ 
    font <- "Alegreya"   #assign font family up front
    
    theme_classic(base_size = 14, base_family = font) %+replace%    #replace elements we want to change
    
    theme(
      text = element_text(family = font),  #set base font family
      
      #text elements
      plot.title = element_text(                 #title
                   family = font,          #set font family
                   size = 24,                    #set font size
                   face = 'bold',                #bold typeface
                   hjust = 0,                    #left align
                   margin = margin(t = 5, r = 0, b = 5, l = 0)), #margin
      plot.title.position = "plot", 
      
      plot.subtitle = element_text(              #subtitle
                   family = font,          #font family
                   size = 14,                   #font size
                   hjust = 0,                   #left align
                   margin = margin(t = 5, r = 0, b = 10, l = 0)), #margin
      
      plot.caption = element_text(               #caption
                   family = font,          #font family
                   size = 9,                     #font size
                   hjust = 1),                   #right align
      
      plot.caption.position = "plot",            #right align
      
      axis.title = element_text(                 #axis titles
                   family = "Roboto Condensed",  #font family
                   size = 12),                   #font size
      
      axis.text = element_text(                  #axis text
                   family = "Roboto Condensed",  #font family
                   size = 9),                    #font size
      
      axis.text.x = element_text(                #margin for axis text
                    margin = margin(5, b = 10))
      
      #since the legend often requires manual tweaking 
      #based on plot content, don't define it here
    )
}

## Use available fonts in ggplot text geoms too!
ggplot2::update_geom_defaults(geom = "text",new = list(
  family = "Roboto Condensed",
  face = "plain",
  size = 3.5,
  color = "#2b2b2b"
)
)

## Set the theme
ggplot2::theme_set(new = theme_custom())

```


## {{< iconify openmoji japanese-symbol-for-beginner >}} Introduction to Inference for Paired Data

### {{< iconify fluent pair-24-filled >}} What is Paired Data?

Sometimes the data is collected on the same set of individual
categories, e.g. scores by *the same* sport persons in two separate tournaments, or
sales of *identical items* in two separate locations of a chain store. Or
say the number of customers in the morning and in the evening, at a set
of store locations. In this case we treat the two sets of observations
as **paired**, since they correspond to the same set of observable
entities ( Qual !!) . This is how some experiments give us paired data.

We would naturally be interested in the differences in **means** across
these two sets, which exploits this paired nature. In this module, we will examine tests for this purpose.

### {{< iconify flat-color-icons workflow >}} Workflow for Inference for Paired Means

```{mermaid}
%%| echo: false
flowchart TD
    A[Inference for Paired Means] -->|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\n Variances: Fisher F-test var.test]
    B --> C{OK?}
    C -->|Yes, both\n Parametric| D[t.test with paired=TRUE]
    C -->|Yes, but not variance\n Parametric| W[t.test with\n Welch Correction, paired =TRUE]
    C -->|No\n Non-Parametric| E[wilcox.test with paired = TRUE]
    C -->|No\n Non-Parametric| P[Bootstrap\n or\n Permutation]
 
```

We will now use a couple to case studies to traverse all the possible
pathways in the Workflow above.

## {{< iconify pajamas issue-type-test-case >}} Case Study #1: Results from a Diving Championship

Here we have swimming records across a Semi-Final and a Final:

### {{< iconify carbon chart-3d >}} Inspecting and Charting Data

```{r}
#| results: hold

data("Diving2017", package = "resampledata3")
Diving2017
Diving2017_inspect <- inspect(Diving2017)
Diving2017_inspect$categorical
Diving2017_inspect$quantitative

```

The data is made up of **paired** observations per swimmer, one for the
*semi-final* and one for the *final* race. There are 12 swimmers and
therefore 12 paired records. How can we quickly visualize this data?

Let us first make this data into long form:

:::: {.columns}
:::{.column}
```{r}
#| label: making-diving-long
#| eval: false

Diving2017_long <- Diving2017 %>%
  pivot_longer(
    cols = c(Final, Semifinal),
    names_to = "race",
    values_to = "scores"
  )
Diving2017_long

```

:::
:::{.column}
```{r}
#| ref.label: making-diving-long
#| echo: false
```

:::
::::

Next, histograms and densities of the two variables at hand:

:::: {.columns}
:::{.column}

```{r}
#| label: diving-density-1
#| eval: false
ggplot2::theme_set(new = theme_custom())

Diving2017_long %>%
  gf_density( ~ scores,
              fill = ~ race,
              alpha = 0.5,
              title = "Diving Scores") %>%
  gf_refine(scale_fill_brewer(palette = "Set1")) %>%
  gf_facet_grid( ~ race) %>%
  gf_fitdistr(dist = "dnorm") %>% 
gf_theme(theme(legend.position = "none")) # No Need!!
  
```

:::
:::{.column}
```{r}
#| ref.label: diving-density-1
#| echo: false
```


:::
::::

:::: {.columns}
:::{.column}

```{r}
#| label: diving-col-1
#| eval: false
ggplot2::theme_set(new = theme_custom())

Diving2017_long %>%
  gf_col(
    fct_reorder(Name, scores) ~ scores,
    fill = ~ race,
    alpha = 0.5,
    position = "dodge",
    xlab = "Scores",
    ylab = "Name",
    title = "Diving Scores"
  )  %>% 
  gf_refine(scale_fill_brewer(palette = "Set1"))

```
:::
:::{.column}
```{r}
#| ref.label: diving-col-1
#| echo: false
```


:::
::::

:::: {.columns}
:::{.column}

```{r}
#| label: diving-boxplot-1
#| eval: false
ggplot2::theme_set(new = theme_custom())

Diving2017_long %>%
  gf_boxplot(
    scores ~ race,
    fill = ~ race,
    alpha = 0.5,
    xlab = "Race",
    ylab = "Scores",
    title = "Diving Scores"
  ) %>% 
    gf_refine(scale_fill_brewer(palette = "Set1")) %>% 
gf_theme(theme(legend.position = "none")) # No Need

```
:::
:::{.column}
```{r}
#| ref.label: diving-boxplot-1
#| echo: false
```


:::
::::

We see that:

-   The data are not normally distributed. With just such few readings
    (n \< 30) it was just possible...more readings would have helped. We
    will verify this aspect formally very shortly.\
-   There is no immediately identifiable **trend** in score changes from
    one race to the other.\
-   Although the two *medians* appear to be different, the box plots
    overlap considerably. So one cannot visually conclude that the two
    sets of race timings have different *means*.

### A. {{< iconify mdi chart-bell-curve size=2x >}} Check for Normality

Let us also complete a check for normality: the `shapiro.wilk` test
checks whether a Quant variable is from a normal distribution; the NULL
hypothesis is that the data *are* from a normal distribution.

```{r}
#| layout-ncol: 2
#| results: hold
shapiro.test(Diving2017$Final)
shapiro.test(Diving2017$Semifinal)

```

Hmmm....the Shapiro-Wilk test suggests that both scores ***are***
normally distributed (!!!), though `Semifinal` is probably marginally
so.

Can we check this comparison also with plots? We can plot Q-Q plots for
both variables, and also compare both data with normally-distributed
data generated with the same means and standard deviations:

```{r}
#| layout-ncol: 2
#| warning: false
ggplot2::theme_set(new = theme_custom())

Diving2017_long %>% 
  gf_qq(~ scores | race, size = 2, colour = ~ race) %>% 
  gf_qqline(ylab = "scores", colour = ~ race,
xlab = "theoretical normal", title = "Q-Q Plots for Paired Race Data") %>% 
gf_theme(theme(legend.position = "none"))

```

We see in the QQ-plots that
the `Final` scores are closer to the straight line than the `Semifinal`
scores. But it is perhaps still hard to accept the data as normally
distributed...hmm.

### B. {{< iconify mdi sigma-lower size=2x >}} Check for Variances

Let us check if the two variables have similar variances: the `var.test`
does this for us, with a NULL hypothesis that the variances are not
significantly different:

```{r}
#| message: false
var.test(scores ~ race, data = Diving2017_long,
         ratio = 1, # What we believe
         conf.int = TRUE,
         conf.level = 0.95) %>% 
  broom::tidy()

```

The variances are not significantly different, as seen by the
$p.value = 0.08$.

So to summarise our data checks:

::: callout-note
### Conditions
-   data are normally distributed\
-   variances are not significantly different\
:::

### {{< iconify academicons hypothesis >}} Hypothesis

Based on the graph, how would we formulate our Hypothesis? We wish to
infer whether there is any change in performance between the **population of swimmers** who might have participated in these two races. So accordingly:

$$
H_0: \mu_{semifinal} = \mu_{final}\\
$$

$$
H_a: \mu_{semifinal} \ne \mu_{final}\
$$

### {{< iconify academicons hypothesis >}} Observed and Test Statistic

What would be the **test statistic** we would use? The **difference in
means**. Is the observed difference in the means between the two groups
of scores non-zero? We use the `diffmean` function, with the argument
`only.2 = FALSE` to allow for *paired data*:

```{r}

obs_diff_swim <- diffmean(scores ~ race, 
                          data = Diving2017_long, 
                          only.2 = FALSE) # paired data

# Can use this also
# formula method is better for permutation test!
# obs_diff_swim <- mean(~ (Final - Semifinal), data = Diving2017)

obs_diff_swim

```

### {{< iconify fluent-mdl2 insights >}} Inference

::: {.panel-tabset .nav-pills style="background: whitesmoke;"}
#### *Paired* t.test

Since the data variables satisfy the assumption of being *normally
distributed*, and the variances are not significantly different, we may
attempt the classical `t.test` with paired data. (we will use the
`mosaic` variant). Type `help(t.test)` in your Console.
Our model would be:

$$
mean(Final(i) - Semi\_final(i)) = \beta_0 \\
$$

And that:
$$
H_0: \mu_{final} - \mu_{semifinal} = 0;
$$
$$
H_a: \mu_{final} - \mu_{semifinal} \ne 0;\\
$$

```{r}
mosaic::t.test(x = Diving2017$Semifinal, 
               y = Diving2017$Final,
               paired = TRUE, var.equal = FALSE) %>% broom::tidy()
```

The `confidence interval` spans the zero value, and the `p.value` is a
high $0.259$, so there is no reason to accept alternative hypothesis
that the means are different. Hence we say that there is no evidence of
a difference between `SemiFinal` and `Final` scores.

#### *Paired* Wilcoxon test

Well, we might consider ( based on knowledge of the sport ) that at
least one of the variables does *not* meet the normality criteria, and
though their variances are not significantly different. So we would
attempt a non-parametric Wilcoxon test, that uses the *signed-rank of
the paired data differences*, instead of the data variables. Our model
would be:

$$
mean(\ sign.rank[\ Final(i) - Semifinal(i)\ ]\ ) = \beta_0 \\
$$
$$
H_0: \mu_{final} - \mu_{semifinal} = 0;
$$
$$
H_a: \mu_{final} - \mu_{semifinal} \ne 0;\\
$$

```{r}
wilcox.test(x = Diving2017$Semifinal, 
            y = Diving2017$Final,
            mu = 0, # belief
            alternative = "two.sided", #difference either way
            paired = TRUE,
            conf.int = TRUE,
            conf.level = 0.95) %>% 
  broom::tidy()

```

Here also with the `p.value` being $0.3804$, we have no reason to accept
the Alternative Hypothesis. The parametric `t.test` and the
non-parametric `wilcox.test` agree in their inferences.

#### Linear Model Method

We can apply the *linear-model-as-inference* interpretation both to the
original data and to the sign.rank data: 

$$
lm(y_i - x_i \sim 1) = \beta_0\\
\\ and\\
lm(\ sign.rank[\ Final(i) - Semifinal(i)\ ] \sim 1) = \beta_0\\
$$

And the Hypothesis for both interpretations would be:\
$$
H_0: \beta_0 = 0\\
\\\
H_a: \beta_0 \ne 0\\
$$

```{r}
#| results: hold
lm(Semifinal - Final ~ 1, data = Diving2017) %>% 
  broom::tidy(conf.int = TRUE, conf.level = 0.95)

# Create a sign-rank function
signed_rank <- function(x) {sign(x) * rank(abs(x))}

lm(signed_rank(Semifinal - Final) ~ 1, 
                   data = Diving2017) %>% 
  broom::tidy(conf.int = TRUE,conf.level = 0.95)

```

We observe that using the linear model method for the original scores and the sign-rank scores both sdo not permit us to reject the $H_0$ Null Hypothesis, since `p.values` are high, and the `confidence.intervals` straddle $0$.

#### Permutation Tests

For the specific data at hand, we need to shuffle
the records between `Semifinal` and `Final` on **a per Swimmer basis** (paired data!!) and take the `test statistic` (difference between the two swim records for *each* swimmer). Another way to look at this is to take the differences between `Semifinal` and `Final` scores and *shuffle the differences to either polarity*. We will follow this method in the code below:

```{r}
#| layout-ncol: 2
#| cache: true
polarity <- c(rep(1, 6), rep(-1, 6)) 
# 12 +/- 1s, 
# 6 each to make sure there is equal probability
polarity
##
null_dist_swim <- do(4999) * 
  mean(data = Diving2017,
       ~ (Final - Semifinal) * # take (pairwise) differences
         mosaic::resample(polarity, # Swap polarity randomly
                          replace = TRUE))
##
null_dist_swim

```

Let us plot the NULL distribution and compare it with the actual
`observed differences` in the race times:

```{r}
#| layout-ncol: 2
ggplot2::theme_set(new = theme_custom())

gf_histogram(data = null_dist_swim, ~ mean) %>%
  gf_vline(xintercept = obs_diff_swim, 
           colour = "red",
           linewidth = 1, title = "Null Distribution by Permutation", 
           subtitle = "Histogram") %>% 
  gf_labs(x = "Difference in Means")

###
gf_ecdf(data = null_dist_swim, ~ mean, linewidth = 1) %>%
  gf_vline(xintercept = obs_diff_swim, 
           colour = "red",
           linewidth = 1, title = "Null Distribution by Permutation", 
           subtitle = "Cumulative Density") %>% 
  gf_labs(x = "Difference in Means")

###
prop1(~ mean <= obs_diff_swim, data = null_dist_swim)


```

Hmm...so by generating 4999 shuffles of score-difference polarities, it
does appear that we can not only obtain the current observed difference
but even surpass it frequently. So it does seem that there is no
difference in means between Semi-Final and Final swimming scores.
:::

### All Tests Together

We can put all the test results together to get a few more insights
about the tests:

```{r}
#| results: hold

mosaic::t.test(x = Diving2017$Semifinal, 
               y = Diving2017$Final,
               paired = TRUE) %>% broom::tidy() %>% 
  gt() %>%
  tab_style(
    style = list(cell_fill(color = "cyan"), cell_text(weight = "bold")),
    locations = cells_body(columns = p.value)) %>% 
  tab_header(title = "t.test")

lm(Semifinal - Final ~ 1, data = Diving2017) %>% 
  broom::tidy(conf.int = TRUE, conf.level = 0.95) %>% 
  gt() %>%
  tab_style(
    style = list(cell_fill(color = "cyan"),cell_text(weight = "bold")),
    locations = cells_body(columns = p.value)) %>% 
  tab_header(title = "Linear Model")

wilcox.test(x = Diving2017$Semifinal, 
               y = Diving2017$Final,
               paired = TRUE) %>% broom::tidy() %>% 
  gt() %>%
  tab_style(
    style = list(cell_fill(color = "palegreen"),cell_text(weight = "bold")),
    locations = cells_body(columns = p.value)) %>% 
  tab_header(title = "Wilcoxon test")

lm(signed_rank(Semifinal - Final) ~ 1, 
                   data = Diving2017) %>% 
  broom::tidy(conf.int = TRUE,conf.level = 0.95) %>% 
  gt() %>%
  tab_style(
    style = list(cell_fill(color = "palegreen"),cell_text(weight = "bold")),
    locations = cells_body(columns = p.value)) %>% 
  tab_header(title = "Linear Model with sign.rank")

```

The linear model and the `t.test` are nearly identical in performance;
the p.values are the same. The same is also true of the `wilcox.test`
and the linear model with sign-rank data differences. This is of course
not surprising!

## {{< iconify pajamas issue-type-test-case >}} Case Study #2: Walmart vs Target

Is there a difference in the price of Groceries sold by the two
retailers Target and Walmart? The data set `Groceries` contains a sample
of grocery items and their prices advertised on their respective web
sites on one specific day. We will:

a)  Inspect the data set, then explain why this is an example of matched
    pairs data.
b)  Compute summary statistics of the prices for each store.
c)  Conduct a permutation test to determine whether or not there is a
    difference in the mean prices.
d)  Create a ~~histogram~~ bar-chart of the difference in prices. What
    is unusual about Quaker Oats Life cereal?
e)  Redo the hypothesis test without this observation. Would we reach
    the same conclusion?

### {{< iconify carbon chart-3d >}} Inspecting and Charting Data

```{r}
#| results: hold
data("Groceries")
Groceries <- Groceries %>% 
  mutate(Product = stringr::str_squish(Product)) # Knock off extra spaces
Groceries
Groceries_inspect <- inspect(Groceries)
Groceries_inspect$categorical
Groceries_inspect$quantitative

```

There are just 30 prices for each vendor....just barely enough to get an idea of what the distribution might be. Let us convert the data to tidy long-form for convenience, and plot the
prices for the products, as box plots after pivoting the data to long
form, [^1] and as bar charts:

[^1]: <https://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/tidyr-pivoting.gif>


```{r}
#| label: making-groceries-long
#| code-fold: true
Groceries_long <- Groceries %>%
  pivot_longer(
    cols = c(Walmart, Target),
    names_to = "store",
    values_to = "prices"
  ) %>% 
  mutate(store = as_factor(store))
Groceries_long

```


Let us plot histograms/densities of the two variables that we wish to compare. We will also overlay a Gaussian distribution for comparison:

:::: {.columns}
:::{.column }

```{r}
#| label: Grocery-histograms-1
#| eval: false
ggplot2::theme_set(new = theme_custom())

Groceries_long %>%
  gf_dhistogram( ~ prices,
              fill = ~ store,
              alpha = 0.5,
              title = "Grocery Costs") %>%
  gf_facet_grid( ~ store) %>%
  gf_fitdistr(dist = "dnorm") %>% 
  gf_refine(scale_fill_brewer(palette = "Set1")) %>% 
gf_theme(theme(legend.position = "none"))
```
:::
:::{.column }
```{r}
#| ref.label: Grocery-histograms-1
#| echo: false
```
:::
::::

:::: {.columns}
:::{.column }
```{r}
#| label: Grocery-density-1
#| eval: false
ggplot2::theme_set(new = theme_custom())

Groceries_long %>%
  gf_density( ~ prices,
              fill = ~ store,
              alpha = 0.5,
              title = "Grocery Costs") %>%
  gf_facet_grid( ~ store) %>%
  gf_fitdistr(dist = "dnorm") %>% 
  gf_refine(scale_fill_brewer(palette = "Set1")) %>% 
gf_theme(theme(legend.position = "none"))

```
:::
:::{.column }
```{r}
#| ref.label: Grocery-density-1
#| echo: false
```
:::
::::

Not close to the Gaussian...there is clearly some skew to the right,
with some items being very costly compared to the rest. More when we
check the assumptions on data for the tests.

How about price differences, what we are interested in? 

:::: {.columns}
:::{.column }
```{r}
#| label: Grocery-boxplot-1
#| eval: false
ggplot2::theme_set(new = theme_custom())

Groceries_long %>% 
  gf_boxplot(prices~ store, 
             fill = ~ store) %>% 
  gf_refine(scale_fill_brewer(palette = "Set1")) 
```
:::
:::{.column }
```{r}
#| ref.label: Grocery-boxplot-1
#| echo: false
```
:::
::::

:::: {.columns}
:::{.column}
```{r}
#| label: Grocery-colplot-1
#| eval: false
ggplot2::theme_set(new = theme_custom())

Groceries_long %>%
  gf_col(fct_reorder(Product, prices) ~ prices,
    fill = ~ store,
    #alpha = 0.5,
    position = "dodge",
    xlab = "Prices",
    ylab = "",
    title = "Groceries Costs"
  ) %>% 
  gf_col(data = 
    Groceries_long %>% 
    filter(
      Product == "Quaker Oats Life Cereal Original"), 
         fct_reorder(Product, prices) ~ prices, 
         fill = ~ store, 
         position = "dodge") %>% 
  gf_refine(scale_fill_brewer(name = "Store", palette = "Set1")) 

```

:::
:::{.column }
```{r}
#| ref.label: Grocery-colplot-1
#| echo: false
```
:::
::::


::: callout-note
We see that the price difference between Walmart and Target prices is
highest for the `Product` named `Quaker Oats Life Cereal Original`.
Apart from this Product, the rest have no discernible trend either way.
Let us check `observed statistic` (the mean difference in prices)
:::

```{r}
#| layout: [[75, 25]]
obs_diff_price <- diffmean(prices ~ store, 
                           data = Groceries_long, 
                           only.2 = FALSE)
# Can also use
# obs_diff_price <-  mean( ~ Walmart - Target, data = Groceries)
obs_diff_price

```

### {{< iconify academicons hypothesis >}} Hypothesis

Based on the graph, how would we formulate our Hypothesis? We wish to
infer whether there is any change in prices, **per product** between the
two Store chains. So accordingly:

$$
H_0: \mu_{Walmart} = \mu_{Target}\\
$$

$$
H_a: \mu_{Walmart} \ne \mu_{Target}\
$$

### Testing for Assumptions on the Data

There are a few checks we need to make of our data, to decide what test procedure to use.

### A. {{< iconify mdi chart-bell-curve size=2x >}} Check for Normality
:::: {.columns}
:::{.column width="55%"}
```{r}
#| label: normality-walmart-1
#| eval: false
shapiro.test(Groceries$Walmart)
shapiro.test(Groceries$Target)

```
:::
:::{.column width="45%"}
```{r}
#| ref.label: normality-walmart-1
#| echo: false
```

:::
::::

For both tests, we see that the `p.value` is very small, indicating that the data are unlikely to be normally distributed. This means we cannot apply a standard `paired t.test` and need to use the non-parametric `wilcox.test`, that does not rely on the assumption of normality.

### B. {{< iconify mdi sigma-lower size=2x >}} Check for Variances

Let us check if the two variables have similar variances:

```{r}
var.test(Groceries$Walmart, Groceries$Target)

```

It appears from the $p.value = 0.9$ and the
$Confidence Interval = [0.4629, 2.0432]$, which includes $1$, that we cannot reject the NULL Hypothesis that the variances are not significantly different.

### {{< iconify fluent-mdl2 insights >}} Inference

::: {.panel-tabset .nav-pills style="background: whitesmoke;"}
#### Using *paired* t.test

Well, the variables are *not* normally distributed, so a standard
`t.test` is not advised, even if the variances are similar. We can still try:

```{r}
mosaic::t_test(Groceries$Walmart, Groceries$Target,paired = TRUE) %>% 
  broom::tidy()
```

The `p.value` is $0.64$ ! And the `Confidence Interval` straddles $0$.
So the `t.test` gives us no reason to reject the Null Hypothesis that
the means are similar. But can we really believe this, given the
non-normality of data?

#### Using non-parametric *paired* Wilcoxon test

However, we have seen that the data variables are *not* normally
distributed. So a Wilcoxon Test, using signed-ranks, is indicated:
(recall the model!)

```{r}
#| warning: false
# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say, 
# such that determination of ties does not depend on very small numeric differences (see the example).

wilcox.test(Groceries$Walmart, Groceries$Target, 
            data = Groceries_long, 
            digits.rank = 7,paired = TRUE, 
            conf.int = TRUE, conf.level = 0.95) %>% 
  broom::tidy()

```

The Wilcoxon test result is very interesting: the `p.value` says there
**is** a significant difference between the two store prices, and the
`confidence.interval` also is unipolar...

#### Using the Linear Model Method

As before we can do the linear model for both the original data and the sign.rank data. The `test statistic` is again the difference between thetwo variables:

```{r}
#| results: hold
lm(Target- Walmart ~ 1, data = Groceries) %>% 
  broom::tidy(conf.int = TRUE,conf.level = 0.95)

# Create a sign-rank function
signed_rank <- function(x) {sign(x) * rank(abs(x))}

lm(signed_rank(Target - Walmart) ~ 1, 
                    data = Groceries) %>% 
  broom::tidy(conf.int = TRUE,conf.level = 0.95)

```

Very interesting results, but confirming what we saw earlier: The Linear Model with the original data reports no significant difference, but the linear model with sign-ranks, suggests there is a significant difference in means prices between stores!

#### Permutation Tests

Let us perform the pair-wise permutation test on prices, by shuffling
the two store names:

```{r}
# | layout: [[15, 85, 15]]
ggplot2::theme_set(new = theme_custom())

polarity <- c(rep(1, 15), rep(-1,15))
##
null_dist_price <- do(9999) * 
                    mean(data = Groceries, 
                      ~ (Target - Walmart) * 
                        resample(polarity, replace = TRUE))
null_dist_price
##
gf_histogram(data = null_dist_price, ~ mean) %>% 
  gf_vline(xintercept = obs_diff_price, colour = "red")

prop1(~ mean, data = null_dist_price)

```

Does not seem to be any significant difference in prices...
:::

### All Tests Together

We can put all the test results together to get a few more insights
about the tests:

```{r}
#| results: hold
#| warning: false
mosaic::t_test(Groceries$Walmart, Groceries$Target,paired = TRUE) %>% 
  broom::tidy() %>%
  gt() %>%
  tab_style(style = list(cell_fill(color = "cyan"),
                         cell_text(weight = "bold")),
            locations = cells_body(columns = p.value)) %>%
  tab_header(title = "t.test")
###
lm(Target - Walmart ~ 1, data = Groceries) %>%
  broom::tidy(conf.int = TRUE, conf.level = 0.95) %>% gt() %>%
  tab_style(style = list(cell_fill(color = "cyan"),
                         cell_text(weight = "bold")),
            locations = cells_body(columns = p.value))  %>%
  tab_header(title = "Linear Model")
###
wilcox.test(Groceries$Walmart, Groceries$Target,
  digits.rank = 7,
  paired = TRUE,
  conf.int = TRUE,
  conf.level = 0.95
) %>%
  broom::tidy() %>% gt() %>%
  tab_style(style = list(cell_fill(color = "palegreen"),
                         cell_text(weight = "bold")),
            locations = cells_body(columns = p.value))  %>%
  tab_header(title = "Wilcoxon Test")
###
lm(signed_rank(Target - Walmart) ~ 1,
   data = Groceries) %>%
  broom::tidy(conf.int = TRUE, conf.level = 0.95) %>% gt() %>%
  tab_style(style = list(cell_fill(color = "palegreen"),
                         cell_text(weight = "bold")),
            locations = cells_body(columns = p.value)) %>%
  tab_header(title = "Linear Model with Sign.Ranks")

```

Clearly, the parametric tests do not detect a significant difference in prices, whereas the non-parametric tests do.

Suppose we knock off the `Quaker Cereal` data item...(note the spaces in the product name)

```{r}
#| layout: [[30,5],[50,5]]
#| warning: false
ggplot2::theme_set(new = theme_custom())

set.seed(12345)
Groceries_less <- Groceries %>%
  filter(Product != "Quaker Oats Life Cereal Original")
##
Groceries_less_long <- Groceries_less %>%
  pivot_longer(
    cols = c(Target, Walmart),
    names_to = "store",
    values_to = "prices"
  )
##
wilcox.test(Groceries_less$Walmart, 
            Groceries_less$Target, 
            paired = TRUE, digits.rank = 7,
            conf.int = TRUE,
            conf.level = 0.95
) %>%
  broom::tidy()
##
obs_diff_price_less <-
  mean(~ (Target - Walmart), data = Groceries_less)
obs_diff_price_less

polarity_less <- c(rep(1, 15), rep(-1, 14))
# Due to resampling this small bias makes no difference
null_dist_price_less <-
  do(9999) * mean(data = Groceries_less,
                  ~ (Target - Walmart) * resample(polarity_less, replace = TRUE))
##
gf_histogram(data = null_dist_price_less, ~ mean) %>%
  gf_vline(xintercept = obs_diff_price_less, 
           colour = "red")
##
mean(null_dist_price_less >= obs_diff_price_less)

```

We see that removing the Quaker Oats product item from the data **does**
give a significant difference in mean prices !!! That one price
difference was in the opposite direction compared to the general trend
in differences, so when it was removed, we obtained a truer picture of
price differences.

Try to do a regular parametric t.test with this reduced data!

## {{< iconify mingcute thought-line >}} Wait, But Why?

`Paired` data is a special case of dependent data, where the observations are paired in some way. This can be due to repeated measures on the same subjects, or matched subjects in a case-control study. The paired t-test and the Wilcoxon signed-rank test are both designed for paired data, and take into account the fact that the observations are not independent.

Can you think of an underlying experiment where the data may be paired?

## {{< iconify fluent-mdl2 decision-solid >}} Conclusion

We have learnt how to perform inference for paired-means. We have looked at the conditions that make the regular `t.test` possible, and learnt
what to do if the conditions of normality and equal variance are not
met. We have also looked at how these tests can be understood as
manifestations of the *linear model*, with data and sign-ranked data. It should also be fairly clear now that we can test for the equivalence of two paired means, using a very simple permutation tests. Given computing power, we can always mechanize this test very quickly to get our results. And that performing this test yields reliable results without having to rely on any assumption relating to underlying distributions and so on.

## {{< iconify openmoji person >}} Your Turn

1. Try the datasets in the `openintro` package. Use `data(package = "openintro")` in your Console to list out the data packages. Then simply type the name of the dataset in a Quarto chunk ( e.g. `babynames`) to read it. 

2. Same with the `resampledata` and `resampledata3` packages.

3. Try the datasets in the `PairedData` package. Yes, install this too, peasants. 

## {{< iconify ooui references-ltr >}} References

1. Paired Independence test with the package `infer`: <https://infer.netlify.app/articles/paired>
1.  Randall Pruim, Nicholas J. Horton, Daniel T. Kaplan, [*StartTeaching with R*](https://github.com/ProjectMOSAIC/LittleBooks/raw/master/Starting/MOSAIC-StartTeaching.pdf)
1.  <https://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307>
1.  <https://statsandr.com/blog/wilcoxon-test-in-r-how-to-compare-2-groups-under-the-non-normality-assumption/>


::: {#refs style="font-size: 60%;"}

###### {{< iconify lucide package-check >}} R Package Citations

```{r}
#| echo: false
#scan_packages()
cite_packages(
  output = "table",
  out.dir = ".",
  out.format = "html",
  pkgs = c("gt", "infer", "MKinfer", "openintro")
) %>%
  knitr::kable(format = "simple")

```
:::
