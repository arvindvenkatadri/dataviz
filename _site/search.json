[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Hi, I’m Arvind Venkatadri.",
    "section": "",
    "text": "Hi, I’m Arvind Venkatadri.\nI’m external Faculty at the Srishti Manipal Institute of Art and Design, both in Bangalore, INDIA, and a Certified Level #1 TRIZ Professional.\nI have a passion for coding in R / ObservableJS / p5.js, Data Visualization, Complexity Science, Literature, and Creative Thinking / Problem Solving with TRIZ.\nOn this website, I share and teach Data Visualization and Statistics in the R language.\nTo get started, you can check out my courses. You can also find me on Twitter, GitHub, and on Medium. Feel free to reach out to me via email !\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Arvind V.",
    "section": "",
    "text": "I’m an an external Faculty Member at the Srishti Manipal Institute of Art, Design, and Technology (SMI), in Bangalore, INDIA. I am passionate about working on R, Data Visualization, Complexity Science, and Creative Thinking and Problem Solving with TRIZ. On this website, I share my course materials and methods. I also blog about TRIZ and Data Science on occasion.\nTo get started, you can check out my courses on this website.\nMy other teaching websites are:\n- Foundation Courses at Srishti\n- Teaching R to Artists and Designers\n- Using AI in R\n\nMy student portfolios are here:\nhttps://we-r-us.netlify.app/portfolio/\nhttps://form-and-structure.netlify.app/portfolio/\n\nYou can find me on Twitter, or GitHub, and on LinkedIn! Feel free to reach out to me via mail too!\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html",
    "title": "\n Densities",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“Never let the future disturb you. You will meet it, if you have to, with the same weapons of reason which today arm you against the present.”\n— Marcus Aurelius",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#slides-and-tutorials",
    "title": "\n Densities",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“Never let the future disturb you. You will meet it, if you have to, with the same weapons of reason which today arm you against the present.”\n— Marcus Aurelius",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#setting-up-r-packages",
    "title": "\n Densities",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\n\n# install.packages(\"remotes\")\n# library(remotes)\n# remotes::install_github(\"wilkelab/ggridges\")\nlibrary(ggridges)\nlibrary(skimr)\nlibrary(palmerpenguins) # Our new favourite dataset\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-graphs-will-we-see-today",
    "title": "\n Densities",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nDensity plot, Ridge Density Plot",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Densities",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n1\n                  How Many / Much / Heavy? Few? Seldom? Often? When?\n                  Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\n                  Quantitative/Ratio\n                  Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\n                  Correlation",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#inspiration",
    "title": "\n Densities",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\nApril is the cruelest month, said T.S Eliot. But December in Nebraska must be tough.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-is-a-density-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#what-is-a-density-plot",
    "title": "\n Densities",
    "section": "\n What is a “Density Plot”?",
    "text": "What is a “Density Plot”?\nAs we saw earlier, Histograms are best to show the distribution of raw Quantitative data, by displaying the number of values that fall within defined ranges, often called buckets or bins.\nSometimes it is useful to consider a chart where the bucket width shrinks to zero!\nYou might imagine a density chart as a histogram where the buckets are infinitesimally small, i.e. zero width. Think of the frequency density as a differentiation (as in calculus) of the histogram. By taking the smallest of steps \\(\\sim 0\\), we get a measure of the slope of distribution. This may seem counter-intuitive, but densities have their uses in spotting the ranges in the data where there are more frequent values. In this, they serve a similar purpose as do histograms, but may offer insights not readily apparent with histograms, especially with default bucket widths. The chunkiness that we see in the histograms is removed and gives us a smooth curve showing in which range the data are more frequent.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#case-study-1-penguins-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#case-study-1-penguins-dataset",
    "title": "\n Densities",
    "section": "\n Case Study-1: penguins dataset",
    "text": "Case Study-1: penguins dataset\nWe will first look at at a dataset that is directly available in R, the penguins dataset. Data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network.\n\n Examine the Data\nAs per our Workflow, we will look at the data using all the three methods we have seen.\n\n\n dplyr\n skimr\n mosaic\n web-r\n\n\n\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\n\n\n\ninspect(penguins)\n\n\ncategorical variables:  \n     name  class levels   n missing\n1 species factor      3 344       0\n2  island factor      3 344       0\n3     sex factor      2 333      11\n                                   distribution\n1 Adelie (44.2%), Gentoo (36%) ...             \n2 Biscoe (48.8%), Dream (36%) ...              \n3 male (50.5%), female (49.5%)                 \n\nquantitative variables:  \n               name   class    min       Q1  median     Q3    max       mean\n1    bill_length_mm numeric   32.1   39.225   44.45   48.5   59.6   43.92193\n2     bill_depth_mm numeric   13.1   15.600   17.30   18.7   21.5   17.15117\n3 flipper_length_mm integer  172.0  190.000  197.00  213.0  231.0  200.91520\n4       body_mass_g integer 2700.0 3550.000 4050.00 4750.0 6300.0 4201.75439\n5              year integer 2007.0 2007.000 2008.00 2009.0 2009.0 2008.02907\n           sd   n missing\n1   5.4595837 342       2\n2   1.9747932 342       2\n3  14.0617137 342       2\n4 801.9545357 342       2\n5   0.8183559 344       0\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nsex: male and female penguins\n\nisland: they have islands to themselves!!\n\nspecies: Three adorable types!\n\n\n\n\n\n\n\n\n\n\nFigure 1: Penguin Species\n\n\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nbill_length_mm: The length of the penguins’ bills\n\nbill_depth_mm: See the picture!!\n\nflipper_length_mm: Flippers! Penguins have “hands”!!\n\nbody_mass_gm: Grams? Grams??? Why, these penguins are like human babies!!❤️\n\n\n\n\n\n\n\n\n\n\nFigure 2: Penguin Features\n\n\n\n\n\n\n\n\nNoteBusiness Insights on Examining the penguins dataset\n\n\n\n\nThis is a smallish dataset (344 rows, 8 columns).\nThere are a few missing values in sex(11 missing entries) and all the Quant variables (2 missing entries each).\n\n\n\n\n Plotting Densities\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n\n\n\npenguins &lt;- penguins %&gt;% drop_na()\n\ngf_density(~body_mass_g, data = penguins) %&gt;%\n  gf_labs(title = \"Plot A: Penguin Masses\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\npenguins %&gt;%\n  gf_density(~body_mass_g,\n    fill = ~species,\n    color = \"black\"\n  ) %&gt;%\n  gf_refine(scale_color_viridis_d(\n    option = \"magma\",\n    aesthetics = c(\"colour\", \"fill\")\n  )) %&gt;%\n  gf_labs(\n    title = \"Plot B: Penguin Body Mass by Species\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;%\n  gf_density(\n    ~body_mass_g,\n    fill = ~species,\n    color = \"black\",\n    alpha = 0.3\n  ) %&gt;%\n  gf_facet_wrap(vars(sex)) %&gt;%\n  gf_labs(title = \"Plot C: Penguin Body Mass by Species and facetted by Sex\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;%\n  gf_density(~body_mass_g, fill = ~species, color = \"black\") %&gt;%\n  gf_facet_wrap(vars(sex), scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(\n    title = \"Plot D: Penguin Body Mass by Species and facetted by Sex\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(\n    angle = 45,\n    hjust = 1\n  )))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins &lt;- penguins %&gt;% drop_na()\n\nggplot(data = penguins) +\n  geom_density(aes(x = body_mass_g)) +\n  labs(title = \"Plot A: Penguin Masses\", caption = \"ggplot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;%\n  ggplot() +\n  geom_density(aes(x = body_mass_g, fill = species),\n    alpha = 0.3,\n    color = \"black\"\n  ) +\n  scale_color_brewer(\n    palette = \"Set1\",\n    aesthetics = c(\"colour\", \"fill\")\n  ) +\n  labs(\n    title = \"Plot B: Penguin Body Mass by Species\",\n    caption = \"ggplot\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;% ggplot() +\n  geom_density(aes(x = body_mass_g, fill = species),\n    color = \"black\",\n    alpha = 0.3\n  ) +\n  facet_wrap(vars(sex)) +\n  labs(title = \"Plot C: Penguin Body Mass by Species and facetted by Sex\", caption = \"ggplot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npenguins %&gt;% ggplot() +\n  geom_density(aes(x = body_mass_g, fill = species),\n    alpha = 0.3,\n    color = \"black\"\n  ) +\n  facet_wrap(vars(sex), scales = \"free_y\", nrow = 2) +\n  labs(\n    title = \"Plot D: Penguin Body Mass by Species and facetted by Sex\",\n    subtitle = \"Free y-scale\", caption = \"ggplot\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(theme(axis.text.x = element_text(angle = 45, hjust = 1)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from penguin Densities\n\n\n\nPretty much similar conclusions as with histograms. Although densities may not be used much in business contexts, they are better than histograms when comparing multiple distributions! So you should use thems!\n\n\n\n\n Ridge Plots\nSometimes we may wish to show the distribution/density of a Quant variable, against several levels of a Qual variable. For instance, the prices of different items of furniture, based on the furniture “style” variable. Or the sales of a particular line of products, across different shops or cities. We did this with both histograms and densities, by colouring based on a Qual variable, and by facetting using a Qual variable. There is a third way, using what is called a ridge plot. ggformula support this plot by importing/depending upon the ggridges package. ggridges provides direct support for ridge plots, and can be used as an extension to # ggplot2 and ggformula.\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n\n\n\ngf_density_ridges(drv ~ hwy,\n  fill = ~drv,\n  alpha = 0.5, # colour saturation\n  rel_min_height = 0.005, # separation between plots\n  data = mpg\n) %&gt;%\n  gf_refine(\n    scale_y_discrete(expand = c(0.01, 0)),\n    scale_x_continuous(expand = c(0.01, 0)),\n    scale_fill_brewer(\n      name = \"Drive Type\",\n      palette = \"Spectral\"\n    )\n  ) %&gt;%\n  gf_labs(\n    title = \"Ridge Plot\", x = \"Highway Mileage\",\n    y = \"Drive Type\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngf_density_ridges(drv ~ hwy,\n  fill = ~drv,\n  alpha = 0.5, # colour saturation\n  rel_min_height = 0.005, data = mpg\n) %&gt;%\n  gf_refine(\n    scale_y_discrete(expand = c(0.01, 0)),\n    scale_x_continuous(expand = c(0.01, 0)),\n    scale_fill_brewer(\n      name = \"Drive Type\",\n      palette = \"Spectral\"\n    )\n  ) %&gt;%\n  gf_labs(\n    title = \"Ridge Plot\", x = \"Highway Mileage\",\n    y = \"Drive Type\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from mpg Ridge Plots\n\n\n\nThis is another way of visualizing multiple distributions, of a Quant variable at different levels of a Qual variable. We see that the distribution of hwy mileage varies substantially with drv type.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#wait-but-why",
    "title": "\n Densities",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nDensities are sometimes easier to compare side by side. That is what Claus Wilke says, at least. Perhaps because they look less “busy” than histograms.\nRidge Density Plots are very cool when it comes to comparing the density of a Quant variable as it varies against the levels of a Qual variable, without having to facet or group.\nIt is possible to plot 2D-densities too, for two Quant variables, which give very evocative contour-like plots. Try to do this with the faithful dataset in R.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#conclusion",
    "title": "\n Densities",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nHistograms and Frequency Distributions are both used for Quantitative data variables\nWhereas Histograms “dwell upon” counts, ranges, means and standard deviations\n\nFrequency Density plots “dwell upon” probabilities and densities\n\nRidge Plots are density plots used for describing one Quant and one Qual variable (by inherent splitting)\nWe can split all these plots on the basis of another Qualitative variable.(Ridge Plots are already split)\nLong tailed distributions need care in visualization and in inference making!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#your-turn",
    "title": "\n Densities",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nNoteStar Trek Books\n\n\n\n\n\n Start Trek Book data\n\n\nWhich would be the Group By variables here? And what would you summarize? With which function?\n\n\n\n\n\n\n\n\nNoteMath Anxiety! Hah! Peasants.\n\n\n\n\n\n Math Anxiety data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/index.html#references",
    "title": "\n Densities",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\n\nMinimal R using mosaic.https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nSebastian Sauer, Plotting multiple plots using purrr::map and ggplot \n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nresampledata3\n1.0\nChihara and Hesterberg (2022)\n\n\nrtrek\n0.5.2\nLeonawicz (2025)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\ntidyplots\n0.3.1\nEngler (2025)\n\n\ntinyplot\n0.4.2\nMcDermott, Arel-Bundock, and Zeileis (2025)\n\n\ntinytable\n0.10.0\nArel-Bundock (2025)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nArel-Bundock, Vincent. 2025. tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,” “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats. https://doi.org/10.32614/CRAN.package.tinytable.\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://doi.org/10.32614/CRAN.package.visualize.\n\n\nChihara, Laura, and Tim Hesterberg. 2022. Resampledata3: Data Sets for “Mathematical Statistics with Resampling and R” (3rd Ed). https://doi.org/10.32614/CRAN.package.resampledata3.\n\n\nEngler, Jan Broder. 2025. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” iMeta, e70018. https://doi.org/10.1002/imt2.70018.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nLeonawicz, Matthew. 2025. rtrek: Data Analysis Relating to Star Trek. https://doi.org/10.32614/CRAN.package.rtrek.\n\n\nMcDermott, Grant, Vincent Arel-Bundock, and Achim Zeileis. 2025. tinyplot: Lightweight Extension of the Base r Graphics System. https://doi.org/10.32614/CRAN.package.tinyplot.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"clarity:bell-curve-line\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTipFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(children = n()) %&gt;%\n  # just to check\n  mutate(\n    No_of_families = as.integer(children / nkids),\n    # Why do we divide\n\n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)\n  )\nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;%\n  gf_col(No_of_families ~ nkids) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(No_of_families,\n    colorBy = \"data\",\n    legend = FALSE\n  ) %&gt;% # Or \"series\"\n\n  # https://echarts4r.john-coene.com/articles/grid.html\n  # echarts4r does not \"automatically\" name the axes!\n  # And look at the \"categorical\" x-axis below!\n\n  e_x_axis(\n    name = \"Family Size\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25, ) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;%\n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"Family Size (nkids)\", nameLocation = \"center\",\n    nameGap = 20, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 20,\n    nameTextStyle = list(align = \"center\"),\n    nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(family, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(family) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 25, nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 5) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nNoteSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;%\n  e_charts() %&gt;%\n  e_histogram(serie = height) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$height)),\n    label = list(\n      label = \"Mean Height\",\n      label.position = \"end\"\n    ),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;%\n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(height) %&gt;%\n  e_mark_line(\n    data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n    # This code colours both v-lines red...how?\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(\n    data = list(xAxis = measures %&gt;%\n      filter(sex == \"F\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n\n    # This piece of code has no effect...wonder why not?\n    # BOTH lines are in red ...why??\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;%\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(father) %&gt;%\n  e_density(mother) %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$mother)),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_mark_line(data = list(\n    xAxis = mean(Galton$father),\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  )) %&gt;%\n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  e_charts(height = 400) %&gt;%\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids, sex) %&gt;%\n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  e_charts(nkids, height = 300) %&gt;%\n  e_bar(mean_height, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\", nameGap = 25,\n    type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;%\n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;%\n  group_by(family, sex) %&gt;%\n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;%\n  select(family, sex, height, diff_height) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(diff_height, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  # Fit a trend line\n  e_lm(height ~ diff_height,\n    name = c(\"Female\", \"Male\")\n  ) %&gt;%\n  e_x_axis(\n    max = 18, min = -5,\n    name = \"Father - Mother Height\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50,\n    name = \"Children's Heights\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n())\nNHANES %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n()) %&gt;%\n  e_charts(Education, height = 300) %&gt;%\n  e_bar(count) %&gt;%\n  e_y_axis(max = 1750) %&gt;%\n  e_x_axis(type = \"category\") %&gt;%\n  e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip()\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(rows = 1, cols = 3) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n::: {.callout-note title=“Question”} Q.4. How are people Ages distributed across levels of Education?\n# Recall there are missing data\n# gf_boxplot(Age ~ Education,\n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;%\n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;%\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;%\n  group_by(Race1) %&gt;%\n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;%\n  group_by(Education, Race1) %&gt;%\n  summarize(n = n()) %&gt;%\n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;%\n  mutate(percapita_educated = (n / population) * 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;% # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;%\n  e_y_axis(max = 35) %&gt;%\n  e_facet(rows = 2, cols = 3) %&gt;%\n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(BMI)\nNHANES %&gt;%\n  group_by(Race1) %&gt;%\n  e_charts(height = 350) %&gt;%\n  e_density(BMI) %&gt;%\n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%\n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(\n  path = \"../data/banned.xlsx\",\n  sheet = \"Sorted by Author & Title\"\n)\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;-\n  banned %&gt;%\n  group_by(State) %&gt;%\n  summarise(total = n()) %&gt;%\n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;%\n  group_by(State, `Type of Ban`) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;%\n  #  pivot_wider(.,id_cols = State,\n  #              names_from = `Type of Ban`,\n  #              values_from = count) %&gt;% janitor::clean_names() %&gt;%\n  #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n  #                  banned_from_libraries = 0,\n  #                  banned_pending_investigation = 0,\n  #                  banned_from_classrooms = 0)) %&gt;%\n  # mutate(total = sum(across(where(is.integer)))) %&gt;%\n  gf_col(count ~ reorder(State, total),\n    fill = ~`Type of Ban`\n  ) %&gt;%\n  gf_labs(\n    x = \"Count of Banned Books\",\n    y = \"State\"\n  ) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html",
    "title": "\n Proportions",
    "section": "",
    "text": "…“Thinking is difficult, that’s why most people judge.”\n— C.G. Jung",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#sec-setting-up-r-packages",
    "title": "\n Proportions",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic) # Our trusted friend\nlibrary(skimr)\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\n\nlibrary(resampledata) # More datasets\n\nlibrary(GGally) # Correlation Plots\nlibrary(visStatistics) # Comprehensive all-in-one stats viz/test package\nlibrary(ca) # Correspondence Analysis, for use some day\n\nlibrary(ggmosaic) # Mosaic Plots\nlibrary(ggpubr) # Colours, Themes and new geometries in ggplot\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#what-graphs-will-we-see-today",
    "title": "\n Proportions",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQual\nQual\nPies, and Mosaic Charts",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Proportions",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n3\n                  How, What Kind, What Sort\n                  A Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\n                  Qualitative/Ordinal\n                  Socioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\n                  Median,Percentile",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#introduction",
    "title": "\n Proportions",
    "section": "\n Introduction",
    "text": "Introduction\nTo recall, a categorical variable is one for which the possible measured or assigned values consist of a discrete set of categories, which may be ordered or unordered. Some typical examples are:\n\nGender, with categories “Male,” “Female.”\nMarital status, with categories “Never married,” “Married,” “Separated,” “Divorced,” “Widowed.”\nFielding position (in baseball cricket), with categories “Slips,”Cover “,”Mid-off “Deep Fine Leg”, “Close-in”, “Deep”…\nSide effects (in a pharmacological study), with categories “None,” “Skin rash,” “Sleep disorder,” “Anxiety,” . . ..\nPolitical attitude, with categories “Left,” “Center,” “Right.”\nParty preference (in India), with categories “BJP” “Congress,” “AAP,” “TMC”…\nTreatment outcome, with categories “no improvement,” “some improvement,” or “marked improvement.”\nAge, with categories “0–9,” “10–19,” “20–29,” “30–39,” . . . .\nNumber of children, with categories 0, 1, 2, . . . .\n\nAs these examples suggest, categorical variables differ in the number of categories: we often distinguish binary variables (or dichotomous variables) such as Gender from those with more than two categories (called polytomous variables).",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#inspiration",
    "title": "\n Proportions",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n(a) Obesity across the World\n\n\n\n\n\n\n\n\n\n(b) Covid Deaths https://datatopics.worldbank.org/sdgatlas/goal-3-good-health-and-well-being?lang=en\n\n\n\n\n\n\nFigure 1: Depicting Proportions\n\n\nFrom Figure 1 (a), it is seen that Egypt, Qatar, and the United States are the only countries with a population greater than 1 million on this list. Poor food habits are once again a factor, with some cultural differences. In Egypt, high food inflation has pushed residents to low-cost high-calorie meals. To combat food insecurity, the government subsidizes bread, wheat flour, sugar and cooking oil, many of which are the ingredients linked to weight gain. In Qatar, a country with one of the highest per capita GDPs in the world, a genetic predisposition towards obesity and sedentary lifestyles worsen the impact of rich diets. And in the U.S., bigger portions are one of the many reasons cited for rampant adult and child obesity. For example, Americans ate 20% more calories in the year 2000 than they did in 1983. They consume 195 lbs of meat annually compared to 138 lbs in 1953. And their grain intake has increased 45% since 1970.\nIt’s worth noting however that this dataset is based on BMI values, which do not fully account for body types with larger bone and muscle mass.\nFrom Figure 1 (b), according to World Bank, six countries (India, Russia, Indonesia, United States, Brazil, and Mexico) accounted for over 60 percent of the total additional deaths in the first two years of the pandemic.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#how-do-these-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#how-do-these-charts-work",
    "title": "\n Proportions",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nWe saw with Bar Charts that when we deal with single Qual variables, we perform counts for each level of the variable. For a single Qual variable, even with multiple levels ( e.g. Education Status: High school, College, Post-Graduate, PhD), we can count the observations as with Bar Charts and plot Pies.\nWe can also plot Pie Charts when the number of levels in a single Qual variable are not too many. Almost always, a Bar chart is preferred. The problem is that humans are pretty bad at reading angles. This ubiquitous chart is much vilified in the industry and bar charts that we have seen earlier, are viewed as better options. On the other hand, pie charts are ubiquitous in design and business circles, and are very much accepted! Do also read this spirited defense of pie charts here. https://speakingppt.com/why-tufte-is-flat-out-wrong-about-pie-charts/\nWhat if there are two Quals? Or even more? The answer is to take them pair-wise, make all combinations of levels for both and calculate counts for these. This is called a Contingency Table. Then we plot that table. We’ll see.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#categorical-data",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#categorical-data",
    "title": "\n Proportions",
    "section": "\n Categorical Data",
    "text": "Categorical Data\nFrom the {vcd package} vignette:\n\nThe first thing you need to know is that categorical data can be represented in three different forms in R, and it is sometimes necessary to convert from one form to another, for carrying out statistical tests, fitting models or visualizing the results.\n\n\nCase Data\nFrequency Data\nCross-Tabular Count Data\n\nLet us first see examples of each.\n\n\nCase Form\nFrequency Data Form\nTable form\n\n\n\nFrom Michael Friendly Discrete Data Analysis and Visualization :\n\nIn many circumstances, data is recorded on each individual or experimental unit. Data in this form is called case data, or data in case form. Containing individual observations with one or more categorical factors, used as classifying variables. The total number of observations is nrow(X), and the number of variables is ncol(X).\n\n\n\n R\n web-r\n\n\n\n\nclass(Arthritis)\n\n[1] \"data.frame\"\n\n# Tibble as HTML for presentation\nArthritis %&gt;%\n  head(10) %&gt;%\n  tt(theme = \"striped\", caption = \"Arthritis Treatments and Effects&lt;br&gt; First 10 Observations\", centering = TRUE)\n\n\n\n    \n\n      \n\nArthritis Treatments and Effects First 10 Observations\n              \nID\n                Treatment\n                Sex\n                Age\n                Improved\n              \n\n\n\n57\n                  Treated\n                  Male\n                  27\n                  Some\n                \n\n46\n                  Treated\n                  Male\n                  29\n                  None\n                \n\n77\n                  Treated\n                  Male\n                  30\n                  None\n                \n\n17\n                  Treated\n                  Male\n                  32\n                  Marked\n                \n\n36\n                  Treated\n                  Male\n                  46\n                  Marked\n                \n\n23\n                  Treated\n                  Male\n                  58\n                  Marked\n                \n\n75\n                  Treated\n                  Male\n                  59\n                  None\n                \n\n39\n                  Treated\n                  Male\n                  59\n                  Marked\n                \n\n33\n                  Treated\n                  Male\n                  63\n                  None\n                \n\n55\n                  Treated\n                  Male\n                  63\n                  None\n                \n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe Arthritis data set has three factors and two integer variables. One of the three factors Improved is an ordered factor.\n\nID\nTreatment: a factor; Placebo or Treated\nSex: a factor, M / F\nAge: integer\nImproved: Ordinal factor; None &lt; Some &lt; Marked\n\nEach row in the Arthritis dataset is a separate case or observation.\n\n\nData in frequency form has already been tabulated and aggregated by counting over the (combinations of) categories of the table variables. When the data are in case form, we can always trace any observation back to its individual identifier or data record, since each row is a unique observation or case; the reverse, with the Frequency Form is rarely possible.\nFrequency Data is usually a data frame, with columns of categorical variables and at least one column containing frequency or count information.\n\n\n R\n web-r\n\n\n\n\n\n\nstr(GSS)\n\n# Tibble as HTML for presentation\nGSS %&gt;%\n  tt(\n    theme = \"striped\", caption = \"General Social Survey\",\n    centering = TRUE\n  )\n\n\n\n\n'data.frame':   6 obs. of  3 variables:\n $ sex  : Factor w/ 2 levels \"female\",\"male\": 1 2 1 2 1 2\n $ party: Factor w/ 3 levels \"dem\",\"indep\",..: 1 1 2 2 3 3\n $ count: num  279 165 73 47 225 191\n\n\n\n\n    \n\n      \n\nGeneral Social Survey\n              \nsex\n                party\n                count\n              \n\n\n\nfemale\n                  dem\n                  279\n                \n\nmale\n                  dem\n                  165\n                \n\nfemale\n                  indep\n                  73\n                \n\nmale\n                  indep\n                  47\n                \n\nfemale\n                  rep\n                  225\n                \n\nmale\n                  rep\n                  191\n                \n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nRespondents in the GSS survey were classified by sex and party identification. As can be seen, there is a count for every combination of the two categorical variables, sex and party.\n\n\nTable Form Data can be a matrix, array or table object, whose elements are the frequencies in an n-way table. The variable names (factors) and their levels are given by dimnames(X).\n\n\n\nHairEyeColor\nclass(HairEyeColor)\n\n\n\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\n[1] \"table\"\n\n\n\nHairEyeColor is a “two-way” table, consisting of two tables, one for Sex = Female and the other for Sex = Male. The total number of observations is sum(X). The number of dimensions of the table is length(dimnames(X)), and the table sizes are given by sapply(dimnames(X), length). The data looks like a n-dimensional cube and needs n-way tables to represent.\n\nsum(HairEyeColor)\n\n[1] 592\n\ndimnames(HairEyeColor)\n\n$Hair\n[1] \"Black\" \"Brown\" \"Red\"   \"Blond\"\n\n$Eye\n[1] \"Brown\" \"Blue\"  \"Hazel\" \"Green\"\n\n$Sex\n[1] \"Male\"   \"Female\"\n\nsapply(dimnames(HairEyeColor), length)\n\nHair  Eye  Sex \n   4    4    2 \n\n\nA good way to think of tabular data is to think of a Rubik’s Cube.\n\n\n\n\n\nRubik’s Cube model for Multi-Table Data\n\n\n\n\n\n\n\nTipRubik’s Cube and Categorical Data Tables\n\n\n\nEach of the edges is an Ordinal Variable, each segment represents a level in the variable. So each face of the Cube represents two ordinal variables. Any segment is at the intersection of two (independent) levels of two variables, and the colour may be visualized as a count. This array of counts on a face is a 2D or 2-Way Table. ( More on this later )\n\n\nSince we can only print 2D tables, we hold one face in front and the image we see is a 2-Way Table. Turning the Cube by 90 degrees gives us another face with 2 variables, with one variable in common with the previous face. If we consider two faces together, we get two 2-way tables, effectively allowing us to contemplate 3 categorical variables.\nMultiple 2-Way tables can be flattened into a single long table that contains all counts for all combinations of categorical variables. This can be visualized as “opening up” and laying flat the Rubik’s cube, as with a cardboard model of it.\n\nftable(HairEyeColor)\n\n            Sex Male Female\nHair  Eye                  \nBlack Brown       32     36\n      Blue        11      9\n      Hazel       10      5\n      Green        3      2\nBrown Brown       53     66\n      Blue        50     34\n      Hazel       25     29\n      Green       15     14\nRed   Brown       10     16\n      Blue        10      7\n      Hazel        7      7\n      Green        7      7\nBlond Brown        3      4\n      Blue        30     64\n      Hazel        5      5\n      Green        8      8\n\n\nFinally, we may need to convert the (multiple) tables into a data frame or tibble:\n## Convert the two tables into a data frame\nHairEyeColor %&gt;%\n  as_tibble()\n# Tibble as HTML for presentation\nHairEyeColor %&gt;%\n  as_tibble() %&gt;% # Convert\n  tt(\n    theme = \"striped\", caption = \"Hair Eye and Color&lt;br&gt; as a Data Frame\",\n    centering = TRUE\n  )\n\n\n\n\n  \n\n\n\n\n\n    \n\n      \n\nHair Eye and Color as a Data Frame\n              \nHair\n                Eye\n                Sex\n                n\n              \n\n\n\nBlack\n                  Brown\n                  Male\n                  32\n                \n\nBrown\n                  Brown\n                  Male\n                  53\n                \n\nRed\n                  Brown\n                  Male\n                  10\n                \n\nBlond\n                  Brown\n                  Male\n                  3\n                \n\nBlack\n                  Blue\n                  Male\n                  11\n                \n\nBrown\n                  Blue\n                  Male\n                  50\n                \n\nRed\n                  Blue\n                  Male\n                  10\n                \n\nBlond\n                  Blue\n                  Male\n                  30\n                \n\nBlack\n                  Hazel\n                  Male\n                  10\n                \n\nBrown\n                  Hazel\n                  Male\n                  25\n                \n\nRed\n                  Hazel\n                  Male\n                  7\n                \n\nBlond\n                  Hazel\n                  Male\n                  5\n                \n\nBlack\n                  Green\n                  Male\n                  3\n                \n\nBrown\n                  Green\n                  Male\n                  15\n                \n\nRed\n                  Green\n                  Male\n                  7\n                \n\nBlond\n                  Green\n                  Male\n                  8\n                \n\nBlack\n                  Brown\n                  Female\n                  36\n                \n\nBrown\n                  Brown\n                  Female\n                  66\n                \n\nRed\n                  Brown\n                  Female\n                  16\n                \n\nBlond\n                  Brown\n                  Female\n                  4\n                \n\nBlack\n                  Blue\n                  Female\n                  9\n                \n\nBrown\n                  Blue\n                  Female\n                  34\n                \n\nRed\n                  Blue\n                  Female\n                  7\n                \n\nBlond\n                  Blue\n                  Female\n                  64\n                \n\nBlack\n                  Hazel\n                  Female\n                  5\n                \n\nBrown\n                  Hazel\n                  Female\n                  29\n                \n\nRed\n                  Hazel\n                  Female\n                  7\n                \n\nBlond\n                  Hazel\n                  Female\n                  5\n                \n\nBlack\n                  Green\n                  Female\n                  2\n                \n\nBrown\n                  Green\n                  Female\n                  14\n                \n\nRed\n                  Green\n                  Female\n                  7\n                \n\nBlond\n                  Green\n                  Female\n                  8",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#simple-plots-for-categorical-data",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#simple-plots-for-categorical-data",
    "title": "\n Proportions",
    "section": "\n Simple Plots for Categorical Data",
    "text": "Simple Plots for Categorical Data\n\nWe have already examined Bar Charts.\nPie Charts are discussed here.\nThese are both good for single Qual variables. Bars are more suited when there are many levels and/or when there is more than one Qual variable, as discussed earlier.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#plotting-nested-proportions",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#plotting-nested-proportions",
    "title": "\n Proportions",
    "section": "\n Plotting Nested Proportions",
    "text": "Plotting Nested Proportions\nWhen we want to visualize proportions based on Multiple Qual variables, we are looking at what Claus Wilke calls nested proportions: groups within groups. Making counts with combinations of levels for two Qual variables gives us a data structure called a Contingency Table, which we will use to build our plot for nested proportions. The Statistical tests for Proportions ( the \\(\\chi^2\\) test ) also needs Contingency Tables. The Frequency Table we encountered earlier is very close to being a full-fledged Contingency Table; one only needs to add the margin counts! So what is a Contingency Table?\n\n Creating Contingency Tables\nFrom Wolfram Alpha:\n\nA contingency table, sometimes called a two-way frequency table, is a tabular mechanism with at least two rows and two columns used in statistics to present categorical data in terms of frequency counts. More precisely, an \\(r \\times c\\) contingency table shows the observed frequency of two variables the observed frequencies of which are arranged into \\(r\\) rows and \\(c\\) columns. The intersection of a row and a column of a contingency table is called a cell.\n\nIn this section we understand how to make Contingency Tables from each of the three forms. We will use vcd, mosaic and the tidyverse packages for our purposes. Then we will see how they can be visualized.\n\n\nUsing mosaic\nUsing base R\nUsing vcd\nUsing tidyverse\n\n\n\nI think this is the simplest and most elegant way of obtaining Contingency Tables:\n\ndata(\"GSS2002\", package = \"resampledata\")\ngss2002 &lt;- GSS2002 %&gt;%\n  dplyr::select(Education, DeathPenalty) %&gt;%\n  tidyr::drop_na(., c(Education, DeathPenalty))\n##\nmosaic::tally(DeathPenalty ~ Education, data = gss2002) %&gt;%\n  addmargins()\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\n\nPlotting this as an HTML table, we get:\n\n\n\n\n\n    \n\n      \n\nGSS Social Survey\n              \nDeathPenalty\n                Left HS\n                HS\n                Jr Col\n                Bachelors\n                Graduate\n                Sum\n              \n\n\n\nFavor\n                  117\n                  511\n                  71\n                  135\n                  64\n                  898\n                \n\nOppose\n                  72\n                  200\n                  16\n                  71\n                  50\n                  409\n                \n\nSum\n                  189\n                  711\n                  87\n                  206\n                  114\n                  1307\n                \n\n\n\n\n\n\nFigure 2: Contingency Table Picture\n\n\n\nHow was this computed?\nSo \\(117\\) is the number of people who Left HS and Favor the death penalty, and \\(71\\) is the count for Bachelors who Oppose the death penalty. And so on.\n\n\n\n# One Way Table ( one variable )\ntable(Arthritis$Treatment) # Contingency Table\n\n\nPlacebo Treated \n     43      41 \n\n# 1-way Contingency Table\ntable(Arthritis$Treatment) %&gt;% addmargins() # Contingency Table with margins\n\n\nPlacebo Treated     Sum \n     43      41      84 \n\n# 2-Way Contingency Tables\n# Choosing Treatment and Improved\ntable(Arthritis$Treatment, Arthritis$Improved) %&gt;% addmargins()\n\n         \n          None Some Marked Sum\n  Placebo   29    7      7  43\n  Treated   13    7     21  41\n  Sum       42   14     28  84\n\n# Choosing Treatment and Sex\ntable(Arthritis$Sex, Arthritis$Improved) %&gt;% addmargins()\n\n        \n         None Some Marked Sum\n  Female   25   12     22  59\n  Male     17    2      6  25\n  Sum      42   14     28  84\n\n\nWe can use table() ( and also xtabs() ) to generate multi-dimensional tables too (More than 2-way) These will be printed out as a series of 2D tables, one for each value/level of the “third” parameter. We can then flatten this set of tables using ftable() and add margins to convert into a Contingency Table:\n\nmy_arth_table &lt;- table(Arthritis$Treatment, Arthritis$Sex, Arthritis$Improved)\nmy_arth_table\n\n, ,  = None\n\n         \n          Female Male\n  Placebo     19   10\n  Treated      6    7\n\n, ,  = Some\n\n         \n          Female Male\n  Placebo      7    0\n  Treated      5    2\n\n, ,  = Marked\n\n         \n          Female Male\n  Placebo      6    1\n  Treated     16    5\n\n# Now flatten\nftable(my_arth_table)\n\n                None Some Marked\n                                \nPlacebo Female    19    7      6\n        Male      10    0      1\nTreated Female     6    5     16\n        Male       7    2      5\n\nftable(my_arth_table) %&gt;% addmargins()\n\n             Sum\n    19  7  6  32\n    10  0  1  11\n     6  5 16  27\n     7  2  5  14\nSum 42 14 28  84\n\n\nA bit strange that the column labels disappear in the ftable when margins are added…maybe need to investigate the FUN argument to add_margins().\n\n\nThe vcd ( Visualize Categorical Data ) package by Michael Friendly has a convenient function to create Contingency Tables: structable(); this function produces a ‘flat’ representation of a high-dimensional contingency table constructed by recursive splits (similar to the construction of mosaic charts/graphs). structable tends to render flat tables, of the kind that can be thought of as a “text representation” of the vcd::mosaic plot:\nThe arguments of structable are:\n\na formula \\(y + p \\sim x + z\\) which shows which variables are to be included as columns and rows respectively on a table;\na data argument, which can indicate a data frame from where the variables are drawn.\n\narth_vcd &lt;- vcd::structable(data = Arthritis, Treatment ~ Improved)\narth_vcd\nclass(arth_vcd)\narth_vcd %&gt;% addmargins()\n\n\n\n         Treatment Placebo Treated\nImproved                          \nNone                    29      13\nSome                     7       7\nMarked                   7      21\n[1] \"structable\" \"ftable\"    \n        Treatment\nImproved Placebo Treated Sum\n  None        29      13  42\n  Some         7       7  14\n  Marked       7      21  28\n  Sum         43      41  84\n\n\n\n# With Margins\narth_vcd %&gt;%\n  as.matrix() %&gt;%\n  addmargins()\n\n\n\n        Treatment\nImproved Placebo Treated Sum\n  None        29      13  42\n  Some         7       7  14\n  Marked       7      21  28\n  Sum         43      41  84\n\n\n\n# HairEyeColor is in multiple table form\nHairEyeColor\n# structable flattens these into one, as for a mosaic chart\nvcd::structable(HairEyeColor)\n# As tibble\nvcd::structable(HairEyeColor) %&gt;% as_tibble()\n\n\n\n, , Sex = Male\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    32   11    10     3\n  Brown    53   50    25    15\n  Red      10   10     7     7\n  Blond     3   30     5     8\n\n, , Sex = Female\n\n       Eye\nHair    Brown Blue Hazel Green\n  Black    36    9     5     2\n  Brown    66   34    29    14\n  Red      16    7     7     7\n  Blond     4   64     5     8\n\n\n             Eye Brown Blue Hazel Green\nHair  Sex                              \nBlack Male          32   11    10     3\n      Female        36    9     5     2\nBrown Male          53   50    25    15\n      Female        66   34    29    14\nRed   Male          10   10     7     7\n      Female        16    7     7     7\nBlond Male           3   30     5     8\n      Female         4   64     5     8\n\n\n\n\n\n  \n\n\n\n\nUCBAdmissions is already in Frequency Form i.e. a Contingency Table. But it is a set of (two-way) Contingency Tables:\nUCBAdmissions\n###\nvcd::structable(UCBAdmissions)\n###\nstructable(UCBAdmissions) %&gt;%\n  as.matrix() %&gt;%\n  addmargins()\n\n\n\n, , Dept = A\n\n          Gender\nAdmit      Male Female\n  Admitted  512     89\n  Rejected  313     19\n\n, , Dept = B\n\n          Gender\nAdmit      Male Female\n  Admitted  353     17\n  Rejected  207      8\n\n, , Dept = C\n\n          Gender\nAdmit      Male Female\n  Admitted  120    202\n  Rejected  205    391\n\n, , Dept = D\n\n          Gender\nAdmit      Male Female\n  Admitted  138    131\n  Rejected  279    244\n\n, , Dept = E\n\n          Gender\nAdmit      Male Female\n  Admitted   53     94\n  Rejected  138    299\n\n, , Dept = F\n\n          Gender\nAdmit      Male Female\n  Admitted   22     24\n  Rejected  351    317\n\n\n              Gender Male Female\nAdmit    Dept                   \nAdmitted A            512     89\n         B            353     17\n         C            120    202\n         D            138    131\n         E             53     94\n         F             22     24\nRejected A            313     19\n         B            207      8\n         C            205    391\n         D            279    244\n         E            138    299\n         F            351    317\n\n\n\n\n            Gender\nAdmit_Dept   Male Female  Sum\n  Admitted_A  512     89  601\n  Admitted_B  353     17  370\n  Admitted_C  120    202  322\n  Admitted_D  138    131  269\n  Admitted_E   53     94  147\n  Admitted_F   22     24   46\n  Rejected_A  313     19  332\n  Rejected_B  207      8  215\n  Rejected_C  205    391  596\n  Rejected_D  279    244  523\n  Rejected_E  138    299  437\n  Rejected_F  351    317  668\n  Sum        2691   1835 4526\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that structable does not permit the adding of margins directly; it needs to be converted to a matrix for addmargins() to do its work.\n\n\n\n\nSo far these packages give Contingency Tables that are easy to see for humans; some of these structures are also capable being passed directly to commands such as stats::chisq.test() or janitor::chisq.test().\nOften we need Contingency Tables that are in tibble form, and we need to perform some data processing using dplyr to get there. Doing this with the tidyverse set of packages may seem counter-intuitive and long-winded, but the workflow is easily understandable.\nFirst we develop the counts:\ndiamonds %&gt;% count(cut)\ndiamonds %&gt;% count(clarity)\ndiamonds %&gt;%\n  group_by(cut, clarity) %&gt;%\n  dplyr::summarise(count = n())\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\nWe need to have the individual levels of cut as rows and the individual levels of clarity as columns. This means that we need to pivot this from “long to wide”1 to obtain a Contingency Table:\n\ndiamonds %&gt;%\n  group_by(cut, clarity) %&gt;%\n  dplyr::summarise(count = n()) %&gt;%\n  pivot_wider(\n    id_cols = cut,\n    names_from = clarity,\n    values_from = count\n  ) %&gt;%\n  # Now add the row and column totals using the `janitor` package\n  janitor::adorn_totals(where = c(\"row\", \"col\")) %&gt;%\n  # Recover to tibble since janitor gives a \"tabyl\" format\n  # ( which can be useful too !)\n  as_tibble() -&gt; diamonds_ct\ndiamonds_ct\n\n\n  \n\n\n### Another Way\ndiamonds %&gt;%\n  group_by(cut, clarity) %&gt;%\n  dplyr::summarise(count = n()) %&gt;%\n  pivot_wider(\n    id_cols = cut,\n    names_from = clarity,\n    values_from = count\n  ) %&gt;%\n  # Now add the row and column totals using the `dplyr` package\n  # From: https://stackoverflow.com/a/67885521\n  mutate(\"row_totals\" = sum(across(where(is.integer)))) %&gt;%\n  ungroup() %&gt;%\n  add_row(cut = \"col_total\", summarize(., across(where(is.integer), sum)))\n\n\n  \n\n\n\n\n\n\nNow then, how does one plot a set of data that looks like this, a matrix? No column is a single variable, nor is each row a single observation, which is what we understand with the idea of tidy data.\n\n Mosaic Plots\nThe answer is provided in the very shape of the data: we plot this as a set of tiles, where \\[ \\pmb{area~of~tile \\sim count} \\] We recursively partition off a (usually) square area into vertical and horizontal pieces whose area is proportional to the count at a specific combination of levels of the two Qual variables. So we might follow the process as shown below:\n\nTake the bottom row of per-column totals and create vertical rectangles with these widths\n\nTake the individual counts in the rows and partition each rectangle based in the counts in these rows.\n\n\n\n\n\n\n\n\n\n\n(a) GSS Mosaic Chart Step #1\n\n\n\n\n\n\n\n\n\n(b) GSS Mosaic Chart Step #2\n\n\n\n\n\n\nFigure 3: Mosaic Chart for GSS Data\n\n\nThe first split shows the various levels of Education and their counts as widths. Order is alphabetical! This splitting corresponds to the bottom ROW of the Figure 2. HS is clearly the largest subgroup in Education.\nIn the second step, the columns from Figure 3 (a) are sliced horizontally into tiles, in proportion to the number of people in each Education category/level who support/do not support DeathPenalty. This is done in proportion to all the entries in each COLUMN, giving us Figure 3 (b).\nLet us now make this plot with a variety of approaches.\n\n\nUsing vcd\nUsing ggmosaic\nUsing ggformula\nUsing visStatistics\n\n\n\nThe vcd::mosaic() function needs the data in contingency table form. We already built one using mosaic::tally() and that is easily plotted:\n\n# Code used earlier\ndata(\"GSS2002\", package = \"resampledata\")\ngss2002 &lt;- GSS2002 %&gt;%\n  # select two categorical variables from the dataset\n  dplyr::select(Education, DeathPenalty) %&gt;%\n  drop_na(Education, DeathPenalty)\ngss2002\n\n\n  \n\n\n# make a tally table\ngss_table &lt;- mosaic::tally(DeathPenalty ~ Education, data = gss2002)\ngss_table %&gt;% addmargins()\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\n# gss_table is *not* a tibble, but a *table* object.\n\nvcd::mosaic(gss_table,\n  gp = shading_hsv,\n  main = \"mosaic::tally() + vcd::mosaic()\"\n)\n\n\n\n\n\n\n\nThere is also a command within vcd itself to create a Contingency Table, vcd::structable():\narthritis_table &lt;- vcd::structable(~ Treatment + Improved,\n  data = Arthritis\n)\narthritis_table\nvcd::mosaic(arthritis_table,\n  gp = shading_max,\n  main = \"Arthritis Treatment Dataset\"\n)\n\n\n\n          Improved None Some Marked\nTreatment                          \nPlacebo              29    7      7\nTreated              13    7     21\n\n\n\n\n\n\n\n\nggmosaic takes a tibble with Qualitative variables, internally computes the counts/table, and plots the mosaic plot:\n\n\n\ngss2002\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# library(ggmosaic)\n#\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n##\nggplot(data = gss2002) +\n  ggmosaic::geom_mosaic(\n    aes(\n      x = product(DeathPenalty, Education),\n      fill = DeathPenalty\n    )\n  ) +\n  theme(legend.position = \"top\") +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\nWarning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n3.5.0.\n\n\nWarning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\nWarning: `unite_()` was deprecated in tidyr 1.2.0.\nℹ Please use `unite()` instead.\nℹ The deprecated feature was likely used in the ggmosaic package.\n  Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;.\n\n\n\n\n\n\n\n\n\n\n\n\nThis needs quite some work, to convert the Contingency Table into a mosaic plot; perhaps not the most intuitive of methods either. This code has been developed using this Stackoverflow post.\n\n# Reference\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\ngss_summary &lt;- gss2002 %&gt;%\n  dplyr::group_by(Education, DeathPenalty) %&gt;%\n  dplyr::summarise(count = n()) %&gt;% # This is good for a chisq test\n\n  # Data is still grouped by `Education`\n  # Add two more columns to facilitate mosaic Plot\n  # These two columns are quite unusual...\n  mutate(\n    edu_count = sum(count),\n    edu_prop = count / sum(count)\n  ) %&gt;%\n  ungroup()\ngss_summary\n\n\n  \n\n\n\n\n\n\n# This works but is not very intuitive...\n\ngf_col(edu_prop ~ Education,\n  data = gss_summary,\n  width = ~edu_count, # Not typically used in a column chart\n  fill = ~DeathPenalty,\n  stat = \"identity\",\n  position = \"fill\",\n  color = \"black\"\n) %&gt;%\n  gf_text(edu_prop ~ Education,\n    label = ~ scales::percent(edu_prop),\n    position = position_stack(vjust = 0.5)\n  ) %&gt;%\n  gf_facet_grid(~Education,\n    scales = \"free_x\",\n    space = \"free_x\"\n  ) %&gt;%\n  gf_theme(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\nWarning: Ignoring unknown aesthetics: width\n\n\n\n\n\n\n\n\n\n\n\n\nvisStatistics is a recent package that allows a very wide variety of statistical charts to be created automagically based on the variables chosen. Let us plot a mosaic chart directly with this package\n\nvisstat(gss2002$DeathPenalty, gss2002$Education)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#coloured-tiles-actual-and-expected-contingency-tables",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#coloured-tiles-actual-and-expected-contingency-tables",
    "title": "\n Proportions",
    "section": "Coloured Tiles: Actual and Expected Contingency Tables",
    "text": "Coloured Tiles: Actual and Expected Contingency Tables\nWe notice that the mosaic plots has coloured some tiles blue and some red. Why was this done? Consider the set of mosaic plots below:\n\nvcd::mosaic(arthritis_table, gp = shading_max, legend = FALSE)\nvcd::mosaic(arthritis_table, type = \"expected\")\nvcd::assoc(arthritis_table)\n\n\n\n\n\n\n\n\nFigure 4: Actual Contingency Table\n\n\n\n\n\n\n\n\n\nFigure 5: Expected Contingency Table\n\n\n\n\n\n\n\n\n\nFigure 6: Tile-Wise Differences\n\n\n\n\n\n\nFrom an inspection of these plots, we see the (tile-wise) difference between situations when Qualitative variables are related to that when they not related.\nThe graph on the left show the mosaic plot of the actual Contingency Table.\nThe graph in the middle shows a similar but fictitious plot but with the cuts neatly horizontal or vertical. This mosaic would be what we would expect, if Education and the opinion on Death Penalty were independent!!\nClearly, there are differences in area of the corresponding tiles in the two mosaics, actual and expected, as shown in the graph on the right. Some differences are positive, and some negative. In the actual mosaic, Figure 4, tiles with large positive differences are coloured blue, and those with large negative differences are coloured red.\nThe higher the absolute values of these differences, the greater the effect of one Qual on the other. More when we get into Inference for Two Proportions.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#dataset-titanic",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#dataset-titanic",
    "title": "\n Proportions",
    "section": "\n Dataset: Titanic",
    "text": "Dataset: Titanic\nBanzai!!! That was quite some journey! Let us end it by quickly looking at a sadly famous dataset:\n\ndata(\"titanic\", package = \"ggmosaic\")\ntitanic\n\n\n  \n\n\n\nThere were 2201 passengers, as per this dataset.\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\nNone.\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nSurvived: (chr) yes or no\n\nClass: (chr) Class of Travel, else “crew”\n\nAge: (chr) Adult, Child\n\nSex: (chr) Male / Female.\n\n\n\n\n Research Questions\n\n\n\n\n\n\nNoteQ.1. What is the dependence of survived upon sex?\n\n\n\n\n\n\nvcd::structable(Survived ~ Sex, data = titanic) %&gt;%\n  vcd::mosaic(gp = shading_max)\n\n\n\n\n\n\n\n\n\n\n\n\nNote the huge imbalance in survived with sex: men have clearly perished in larger numbers than women. Which is why the colouring by the Pearson Residuals show large positive residuals for men who died, and large negative residuals for women who died.\nSo sadly Jack is far more likely to have died than Rose.\n\n\n\n\n\n\n\n\nNoteQ.2. How does Survived depend upon Class?\n\n\n\n\n\n\nvcd::structable(Survived ~ Class, data = titanic) %&gt;%\n  vcd::mosaic(gp = shading_max)\n\n\n\n\n\n\n\n\n\n\n\n\nCrew has seen deaths in large numbers, as seen by the large negative residual for crew-survivals. First Class passengers have had speedy access to the boats and have survived in larger proportions than say second or third class. There is a large positive residual for first-class survivals.\nRose travelled first class and Jack was third class. So again the odds are stacked against him.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#balloon-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#balloon-plots",
    "title": "\n Proportions",
    "section": "\n Balloon Plots",
    "text": "Balloon Plots\nThere is another visualization of Categorical Data, called a Balloon Plot. We will use the housetasks dataset from the package ggpubr.\n\n Dataset: Who Does the Housework?\n\nhousetasks &lt;- read.delim(\n  system.file(\"demo-data/housetasks.txt\",\n    package = \"ggpubr\"\n  ),\n  row.names = 1\n)\nhousetasks\n\n\n  \n\n\ninspect(housetasks)\n\n\nquantitative variables:  \n         name   class min Q1 median Q3 max     mean       sd  n missing\n1        Wife integer   0 10     32 77 156 46.15385 50.05971 13       0\n2 Alternating integer   1 11     14 24  51 19.53846 16.26149 13       0\n3     Husband integer   1  5      9 23 160 29.30769 44.97663 13       0\n4     Jointly integer   2  4     15 57 153 39.15385 44.09808 13       0\n\n\nWe see that we have 13 observations.\n\n\n\n\n\n\nImportant\n\n\n\nThis data is already in Contingency Table form (without the margin totals)!\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nFreq: (int) No of times a task was carried out by specific people\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nWho: (chr) Who carried out the task?\n\nTask: (chr) Task? Which task? Can’t you see I’m tired?\n\n\n\n\n\n\nggpubr::ggballoonplot(housetasks,\n  fill = \"value\",\n  ggtheme = theme_pubr(base_family = \"Alegreya\")\n) +\n  scale_fill_viridis_c(option = \"C\") +\n  labs(title = \"A Balloon Plot for Categorical Data\")\n\n\n\n\n\n\n\n\n\n\n\n\nAnd repeat with the familiar HairEyeColor dataset:\n\n\n\ndf &lt;- as_tibble(HairEyeColor)\ndf\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nggballoonplot(df,\n  x = \"Hair\",\n  y = \"Eye\", size = \"n\",\n  fill = \"n\",\n  ggtheme = theme_pubr(base_family = \"Alegreya\")\n) +\n  scale_fill_viridis_c(option = \"C\") +\n  labs(title = \"Balloon Plot\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Balloon Plot with facetting\nggballoonplot(df,\n  x = \"Hair\",\n  y = \"Eye\", size = \"n\",\n  fill = \"n\",\n  facet.by = \"Sex\",\n  ggtheme = theme_pubr(base_family = \"Alegreya\")\n) +\n  scale_fill_viridis_c(option = \"C\") +\n  labs(\n    title = \"Balloon Plot with Facetting by Sex\",\n    subtitle = \"Hair and Eye Color\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nNote the somewhat different syntax with ggballoonplot: the variable names are enclosed in quotes.\nBalloon Plots work because they use color and size aesthetics to represent categories and counts respectively.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#wait-but-why",
    "title": "\n Proportions",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nWe can detect correlation between Quant variables using the scatter plots and regression lines\n\nAnd we can detect association between Qual variables using mosaics, sieves (which we did not see here, but is possible in R), and with balloon plots.\nYour project primary research data may be pure Qualitative too, as with a Questionnaire / Survey instrument.\nOne such Qual variable therein will be your target variable\n\nYou will need to justify whether the target variable is dependent upon the other Quals, and then to decide what to do about that.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#ai-generated-summary-and-podcast",
    "title": "\n Proportions",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis excerpt from a course on Data Analysis using Metaphors focuses on the importance of understanding and visualizing categorical data. It discusses different ways to represent categorical data in R, including case data, frequency data, and cross-tabular count data. The text also explores various visualization techniques like bar plots, pie charts, mosaic plots, and balloon plots. It emphasizes the use of contingency tables for analyzing relationships between categorical variables, illustrating how to create them and visualize them using R packages. Additionally, the text delves into the concept of Pearson residuals, which help to identify associations between categorical variables and highlight deviations from independence.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#conclusion",
    "title": "\n Proportions",
    "section": "\n Conclusion",
    "text": "Conclusion\nHow are the bar plots for categorical data different from histograms? Why don’t “regular” scatter plots simply work for Categorical data? Discuss!\nThere are quite a few things we can do with Qualitative/Categorical data:\n\nMake simple bar charts with colours and facetting\nMake Contingency Tables for a \\(X^2\\)-test\nMake Mosaic Plots to show how the categories stack up\nMake Balloon Charts as an alternative\nThen, draw your inferences and tell the story!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#your-turn",
    "title": "\n Proportions",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTake some of the categorical datasets from the vcd and vcdExtra packages and recreate the plots from this module. Go to https://vincentarelbundock.github.io/Rdatasets/articles/data.html and type “vcd” in the search box. You can directly load CSV files from there, using read_csv(\"url-to-csv\").\nTry the housetasks dataset that we used for Balloon Plots, to create a mosaic plot with Pearson Residuals.\n\n\n\n\n\n\n\nNoteClothing and Intelligence Rating of Children!!\n\n\n\nAre well-dressed students actually smarter?\n Download the Gilby Study dataset \n\n\n\n\n\n\n\n\nNotePre-marital Sex and Divorce.\n\n\n\n Download the pre- and extra-marital sex and divorce dataset \n\n\n\n\n\n\n\n\nNoteAre Emily and Greg More Employable Than Lakisha and Jamal?\n\n\n\nAre first names a basis for racial discrimination, in the US?\nThis dataset was generated as part of a landmark research study done by Marianne Bertrand and Senthil Mullainathan. Read the description therein to really understand how you can prove causality with a well-crafted research experiment.\n Download the Resume Name dataset",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#ai-generated-summary-and-podcast-1",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#ai-generated-summary-and-podcast-1",
    "title": "\n Proportions",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis module focuses on the importance of understanding and visualizing categorical data. It discusses different ways to represent categorical data in R, including case data, frequency data, and cross-tabular count data. The text also explores various visualization techniques like bar plots, pie charts, mosaic plots, and balloon plots. It emphasizes the use of contingency tables for analyzing relationships between categorical variables, illustrating how to create them and visualize them using R packages. Additionally, the text delves into the concept of Pearson residuals, which help to identify associations between categorical variables and highlight deviations from independence.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#references",
    "title": "\n Proportions",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nNice Chi-square interactive story at https://statisticalstories.xyz/chi-square\n\nChittaranjan Andrade(July 22, 2015). Understanding Relative Risk, Odds Ratio, and Related Terms: As Simple as It Can Get. https://www.psychiatrist.com/jcp/understanding-relative-risk-odds-ratio-related-terms/\n\nMine Cetinkaya-Rundel and Johanna Hardin. An Introduction to Modern Statistics, Chapter 4. https://openintro-ims.netlify.app/explore-categorical.html\n\nUsing the strcplot command from vcd, https://cran.r-project.org/web/packages/vcd/vignettes/strucplot.pdf\n\nCreating Frequency Tables with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/A_creating.html\n\nCreating mosaic plots with vcd, https://cran.r-project.org/web/packages/vcdExtra/vignettes/D_mosaics.html\n\nMichael Friendly, Corrgrams: Exploratory displays for correlation matrices. The American Statistician August 19, 2002 (v1.5). https://www.datavis.ca/papers/corrgram.pdf\n\n\nVisualizing Categorical Data in R\n\nH. Riedwyl & M. Schüpbach (1994), Parquet diagram to plot contingency tables. In F. Faulbaum (ed.), Softstat ’93: Advances in Statistical Software, 293–299. Gustav Fischer, New York.\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggmosaic\n0.3.3\nJeppson, Hofmann, and Cook (2021)\n\n\nggpubr\n0.6.1\nKassambara (2025)\n\n\ntidyplots\n0.3.1\nEngler (2025)\n\n\ntinyplot\n0.4.2\nMcDermott, Arel-Bundock, and Zeileis (2025)\n\n\ntinytable\n0.10.0\nArel-Bundock (2025)\n\n\nvcd\n1.4.13\n\nMeyer, Zeileis, and Hornik (2006); Zeileis, Meyer, and Hornik (2007); Meyer et al. (2024)\n\n\n\nvcdExtra\n0.8.5\nFriendly (2023)\n\n\nvisStatistics\n0.1.7\nSchilling (2025)\n\n\n\n\n\n\nArel-Bundock, Vincent. 2025. tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,” “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats. https://doi.org/10.32614/CRAN.package.tinytable.\n\n\nEngler, Jan Broder. 2025. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” iMeta, e70018. https://doi.org/10.1002/imt2.70018.\n\n\nFriendly, Michael. 2023. vcdExtra: “vcd” Extensions and Additions. https://doi.org/10.32614/CRAN.package.vcdExtra.\n\n\nJeppson, Haley, Heike Hofmann, and Di Cook. 2021. ggmosaic: Mosaic Plots in the “ggplot2” Framework. https://doi.org/10.32614/CRAN.package.ggmosaic.\n\n\nKassambara, Alboukadel. 2025. ggpubr: “ggplot2” Based Publication Ready Plots. https://doi.org/10.32614/CRAN.package.ggpubr.\n\n\nMcDermott, Grant, Vincent Arel-Bundock, and Achim Zeileis. 2025. tinyplot: Lightweight Extension of the Base r Graphics System. https://doi.org/10.32614/CRAN.package.tinyplot.\n\n\nMeyer, David, Achim Zeileis, and Kurt Hornik. 2006. “The Strucplot Framework: Visualizing Multi-Way Contingency Tables with Vcd.” Journal of Statistical Software 17 (3): 1–48. https://doi.org/10.18637/jss.v017.i03.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2024. vcd: Visualizing Categorical Data. https://doi.org/10.32614/CRAN.package.vcd.\n\n\nSchilling, Sabine. 2025. visStatistics: Automated Selection and Visualisation of Statistical Hypothesis Tests. https://doi.org/10.32614/CRAN.package.visStatistics.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007. “Residual-Based Shadings for Visualizing (Conditional) Independence.” Journal of Computational and Graphical Statistics 16 (3): 507–25. https://doi.org/10.1198/106186007X237856.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/40-CatData/index.html#footnotes",
    "title": "\n Proportions",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://tidyr.tidyverse.org/articles/pivot.html↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:proportional-scaling\"></iconify-icon> Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html",
    "title": "\n Parts of a Whole",
    "section": "",
    "text": "“There is no such thing as a”self-made” man. We are made up of thousands of others. Everyone who has ever done a kind deed for us, or spoken one word of encouragement to us, has entered into the make-up of our character and of our thoughts.”\n— George Matthew Adams, newspaper columnist (23 Aug 1878-1962)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#setting-up-the-packages",
    "title": "\n Parts of a Whole",
    "section": "\n Setting up the Packages",
    "text": "Setting up the Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(plotrix) # Fan, Pyramid Chart\n# devtools::install_github(\"zmeers/ggparliament\")\nlibrary(ggparliament) # Parliament Chart\nlibrary(ggpol) # Parliament, Arc-Bar and other interesting charts\nlibrary(data.tree) # Many plots related to heirarchical data\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\nlibrary(waffle)\nlibrary(tidygraph) # Trees, Dendros, and Circle Packings\nlibrary(ggraph) # Trees, Dendros, and Circle Packings\nlibrary(echarts4r) # Interactive Charts\n\nlibrary(patchwork) # Arrange your plots\n\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#what-graphs-will-we-see-today",
    "title": "\n Parts of a Whole",
    "section": "\n What Graphs will we see today?",
    "text": "What Graphs will we see today?\nThere are a good few charts available to depict things that constitute other bigger things. We will discuss a few of these: Pie, Fan, and Donuts; Waffle and Parliament charts; Trees, Dendrograms, and Circle Packings. (The last three visuals we will explore along with network diagrams in a later module.)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#pies-and-fans",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#pies-and-fans",
    "title": "\n Parts of a Whole",
    "section": "\n Pies and Fans",
    "text": "Pies and Fans\nSo let us start with “eating humble pie”: discussing a Pie chart first.\nA pie chart is a circle divided into sectors that each represent a proportion of the whole. It is often used to show percentage, where the sum of the sectors equals 100%.\nThe problem is that humans are pretty bad at reading angles. This ubiquitous chart is much vilified in the industry and bar charts that we have seen earlier, are viewed as better options. On the other hand, pie charts are ubiquitous in business circles, and are very much accepted! Do also read this spirited defense of pie charts here. https://speakingppt.com/why-tufte-is-flat-out-wrong-about-pie-charts/\nAnd we will also see that there is an attractive, and similar-looking alternative, called a fan chart which we will explore here.\n\n\nUsing Base R\nUsing ggformula\nUsing echarts4r\n\n\n\nBase R has a simple pie command that does the job. Let’s create some toy data first:\n\n\n\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12),\n\n  # Labels MUST be character entries for `pie` to work\n  labels = c(\n    \"Blueberry\", \"Cherry\", \"Apple\", \"Boston Cream\",\n    \"Other\", \"Vanilla Cream\"\n  )\n)\npie_data\n\n\n  \n\n\npie(\n  x = pie_data$sales,\n  labels = pie_data$labels, # Character Vector is a MUST\n\n  # Pie is within a square of 1 X 1 units\n  # Reduce radius if needed to see labels properly\n  radius = 0.95,\n  init.angle = 90, # First slice starts at 12 o'clock position\n\n  # Change the default colours. Comment this and see what happens.\n  col = grDevices::hcl.colors(palette = \"Plasma\", n = 6)\n)\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nWe create a bar chart or a column chart as appropriate, with bars filled by category. The width parameter is set to 1 so that the bars touch. The bars have a fixed width along the x-axis; the height of the bar varies based on the number we wish to show. Then the coord_polar(theta = \"y\") converts the bar plot into a pie.\n# Using gf_col since we have a count/value column already\npie_data %&gt;%\n  gf_col(sales ~ 1, fill = ~labels, width = 1, color = \"black\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\npie_data %&gt;%\n  gf_col(sales ~ 1, fill = ~labels, width = 1, color = \"black\") %&gt;%\n  gf_refine(coord_polar(theta = \"y\")) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n# Using gf_bar since we don't have ready made counts\ngf_bar(\n  data = mpg,\n  ~1,\n  fill = ~drv,\n  color = \"black\", # border for the bars/slices\n  width = 1\n) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\ngf_bar(\n  data = mpg,\n  ~0.5,\n  fill = ~drv,\n  color = \"black\", # border for the bars/slices\n  width = 1\n) %&gt;%\n  gf_refine(coord_polar(theta = \"y\")) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is a basic interactive pie chart withecharts4r:\n\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12),\n  labels = c(\n    \"Blueberry\", \"Cherry\", \"Apple\", \"Boston Cream\", \"Other\",\n    \"Vanilla Cream\"\n  )\n)\npie_data %&gt;%\n  e_charts(x = labels) %&gt;%\n  e_pie(\n    serie = sales, clockwise = TRUE,\n    startAngle = 90\n  ) %&gt;%\n  e_legend(list(\n    orient = \"vertical\",\n    left = \"right\"\n  )) %&gt;%\n  e_tooltip()\n\n\n\n\n\nWe can add more bells and whistles to the humble-pie chart, and make a Nightingale rosechart out of it:\npie_data &lt;- tibble(\n  sales = c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12),\n  labels = c(\n    \"Blueberry\", \"Cherry\", \"Apple\", \"Boston Cream\", \"Other\",\n    \"Vanilla Cream\"\n  )\n)\npie_data %&gt;%\n  e_charts(x = labels) %&gt;%\n  e_pie(\n    serie = sales, clockwise = TRUE,\n    startAngle = 90,\n    roseType = \"area\"\n  ) %&gt;% # try \"radius\"\n\n  # Lets move the legend\n  e_legend(left = \"right\", orient = \"vertical\") %&gt;%\n  e_tooltip()\npie_data %&gt;%\n  e_charts(x = labels) %&gt;%\n  e_pie(\n    serie = sales, clockwise = TRUE,\n    startAngle = 90,\n    roseType = \"radius\"\n  ) %&gt;%\n  # Lets move the legend\n  e_legend(left = \"right\", orient = \"vertical\") %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nFor more information and customization look at https://echarts.apache.org/en/option.html#series-pie\n\n\n\nThe fan Plot\nThe fan plot (from the plotrix package) displays numerical values as arcs of overlapping sectors. This allows for more effective comparison:\n\n\n\nplotrix::fan.plot(\n  x = pie_data$sales,\n  labels = pie_data$labels,\n  col = grDevices::hcl.colors(palette = \"Lajolla\", n = 6), # Try hcl.pals()\n  shrink = 0.03,\n  # How much to shrink each successive sector\n\n  label.radius = 1.15,\n  main = \"Fan Plot of Ice Cream Flavours\",\n  # ticks = 360,\n  # if we want tick marks on the circumference\n\n  max.span = pi\n)\n\n\n\n\n\n\n\n\n\n\n\n\nThere is no fan plot possible with echarts4r, as far as I know.\nThe Donut Chart\nThe donut chart suffers from the same defects as the pie, so should be used with discretion. The donut chart is essentially a gf_rect from ggformula, plotted on a polar coordinate set of of axes:\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\nLet us make some toy data:\n# Data\ndf &lt;- tibble(\n  group = LETTERS[1:3],\n  value = c(25, 20, 35)\n)\n\ndf &lt;-\n  df %&gt;%\n  dplyr::mutate(\n    fraction = value / sum(value), # percentages\n    ymax = cumsum(fraction), # cumulative percentages\n    ymin = lag(ymax, 1, default = 0),\n    # bottom edge of each\n    label = paste0(group, \"\\n value: \", value),\n    labelPosition = (ymax + ymin) / 2 # labels midway on arcs\n  )\n\ndf\ndf %&gt;%\n  # gf_rect() formula: ymin + ymax ~ xmin + xmax\n  # Bars with varying thickness (y) proportional to data\n  # Fixed length x (2 to 4)\n  gf_rect(ymin + ymax ~ 2 + 4,\n    fill = ~group, colour = \"black\"\n  ) %&gt;%\n  gf_label(labelPosition ~ 3.5,\n    label = ~label, colour = \"black\",\n    size = 4\n  ) %&gt;%\n  # When switching to polar coords:\n  # x maps to radius\n  # y maps to angle theta\n  # so we create a \"hole\" in the radius, in x\n  gf_refine(coord_polar(\n    theta = \"y\",\n    direction = 1\n  )) %&gt;%\n  # Up to here will give us a pie chart\n\n  # Now to create the hole\n  # try to play with the \"0\"\n  # Recall x = [2,4]\n  gf_refine(xlim(c(-2, 5)), scale_fill_brewer(palette = \"Spectral\")) %&gt;%\n  gf_theme(theme = theme_void()) %&gt;%\n  gf_theme(legend.position = \"none\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nThe donut chart is simply a variant of the pie chart in echarts4r:\ndf &lt;- tibble(\n  group = LETTERS[1:3],\n  value = c(25, 20, 35)\n)\n\ndf &lt;-\n  df %&gt;%\n  dplyr::mutate(\n    fraction = value / sum(value), # percentages\n    ymax = cumsum(fraction), # cumulative percentages\n    ymin = lag(ymax, 1, default = 0),\n    # bottom edge of each\n    label = paste0(group, \"\\n value: \", value),\n    labelPosition = (ymax + ymin) / 2 # labels midway on arcs\n  )\ndf\ndf %&gt;%\n  e_charts(x = group, width = 400) %&gt;%\n  e_pie(\n    serie = value,\n    clockwise = TRUE,\n    startAngle = 90,\n    radius = c(\"50%\", \"70%\")\n  ) %&gt;%\n  e_legend(left = \"right\", orient = \"vertical\") %&gt;%\n  e_tooltip()",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#waffle-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#waffle-charts",
    "title": "\n Parts of a Whole",
    "section": "\n Waffle Charts",
    "text": "Waffle Charts\nWaffle charts are often called “square pie charts” !\nHere we will need to step outside of ggformula and get into ggplot itself momentarily. (Always remember that ggformula is a simplified and intuitive method that runs on top of ggplot.) We will use the waffle package.\n\n\n\n# install.packages(\"waffle\", repos = \"https://cinc.rud.is\")\nlibrary(waffle)\n\n# Data\ndf &lt;- tibble(\n  group = LETTERS[1:3],\n  value = c(25, 20, 35)\n)\ndf\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Waffle plot\n# Using ggplot, sadly not yet ggformula\n\n\nggplot(df, aes(fill = group, values = value)) +\n  geom_waffle(\n    n_rows = 8,\n    size = 0.33,\n    colour = \"white\",\n    na.rm = TRUE\n  ) +\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#BA182A\", \"#FF8288\", \"#FFDBDD\"),\n    labels = c(\"A\", \"B\", \"C\")\n  ) +\n  labs(\n    title = \"Waffle Chart\",\n    subtitle = \"A square pie chart\",\n    caption = \"Source: Toy Data\"\n  ) +\n  coord_equal()",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#parliament-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#parliament-charts",
    "title": "\n Parts of a Whole",
    "section": "\n Parliament Charts",
    "text": "Parliament Charts\nThe package ggpol offers an interesting visualization in the shape of a array of “seats” in a parliament. (There is also a package called ggparliament which in my opinion is a bit cumbersome, having a two-step procedure to convert data into “parliament form” etc. )\n\n\n\n# Same toy dataset\n# df &lt;- tibble(group = LETTERS[1:3],\n#                  value = c(25, 20, 35))\n#\n# Parliament Plot\nggplot(df) +\n  ggpol::geom_parliament(\n    aes(\n      seats = value,\n      fill = group\n    ),\n    r0 = 2, # inner radius\n    r1 = 4 # Outer radius\n  ) +\n  scale_fill_manual(\n    name = NULL,\n    values = c(\"#BA182A\", \"#FF8288\", \"#FFDBDD\"),\n    labels = c(\"A\", \"B\", \"C\")\n  ) +\n  labs(\n    title = \"Parliament Chart\",\n    subtitle = \"A circular array of seats\",\n    caption = \"Source: Toy Data\"\n  ) +\n  coord_equal()",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#trees-dendrograms-and-circle-packings",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#trees-dendrograms-and-circle-packings",
    "title": "\n Parts of a Whole",
    "section": "Trees, Dendrograms, and Circle Packings",
    "text": "Trees, Dendrograms, and Circle Packings\nThere are still more esoteric plots to explore, if you are hell-bent on startling people ! There is an R package called ggraph, that can do these charts, and many more:\n\nggraph is an extension of ggplot2 aimed at supporting relational data structures such as networks, graphs, and trees. While it builds upon the foundation of ggplot2 and its API it comes with its own self-contained set of geoms, facets, etc., as well as adding the concept of layouts to the grammar.\n\nWe will explore these charts when we examine network diagrams. For now, we can quickly see what these diagrams look like. Although the R-code is visible to you, it may not make sense at the moment!\n\n Dendrograms\nFrom the R Graph Gallery Website :\n\nDendrograms can be built from:\n\nHierarchical dataset: think about a CEO managing team leads managing employees and so on.\nClustering result: clustering divides a set of individuals in group according to their similarity. Its result can be visualized as a tree.\n\n\n\n\n\n# create an edge list data frame giving the hierarchical structure of your individuals\nd1 &lt;- tibble(from = \"origin\", to = paste(\"group\", seq(1, 5), sep = \"\"))\nd2 &lt;- tibble(from = rep(d1$to, each = 5), to = paste(\"subgroup\", seq(1, 25), sep = \"_\"))\nedges &lt;- rbind(d1, d2)\nedges\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Create a graph object\nmygraph1 &lt;- tidygraph::as_tbl_graph(edges)\nmygraph1\n\n\n\n\n# A tbl_graph: 31 nodes and 30 edges\n#\n# A rooted tree\n#\n# Node Data: 31 × 1 (active)\n   name      \n   &lt;chr&gt;     \n 1 origin    \n 2 group1    \n 3 group2    \n 4 group3    \n 5 group4    \n 6 group5    \n 7 subgroup_1\n 8 subgroup_2\n 9 subgroup_3\n10 subgroup_4\n# ℹ 21 more rows\n#\n# Edge Data: 30 × 2\n   from    to\n  &lt;int&gt; &lt;int&gt;\n1     1     2\n2     1     3\n3     1     4\n# ℹ 27 more rows\n\n\n\n\n\n\n\n# Basic tree\nggraph(mygraph1,\n  layout = \"dendrogram\",\n  circular = TRUE\n) +\n  geom_edge_diagonal() +\n  geom_node_point(size = 3) +\n  geom_node_label(aes(label = name),\n    size = 3, repel = TRUE\n  ) +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# create a data frame\ndata &lt;- tibble(\n  level1 = \"CEO\",\n  level2 = c(rep(\"boss1\", 4), rep(\"boss2\", 4)),\n  level3 = paste0(\"mister_\", letters[1:8])\n)\n\n# transform it to a edge list!\nedges_level1_2 &lt;- data %&gt;%\n  select(level1, level2) %&gt;%\n  unique() %&gt;%\n  rename(from = level1, to = level2)\n\nedges_level2_3 &lt;- data %&gt;%\n  select(level2, level3) %&gt;%\n  unique() %&gt;%\n  rename(from = level2, to = level3)\n\nedge_list &lt;- rbind(edges_level1_2, edges_level2_3)\nedge_list\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nmygraph2 &lt;- as_tbl_graph(edge_list)\nmygraph2\n\n\n\n\n# A tbl_graph: 11 nodes and 10 edges\n#\n# A rooted tree\n#\n# Node Data: 11 × 1 (active)\n   name    \n   &lt;chr&gt;   \n 1 CEO     \n 2 boss1   \n 3 boss2   \n 4 mister_a\n 5 mister_b\n 6 mister_c\n 7 mister_d\n 8 mister_e\n 9 mister_f\n10 mister_g\n11 mister_h\n#\n# Edge Data: 10 × 2\n   from    to\n  &lt;int&gt; &lt;int&gt;\n1     1     2\n2     1     3\n3     2     4\n# ℹ 7 more rows\n\n\n\n\n\n\n\n# Now we can plot that\nggraph(mygraph2, layout = \"dendrogram\", circular = FALSE) +\n  geom_edge_diagonal() +\n  geom_node_point(size = 3) +\n  geom_node_label(aes(label = name), repel = TRUE) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\nCircle Packing\n\n\n\ngraph_flare &lt;- tbl_graph(flare$vertices, flare$edges)\ngraph_flare\n\n\n\n\n# A tbl_graph: 252 nodes and 251 edges\n#\n# A rooted tree\n#\n# Node Data: 252 × 3 (active)\n   name                                            size shortName            \n   &lt;chr&gt;                                          &lt;dbl&gt; &lt;chr&gt;                \n 1 flare.analytics.cluster.AgglomerativeCluster    3938 AgglomerativeCluster \n 2 flare.analytics.cluster.CommunityStructure      3812 CommunityStructure   \n 3 flare.analytics.cluster.HierarchicalCluster     6714 HierarchicalCluster  \n 4 flare.analytics.cluster.MergeEdge                743 MergeEdge            \n 5 flare.analytics.graph.BetweennessCentrality     3534 BetweennessCentrality\n 6 flare.analytics.graph.LinkDistance              5731 LinkDistance         \n 7 flare.analytics.graph.MaxFlowMinCut             7840 MaxFlowMinCut        \n 8 flare.analytics.graph.ShortestPaths             5914 ShortestPaths        \n 9 flare.analytics.graph.SpanningTree              3416 SpanningTree         \n10 flare.analytics.optimization.AspectRatioBanker  7074 AspectRatioBanker    \n# ℹ 242 more rows\n#\n# Edge Data: 251 × 2\n   from    to\n  &lt;int&gt; &lt;int&gt;\n1   221     1\n2   221     2\n3   221     3\n# ℹ 248 more rows\n\n\n\n\n\n\n\nset.seed(1)\nggraph(graph_flare, \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = as_factor(depth)), size = 0.25, n = 50) +\n  coord_fixed() +\n  scale_fill_brewer(name = \"Depth\", palette = \"Set1\")",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#your-turn",
    "title": "\n Parts of a Whole",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nUse the penguins dataset from the palmerpenguins package and plot pies, fans, and donuts as appropriate.\nLook at the whigs and highschool datasets in the package ggraph. Plot Pies, Fans and if you are feeling confident, Trees, Dendrograms, and Circle Packings as appropriate for these.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/index.html#references",
    "title": "\n Parts of a Whole",
    "section": "\n References",
    "text": "References\n\nIaroslava.2020. A Parliament Diagram in R, https://datavizstory.com/a-parliament-diagram-in-r/\n\nVenn Diagrams in R, Venn diagram in ggplot2 | R CHARTS (r-charts.com)\n\nGenerate icon-array charts without code! https://iconarray.com\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ndata.tree\n1.1.0\nGlur (2023)\n\n\necharts4r\n0.4.5\nCoene (2023)\n\n\nggparliament\n3.1.6\nHickman, Meers, and Leeper (2024)\n\n\nggpol\n0.0.7\nTiedemann (2020)\n\n\nggraph\n2.2.1\nPedersen (2024a)\n\n\nplotrix\n3.8.4\nJ (2006)\n\n\ntidygraph\n1.3.1\nPedersen (2024b)\n\n\nwaffle\n1.0.2\nRudis and Gandy (2023)\n\n\n\n\n\n\nCoene, John. 2023. Echarts4r: Create Interactive Graphs with “Echarts JavaScript” Version 5. https://doi.org/10.32614/CRAN.package.echarts4r.\n\n\nGlur, Christoph. 2023. data.tree: General Purpose Hierarchical Data Structure. https://doi.org/10.32614/CRAN.package.data.tree.\n\n\nHickman, Robert, Zoe Meers, and Thomas J. Leeper. 2024. ggparliament: Parliament Plots. https://github.com/zmeers/ggparliament.\n\n\nJ, Lemon. 2006. “Plotrix: A Package in the Red Light District of r.” R-News 6 (4): 8–12.\n\n\nPedersen, Thomas Lin. 2024a. ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://doi.org/10.32614/CRAN.package.ggraph.\n\n\n———. 2024b. tidygraph: A Tidy API for Graph Manipulation. https://doi.org/10.32614/CRAN.package.tidygraph.\n\n\nRudis, Bob, and Dave Gandy. 2023. waffle: Create Waffle Chart Visualizations. https://doi.org/10.32614/CRAN.package.waffle.\n\n\nTiedemann, Frederik. 2020. ggpol: Visualizing Social Science Data with “ggplot2”. https://doi.org/10.32614/CRAN.package.ggpol.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ic:round-pie-chart-outline\"></iconify-icon> Parts of a Whole"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\nlibrary(corrplot) # For Correlogram plots\nlibrary(plotly)\nlibrary(echarts4r)\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\n\nLet us inspect what datasets are available in the package mosaicData. Run this command in your Console: data(package = “mosaicData”)\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it: (We will save the inspect output as an R object for use later)\n\ndata(\"Galton\")\ngalton_describe &lt;- inspect(Galton)\ngalton_describe$categorical\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n  \n\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\nThe dataset is described as:Try help(\"Galton\") in your Console.\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nNotePair-wise Correlation Plot\n\n\n\nQ.1 Which are the variables that have significant pair-wise correlations? What polarity are these correlations?\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  diag = list(\"densityDiag\"),\n  title = \"Galton Data Correlations Plot\"\n) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: There are significant, but low value correlations in the Galton dataset. height is best correlated with father (\\(0.275\\)). The Scatter Plots shown in the plot also visually demonstrate the (lack of) large value correlations.\nWe cannot have too many variables in this kind of plot. We will shortly see how to plot correlations when there are a large number of variables.\n\n\n\n\n\n\n\n\nNoteHeatmap\n\n\n\necharts4r does not have a comprehensive combination plot like what GGally offers. However, we can plot a Correlation Heatmap using echarts4r:\n\nGalton %&gt;%\n  select(where(is.numeric)) %&gt;%\n  mosaic::cor() %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_correlations(order = \"hclust\", visual_map = TRUE) %&gt;%\n  e_title(\"Galton Correlations Heatmap\")\n\n\n\n\n\nInsight: Moving the cursor over the heatmap gives us the an indication of the correlation scores between variables. The visual map slider moves automatically to indicate the scores. We can also move the slider ourselves to “filter” the heatmap!\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2: Can we plot a Correlogram for this dataset?\n\n# library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(\n    method = \"ellipse\",\n    type = \"lower\",\n    main = \"Correlogram for Galton dataset\"\n  )\n\n\n\n\n\n\n\nInsight: Again, height is positively correlated to father and mother as depicted by the rightward-sloping blue ellipses. And height is negatively correlated (very slightly) with nkids, with leftward-sloping reddish ellipses. (See the color palette + legend below the figure).\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3: What do the correlation tests tell us?\nmosaic::cor_test(height ~ father, data = Galton)\nmosaic::cor_test(height ~ mother, data = Galton)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 8.5737, df = 896, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2137851 0.3347455\nsample estimates:\n      cor \n0.2753548 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.1628, df = 896, p-value = 1.079e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1380554 0.2635982\nsample estimates:\n      cor \n0.2016549 \n\n\n\nInsight: The tests give us the same values seen before, along with the confidence intervals for the correlation estimate. These represent the uncertainty that exists in our estimates.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4: What does this correlation look when split by sex of Child?\nWe will use the mosaic function cor_test to get these results:\n# For the sons\nmosaic::cor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"M\")\n)\ncor_test(height ~ mother, data = Galton %&gt;%\n  filter(sex == \"M\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 9.1498, df = 463, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3114667 0.4656805\nsample estimates:\n      cor \n0.3913174 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 7.628, df = 463, p-value = 1.367e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2508178 0.4125305\nsample estimates:\n      cor \n0.3341309 \n\n\n\n# For the daughters\ncor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"F\")\n)\ncor_test(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"F\")\n)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 10.719, df = 431, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3809944 0.5300812\nsample estimates:\n      cor \n0.4587605 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.8588, df = 431, p-value = 2.421e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2261463 0.3962226\nsample estimates:\n      cor \n0.3136984 \n\n\n\nInsight: Son’s heights are correlated more with father than with mother. This trend is even more so for daughters! Hmmm…mother’s influence on children is clearly not with height.\n\n\n\n\n\n\n\n\n\nNoteCorrelation Tests and Uncertainty\n\n\n\nNote how the cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found. Note that GGally too reports the significance of the correlation scores using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\nWe can also visualise this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse: Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows:\n\nall_corrs &lt;- Galton %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off height to get all the remaining ones\n  select(-height) %&gt;%\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, Galton$height)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nall_corrs\n\n\n  \n\n\n\n\nall_corrs %&gt;%\n  e_charts(predictor) %&gt;%\n  e_bar(estimate, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_error_bar(lower = conf.low, upper = conf.high) %&gt;%\n  e_y_axis(\n    name = \"Correlation with `height`\",\n    nameLocation = \"middle\", nameGap = 35\n  ) %&gt;%\n  e_x_axis(\n    name = \"Parameter\", nameLocation = \"center\",\n    nameGap = 35, type = \"category\"\n  ) %&gt;%\n  e_tooltip()\n\n\n\n\nall_corrs %&gt;%\n  mutate(sd = (conf.high - conf.low) / 2) %&gt;%\n  plot_ly() %&gt;%\n  add_bars(\n    y = ~estimate, x = ~predictor,\n    error_y = ~ list(array = sd, color = \"black\")\n  )\n\n\n\n\n\nInsight: We can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very slightly, in a negative way.\nThis kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n# For the father\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(father, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  e_lm(height ~ father, legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"father\", nameLocation = \"middle\", nameGap = 35,\n    min = 60, max = 80\n  ) %&gt;%\n  e_y_axis(\n    name = \"height\", nameLocation = \"middle\", nameGap = 35,\n    min = 50, max = 80\n  ) %&gt;%\n  e_tooltip()\n# for the mother\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(mother, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  e_lm(height ~ mother, legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"mother\", nameLocation = \"middle\", nameGap = 35,\n    min = 55, max = 75\n  ) %&gt;%\n  e_y_axis(\n    name = \"height\", nameLocation = \"middle\", nameGap = 35,\n    min = 50, max = 80\n  ) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Visibly the scatter plots are slightly tilted upward to the right, showing a positive correlation for both sons’ and daughters’ heights with that of the father and mother.\n\n\n\n\n\n\n\n\nNoteGalton’s Plot\n\n\n\nAn approximation to Galton’s famous plot (see Wikipedia):\n\ngf_point(height ~ (father + mother) / 2, data = Galton) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_density_2d(n = 8) %&gt;%\n  gf_abline(slope = 1) %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\nInsight: How would you interpret this plot1? As yet we are not able to reproduce this with charts4r.\n\n\n\nWe will “live code” this in class!\n\nWe have a decent Correlations related workflow in R:\n- load the dataset\n- inspect the dataset, identify Quant and Qual variables\n- Develop Pair-Wise plots + Correlations using GGally::ggpairs()\n- Develop Correlogram corrplot::corrplot\n- Check everything with a cor_test\n- Use purrr + cor.test to plot correlations and confidence intervals for multiple Quant variables\n- Plot scatter plots using gf_point.\n- Add extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#setting-up-r-packages",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\nlibrary(corrplot) # For Correlogram plots\nlibrary(plotly)\nlibrary(echarts4r)\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-1-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-1-dataset-from-mosaicdata",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "Let us inspect what datasets are available in the package mosaicData. Run this command in your Console: data(package = “mosaicData”)\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it: (We will save the inspect output as an R object for use later)\n\ndata(\"Galton\")\ngalton_describe &lt;- inspect(Galton)\ngalton_describe$categorical\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n  \n\n\n\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\nThe dataset is described as:Try help(\"Galton\") in your Console.\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! What can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Discuss!\n\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nNotePair-wise Correlation Plot\n\n\n\nQ.1 Which are the variables that have significant pair-wise correlations? What polarity are these correlations?\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  diag = list(\"densityDiag\"),\n  title = \"Galton Data Correlations Plot\"\n) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: There are significant, but low value correlations in the Galton dataset. height is best correlated with father (\\(0.275\\)). The Scatter Plots shown in the plot also visually demonstrate the (lack of) large value correlations.\nWe cannot have too many variables in this kind of plot. We will shortly see how to plot correlations when there are a large number of variables.\n\n\n\n\n\n\n\n\nNoteHeatmap\n\n\n\necharts4r does not have a comprehensive combination plot like what GGally offers. However, we can plot a Correlation Heatmap using echarts4r:\n\nGalton %&gt;%\n  select(where(is.numeric)) %&gt;%\n  mosaic::cor() %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_correlations(order = \"hclust\", visual_map = TRUE) %&gt;%\n  e_title(\"Galton Correlations Heatmap\")\n\n\n\n\n\nInsight: Moving the cursor over the heatmap gives us the an indication of the correlation scores between variables. The visual map slider moves automatically to indicate the scores. We can also move the slider ourselves to “filter” the heatmap!\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2: Can we plot a Correlogram for this dataset?\n\n# library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(\n    method = \"ellipse\",\n    type = \"lower\",\n    main = \"Correlogram for Galton dataset\"\n  )\n\n\n\n\n\n\n\nInsight: Again, height is positively correlated to father and mother as depicted by the rightward-sloping blue ellipses. And height is negatively correlated (very slightly) with nkids, with leftward-sloping reddish ellipses. (See the color palette + legend below the figure).\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3: What do the correlation tests tell us?\nmosaic::cor_test(height ~ father, data = Galton)\nmosaic::cor_test(height ~ mother, data = Galton)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 8.5737, df = 896, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2137851 0.3347455\nsample estimates:\n      cor \n0.2753548 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.1628, df = 896, p-value = 1.079e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1380554 0.2635982\nsample estimates:\n      cor \n0.2016549 \n\n\n\nInsight: The tests give us the same values seen before, along with the confidence intervals for the correlation estimate. These represent the uncertainty that exists in our estimates.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4: What does this correlation look when split by sex of Child?\nWe will use the mosaic function cor_test to get these results:\n# For the sons\nmosaic::cor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"M\")\n)\ncor_test(height ~ mother, data = Galton %&gt;%\n  filter(sex == \"M\"))\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 9.1498, df = 463, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3114667 0.4656805\nsample estimates:\n      cor \n0.3913174 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 7.628, df = 463, p-value = 1.367e-13\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2508178 0.4125305\nsample estimates:\n      cor \n0.3341309 \n\n\n\n# For the daughters\ncor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"F\")\n)\ncor_test(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"F\")\n)\n\n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and father\nt = 10.719, df = 431, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3809944 0.5300812\nsample estimates:\n      cor \n0.4587605 \n\n\n\n    Pearson's product-moment correlation\n\ndata:  height and mother\nt = 6.8588, df = 431, p-value = 2.421e-11\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2261463 0.3962226\nsample estimates:\n      cor \n0.3136984 \n\n\n\nInsight: Son’s heights are correlated more with father than with mother. This trend is even more so for daughters! Hmmm…mother’s influence on children is clearly not with height."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#correlation-tests-and-uncertainty",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#correlation-tests-and-uncertainty",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "NoteCorrelation Tests and Uncertainty\n\n\n\nNote how the cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found. Note that GGally too reports the significance of the correlation scores using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\nWe can also visualise this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse: Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows:\n\nall_corrs &lt;- Galton %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off height to get all the remaining ones\n  select(-height) %&gt;%\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, Galton$height)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nall_corrs\n\n\n  \n\n\n\n\nall_corrs %&gt;%\n  e_charts(predictor) %&gt;%\n  e_bar(estimate, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_error_bar(lower = conf.low, upper = conf.high) %&gt;%\n  e_y_axis(\n    name = \"Correlation with `height`\",\n    nameLocation = \"middle\", nameGap = 35\n  ) %&gt;%\n  e_x_axis(\n    name = \"Parameter\", nameLocation = \"center\",\n    nameGap = 35, type = \"category\"\n  ) %&gt;%\n  e_tooltip()\n\n\n\n\nall_corrs %&gt;%\n  mutate(sd = (conf.high - conf.low) / 2) %&gt;%\n  plot_ly() %&gt;%\n  add_bars(\n    y = ~estimate, x = ~predictor,\n    error_y = ~ list(array = sd, color = \"black\")\n  )\n\n\n\n\n\nInsight: We can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very slightly, in a negative way.\nThis kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n# For the father\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(father, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  e_lm(height ~ father, legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"father\", nameLocation = \"middle\", nameGap = 35,\n    min = 60, max = 80\n  ) %&gt;%\n  e_y_axis(\n    name = \"height\", nameLocation = \"middle\", nameGap = 35,\n    min = 50, max = 80\n  ) %&gt;%\n  e_tooltip()\n# for the mother\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(mother, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  e_lm(height ~ mother, legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"mother\", nameLocation = \"middle\", nameGap = 35,\n    min = 55, max = 75\n  ) %&gt;%\n  e_y_axis(\n    name = \"height\", nameLocation = \"middle\", nameGap = 35,\n    min = 50, max = 80\n  ) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Visibly the scatter plots are slightly tilted upward to the right, showing a positive correlation for both sons’ and daughters’ heights with that of the father and mother.\n\n\n\n\n\n\n\n\nNoteGalton’s Plot\n\n\n\nAn approximation to Galton’s famous plot (see Wikipedia):\n\ngf_point(height ~ (father + mother) / 2, data = Galton) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_density_2d(n = 8) %&gt;%\n  gf_abline(slope = 1) %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\nInsight: How would you interpret this plot1? As yet we are not able to reproduce this with charts4r."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We will “live code” this in class!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#conclusion",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "",
    "text": "We have a decent Correlations related workflow in R:\n- load the dataset\n- inspect the dataset, identify Quant and Qual variables\n- Develop Pair-Wise plots + Correlations using GGally::ggpairs()\n- Develop Correlogram corrplot::corrplot\n- Check everything with a cor_test\n- Use purrr + cor.test to plot correlations and confidence intervals for multiple Quant variables\n- Plot scatter plots using gf_point.\n- Add extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations-interactive.html#footnotes",
    "title": "EDA: Interactive Correlation Graphs in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.researchgate.net/figure/Galtons-smoothed-correlation-diagram-for-the-data-on-heights-of-parents-and-children_fig15_226400313↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html",
    "title": "\n Ratings and Rankings",
    "section": "",
    "text": "“I have no respect for people who deliberately try to be weird to attract attention, but if that’s who you honestly are, you shouldn’t try to”normalize” yourself.”\n— Alicia Witt, actress, singer-songwriter, and pianist (b. 21 Aug 1975)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#inspiration",
    "title": "\n Ratings and Rankings",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n(a) Energy Sources in the USA in 2024\n\n\n\n\n\n\n\n(b) 5 tools Players in Baseball\n\n\n\n\nFigure 1: Dumbbell and Radar Charts for Ranking\n\n\nWhat do we see here? From https://www.visualcapitalist.com/sp/americas-cheapest-sources-of-electricity-in-2024/ :\nFrom Figure 1 (a):\n\n\nOnshore wind power effectively costs USD0 per megawatt-hour (MWh) when subsidies are included!\n\nDemand for storage solutions is rising quickly. If storage is included, the minimum cost for onshore wind increases to $8 per MWh.\n\nSolar photovoltaics (PV) have similarly attractive economics. With subsidies, the minimum cost is USD6 per MWh. When including storage, USD38 per MWh. Notably, the maximum cost of solar PV with storage has significantly increased from USD102 in 2023 to USD 210 in 2024.\n\nFor gas-combined cycle plants, which combine natural gas and steam turbines for efficient electricity generation, the maximum price has climbed $7 year-over-year to $108 per MWh.\n\n\nAnd from From Figure 1 (b)?\n\nThere is a clear difference in the capabilities of the three players compared, though all of them are classified as “5 tools” players.\n\nEach player is better than the others at one unique skill: Betts at Throwing, Judge at Hit_power, and Trout at Hit_avg.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#setting-up-r-packages",
    "title": "\n Ratings and Rankings",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # includes ggplot for plotting\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(RColorBrewer) # colour palettes\n\nlibrary(ggbump) # Bump Charts\nlibrary(ggiraphExtra) # Radar, Spine, Donut and Donut-Pie combo charts !!\nlibrary(ggalt) # New geometries, coordinate systems, statistical transformations, scales and fonts\n\n# install.packages(\"devtools\")\n# devtools::install_github(\"ricardo-bion/ggradar\")\nlibrary(ggradar) # Radar Plots\n\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#what-graphs-are-we-going-to-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#what-graphs-are-we-going-to-see-today",
    "title": "\n Ratings and Rankings",
    "section": "\n What graphs are we going to see today?",
    "text": "What graphs are we going to see today?\nWhen we wish to compare the size of things and rank them, there are quite a few ways to do it.\nBar Charts and Lollipop Charts are immediately obvious when we wish to rank things on one aspect or parameter, e.g. mean income vs education. We can also put two lollipop charts back-to-back to make a Dumbbell Chart to show comparisons/ranks across two datasets based on one aspect, e.g change in mean income over two years, across gender.\nWhen we wish to rank the multiple objects against multiple aspects or parameters, then we can use Bump Charts and Radar Charts, e.g performance of one or more products against multiple criteria (cost, size, performance…)s.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#lollipop-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#lollipop-charts",
    "title": "\n Ratings and Rankings",
    "section": "\n Lollipop Charts",
    "text": "Lollipop Charts\nLet’s make a toy dataset of Products and Ratings:\n\n# Sample data set\nset.seed(1)\ndf1 &lt;- tibble(\n  product = LETTERS[1:10],\n  rank = sample(20:35, 10, replace = TRUE)\n)\ndf1\n\n\n  \n\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nUsing ggalt\n\n\n\n\n\n\n###\ngf_segment(0 + rank ~ product + product, data = df1) %&gt;%\n  # A formula with shape y + yend ~ x + xend.\n\n  gf_point(rank ~ product,\n    colour = ~product,\n    size = 5,\n    ylab = \"Rank\",\n    xlab = \"Product\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngf_segment(\n  0 + rank ~ fct_reorder(product, -rank) +\n    fct_reorder(product, -rank),\n  data = df1\n) %&gt;%\n  # A formula with shape y + yend ~ x + xend.\n\n  gf_point(rank ~ product, colour = ~product, size = 5) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_labs(x = \"Product\", y = \"Rank\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe have flipped the chart horizontally and reordered the \\(x\\) categories in order of decreasing ( or increasing ) \\(y\\), using forcats::fct_reorder.\n\n\nggplot(df1) +\n  geom_segment(aes(\n    y = 0, yend = rank,\n    x = product,\n    xend = product\n  )) +\n  geom_point(aes(y = rank, x = product, colour = product), size = 5) +\n  labs(title = \"Product Ratings\", x = \"Product\", y = \"Rank\")\n###\nggplot(df1) +\n  geom_segment(aes(\n    y = 0, yend = rank,\n    x = fct_reorder(product, -rank),\n    xend = fct_reorder(product, -rank)\n  )) +\n  geom_point(aes(x = product, y = rank, colour = product), size = 5) +\n  labs(title = \"Product Ratings\", x = \"Product\", y = \"Rank\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n\n\nYes, R has ( nearly) everything, including a geom_lollipop command: Here!\n\n\n\nggplot(df1) +\n  geom_lollipop(aes(x = rank, y = product),\n    point.size = 3, horizontal = F\n  ) +\n  labs(title = \"What is this BS chart?\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(df1) +\n  geom_lollipop(aes(y = rank, x = product),\n    point.size = 3, horizontal = T\n  ) +\n  labs(title = \"This also looks like BS\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(df1) +\n  geom_lollipop(aes(y = rank, x = product),\n    point.size = 3, , horizontal = F\n  ) +\n  labs(\n    title = \"Yeah, but I want this horizontal...\",\n    subtitle = \"And with colour and sorted and...\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(df1) +\n  geom_lollipop(\n    aes(\n      x = rank,\n      y = reorder(product, rank),\n      colour = product\n    ),\n    stroke = 2,\n    point.size = 3, horizontal = T\n  ) +\n  labs(\n    title = \"Now you're talking\",\n    x = \"Rank\", y = \"Product\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Lollipop Plots\n\n\n\n\nVery simple chart, almost like a bar chart\nDifferences between the same set of data across one aspect (i.e. rank) is very quickly apparent\n\nOrdering the dataset by the attribute (i.e ordering product by rank) makes the message very clear.\nEven a large number of data can safely be visualized and understood",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#dumbbell-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#dumbbell-charts",
    "title": "\n Ratings and Rankings",
    "section": "\n Dumbbell Charts",
    "text": "Dumbbell Charts\nA lollipop chart compares a set of data against one aspect. What if we have more than one? Say sales in many product lines across two years?\nLet us once again construct a very similar looking toy dataset, but with two columns for ratings, one for each of two years:\n\n# Sample data set\n# Wide Format data!\nset.seed(2)\ndf2 &lt;- tibble(\n  product = LETTERS[1:10],\n  rank_year1 = sample(20:35, 10, replace = TRUE),\n  rank_year2 = sample(15:45, 10, replace = TRUE)\n)\ndf2\n\n\n  \n\n\n\n\nA short diversion: we can also make this data into long form: this will become useful very shortly!\n\n\n\n\n\n\nNote Wide Form and Long Form Data\n\n\n\nLook at the data: this is wide form data. The columns pertaining to each of the Product-Features would normally be stacked into two columns, one with the Feature and the other with the score. Note the trio: Qual(product) + Qual(year) + Quant(scores):\n\n# With Long Format Data\ndf2_long &lt;- df2 %&gt;%\n  pivot_longer(\n    cols = c(dplyr::starts_with(\"rank\")),\n    names_to = \"year\", values_to = \"scores\"\n  )\ndf2_long\n\n\n  \n\n\n\nA cool visualization of this operation was created by Garrick Aden-Buie:\n\n\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nUsing ggalt\nComparison barchart\n\n\n\n\n\n\n## With Wide Form Data\n##\ndf2 %&gt;%\n  gf_segment(product + product ~ rank_year1 + rank_year2,\n    size = 3, color = \"#e3e2e1\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\"),\n      ends = \"last\", type = \"open\"\n    )\n  ) %&gt;%\n  gf_point(product ~ rank_year1,\n    size = 3,\n    colour = \"#123456\"\n  ) %&gt;%\n  gf_point(product ~ rank_year2,\n    size = 3,\n    colour = \"#bad744\"\n  ) %&gt;%\n  gf_labs(x = \"Rank\", y = \"Product\", title = \"Product Ranks in Year1 and Year2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Rearranging `product` in order of rank_year2\ndf2 %&gt;%\n  gf_segment(\n    reorder(product, rank_year2) +\n      reorder(product, rank_year2) ~\n      rank_year1 + rank_year2,\n    size = 3, color = \"#e3e2e1\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\")\n    )\n  ) %&gt;%\n  gf_point(product ~ rank_year1,\n    size = 3,\n    colour = \"#123456\"\n  ) %&gt;%\n  gf_point(product ~ rank_year2,\n    size = 3,\n    colour = \"#bad744\"\n  ) %&gt;%\n  gf_labs(\n    x = \"Rank\", y = \"Product\",\n    title = \"In Decreasing order of Year2 Rank\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## With Wide Format Data\nggplot(df2, aes(y = product, yend = product, x = rank_year1, xend = rank_year2)) +\n  geom_segment(\n    size = 3, color = \"#e3e2e1\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\")\n    )\n  ) +\n  geom_point(aes(rank_year1, product),\n    colour = \"#5b8124\", size = 3\n  ) +\n  geom_point(aes(rank_year2, product),\n    colour = \"#bad744\", size = 3\n  ) +\n  labs(x = \"Rank\", y = \"Product\")\n\n\n\n\n\n\n## Rearranging `product` in order of rank_year2\nggplot(df2, aes(y = reorder(product, rank_year2), yend = reorder(product, rank_year2), x = rank_year1, xend = rank_year2)) +\n  geom_segment(\n    size = 3, color = \"#e3e2e1\",\n    arrow = arrow(\n      angle = 30,\n      length = unit(0.25, \"inches\")\n    )\n  ) +\n  geom_point(aes(rank_year1, product),\n    colour = \"#5b8124\", size = 3\n  ) +\n  geom_point(aes(rank_year2, product),\n    colour = \"#bad744\", size = 3\n  ) +\n  labs(\n    x = \"Rank\", y = \"Product\",\n    title = \"In Decreasing order of Year2 Rank\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\ndf2 %&gt;% ggplot() +\n  geom_dumbbell(\n    aes(\n      y = reorder(product, rank_year2),\n      x = rank_year1,\n      xend = rank_year2\n    ),\n    size = 3, color = \"#e3e2e1\",\n    colour_x = \"#5b8124\",\n    colour_xend = \"#bad744\",\n    dot_guide = TRUE, # Try FALSE\n    dot_guide_size = 0.25\n  ) +\n  labs(\n    x = NULL, y = NULL,\n    title = \"ggplot2 geom_dumbbell with dot guide\",\n    subtitle = \"Products in Decreasing order of Year2 Rank\",\n    caption = \"Made with ggalt\"\n  ) +\n  theme(panel.grid.major.x = element_line(size = 0.05)) +\n  theme(panel.grid.major.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf2_long %&gt;%\n  gf_col(product ~ scores,\n    group = ~year,\n    fill = ~year, position = \"dodge\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Dumbbell Plots\n\n\n\n\nDumbbell Plots are clearly they are more intuitive and clear than the bar chart\nDifferences between the same set of data at two different aspects is very quickly apparent\n\nDifferences in differences(DID) are also quite easily apparent. Experiments do use these metrics and these plots would be very useful there.\n\nggalt works nicely with additional visible guides rendered in the chart\n\n\n\n\n\n\n\n Bump Charts\nBump Charts track the ranking of several objects based on other parameters, such as time/month or even category. For instance, what is the opinion score of a set of products across various categories of users?\n\nyear &lt;- rep(2019:2021, 4)\nposition &lt;- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nproduct &lt;- c(\n  \"A\", \"A\", \"A\",\n  \"B\", \"B\", \"B\",\n  \"C\", \"C\", \"C\",\n  \"D\", \"D\", \"D\"\n)\n\ndf3 &lt;- tibble(year, position, product)\n\ndf3\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteggbump uses ggplot syntax\n\n\n\nWe need to use a new package called, what else, ggbump to create our Bump Charts: Here again we do not yet have a ggformula equivalent. ( Though it may be possible with a combination of gf_point and gf_polygon, and pre-computing the coordinates. Seems long-winded.)\nNote the + syntax with ggplot code!!\n\n\n\n\n\n\ndf3 %&gt;%\n  ggplot() +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n    size = 6\n  ) +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  scale_color_brewer(palette = \"RdBu\") + # Change Colour Scale\n  scale_x_discrete(limits = c(2019:2021)) # Check warning here...\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add labels along the “bump lines” and remove the legend altogether:\n\n\n\nggplot(df3) +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n    size = 6\n  ) +\n  scale_color_brewer(palette = \"RdBu\") + # Change Colour Scale\n  # Same as before up to here\n  # Add the labels at start and finish\n\n  geom_text(\n    data = df3 %&gt;% filter(year == min(year)),\n    aes(\n      x = year - 0.1, label = product,\n      y = position\n    ),\n    size = 5, hjust = 1\n  ) +\n  geom_text(\n    data = df3 %&gt;% filter(year == max(year)),\n    aes(\n      x = year + 0.1, label = product,\n      y = position\n    ),\n    size = 5, hjust = 0\n  ) +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  scale_x_discrete(limits = c(2019:2021)) +\n  theme(legend.position = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Bump Charts\n\n\n\n\nBump charts are good for depicting Ranks/Scores pertaining to a set of data, as they vary over another aspect, for a set of products\nCannot have too many levels in the aspect parameter, else the graph gets too hard to make sense with.\nFor instance if we had 10 years in the data above, we would have lost the plot, literally! Perhaps better to use a Sankey in that case!!\n\n\n\n\n\n Radar Charts\nWhat if your marketing folks had rated some products along several different desirable criteria? Such data, where a certain set of items (Qualitative!!) are rated (Quantitative!) against another set (Qualitative again!!) can be plotted on a roughly circular set of axes, with the radial distance defining the rank against each axes. Such a plot is called a radar plot.\nOf course, we will use the aptly named ggradar, which is at this time (Feb 2023) a development version and not yet part of CRAN. We will still try it, and another package ggiraphExtra which IS a part of CRAN (and has some other capabilities too, which are worth exploring!)\nLet us generate some toy data first:\n\nset.seed(4)\ndf4 &lt;- tibble(\n  Product = c(\"G1\", \"G2\", \"G3\"),\n  Power = runif(3),\n  Cost = runif(3),\n  Harmony = runif(3),\n  Style = runif(3),\n  Size = runif(3),\n  Manufacturability = runif(3),\n  Durability = runif(3),\n  Universality = runif(3)\n)\ndf4\n\n\n  \n\n\n\nAnd now plot it with both packages.\n\n\n\n Using ggradar\n Using ggiraphExtra\n\n\n\n\n\n\nggradar::ggradar(\n  plot.data = df4,\n  axis.label.size = 3, # Titles of Params\n  grid.label.size = 4, # Score Values/Circles\n  group.point.size = 3, # Product Points Sizes\n  group.line.width = 1, # Product Line Widths\n  group.colours = c(\"#123456\", \"#fad744\", \"#03e2e1\"), # Product Colours\n  fill = TRUE, # fill the radar polygons\n  fill.alpha = 0.3, # Not too dark, Arvind\n  legend.title = \"Product\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the ggiraphExtra website:\n\nPackage ggiraphExtra contains many useful functions for exploratory plots. These functions are made by both ‘ggplot2’ and ‘ggiraph’ packages. You can make a static ggplot or an interactive ggplot by setting the parameter interactive=TRUE.\n\n\n\n\nggiraphExtra::ggRadar(\n  data = df4,\n  aes(colour = Product),\n  interactive = FALSE, # try TRUE\n  rescale = FALSE,\n  title = \"Using ggiraphExtra\"\n) + # recale = TRUE makes it look different...try!!\n  theme_minimal(base_family = \"Roboto Condensed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Radar Plots\n\n\n\n\nDifferences in scores for a given item across several aspect or parameters are readily apparent.\nThese can also be compared, parameter for parameter, with more than one item\nthe same set of data at two different aspects is very quickly apparent\nData is clearly in wide form\nBoth ggradar and ggiraphExtra render very similar-looking radar charts and the syntax is not too intimidating!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#bump-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#bump-charts",
    "title": "\n Ratings and Rankings",
    "section": "\n Bump Charts",
    "text": "Bump Charts\nBump Charts track the ranking of several objects based on other parameters, such as time/month or even category. For instance, what is the opinion score of a set of products across various categories of users?\n\nyear &lt;- rep(2019:2021, 4)\nposition &lt;- c(4, 2, 2, 3, 1, 4, 2, 3, 1, 1, 4, 3)\nproduct &lt;- c(\n  \"A\", \"A\", \"A\",\n  \"B\", \"B\", \"B\",\n  \"C\", \"C\", \"C\",\n  \"D\", \"D\", \"D\"\n)\n\ndf3 &lt;- tibble(year, position, product)\n\ndf3\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteggbump uses ggplot syntax\n\n\n\nWe need to use a new package called, what else, ggbump to create our Bump Charts: Here again we do not yet have a ggformula equivalent. ( Though it may be possible with a combination of gf_point and gf_polygon, and pre-computing the coordinates. Seems long-winded.)\nNote the + syntax with ggplot code!!\n\n\n\n\n\n\ndf3 %&gt;%\n  ggplot() +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n    size = 6\n  ) +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  scale_color_brewer(palette = \"RdBu\") + # Change Colour Scale\n  scale_x_discrete(limits = c(2019:2021)) # Check warning here...\n\n\n\n\n\n\n\n\n\n\n\n\nWe can add labels along the “bump lines” and remove the legend altogether:\n\n\n\nggplot(df3) +\n  geom_bump(aes(x = year, y = position, color = product)) +\n  geom_point(aes(x = year, y = position, color = product),\n    size = 6\n  ) +\n  scale_color_brewer(palette = \"RdBu\") + # Change Colour Scale\n  # Same as before up to here\n  # Add the labels at start and finish\n\n  geom_text(\n    data = df3 %&gt;% filter(year == min(year)),\n    aes(\n      x = year - 0.1, label = product,\n      y = position\n    ),\n    size = 5, hjust = 1\n  ) +\n  geom_text(\n    data = df3 %&gt;% filter(year == max(year)),\n    aes(\n      x = year + 0.1, label = product,\n      y = position\n    ),\n    size = 5, hjust = 0\n  ) +\n  xlab(\"Year\") +\n  ylab(\"Rank\") +\n  scale_x_discrete(limits = c(2019:2021)) +\n  theme(legend.position = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Bump Charts\n\n\n\n\nBump charts are good for depicting Ranks/Scores pertaining to a set of data, as they vary over another aspect, for a set of products\nCannot have too many levels in the aspect parameter, else the graph gets too hard to make sense with.\nFor instance if we had 10 years in the data above, we would have lost the plot, literally! Perhaps better to use a Sankey in that case!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#radar-charts",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#radar-charts",
    "title": "\n Ratings and Rankings",
    "section": "\n Radar Charts",
    "text": "Radar Charts\nWhat if your marketing folks had rated some products along several different desirable criteria? Such data, where a certain set of items (Qualitative!!) are rated (Quantitative!) against another set (Qualitative again!!) can be plotted on a roughly circular set of axes, with the radial distance defining the rank against each axes. Such a plot is called a radar plot.\nOf course, we will use the aptly named ggradar, which is at this time (Feb 2023) a development version and not yet part of CRAN. We will still try it, and another package ggiraphExtra which IS a part of CRAN (and has some other capabilities too, which are worth exploring!)\nLet us generate some toy data first:\n\nset.seed(4)\ndf4 &lt;- tibble(\n  Product = c(\"G1\", \"G2\", \"G3\"),\n  Power = runif(3),\n  Cost = runif(3),\n  Harmony = runif(3),\n  Style = runif(3),\n  Size = runif(3),\n  Manufacturability = runif(3),\n  Durability = runif(3),\n  Universality = runif(3)\n)\ndf4\n\n\n  \n\n\n\nAnd now plot it with both packages.\n\n\n\n Using ggradar\n Using ggiraphExtra\n\n\n\n\n\n\nggradar::ggradar(\n  plot.data = df4,\n  axis.label.size = 3, # Titles of Params\n  grid.label.size = 4, # Score Values/Circles\n  group.point.size = 3, # Product Points Sizes\n  group.line.width = 1, # Product Line Widths\n  group.colours = c(\"#123456\", \"#fad744\", \"#03e2e1\"), # Product Colours\n  fill = TRUE, # fill the radar polygons\n  fill.alpha = 0.3, # Not too dark, Arvind\n  legend.title = \"Product\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the ggiraphExtra website:\n\nPackage ggiraphExtra contains many useful functions for exploratory plots. These functions are made by both ‘ggplot2’ and ‘ggiraph’ packages. You can make a static ggplot or an interactive ggplot by setting the parameter interactive=TRUE.\n\n\n\n\nggiraphExtra::ggRadar(\n  data = df4,\n  aes(colour = Product),\n  interactive = FALSE, # try TRUE\n  rescale = FALSE,\n  title = \"Using ggiraphExtra\"\n) + # recale = TRUE makes it look different...try!!\n  theme_minimal(base_family = \"Roboto Condensed\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Radar Plots\n\n\n\n\nDifferences in scores for a given item across several aspect or parameters are readily apparent.\nThese can also be compared, parameter for parameter, with more than one item\nthe same set of data at two different aspects is very quickly apparent\nData is clearly in wide form\nBoth ggradar and ggiraphExtra render very similar-looking radar charts and the syntax is not too intimidating!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#wait-but-why",
    "title": "\n Ratings and Rankings",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nBump Charts can show changes in Rating and Ranking over time, or some other Qual variable too!\nLollipop Charts are useful in comparing multiple say products or services, with only one aspect for comparison, or which defines the rank\nRadar Charts are also useful in comparing multiple say products or services, but against several aspects or parameters for simultaneous comparisons.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#conclusion",
    "title": "\n Ratings and Rankings",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nThese are easy and simple charts to use and are easily understood too\nBear in mind the data structure requirements for different charts/packages: Wide vs Long.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#your-turn",
    "title": "\n Ratings and Rankings",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTake the HELPrct dataset from our well used mosaicData package. Plot ranking charts using each of the public health issues that you can see in that dataset. What choice will you make for the the axes?\nTry the SaratogaHouses dataset also from mosaicData.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/80-Ranking/index.html#references",
    "title": "\n Ratings and Rankings",
    "section": "\n References",
    "text": "References\n\nHighcharts Blog. Why you need to start using dumbbell chartshttps://github.com/hrbrmstr/ggalt#lollipop-charts\n\nSee this use of Radar Charts in Education. Choose the country/countries of choice and plot their ranks on various educational parameters in a radar chart. https://gpseducation.oecd.org/Home\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggalt\n0.4.0\nRudis, Bolker, and Schulz (2017)\n\n\nggbump\n0.1.0\nSjoberg (2020)\n\n\nggiraphExtra\n0.3.0\nMoon (2020)\n\n\nggradar\n0.2\nBion (2025)\n\n\n\n\n\n\nBion, Ricardo. 2025. ggradar: Create Radar Charts Using Ggplot2. https://github.com/ricardo-bion/ggradar.\n\n\nMoon, Keon-Woong. 2020. ggiraphExtra: Make Interactive “ggplot2.” Extension to “ggplot2” and “ggiraph”. https://doi.org/10.32614/CRAN.package.ggiraphExtra.\n\n\nRudis, Bob, Ben Bolker, and Jan Schulz. 2017. ggalt: Extra Coordinate Systems, “Geoms,” Statistical Transformations, Scales and Fonts for “ggplot2”. https://doi.org/10.32614/CRAN.package.ggalt.\n\n\nSjoberg, David. 2020. ggbump: Bump Chart and Sigmoid Curves. https://doi.org/10.32614/CRAN.package.ggbump.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"ph:ranking-bold\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ratings and Rankings"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html",
    "title": "🕔 Time Series Wrangling",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure (like in an old-fashioned library), with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original! If you have messed up the code there, then you can hit the “recycle” button on the web-r tab to go back to the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#using-web-r",
    "title": "🕔 Time Series Wrangling",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure (like in an old-fashioned library), with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original! If you have messed up the code there, then you can hit the “recycle” button on the web-r tab to go back to the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#setting-up-r-packages",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nknitr::opts_chunk$set(tidy = \"styler\")\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(ggformula) # Our Formula based graphing package\nlibrary(scales) # Some nice time-oriented scales in graphs!\nlibrary(tsibble)\nlibrary(timetk)\n\n# Datasets\nlibrary(tsibbledata)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#introduction",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Introduction",
    "text": "Introduction\nWe have now arrived at the need to start from raw, multiple time series data and filter, group, and summarize these time series grasp their meaning, a process known as “wrangling”.\n\n\n\n\n\n\nNoteWrangling with dplyr\n\n\n\nThe tutorial for wrangling using dplyr is here.\n\n\nHere, we will first use the births data we encountered earlier which had a single time series, and then proceed to a more complex example which has multiple time-series."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#time-series-wrangling",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#time-series-wrangling",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Time-Series Wrangling",
    "text": "Time-Series Wrangling\nWe can do this in two ways, and with two packages:\n\n\n\n\n\n\nNoteTwo Wrangling “Dimensions”\n\n\n\nFor all the above operations, we can either use time variable as the basis, by filtering for specific periods, or computing summaries over larger intervals of time e.g. month, quarter, year;\nAND/OR\nWe can do the same over space variables, i.e. the Qualitative variables that define individual time series, and based on which we can filter and and analyze these specific time series. Each unique setting of these Qualitative variables could potentially define a time series! There are 336 groups/combinations of them in PBS, but not all are unique time series, since some of the Qual variables are nested inside others, e.g ATC1_desc provides more info on each value of ATC1 and is not truly a separate Qual variable.\n\n\nAnd the packages are:\n\n\n\n\n\n\nTiptsibble has dplyr-like functions\n\n\n\nUsing tsibble data, the tsibble package has specialized filter and group_by functions to do with the index (i.e time) variable and the key variables, such as index_by() and group_by_key().\n(Filtering based on Qual variables can be done with dplyr. We can use dplyr functions such as group_by, mutate(), filter(), select() and summarise() to work with tsibble objects.)\n\n\n\n\n\n\n\n\nTiptimetk also has dplyr-like functions!\n\n\n\nUsing tibbles, timetk provides functions such as summarize_by_time, filter_by_time and slidify that are quite powerful. Again, as with tsibble, dplyr can always be used for other Qual variables (i.e non-time)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-1-births-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-1-births-dataset",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Case Study #1: Births Dataset",
    "text": "Case Study #1: Births Dataset\nAs a second example let us read and inspect in the now familiar US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project.\n Download the US Births data \n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n R\n web-r\n\n\n\n\n# Step1: Read the data\nbirths_2000_2014 &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/births/US_births_2000-2014_SSA.csv\")\n\nLet us make a date column out of the individual year/month/day columns:\n\n\n\n# Step2: Convert year + month + date_of_month to \"date\"\nbirths_timeseries &lt;-\n  births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  select(date, births)\n\nbirths_timeseries\nclass(births_timeseries)\n\nNote that this is still just a tibble, with a time-formatted column. Next let us create a full-blown tsibble with the same data:\n\n\n\n\n  \n\n\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\n\n\n\n# Step3: Convert to tsibble\n# combine the year/month/date_of_month columns into a date\n# drop them thereafter\nbirths_tsibble &lt;-\n  births_2000_2014 %&gt;%\n  mutate(index = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  tsibble::as_tsibble(index = index) %&gt;%\n  select(index, births)\n\nbirths_tsibble\nclass(births_tsibble)\n\nBoth data frames look identical, except for data class difference. This is DAILY data of course.\n\n\n\n\n  \n\n\n\n[1] \"tbl_ts\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n\n\nWe will (sadly) need both formats; the tsibble packages needs, well, tsibble-formats, and timetk cannot, it seems, handle tsibble-formats and needs regular tibbles. Sigh.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#basic-time-series-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#basic-time-series-plot",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Basic Time Series Plot",
    "text": "Basic Time Series Plot\nLet us plot the timeseries using the tsibble data, with both ggformula and timetk:\n\n\n R\n web-r\n\n\n\nLet us try a basic plot with both tsibble vs timetk packages.\n# column: body-outset-right\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nbirths_tsibble %&gt;%\n  gf_line(births ~ index,\n    data = .,\n    title = \"Basic tsibble plotted with ggformula\"\n  )\n# timetk **can** plot tsibbles.\nbirths_tsibble %&gt;%\n  timetk::plot_time_series(\n    .date_var = index,\n    .value = births, .interactive = FALSE,\n    .title = \"Tsibble Plotted with timetk\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#aggregation-and-averaging",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#aggregation-and-averaging",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Aggregation and Averaging",
    "text": "Aggregation and Averaging\nLet us plot the time series using the tsibble data, with both ggformula and timetk, this time grouping by month and get monthly aggregates to get a summary:\n\n\n R\n web-r\n\n\n\nHere we plot Monthly Aggregates with both ggformula and timetk:\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nbirths_tsibble %&gt;%\n  tsibble::index_by(month_index = ~ tsibble::yearmonth(.)) %&gt;%\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  gf_point(mean_births ~ month_index,\n    data = .,\n    title = \"Monthly Aggregate with tsibble + ggformula\"\n  ) %&gt;%\n  gf_line() %&gt;%\n  gf_smooth(se = FALSE, method = \"loess\") %&gt;%\n  gf_labs(x = \"Year\", y = \"Mean Monthly Births\")\n\n##\n##\n##\n##\nbirths_timeseries %&gt;%\n  # cannot use tsibble here\n  # tsibble format cannot be summarized/wrangled by timetk\n\n  timetk::summarize_by_time(\n    .date_var = date,\n    .by = \"month\",\n    month_mean = mean(births)\n  ) %&gt;%\n  timetk::plot_time_series(date, month_mean,\n    .title = \"Monthly aggregate births with timetk\",\n    .interactive = FALSE,\n    .x_lab = \"year\",\n    .y_lab = \"Mean Monthly Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nApart from the bump during in 2006-2007, there are also seasonal trends that repeat each year, which we glimpsed earlier. We will analyse seasonal trends in another module.\nLet us try getting annual aggregates.\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  tsibble::index_by(year_index = ~ lubridate::year(.)) %&gt;%\n  ## tsibble does not have a \"year\" function? So using lubridate..\n  ## Summarize\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  ## Plot\n  gf_point(mean_births ~ year_index, data = .) %&gt;%\n  gf_line() %&gt;%\n  gf_smooth(se = FALSE, method = \"loess\")\n##\n##\n##\n##\n##\nbirths_timeseries %&gt;%\n  ## Summarize\n  timetk::summarise_by_time(\n    .date_var = date,\n    .by = \"year\",\n    mean = mean(births)\n  ) %&gt;%\n  ## Plot\n  timetk::plot_time_series(date, mean,\n    .title = \"Yearly aggregate births with timetk\",\n    .interactive = FALSE,\n    .x_lab = \"year\",\n    .y_lab = \"Mean Yearly Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n A small detour\nAh yes….errors. There is a curious interplay between dplyr and tsibble…they play together but not all the time, it would seem.\nThe original births tibble dataset allows dplyr:group_by + summarize:\n\n# The original dataset allows dplyr:group_by + summarize\nbirths_2000_2014 %&gt;%\n  dplyr::group_by(year) %&gt;%\n  summarise(mean_births = mean(births, na.rm = TRUE))\n\n\n  \n\n\n\nHowever, tsibble-converted data does not quite work with dplyr::group_by+summarize:\n\n```{r}\n#| label: Errors-2\n#| eval: false\n\n# This code will not work\nbirths_tsibble %&gt;%\n  # Grouping does not work. Here is the problem\n  dplyr::group_by(index) %&gt;%\n  # Trying to get Annual Birth Average as before\n  # Should give 15 rows, one per year, but does not!\n  summarise(mean_births = mean(births, na.rm = TRUE))\n```\n\nEven if we pull out the year information in index, it gives confusing results…\n\nbirths_tsibble %&gt;%\n  # All right, try to pull the year info from `index` then\n  mutate(dplyr_year = lubridate::year(index)) %&gt;%\n  # Grouping does not work\n  dplyr::group_by(dplyr_year) %&gt;%\n  # Trying to get Annual Birth Average as before\n  # Should give 15 rows, one per year, but does not!\n  summarise(mean_births = mean(births, na.rm = TRUE))\n\n\n  \n\n\n\nThis grouping does not give a proper result (though it does show 15 groups.)\nUsing tsibble::index_by() and then dplyr::summarize() does the trick…so all right. The index_by() operation is different from that of dplyr::group_by()!\n\n# tsibble works with index_by + summarize\n# 15 rows, one for each year\nbirths_tsibble %&gt;%\n  # tsibble can get year info from index\n  tsibble::index_by(year_date = year(index)) %&gt;%\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#candle-stick-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#candle-stick-plots",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Candle-Stick Plots",
    "text": "Candle-Stick Plots\nHmm…can we try to plot boxplots over time (Candle-Stick Plots)? Over month, quarter or year?\n\n Monthly Box Plots\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  index_by(month_index = ~ yearmonth(.)) %&gt;%\n  # 15 years\n  # No need to summarise, since we want boxplots per year / month\n  # Plot the groups\n  # 180 plots!!\n  gf_boxplot(births ~ index,\n    group = ~month_index,\n    fill = ~month_index,\n    data = .,\n    title = \"Boxplots of Births by Month\",\n    caption = \"tsibble + ggformula\"\n  )\n\n\n####\n####\n####\n####\nbirths_tsibble %&gt;% # Can try births_timeseries too\n  timetk::plot_time_series_boxplot(\n    index, births,\n    .period = \"month\",\n    .plotly_slider = TRUE,\n    .title = \"Boxplots of Births by Month\",\n    .interactive = TRUE,\n    .x_lab = \"year\",\n    .y_lab = \"Mean Monthly Births\"\n  )\n\ntimetk can take tsibble-format data to plot with, but cannot perform aggregation: summarize_by_time() will throw an error!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe see 180 boxplots…yes this is still too busy a plot for us to learn much from."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#quarterly-boxplots",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#quarterly-boxplots",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Quarterly boxplots",
    "text": "Quarterly boxplots\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  index_by(qrtr_index = ~ yearquarter(.)) %&gt;% # 60 quarters over 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ index,\n    group = ~qrtr_index,\n    fill = ~qrtr_index,\n    data = .\n  ) # 60 plots!!\n###\n###\n###\n###\n###\n\nbirths_tsibble %&gt;% # Can try births_timeseries too\n  timetk::plot_time_series_boxplot(\n    index, births,\n    .period = \"quarter\",\n    .title = \"Quarterly births with timetk\",\n    .interactive = TRUE,\n    .plotly_slider = TRUE,\n    .x_lab = \"year\",\n    .y_lab = \"Mean Quarterly Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe have 60 boxplots…over a period of 15 years, one box plot per quarter…\n\n Yearwise boxplots\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tsibble %&gt;%\n  index_by(year_index = ~ lubridate::year(.)) %&gt;% # 15 years, 15 groups\n  # No need to summarise, since we want boxplots per year / month\n\n  gf_boxplot(births ~ index,\n    group = ~year_index,\n    fill = ~year_index,\n    data = .\n  ) %&gt;% # plot the groups 15 plots\n  gf_theme(scale_fill_distiller(palette = \"Spectral\"))\n####\n####\n####\n####\n####\n####\n\nbirths_tsibble %&gt;%\n  timetk::plot_time_series_boxplot(\n    index, births,\n    .period = \"year\",\n    .title = \"Yearly aggregate births with timetk\",\n    .interactive = TRUE,\n    .plotly_slider = TRUE,\n    .x_lab = \"year\",\n    .y_lab = \"Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThis looks much better…We can more easily see that 2006-2009 the births were somewhat higher, because the medians in these years are the highest."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-2-pbs-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#case-study-2-pbs-dataset",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Case Study #2: PBS Dataset",
    "text": "Case Study #2: PBS Dataset\nWe previously encountered the PBS dataset from the tsibbledata package earlier, which is a dataset containing Monthly Medicare prescription data in Australia. We will resume from there:\n\n\n R\n web-r\n\n\n\n\ndata(\"PBS\", package = \"tsibbledata\")\nPBS\n\n\n  \n\n\nglimpse(PBS)\n\nRows: 67,596\nColumns: 9\nKey: Concession, Type, ATC1, ATC2 [336]\n$ Month      &lt;mth&gt; 1991 Jul, 1991 Aug, 1991 Sep, 1991 Oct, 1991 Nov, 1991 Dec,…\n$ Concession &lt;chr&gt; \"Concessional\", \"Concessional\", \"Concessional\", \"Concession…\n$ Type       &lt;chr&gt; \"Co-payments\", \"Co-payments\", \"Co-payments\", \"Co-payments\",…\n$ ATC1       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n$ ATC1_desc  &lt;chr&gt; \"Alimentary tract and metabolism\", \"Alimentary tract and me…\n$ ATC2       &lt;chr&gt; \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A0…\n$ ATC2_desc  &lt;chr&gt; \"STOMATOLOGICAL PREPARATIONS\", \"STOMATOLOGICAL PREPARATIONS…\n$ Scripts    &lt;dbl&gt; 18228, 15327, 14775, 15380, 14371, 15028, 11040, 15165, 168…\n$ Cost       &lt;dbl&gt; 67877.00, 57011.00, 55020.00, 57222.00, 52120.00, 54299.00,…\n\n# inspect(PBS) # does not work since mosaic cannot handle tsibbles\n# skimr::skim(PBS) # does not work, need to investigate\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Counts by Qual variables\nLet us first see how many observations there are for each combo of keys:\n\n\n R\n web-r\n\n\n\n\n\n\n## Types\nPBS %&gt;%\n  dplyr::count(Type) # 2 Types\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n## Concessions\nPBS %&gt;% count(Concession) # 2 Types\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n## ATC1\nPBS %&gt;% count(ATC1) # 15 ATC1 groups\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n## ATC2\nPBS %&gt;% count(ATC2) # 84 ATC2 groups\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# dplyr grouping with ATC1 and ATC2\nPBS %&gt;%\n  dplyr::group_by(ATC1, ATC2) %&gt;%\n  count() # Still 84; ATC2 is nested in ATC1\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n## All possible groups\nPBS %&gt;%\n  group_by(ATC1, ATC2, Concession, Type) %&gt;%\n  count() # 336 overall groups\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteData Dictionary for PBS\n\n\n\nThis is a large-ish dataset: (Run PBS in your console)\n\n67K observations\nQuant Variables: Two Quant variables (Scripts and Cost)\n\nTime Variable:\n\nData appears to be monthly, as indicated by the 1M.\nthe time index variable is called Month\n\nformatted as yearmonth, a new type of variable introduced in the tsibble package. yearmonth does not show in glimpse output!\n\n\n\nQual variables:\n\n\nConcession: Concessional and General (Concessional scripts are given to pensioners, unemployed, dependents, and other card holders)\n\nType: Co-payments and Safety Net\n\n\nATC1: Anatomical Therapeutic Chemical index (level 1).\n\n15 types\n\n\n\n\n\nATC2: Anatomical Therapeutic Chemical index (level 2).\n\n84 types, nested inside ATC1.\n\n\n\n\n\nWe will start with the familiar basic messy plot, and work our way towards filtering, summaries, and averages.\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  gf_point(Cost ~ Month, data = .) %&gt;%\n  gf_line(\n    title = \"PBS Costs vs time\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nAs noted earlier, this basic plot is quite messy. Other than an overall rising trend and more vigorous variations pointing to a multiplicative process, we cannot say more. There is simply too much happening here and it is now time (sic!) for us to look at summaries of the data using dplyr-like verbs. We will perform summaries with tsibble and plots with ggformula first. Then we will use timetk to perform both operations.\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# Costs variable for a specific combo of Qual variables(keys)\nPBS %&gt;%\n  dplyr::filter(\n    Concession == \"General\",\n    ATC1 == \"A\"\n  ) %&gt;%\n  gf_line(Cost ~ Month,\n    colour = ~Type,\n    data = .\n  ) %&gt;%\n  gf_point(title = \"Costs per Month for General A category patients\") %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteInsights\n\n\n\nAs can be seen:\n\nstrongly seasonal for both Types of graphs;\nseasonal variation increasing over the years, a clear sign of a multiplicative time series, especially for Safety net.\nUpward trend with both types of subsidies, Safety net and Co-payments.\n\nCo-payments type have some kind of dip around the year 2000…\nBut this is still messy and overwhelming and we could certainly use some summaries/aggregates/averages.\n\n\n\nWe can now use tsibble’s dplyr-like commands to develop summaries by year, quarter, month(original data): Look carefully at the new time variable created each time, and the size the data frame decrease with each aggregation:\n\n\n R\n web-r\n\n\n\n\n\n\n# Cost Summary by Month, which is the original data\n# New Variable Name to make grouping visible\nPBS_month &lt;- PBS %&gt;%\n  dplyr::filter(\n    Concession == \"General\",\n    ATC1 == \"A\"\n  ) %&gt;%\n  tsibble::index_by(Month_Date = Month) %&gt;%\n  dplyr::summarise(\n    across(\n      .cols = c(Cost, Scripts),\n      .fn = mean,\n      .names = \"mean_{.col}\"\n    )\n  )\n\nPBS_month\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\nPBS_month %&gt;%\n  mutate(Month_Date = as_date(Month_Date)) %&gt;%\n  gf_line(mean_Cost ~ Month_Date) %&gt;%\n  gf_line(mean_Scripts ~ Month_Date,\n    title = \"Mean Costs and Scripts for General + A category\",\n    subtitle = \"Means over General + A category \"\n  ) %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteInsights\n\n\n\nAs can be seen: To Be Written Up !!!\n\n\n\n\n R\n web-r\n\n\n\n\n\n\n# Cost Summary by Quarter\nPBS_quarter &lt;-\n  PBS %&gt;%\n  tsibble::index_by(Quarter_Date = yearquarter(Month)) %&gt;% # And the change here!\n  dplyr::summarise(across(\n    .cols = c(Cost, Scripts),\n    .fn = mean,\n    .names = \"mean_{.col}\"\n  ))\nPBS_quarter\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nPBS_quarter %&gt;%\n  gf_line(mean_Cost ~ Quarter_Date) %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteInsights\n\n\n\nAs can be seen: TBD\n\n\n\n\n R\n web-r\n\n\n\n\n\n\n# Cost Summary by Year\nPBS_year &lt;- PBS %&gt;%\n  index_by(Year_Date = year(Month)) %&gt;% # Note this change!!!\n  dplyr::summarise(across(\n    .cols = c(Cost, Scripts),\n    .fn = mean,\n    .names = \"mean_{.col}\"\n  ))\nPBS_year\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nPBS_year %&gt;%\n  gf_line(mean_Cost ~ Year_Date) %&gt;%\n  gf_refine(scale_y_continuous(labels = scales::label_comma()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteInsights\n\n\n\nAs can be seen: TBD. I must write this up soon!\n\n\nUsing timetk\n\n\n\n\n\n\n\nNoteThe time variable for timetk\n\n\n\nThe PBS-derived tsibbles have their “time-oriented” variables formatted asyearmonth,yearquarter and dbl, as seen. We need to mutate these into a proper date format for the timetk package to summarise them successfully. (Plotting a tsibble with timetk is possible, as seen earlier.)\n\n\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  mutate(Month_Date = lubridate::as_date(Month)) %&gt;%\n  ##\n  timetk::summarise_by_time(\n    .date_var = Month_Date,\n    .by = \"month\",\n    mean_Cost = mean(Cost)\n  ) %&gt;%\n  ##\n  timetk::plot_time_series(\n    .date_var = Month_Date,\n    .value = mean_Cost,\n    .interactive = FALSE,\n    .x_lab = \"Time\", .y_lab = \"Costs\",\n    .title = \"Mean Costs by Month\"\n  ) +\n  labs(caption = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  mutate(Month_Date = lubridate::as_date(Month)) %&gt;%\n  as_tibble() %&gt;%\n  ##\n  timetk::summarise_by_time(\n    .date_var = Month_Date,\n    .by = \"quarter\",\n    mean_Cost = mean(Cost)\n  ) %&gt;%\n  ##\n  timetk::plot_time_series(\n    .date_var = Month_Date,\n    .value = mean_Cost,\n    .interactive = FALSE,\n    .x_lab = \"Time\", .y_lab = \"Costs\",\n    .title = \"Mean Costs by Quarter\"\n  ) +\n  labs(caption = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n R\n web-r\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\nPBS %&gt;%\n  mutate(Month_Date = lubridate::as_date(Month)) %&gt;%\n  as_tibble() %&gt;%\n  ##\n  timetk::summarise_by_time(\n    .date_var = Month_Date,\n    .by = \"year\",\n    mean_Cost = mean(Cost)\n  ) %&gt;%\n  ##\n  timetk::plot_time_series(\n    .date_var = Month_Date,\n    .value = mean_Cost,\n    .interactive = FALSE,\n    .x_lab = \"Time\", .y_lab = \"Costs\",\n    .title = \"Mean Costs by Year\"\n  ) +\n  labs(caption = \"Tsibble Plotted with timetk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#conclusion",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have learnt how to filter, summarize and compute various aggregate metrics from them and to plot these. Both tsibble and timetk offer similar capability here."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#your-turn",
    "title": "🕔 Time Series Wrangling",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nChoose some of the data sets in the tsdl and in the tsibbledata packages. Plot basic, filtered and summarized graphs for these and interpret."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-wrangling.html#references",
    "title": "🕔 Time Series Wrangling",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition). available online\n\n\nTime Series Analysis at Our Coding Club\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ngapminder\n1.0.1\n(gapminder?)\n\n\ntimetk\n2.9.0\n(timetk?)\n\n\ntsibble\n1.1.6\n(tsibble?)\n\n\ntsibbledata\n0.4.1\n(tsibbledata?)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html",
    "title": "\n Time",
    "section": "",
    "text": "TimeSeries Wrangling  \n\n  Time Series Analysis-WIP \n\n\n\n\n“Remember that sometimes not getting what you want is a wonderful stroke of luck.”\n— Dalai Lama XIV",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#sec-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#sec-slides-and-tutorials",
    "title": "\n Time",
    "section": "",
    "text": "TimeSeries Wrangling  \n\n  Time Series Analysis-WIP \n\n\n\n\n“Remember that sometimes not getting what you want is a wonderful stroke of luck.”\n— Dalai Lama XIV",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#setting-up-r-packages",
    "title": "\n Time",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\nlibrary(skimr)\nlibrary(fpp3)\n\n# Wrangling\n# library(lubridate)  # Deal with dates. Loads with tidyverse\n# library(tsibble) # loads with ffp3\n# library(tsibbledata) # loads with fpp3\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl)\nlibrary(TSstudio)\nlibrary(timetk)\nlibrary(tsbox)\nlibrary(gghighlight) # Highlight specific parts of charts\nlibrary(ggtime) # Mitchell Ohara-Wild June 2025\n\nThe fpp3 packages loads a good few other packages:\n\n\n [1] \"cli\"         \"crayon\"      \"dplyr\"       \"fable\"       \"fabletools\" \n [6] \"feasts\"      \"ggplot2\"     \"lubridate\"   \"purrr\"       \"rstudioapi\" \n[11] \"tibble\"      \"tidyr\"       \"tsibble\"     \"tsibbledata\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#what-graphs-will-we-see-today",
    "title": "\n Time",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nQual\nLine Chart, CandleStick Plot, Heatmap",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Time",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#inspiration",
    "title": "\n Time",
    "section": "\n Inspiration",
    "text": "Inspiration\nShown below are the temperatures over time in two US cities:\n\n\nWhere would need ACs in all rooms? And heaters?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#introduction",
    "title": "\n Time",
    "section": "\n Introduction",
    "text": "Introduction\nAny metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply…).\nWhat can we do with Time Series? As with other datasets, we have to begin by answering fundamental questions, such as:\n\nWhat are the types of time series?\nHow do we visualize time series?\nHow might we summarize time series to get aggregate numbers, say by week, month, quarter or year?\nHow do we decompose the time series into level, trend, and seasonal components?\nHow might we make a model of the underlying process that creates these time series?\nHow do we make useful forecasts with the data we have?\n\nWe will first look at the multiple data formats for time series in R. Alongside we will look at the R packages that work with these formats and create graphs and measures using those objects. Then we examine data wrangling of time series, where we look at packages that offer dplyr-like ability to group and summarize time series using the time variable. We will finally look at obtaining the components of the time series and try our hand at modelling and forecasting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#time-series-formats-conversion-and-plotting",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#time-series-formats-conversion-and-plotting",
    "title": "\n Time",
    "section": "\n Time Series Formats, Conversion, and Plotting",
    "text": "Time Series Formats, Conversion, and Plotting\nThere are multiple formats for time series data. The ones that we are likely to encounter most are:\n\nThe ts format: We may simply have a single series of measurements that are made over time, stored as a numerical vector. The stats::ts() function will convert a numeric vector into an R time series ts object, which is the most basic time series object in R. The base-R ts object is used by established packages forecast and is also supported by newer packages such as tsbox.\nThe tibble format: the simplest and most familiar data format is of course the standard tibble/data frame, with or without an explicit time column/variable to indicate that the other variables vary with time. The standard tibble object is used by many packages, e.g. timetk & modeltime.\nThe tsibble format: this is a new format for time series analysis. The special tsibble object (“time series tibble”) is used by fable, feasts and others from the tidyverts set of packages.\n\nThere are many other time-oriented data formats too…probably too many, such a tibbletime and TimeSeries objects. For now the best way to deal with these, should you encounter them, is to convert them (Using the package tsbox) to a tibble or a tsibble and work with these.\n\n\n\n\n\nStandards\n\nTo start, we will use simple ts data first, and then do another with a “vanilla” tibble format that we can plot as is. We will then look at a tibbledata that does have a time-oriented variable. We will then perform conversion to tsibble format to plot it, and then a final example with a ground-up tsibble dataset.\n\n Base-R ts format data\nThere are a few datasets in base R that are in ts format already.\n\n\n R\n web-r\n\n\n\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThis can be easily plotted using base R:\n\n\n R\n web-r\n\n\n\n\n\n\n# Base R\nplot(AirPassengers)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nOne can see that there is an upward trend and also seasonal variations that also increase over time. This is an example of a multiplicative time series, which we will discuss later.\nLet us take data that is “time oriented” but not in ts format. We use the command ts to convert a numeric vector to ts format: the syntax of ts() is:\nSyntax: objectName &lt;- ts(data, start, end, frequency), where,\n\n\ndata : represents the data vector\n\nstart : represents the first observation in time series\n\nend : represents the last observation in time series\n\nfrequency : represents number of observations per unit time. For example 1=annual, 4=quarterly, 12=monthly, 7=weekly, etc.\n\nWe will pick simple numerical vector data ( i.e. not a time series ) ChickWeight:\n\n\n R\n web-r\n\n\n\n\ndata(ChickWeight)\nstr(ChickWeight)\n\nClasses 'nfnGroupedData', 'nfGroupedData', 'groupedData' and 'data.frame':  578 obs. of  4 variables:\n $ weight: num  42 51 59 64 76 93 106 125 149 171 ...\n $ Time  : num  0 2 4 6 8 10 12 14 16 18 ...\n $ Chick : Ord.factor w/ 50 levels \"18\"&lt;\"16\"&lt;\"15\"&lt;..: 15 15 15 15 15 15 15 15 15 15 ...\n $ Diet  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 1 1 1 1 1 1 1 1 1 ...\n - attr(*, \"formula\")=Class 'formula'  language weight ~ Time | Chick\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"outer\")=Class 'formula'  language ~Diet\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_EmptyEnv&gt; \n - attr(*, \"labels\")=List of 2\n  ..$ x: chr \"Time\"\n  ..$ y: chr \"Body weight\"\n - attr(*, \"units\")=List of 2\n  ..$ x: chr \"(days)\"\n  ..$ y: chr \"(gm)\"\n\nhead(ChickWeight)\n\n\n  \n\n\n\n\n# Filter for Chick #1 and for Diet #1\nChickWeight_ts &lt;- ChickWeight %&gt;%\n  dplyr::filter(Chick == 1, Diet == 1) %&gt;%\n  dplyr::select(weight, Time)\n\n## stats::ts does not accept pipe format\nChickWeight_ts &lt;- stats::ts(ChickWeight_ts$weight,\n  frequency = 2\n)\nstr(ChickWeight_ts)\n\n Time-Series [1:12] from 1 to 6.5: 42 51 59 64 76 93 106 125 149 171 ...\n\n\n\n\n\nplot(ChickWeight_ts) # Using base-R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe see that the weights of a young chick specimen #1 increases over time.\n\ntibble data\nThe ts data format can handle only one time series; in the above example, we could not have plotted the weight of two chicks, if we had wanted to. If we want to plot/analyze multiple time series, based on say Qualitative variables, (e.g. sales figures over time across multiple products and locations) we need other data formats. Using the familiar tibble structure opens up new possibilities.\n\nWe can have multiple time series within a tibble (think of numerical time-series data like GDP, Population, Imports, Exports for multiple countries as with the gapminder1data we saw earlier).\n\n\ngapminder data\n\n\n\n\ncountry\nyear\ngdpPercap\npop\nlifeExp\ncontinent\n\n\n\nAfghanistan\n1952\n779.4453\n8425333\n28.801\nAsia\n\n\nAfghanistan\n1957\n820.8530\n9240934\n30.332\nAsia\n\n\nAfghanistan\n1962\n853.1007\n10267083\n31.997\nAsia\n\n\nAfghanistan\n1967\n836.1971\n11537966\n34.020\nAsia\n\n\nAfghanistan\n1972\n739.9811\n13079460\n36.088\nAsia\n\n\n\n\n\n\nIt also allows for data processing with dplyr such as filtering and summarizing.\n\n\nLet us read and inspect in the US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project.\n Download the US Births data \nRead this data in and inspect it.\n\n\n R\n web-r\n\n\n\n\nbirths_2000_2014 &lt;- read_csv(\"data/US_births_2000-2014_SSA.csv\")\nglimpse(births_2000_2014)\n\nRows: 5,479\nColumns: 5\n$ year          &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 20…\n$ month         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ date_of_month &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ day_of_week   &lt;dbl&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,…\n$ births        &lt;dbl&gt; 9083, 8006, 11363, 13032, 12558, 12466, 12516, 8934, 794…\n\ninspect(births_2000_2014)\n\n\nquantitative variables:  \n           name   class  min   Q1 median    Q3   max         mean          sd\n1          year numeric 2000 2003   2007  2011  2014  2006.999270    4.321085\n2         month numeric    1    4      7    10    12     6.522723    3.449075\n3 date_of_month numeric    1    8     16    23    31    15.730243    8.801151\n4   day_of_week numeric    1    2      4     6     7     3.999817    2.000502\n5        births numeric 5728 8740  12343 13082 16081 11350.068261 2325.821049\n     n missing\n1 5479       0\n2 5479       0\n3 5479       0\n4 5479       0\n5 5479       0\n\nskim(births_2000_2014)\n\n\nData summary\n\n\nName\nbirths_2000_2014\n\n\nNumber of rows\n5479\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nyear\n0\n1\n2007.00\n4.32\n2000\n2003\n2007\n2011\n2014\n▇▇▇▇▇\n\n\nmonth\n0\n1\n6.52\n3.45\n1\n4\n7\n10\n12\n▇▅▅▅▇\n\n\ndate_of_month\n0\n1\n15.73\n8.80\n1\n8\n16\n23\n31\n▇▇▇▇▆\n\n\nday_of_week\n0\n1\n4.00\n2.00\n1\n2\n4\n6\n7\n▇▃▃▃▇\n\n\nbirths\n0\n1\n11350.07\n2325.82\n5728\n8740\n12343\n13082\n16081\n▂▂▁▇▁\n\n\n\n\nbirths_2000_2014\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThis is just a tibble containing a single data variable births that varies over time. All other variables, although depicting time, are numerical columns and not explicitly time columns. There are no Qualitative variables (yet!).\nPlotting tibble-oriented time data\n\n\nUsing ggformula\nUsing ggplot\n\n\n\nWe will now plot this using ggformula. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# grouping by day_of_week\nbirths_2000_2014 %&gt;%\n  gf_line(births ~ year,\n    group = ~day_of_week,\n    color = ~day_of_week\n  ) %&gt;%\n  gf_point(\n    title = \"Births, By Day of Week\",\n    subtitle = \"Over the Years\"\n  ) %&gt;%\n  gf_theme(scale_colour_distiller(palette = \"Paired\"))\n\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;%\n  gf_line(births ~ year,\n    group = ~date_of_month,\n    color = ~date_of_month\n  ) %&gt;%\n  gf_point(\n    title = \"Births, By Date of Month\",\n    subtitle = \"Over the Years\"\n  ) %&gt;%\n  gf_theme(scale_colour_distiller(palette = \"Paired\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot particularly illuminating. This is because the data is daily and we have considerable variation over time, and here we have too much data to visualize.\nSummaries will help, so we could calculate the the mean births per month in each year and plot that:\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;%\n  # Convert month to factor/Qual variable!\n  # So that we can have discrete colours for each month\n  # Using base::factor()\n  # Could use forcats::as_factor() also\n\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n  # `month.abb` is a built-in dataset containing names of months.\n\n  group_by(year, month) %&gt;%\n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\n####\nbirths_2000_2014_monthly %&gt;%\n  ##\n  gf_line(mean_monthly_births ~ year,\n    group = ~month,\n    colour = ~month, linewidth = 1\n  ) %&gt;%\n  ##\n  gf_point(\n    size = 1.5,\n    title = \"Summaries of Monthly Births over the years\"\n  ) %&gt;%\n  ## palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese are graphs for the same month each year: we have a January graph and a February graph and so on. So…average births per month were higher in all months during 2005 to 2007 and have dropped since.\n\n\nWe can do similar graphs using day_of_week as our basis for grouping, instead of month:\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;%\n  mutate(day_of_week = base::factor(day_of_week,\n    levels = c(1, 2, 3, 4, 5, 6, 7),\n    labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n  )) %&gt;%\n  group_by(year, day_of_week) %&gt;%\n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\n##\nbirths_2000_2014_weekly\n##\nbirths_2000_2014_weekly %&gt;%\n  gf_line(mean_daily_births ~ year,\n    group = ~day_of_week,\n    colour = ~day_of_week,\n    linewidth = 1,\n    data = .\n  ) %&gt;%\n  gf_point(size = 2, title = \"Births over the Years by Day of Week\") %&gt;%\n  # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nWe will now plot this using ggplot for completeness. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n# Set graph theme\ntheme_set(new = theme_custom())\n\n# grouping by day_of_week\nbirths_2000_2014 %&gt;%\n  ggplot(aes(year, births,\n    group = day_of_week,\n    color = day_of_week\n  )) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Births, By Day of Week\",\n    subtitle = \"Over the Years\"\n  ) +\n  scale_colour_distiller(palette = \"Paired\")\n##\n\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;%\n  ggplot(aes(year, births,\n    color = date_of_month,\n    group = date_of_month\n  )) +\n  geom_line() +\n  geom_point() +\n  labs(\n    title = \"Births, By Date of Month\",\n    subtitle = \"Over the Years\"\n  ) +\n  scale_colour_distiller(palette = \"Paired\")\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;%\n  # Convert month to factor/Qual variable!\n  # So that we can have discrete colours for each month\n  # Using base::factor()\n  # Could use forcats::as_factor() also\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n  # `month.abb` is a built-in dataset containing names of months.\n\n  group_by(year, month) %&gt;%\n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\nbirths_2000_2014_monthly %&gt;%\n  ggplot(aes(year, mean_monthly_births,\n    group = month,\n    colour = month\n  )) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1.5) +\n  labs(title = \"Summaries of Monthly Births over the years\") +\n\n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\")\n\n\n\n\n  \n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;%\n  mutate(day_of_week = base::factor(day_of_week,\n    levels = c(1, 2, 3, 4, 5, 6, 7),\n    labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n  )) %&gt;%\n  group_by(year, day_of_week) %&gt;%\n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_weekly\nbirths_2000_2014_weekly %&gt;%\n  ggplot(aes(year, mean_daily_births,\n    group = day_of_week,\n    colour = day_of_week\n  )) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 2) +\n\n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\") +\n  labs(title = \"Births over the Years by Day of Week\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n Small Multiples using gghighlight\nInstead of looking at multiple overlapping time series graphs, we could split these up into small multiples or facets and still retain the overall picture that is offered by the overlapping graphs. The trick here is the highlight one of the graphs at a time, while keeping all other graphs in the background. We can do this with the gghighlight package.\n\n\n\n\n# library(gghighlight)\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_monthly\n###\nbirths_2000_2014_monthly %&gt;% ggplot() +\n  geom_line(aes(\n    y = mean_monthly_births,\n    x = year,\n    group = month\n  )) +\n  labs(\n    x = \"Year\", y = \"Mean Monthly Births over the Years\",\n    title = \"Mean Births by Month\",\n    caption = \"Using gghighlight package\"\n  ) +\n\n  ### Add highlighting\n  gghighlight(\n    use_direct_label = F,\n    unhighlighted_params = list(colour = alpha(\"grey85\", 1))\n  ) +\n\n  ### Add faceting\n  facet_wrap(vars(month))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014_weekly\n###\nbirths_2000_2014_weekly %&gt;% ggplot() +\n  geom_line(aes(y = mean_daily_births, x = year, group = day_of_week)) +\n  labs(\n    x = \"Year\", y = \"Mean Daily Births over the Years\",\n    title = \"Mean Births by Day of Week\",\n    caption = \"Using gghighlight package\"\n  ) +\n\n  ### Add highlighting\n  gghighlight(\n    use_direct_label = F,\n    unhighlighted_params = list(colour = alpha(\"grey85\", 1))\n  ) +\n\n  ### Add faceting\n  facet_wrap(vars(day_of_week))\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy are fewer babies born on weekends?\n\n\n\nLooks like an interesting story here…there are significantly fewer births on average on Sat and Sun, over the years! Why? Should we watch Grey’s Anatomy ?\nAnd more births in September? That should be a no-brainer!! 😀\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that this is still using just tibble data, without converting it into a time series format. So far we are simply treating the year/month/day variables are simple variables and using dplyr to group and summarize. We have not created an explicit time or date variable.\n\n\nPlotting tibble time-series\nNow, we can convert the time-oriented columns in this dataset into a single date variable, giving us a proper tibble time-series:\n\nbirths_tibble_timeseries &lt;-\n  births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date(year, month, date_of_month)) %&gt;%\n  ## Drop off the individual columns ( year, month, day_of_month)\n  select(date, births)\n\nbirths_tibble_timeseries\n\n\n  \n\n\n\nNote that we have a proper date formatted column, as desired. This is a single time series, but if we had other Qualitative variables such as say city, we could easily have had multiple series here. We can plot this with ggformula/ggplot as we have done before, and with now with timetk:\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_tibble_timeseries %&gt;%\n  timetk::plot_time_series(\n    .date_var = date,\n    .value = births,\n    .interactive = FALSE,\n    .title = \"Births over Time\",\n    .x_lab = \"Time\",\n    .y_lab = \"Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsibble data\nFinally, we have tsibble (“time series tibble”) format data, which contains three main components:\n\nan index variable that defines time;\na set of key variables, usually categorical, that define sets of observations, over time. This allows for each combination of the categorical variables to define a separate time series.\na set of quantitative variables, that represent the quantities that vary with time (i.e index)\n\nHere is Robert Hyndman’s video introducing tsibbles:\n\nThe package tsibbledata contains several ready made tsibble format data. Run data(package = \"tsibbledata\") in your Console to find out about these.\nLet us try PBS, which is a dataset containing Monthly Medicare prescription data in Australia.\n\ndata(PBS, package = \"tsibbledata\")\nPBS\n\n\n  \n\n\nglimpse(PBS)\n\nRows: 67,596\nColumns: 9\nKey: Concession, Type, ATC1, ATC2 [336]\n$ Month      &lt;mth&gt; 1991 Jul, 1991 Aug, 1991 Sep, 1991 Oct, 1991 Nov, 1991 Dec,…\n$ Concession &lt;chr&gt; \"Concessional\", \"Concessional\", \"Concessional\", \"Concession…\n$ Type       &lt;chr&gt; \"Co-payments\", \"Co-payments\", \"Co-payments\", \"Co-payments\",…\n$ ATC1       &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\",…\n$ ATC1_desc  &lt;chr&gt; \"Alimentary tract and metabolism\", \"Alimentary tract and me…\n$ ATC2       &lt;chr&gt; \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A01\", \"A0…\n$ ATC2_desc  &lt;chr&gt; \"STOMATOLOGICAL PREPARATIONS\", \"STOMATOLOGICAL PREPARATIONS…\n$ Scripts    &lt;dbl&gt; 18228, 15327, 14775, 15380, 14371, 15028, 11040, 15165, 168…\n$ Cost       &lt;dbl&gt; 67877.00, 57011.00, 55020.00, 57222.00, 52120.00, 54299.00,…\n\n\nData Dictionary\n\n\n\n\n\n\nNote\n\n\n\nData Description: This is a large-ish dataset.Run PBS in your console)\n\n67K observations\n336 combinations of key variables (Concession, Type, ATC1, ATC2) which are categorical, as foreseen.\nData appears to be monthly, as indicated by the 1M.\nthe time index variable is called Month, formatted as yearmonth, a new type of variable introduced in the tsibble package.\n\nNote that there are multiple Quantitative variables (Scripts,Cost), each sliced into 336 time-series, a feature which is not supported in the ts format, but is supported in a tsibble. The Qualitative Variables are described below. (Type help(\"PBS\") in your Console.)\nThe data is dis-aggregated/grouped using four keys:\n- Concession: Concessional scripts are given to pensioners, unemployed, dependents, and other card holders\n- Type: Co-payments are made until an individual’s script expenditure hits a threshold ($290.00 for concession, $1141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount.\n- ATC1: Anatomical Therapeutic Chemical index (level 1). 15 types\n- ATC2: Anatomical Therapeutic Chemical index (level 2). 84 types, nested inside ATC1.\n\n\nLet us simply plot Cost over time:\n\n\nUsing ggformula\nUsing ggplot\nUsing timetk\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  gf_point(Cost ~ Month, data = .) %&gt;%\n  gf_line(title = \"PBS Costs vs time\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  ggplot(aes(Month, Cost)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"PBS Costs vs time\", caption = \"ggplot\")\n\n\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nPBS %&gt;%\n  timetk::plot_time_series(\n    .date_var = Month, .value = Cost,\n    .interactive = FALSE,\n    .smooth = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\nThis basic plot is quite messy. Other than an overall rising trend and more vigorous variations pointing to a multiplicative process, we cannot say more. There is simply too much happening here and it is now time (sic!) for us to look at summaries of the data using dplyr-like verbs.\nWe will do that in the Section 1.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#time-series-heatmaps",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#time-series-heatmaps",
    "title": "\n Time",
    "section": "\n Time Series Heatmaps",
    "text": "Time Series Heatmaps\nHow about a heatmap? We can cook up a categorical variable based on the number of births (low, fine, high) and use that to create a heatmap:\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n\nbirths_2000_2014 %&gt;%\n  mutate(birthrate = case_when(\n    births &gt;= 10000 ~ \"high\",\n    births &lt;= 8000 ~ \"low\",\n    TRUE ~ \"fine\"\n  )) %&gt;%\n  mutate(birthrate = base::factor(birthrate,\n    labels = c(\"high\", \"fine\", \"low\"),\n    ordered = TRUE\n  )) %&gt;%\n  gf_tile(\n    data = .,\n    year ~ month,\n    fill = ~birthrate,\n    color = \"black\"\n  ) %&gt;%\n  gf_theme(scale_x_time(\n    breaks = 1:12,\n    labels = c(\n      \"Jan\", \"Feb\", \"Mar\", \"Apr\",\n      \"May\", \"Jun\", \"Jul\", \"Aug\",\n      \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n    )\n  )) %&gt;%\n  gf_theme(scale_fill_brewer(type = \"qual\", palette = \"OrRd\", direction = -1))\n\n\n\n\n\n\n\n\n\n\n\n\nNote how both X and Y axis seem to be a time-oriented variable in a heatmap!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#your-turn",
    "title": "\n Time",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nChoose some of the datasets in the tsdl and in the tsibbledata packages. (Install and load them first! ) Plot basic, filtered and model-based graphs for these and interpret.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#wait-but-why",
    "title": "\n Time",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nMany datasets show quantities varying over time. These are called time-series data.\nThe X-axis in these cases becomes a time axis.\nTime-series data come in many different formats!\nThe time-aspect in a dataset creates for two dimensions of data-aggregation and averaging: One based on factors as before, and a new one based on intervals of time\nWe are interested in decomposing a time-series into averages, trends, seasonal components, and random variations\nWe are also interested in modelling a time-series as additive or multiplicative time-series, using techniques such as Holt-Winters, and ARIMA\nAnd of course we are interested in forecasting!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#conclusion",
    "title": "\n Time",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen a good few data formats for time series, and how to work with them and plot them.\nIn the Tutorial Section 1, we will explore:\n\nwrangling with Time series to produce grouped and filtered aggregates/summaries and plots with these\nhow to decompose time series into periodic and aperiodic components, which can be used to make business decisions.\nProducing Interactive Plots for Time Series\n\nmodelling and forecasting of time series.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#references",
    "title": "\n Time",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition).available online\n\n\nTime Series Analysis at Our Coding Club\n\n\nThe Nuclear Threat—The Shadow Peace, part 1\n\n\n11 Ways to Visualize Changes Over Time – A Guide\n\n\nWhat is seasonal adjustment and why is it used?\n\n\nThe start-at-zero rule\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nfpp3\n1.0.1\nHyndman (2024)\n\n\ngghighlight\n0.5.0\nYutani (2025)\n\n\ntimetk\n2.9.0\nDancho and Vaughan (2023)\n\n\ntsbox\n0.4.2\nSax (2021)\n\n\ntsdl\n0.1.0\nHyndman and Yang (2025)\n\n\ntsibble\n1.1.6\nWang, Cook, and Hyndman (2020)\n\n\ntsibbledata\n0.4.1\nO’Hara-Wild et al. (2022)\n\n\nTSstudio\n0.1.7\nKrispin (2023)\n\n\n\n\n\n\nDancho, Matt, and Davis Vaughan. 2023. timetk: A Tool Kit for Working with Time Series. https://doi.org/10.32614/CRAN.package.timetk.\n\n\nHyndman, Rob. 2024. Fpp3: Data for “Forecasting: Principles and Practice” (3rd Edition). https://doi.org/10.32614/CRAN.package.fpp3.\n\n\nHyndman, Rob, and Yangzhuoran Yang. 2025. tsdl: Time Series Data Library. https://github.com/FinYang/tsdl.\n\n\nKrispin, Rami. 2023. TSstudio: Functions for Time Series Analysis and Forecasting. https://doi.org/10.32614/CRAN.package.TSstudio.\n\n\nO’Hara-Wild, Mitchell, Rob Hyndman, Earo Wang, and Rakshitha Godahewa. 2022. tsibbledata: Diverse Datasets for “tsibble”. https://doi.org/10.32614/CRAN.package.tsibbledata.\n\n\nSax, Christoph. 2021. tsbox: Class-Agnostic Time Series in in R. https://docs.ropensci.org/tsbox/.\n\n\nWang, Earo, Dianne Cook, and Rob J Hyndman. 2020. “A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data.” Journal of Computational and Graphical Statistics 29 (3): 466–78. https://doi.org/10.1080/10618600.2019.1695624.\n\n\nYutani, Hiroaki. 2025. gghighlight: Highlight Lines and Points in “ggplot2”. https://doi.org/10.32614/CRAN.package.gghighlight.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/index.html#footnotes",
    "title": "\n Time",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.gapminder.org/data/↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:hour-glass\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Time"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html",
    "title": "\n Data",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure ( like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.\n\n\n\n\n\n\n\nImportantClick on any Picture to Zoom\n\n\n\nAll embedded figures are displayed full-screen when clicked.\n\n\n\n\n“Difficulties strengthen the mind, as labor does the body.”\n— Seneca",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#using-web-r",
    "title": "\n Data",
    "section": "",
    "text": "This tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure ( like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\n\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.\n\n\n\n\n\n\n\nImportantClick on any Picture to Zoom\n\n\n\nAll embedded figures are displayed full-screen when clicked.\n\n\n\n\n“Difficulties strengthen the mind, as labor does the body.”\n— Seneca",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#setting-up-r-packages",
    "title": "\n Data",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # Data processing with tidy principles\nlibrary(mosaic) # Our go-to package for almost everything\n\n# devtools::install_github(\"rpruim/Lock5withR\")\nlibrary(Lock5withR)\nlibrary(Lock5Data) # Some neat little datasets from a lovely textbook\nlibrary(kableExtra)\n\nPlot Fonts and Theme\n\nShow the Code```{r}\n#| label: plot-theme\n#| code-fold: true\n#| messages: false\n#| warning: false\n\nlibrary(showtext)\nfont_add(family = \"Alegreya\", regular = \"../../../../fonts/Alegreya/Alegreya-Regular.ttf\")\nfont_add(family = \"Roboto Condensed\", regular = \"../../../../fonts/RobotoCondensed-Regular.ttf\")\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = \"Alegreya\", # set font family\n        size = 18, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = \"Alegreya\", # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = \"Alegreya\", # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#sec-where-data",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#sec-where-data",
    "title": "\n Data",
    "section": "\n Where does Data come from?",
    "text": "Where does Data come from?\nWe will need to form a basic understanding of basic scientific enterprise. Let us look at the slides. (Also embedded below!)\n    View slides in full screen",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#what-are-data-types",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#what-are-data-types",
    "title": "\n Data",
    "section": "\n What are Data Types?",
    "text": "What are Data Types?\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantTidy Data\n\n\n\nEach variable is a column; a column contains one kind of data. Each observation or case is a row.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#sec-data-types",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#sec-data-types",
    "title": "\n Data",
    "section": "\n How do we Spot Data Variable Types?",
    "text": "How do we Spot Data Variable Types?\nBy asking questions! Shown below is a table of different kinds of questions you could use to query a dataset. The variable or variables that “answer” the question would be in the category indicated by the question.\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\nAs you go from Qualitative to Quantitative data types in the table, I hope you can detect a movement from fuzzy groups/categories to more and more crystallized numbers.\n\n\n\n\n\nType of Variables\n\nEach variable/scale can be subjected to the operations of the previous group. In the words of S.S. Stevens\n\nthe basic operations needed to create each type of scale is cumulative: to an operation listed opposite a particular scale must be added all those operations preceding it.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#some-examples-of-data-variables",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#some-examples-of-data-variables",
    "title": "\n Data",
    "section": "Some Examples of Data Variables",
    "text": "Some Examples of Data Variables\nExample 1: AllCountries\n\n\n\nBase R\n web-r\n\n\n\n\nhead(AllCountries, 5) %&gt;% arrange(desc(Internet))\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ1. How many people in Andorra have internet access?\nA1. This leads to the Internet variable, which is a Quantitative variable, a proportion.1 The answer is \\(70.5\\%\\).\n\n\nExample 2:StudentSurveys\n\n\n\nBase R\n web-r\n\n\n\n\nhead(StudentSurvey, 5)\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteQuestions\n\n\n\nQ.1. What kind of students are these?\nA.1. The variables Gender, and Year both answer to this Question. And they are both Qualitative/Categorical variables, of course.\nQ.2. What is their status in their respective families?\nA.2. Hmm…they are either first-born, or second-born, or third…etc. While this is recorded as a number, it is still a Qualitative variable2! Think! Can you do math operations with BirthOrder? Like mean or median?\nQ.3.How big are the families?\nA.3. Clearly, the variable that answers is Siblings and since the question is synonymous with “how many”, this is a Quantitative variable.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#conclusion",
    "title": "\n Data",
    "section": "\n Conclusion",
    "text": "Conclusion\nLet us take a look at Wickham and Grolemund’s Data Science workflow picture:\n\n\n\n\n\nFigure 1: Data Science Workflow\n\n\nSo there we have it:\n\nWe import and clean the data\n\nQuestions lead us to identify Types of Variables (Quant and Qual)\n\nSometimes we may need to transform the data (long to wide, summarize, create new variables…)\nFurther Questions lead to relationships between variables, which we describe using Data Visualizations\n\nWhich is finally Communicated\n\nYou might think of all these Questions, Answers, Mapping as being equivalent to metaphors as a language in itself. And indeed, in R we use a philosophy called the Grammar of Graphics! We will use this grammar in the R graphics packages that we will encounter when we make Graphs next. Other parts of the Workflow (Transformation, Analysis and Modelling) are also following similar grammars, as we shall see.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#ai-generated-summary-and-podcast",
    "title": "\n Data",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis is a tutorial on data visualization using the R programming language. It introduces concepts such as data types, variables, and visualization techniques. The tutorial utilizes metaphors to explain these concepts, emphasizing the use of geometric aesthetics to represent data. It also highlights the importance of both visual and analytic approaches in understanding data. The tutorial then demonstrates basic chart types, including histograms, scatterplots, and bar charts, and discusses the “Grammar of Graphics” philosophy that guides data visualization in R. The text concludes with a workflow diagram for data science, emphasizing the iterative process of data import, cleaning, transformation, visualization, hypothesis generation, analysis, and communication.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#references",
    "title": "\n Data",
    "section": "\n References",
    "text": "References\n\nRandomized Trials:\n\n\n\n \nMartyn Shuttleworth, Lyndsay T Wilson (Jun 26, 2009). What is the Scientific Method? Retrieved Mar 12, 2024 from Explorable.com: https://explorable.com/what-is-the-scientific-method\n\nAdam E.M. Eltorai, Jeffrey A. Bakal, Paige C. Newell, Adena J. Osband (editors). (March 22, 2023) Translational Surgery: Handbook for Designing and Conducting Clinical and Translational Research. A very lucid and easily explained set of chapters. ( I have a copy. Yes.)\n\nPart III. Clinical: fundamentals\nPart IV: Statistical principles\n\n\nhttps://safetyculture.com/topics/design-of-experiments/\nEmi Tanaka. https://emitanaka.org/teaching/monash-wcd/2020/week09-DoE.html\n\nOpen Intro Stats: Types of Variables\nLock, Lock, Lock, Lock, and Lock. Statistics: Unlocking the Power of Data, Third Edition, Wiley, 2021. https://www.wiley.com/en-br/Statistics:+Unlocking+the+Power+of+Data,+3rd+Edition-p-9781119674160)\n\nClaus Wilke. Fundamentals of Data Visualization. https://clauswilke.com/dataviz/\n\nTim C. Hesterberg (2015). What Teachers Should Know About the Bootstrap: Resampling in the Undergraduate Statistics Curriculum, The American Statistician, 69:4, 371-386, DOI:10.1080/00031305.2015.1089789. PDF here\n\nAlbert Rapp. Adding images to ggplot. https://albert-rapp.de/posts/ggplot2-tips/27_images/27_images\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\nLock5Data\n3.0.0\nLock (2021)\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://doi.org/10.32614/CRAN.package.ggformula.\n\n\nLock, Robin. 2021. Lock5Data: Datasets for “Statistics: UnLocking the Power of Data”. https://doi.org/10.32614/CRAN.package.Lock5Data.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/05-NatureData/index.html#footnotes",
    "title": "\n Data",
    "section": "Footnotes",
    "text": "Footnotes\n\nHow might this data have been obtained? By asking people in a survey and getting Yes/No answers!↩︎\nQualitative variables are called Factor variables in R, and are stored, internally, as numeric variables together with their levels. The actual values of the numeric variable are 1, 2, and so on.↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-twotone:data-user\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Data"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "# Fonts\n# Run these next few commands IN YOUR CONSOLE once.\n# install.packages(\"extrafontdb\")\n# library(extrafont)\n# extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\n# Fonts\nlibrary(ggtext) # Claus Wilke's package\nlibrary(showtext)\nlibrary(fontawesome)\n# For repeatable layouts, some can be random!!\nset.seed(12345)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#setting-up-r-packages",
    "title": "The Grammar of Networks",
    "section": "",
    "text": "# Fonts\n# Run these next few commands IN YOUR CONSOLE once.\n# install.packages(\"extrafontdb\")\n# library(extrafont)\n# extrafont::font_import(paths = NULL, recursive = TRUE, prompt = TRUE,pattern = NULL)\n\n########################################\n# For General Data Manipulation\nlibrary(tidyverse)\n\n########################################\n# Network Analysis Library (Handle data and Viz)\nlibrary(igraph)\n\n########################################\n# For Network \"Manipulation\"\nlibrary(tidygraph)\n\n# For Network Visualization\nlibrary(ggraph)\nlibrary(graphlayouts)\nlibrary(visNetwork)\n\n# For \"Network\" Datasets\nlibrary(igraphdata)\n\n# Fonts\nlibrary(ggtext) # Claus Wilke's package\nlibrary(showtext)\nlibrary(fontawesome)\n# For repeatable layouts, some can be random!!\nset.seed(12345)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#introduction",
    "title": "The Grammar of Networks",
    "section": "\n Introduction",
    "text": "Introduction\nNetwork graphs show relationships between entities: what sort they are, how strong they are, and even of they change over time.\nWe will examine data structures pertaining both to the entities and the relationships between them and look at the data object that can combine these aspects together. Then we will see how these are plotted, what the structure of the plot looks like. There are also metrics that we can calculate for the network, based on its structure. We will of course examine geometric metaphors that can represent various classes of entities and their relationships.\nNetwork graphs can be rendered both as static and interactive and we will examine R packages that render both kinds of plots.\nThere is a another kind of structure: one that combines spatial and network data in one. We will defer that for a future module !"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#what-kind-network-graphs-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#what-kind-network-graphs-will-we-make",
    "title": "The Grammar of Networks",
    "section": "What kind Network graphs will we make?",
    "text": "What kind Network graphs will we make?\nHere is a network map of the characters in Victor Hugo’s Les Miserables:\n\n\nAnd this: the well known Zachary’s Karate Club dataset visualized as a network"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#goals",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#goals",
    "title": "The Grammar of Networks",
    "section": "\n Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g. biology/ecology, ideas/influence, technology, transportation, to name a few)"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#pedagogical-note",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#pedagogical-note",
    "title": "The Grammar of Networks",
    "section": "\n Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#graph-metaphors",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#graph-metaphors",
    "title": "The Grammar of Networks",
    "section": "\n Graph Metaphors",
    "text": "Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?…)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\n\n\n\n\n\n\nTipExamples\n\n\n\n\nThe World Wide Web is an example of a directed network because hyperlinks connect one Web page to another, but not necessarily the other way around.\nCo-authorship networks represent examples of un-directed networks, where nodes are authors and they are connected by an edge if they have written a publication together\nWhen people send e-mail to each other, the distinction between the sender (source) and the recipient (target) is clearly meaningful, therefore the network is directed.\n\n\n\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictruninfer-1",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictruninfer-1",
    "title": "The Grammar of Networks",
    "section": "Predict/Run/Infer-1",
    "text": "Predict/Run/Infer-1\nUsing tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -&gt; “Network Object” in R.\n\nggraph Network Object -&gt; Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey’s Anatomy dataset in our first foray into networks.\nStep1. Read the data\nDownload these two datasets into your current project-&gt; data folder.\n Grey’s Anatomy Nodes \n Grey’s Anatomy Edges \ngrey_nodes &lt;- read_csv(\"data/grey_nodes.csv\")\ngrey_edges &lt;- read_csv(\"data/grey_edges.csv\")\n\ngrey_nodes\ngrey_edges\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #1\n\n\n\nLook at the output thumbnails. What attributes (i.e. extra information) are seen for Nodes and Edges?\n\n\nStep 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka “tibble graph”). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga &lt;- tbl_graph(\n  nodes = grey_nodes,\n  edges = grey_edges,\n  directed = FALSE\n)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Node Data: 54 × 7 (active)\n   name               sex   race  birthyear position  season sign    \n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   \n 1 Addison Montgomery F     White      1967 Attending      1 Libra   \n 2 Adele Webber       F     Black      1949 Non-Staff      2 Leo     \n 3 Teddy Altman       F     White      1969 Attending      6 Pisces  \n 4 Amelia Shepherd    F     White      1981 Attending      7 Libra   \n 5 Arizona Robbins    F     White      1976 Attending      5 Leo     \n 6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini  \n 7 Jackson Avery      M     Black      1981 Resident       6 Leo     \n 8 Miranda Bailey     F     Black      1969 Attending      1 Virgo   \n 9 Ben Warren         M     Black      1972 Other          6 Aquarius\n10 Henry Burton       M     White      1972 Non-Staff      7 Cancer  \n# ℹ 44 more rows\n#\n# Edge Data: 57 × 4\n   from    to weight type    \n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ℹ 54 more rows\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #2\n\n\n\nWhat information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\nStep 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #3\n\n\n\nDescribe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as “points”. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link0(aes(.....)): Draws edges as “links”. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line\n# About what that line does for the overall graph\n\nggraph(graph = ga, layout = \"kk\") +\n  #\n  geom_edge_link0(width = 2, color = \"pink\") +\n  #\n  geom_node_point(\n    shape = 21, size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n\n  labs(\n    title = \"Whoo Hoo! My First Silly Grey's Anatomy graph in R!\",\n    subtitle = \"Why did I ever get in this course...\",\n    caption = \"Bro, they are doing cool things in the other classes...\\n And the show is even more cool!\"\n  ) +\n\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #4:\n\n\n\nWhat parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc.\n\nggraph(graph = ga, layout = \"kk\") +\n  geom_edge_link0(width = 2) +\n  geom_node_point(\n    shape = 21, size = 4,\n    fill = \"moccasin\",\n    color = \"firebrick\",\n    stroke = 2\n  ) +\n  labs(\n    title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n    subtitle = \"Why did I ever get in this course...\",\n    caption = \"Bro, they are doing cool things in the other classes...\"\n  ) +\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #5\n\n\n\nWhat did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon’t try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges” geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n\n  geom_node_point(aes(color = race), size = 6) + # change variable here\n\n  labs(\n    title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\",\n    subtitle = \"Colouring Nodes by Attribute\",\n    caption = \"Grey's Anatomy\"\n  ) +\n\n  scale_edge_width(range = c(0.2, 2)) +\n  set_graph_style(family = \"roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #6\n\n\n\nDescribe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictreuseinfer-2",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#predictreuseinfer-2",
    "title": "The Grammar of Networks",
    "section": "Predict/Reuse/Infer-2",
    "text": "Predict/Reuse/Infer-2\n\n# Arc diagram\n\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(edge_width = \"Weight\", title = \"Grey's Anatomy\", subtitle = \"Arc Layout\") +\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #7\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + # Note the layout!\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n\n  geom_node_point(size = 3, colour = \"red\") +\n  geom_node_text(aes(label = name),\n    repel = TRUE, size = 2, check_overlap = TRUE,\n    max.overlaps = 25\n  ) +\n  labs(edge_width = \"Weight\") +\n  theme(aspect.ratio = 1) +\n  set_graph_style(family = \"Roboto\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #8\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#hierarchical-layouts",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#hierarchical-layouts",
    "title": "The Grammar of Networks",
    "section": "Hierarchical layouts",
    "text": "Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\n# set_graph_style()\n\n# This dataset contains the graph that describes the class\n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n  \n\n\nhead(flare$edges)\n\n\n  \n\n\n# flare class hierarchy\ngraph &lt;- tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n##\nset_graph_style(family = \"Roboto\")\n##\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") +\n  geom_edge_diagonal() +\n  labs(title = \"Dendrogram\")\n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) +\n  geom_edge_diagonal0() +\n  geom_node_point(aes(filter = leaf)) +\n  coord_fixed() +\n  labs(title = \"Circular Dendrogram\")\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) +\n  geom_node_tile(aes(fill = depth), size = 0.25) +\n  scale_fill_distiller(palette = \"Pastel1\") +\n  labs(title = \"Rectangular Tree Map\")\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) +\n  scale_fill_distiller(palette = \"Accent\") +\n  coord_fixed() +\n  labs(title = \"Circular Tree Map\")\n\n\n# icicle\nggraph(graph, layout = \"partition\") +\n  geom_node_tile(aes(y = -y, fill = depth)) +\n  scale_fill_distiller(palette = \"Set3\") +\n  labs(title = \"Icicle Chart\")\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  scale_fill_distiller(palette = \"Spectral\") +\n  coord_fixed() +\n  labs(title = \"Circular Icicle\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #9\n\n\n\nHow do graphs look “metaphorically” different? Do they reveal different aspects of the group? How?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#faceting",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#faceting",
    "title": "The Grammar of Networks",
    "section": "Faceting",
    "text": "Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\n##\nset_graph_style(family = \"Roboto\", size = 8)\n##\n# facet edges by type\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0(aes(color = type)) +\n  geom_node_point() +\n  facet_edges(~type) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet nodes by sex\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0() +\n  geom_node_point() +\n  facet_nodes(~race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet both nodes and edges\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0(aes(color = type)) +\n  geom_node_point() +\n  facet_graph(type ~ race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #10\n\n\n\nDoes splitting up the main graph into sub-networks give you more insight? Describe some of these."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#network-analysis-with-tidygraph",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#network-analysis-with-tidygraph",
    "title": "The Grammar of Networks",
    "section": "Network analysis with tidygraph",
    "text": "Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\nNetwork Centrality: Go-To and Go-Through People!\nCentrality is a an “ill-defined” metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\n\n\n\nStandards\n\nLet’s add a few columns to the nodes and edges based on network centrality measures:\n\nga %&gt;%\n  activate(nodes) %&gt;%\n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %&gt;%\n  filter(degree &gt; 0) %&gt;%\n  activate(edges) %&gt;%\n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Edge Data: 57 × 5 (active)\n    from    to weight type         betweenness\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1     5    47      2 friends             20.3\n 2    21    47      4 benefits            44.7\n 3     5    46      1 friends             39  \n 4     5    41      1 friends             66.3\n 5    18    41      6 friends             39  \n 6    21    41     12 benefits            91.5\n 7    37    41      5 professional       164. \n 8    31    41      2 professional        98.8\n 9    20    31      3 professional        47.2\n10    17    31      4 friends            102. \n# ℹ 47 more rows\n#\n# Node Data: 54 × 8\n  name               sex   race  birthyear position  season sign   degree\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ℹ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n##\nset_graph_style(family = \"Roboto\")\n##\nggraph(ga, layout = \"nicely\") +\n  geom_edge_link0(aes(alpha = centrality_edge_betweenness())) +\n\n  geom_node_point(aes(\n    colour = centrality_degree(),\n    size = centrality_degree()\n  )) +\n\n  geom_node_text(aes(label = name), repel = TRUE, size = 1.5) +\n\n  scale_size(name = \"Degree\", range = c(0.5, 5)) +\n\n  scale_color_gradient(\n    name = \"Degree\", # SAME NAME!!\n    low = \"blue\", high = \"red\",\n    aesthetics = c(\"colour\", \"fill\"),\n    guide = guide_legend(reverse = FALSE)\n  ) +\n\n  scale_edge_alpha(name = \"Betweenness\", range = c(0.05, 1)) +\n  labs(\n    title = \"Grey's Anatomy\",\n    subtitle = \"Nodes Scaled by Degree, Edges shaded by Betweenness\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #11\n\n\n\nHow do the Centrality Measures show up in the graph? Would you “agree” with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\nAnalysis and Visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n##\nset_graph_style(family = \"Roboto\")\n##\n# visualize communities of nodes\nga %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_louvain())) %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(aes(color = community), size = 3) +\n  labs(title = \"Grey's Anatomy\", subtitle = \"Nodes Coloured by Community Detection Algorithm (Louvain)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #12\n\n\n\nIs the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\nInteractive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list.\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\ngrey_edges\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis &lt;- grey_nodes %&gt;%\n  rowid_to_column(var = \"id\") %&gt;%\n  rename(\"label\" = name) %&gt;%\n  mutate(sex = case_when(\n    sex == \"F\" ~ \"Female\",\n    sex == \"M\" ~ \"Male\"\n  )) %&gt;%\n  replace_na(., list(sex = \"Transgender?\")) %&gt;%\n  rename(\"group\" = sex)\ngrey_nodes_vis\ngrey_edges_vis &lt;- grey_edges %&gt;%\n  select(from, to) %&gt;%\n  left_join(., grey_nodes_vis,\n    by = c(\"from\" = \"label\")\n  ) %&gt;%\n  left_join(., grey_nodes_vis,\n    by = c(\"to\" = \"label\")\n  ) %&gt;%\n  select(\"from\" = id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %&gt;%\n  visNetwork(nodes = ., edges = grey_edges_vis) %&gt;%\n  visNodes(font = list(size = 40)) %&gt;%\n  # Colour and icons for each of the gender-groups\n  visGroups(\n    groupname = \"Female\", shape = \"icon\",\n    icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  visGroups(\n    groupname = \"Male\", shape = \"icon\",\n    icon = list(code = \"f183\", size = 75, color = \"slateblue\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  visGroups(\n    groupname = \"Transgender?\", shape = \"icon\",\n    icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  # visLegend() %&gt;%\n  # Add the fontawesome icons!!\n  addFontAwesome(version = \"4.7.0\") %&gt;%\n  # Add Interaction Controls\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let’s see how they look:\n\ngrey_nodes_vis %&gt;%\n  visNetwork(nodes = ., edges = grey_edges_vis, ) %&gt;%\n  visLayout(randomSeed = 12345) %&gt;%\n  visNodes(font = list(size = 50)) %&gt;%\n  visEdges(color = \"green\") %&gt;%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %&gt;%\n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %&gt;%\n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %&gt;%\n  visLegend() %&gt;%\n  addIonicons() %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes &lt;- read_csv(\"data/star-wars-network-nodes.csv\")\nstarwars_edges &lt;- read_csv(\"data/star-wars-network-edges.csv\")\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis &lt;-\n  starwars_nodes %&gt;%\n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis &lt;-\n  starwars_edges %&gt;%\n  # Matching Source &lt;- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %&gt;%\n  # Matching Target &lt;- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %&gt;%\n  # Select \"id.x\" and \"id.y\" ONLY\n  # Rename them as \"from\" and \"to\"\n  # keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\nstarwars_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nOk, let’s make things move and shake!!\n\nvisNetwork(\n  nodes = starwars_nodes_vis,\n  edges = starwars_edges_vis\n) %&gt;%\n  visNodes(font = list(size = 30)) %&gt;%\n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(\n  nodes = starwars_nodes_vis,\n  edges = starwars_edges_vis\n) %&gt;%\n  visNodes(\n    font = list(size = 30), shape = \"icon\",\n    icon = list(code = \"f1e3\", size = 75)\n  ) %&gt;%\n  visEdges(color = list(color = \"red\", hover = \"green\", highlight = \"black\")) %&gt;%\n  visInteraction(hover = TRUE) %&gt;%\n  addFontAwesome(version = \"4.7.0\")"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#your-assignments",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#your-assignments",
    "title": "The Grammar of Networks",
    "section": "Your Assignments:",
    "text": "Your Assignments:\nMake-1 : With a readymade dataset\nStep 0. Sine qua non! Fire up a New Project! Always!\nStep 1. Fire up a new Quarto document. Fill in the YAML header.\nStep 2. Take any one of the “Make1-Datasets” datasets described below.\nStep 3. Document contents:\n\nIntroduce / Inspect in R your data and describe\nIntroduce your Purpose\nCreate graph objects\nTry different layouts\nWrite comments in the code\nWrite narrative in text with sections, bold ,italic etc.\n\nStep 4. Knit before you submit. Submit your entire project in a .zip file.\nMake1 - Datasets:\n\n\n\n\n\n\nNoteAirline Data:\n\n\n\n Airlines Nodes \n Airlines Edges \nStart with this bit of code in your second chunk, after set up\n\n```{r}\n#| label: start up code for Airlines\n#| eval: false ## remove this!!\nairline_nodes &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %&gt;%\n  mutate(Id = Id + 1)\n\nairline_edges &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %&gt;%\n  mutate(Source = Source + 1, Target = Target + 1)\n```\n\n\n\n\n\n\n\n\n\nNoteThe Famous Zachary Karate Club dataset\n\n\n\n\nStart with pulling this data into your Quarto:\n\n\n```{r}\n#| eval: false ## remove this!\ndata(\"karate\", package = \"igraphdata\")\nkarate\n```\n\n\nTry ?karate in the console\n\nNote that this is not a set of nodes, nor edges, but already a graph-object!\n\nSo no need to create a graph object using tbl_graph.\n\nYou will need to just go ahead and plot using ggraph.\n\n\n\n\n\n\n\n\n\nNoteGame of Thrones:\n\n\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\n```{r}\n#| label: start-up code for GoT\n#| eval: false ## remove this!!\n\nGoT &lt;- read_rds(\"data/GoT.RDS\")\n```\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1…7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\n\n\n\n\n\n\n\nNoteOther Datasets\n\n\n\n\nChoose any other graph dataset from igraphdata\n\n(type ?igraphdata in console)\n\nAsk me for help if you need any\n\n\n\n\nMake-2: Literary Network with TV Show / Book / Story / Play\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV…or Hamlet, Little Women , Pride and Prejudice, or LoTR)\n\nStep 1. Go to: Literary Networks for instructions.\n\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\n\nIn your edges excel, use from and to as your first columns.\n\nEntries in these columns can be names or ids but be consistent and don’t mix.\n\n\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\n\nAsk me for clarifications on what to do after you have read the Instructions in your group."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/files/GoN.html#references",
    "title": "The Grammar of Networks",
    "section": "\n References",
    "text": "References\n\n\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/networks\n\nOmar Lizardo and Isaac Jilbert, Social Networks: An Introduction. https://bookdown.org/omarlizardo/_main/\n\nMark Hoffman, Methods for Network Analysis. https://bookdown.org/markhoff/social_network_analysis/\n\n\nStatistical Analysis of Network Data with R, 2nd Edition.https://github.com/kolaczyk/sand\n\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\n\nTyner, Sam, François Briatte, and Heike Hofmann. 2017. “Network Visualization with ggplot2.” The R Journal 9 (1): 27–59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html\n\nNetwork Datasets https://icon.colorado.edu/#!/networks\n\nYunran Chen, Introduction to Network Analysis Using R\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggraph\n2.2.1\nPedersen (2024a)\n\n\nggtext\n0.1.2\nWilke and Wiernik (2022)\n\n\ngraphlayouts\n1.2.2\nDavid Schoch (2023)\n\n\nigraph\n2.1.4\n\nCsardi and Nepusz (2006); Csárdi et al. (2025)\n\n\n\nigraphdata\n1.0.1\nCsardi (2015)\n\n\nsand\n2.0.0\nKolaczyk and Csárdi (2020)\n\n\nshowtext\n0.9.7\nQiu and See file AUTHORS for details. (2024)\n\n\ntidygraph\n1.3.1\nPedersen (2024b)\n\n\nvisNetwork\n2.1.2\nAlmende B.V. and Contributors and Thieurmel (2022)\n\n\n\n\n\n\nAlmende B.V. and Contributors, and Benoit Thieurmel. 2022. visNetwork: Network Visualization Using “vis.js” Library. https://doi.org/10.32614/CRAN.package.visNetwork.\n\n\nCsardi, Gabor. 2015. igraphdata: A Collection of Network Data Sets for the “igraph” Package. https://doi.org/10.32614/CRAN.package.igraphdata.\n\n\nCsardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org.\n\n\nCsárdi, Gábor, Tamás Nepusz, Vincent Traag, Szabolcs Horvát, Fabio Zanini, Daniel Noom, and Kirill Müller. 2025. igraph: Network Analysis and Visualization in r. https://doi.org/10.5281/zenodo.7682609.\n\n\nDavid Schoch. 2023. “graphlayouts: Layout Algorithms for Network Visualizations in r.” Journal of Open Source Software 8 (84): 5238. https://doi.org/10.21105/joss.05238.\n\n\nKolaczyk, Eric, and Gábor Csárdi. 2020. sand: Statistical Analysis of Network Data with r, 2nd Edition. https://doi.org/10.32614/CRAN.package.sand.\n\n\nPedersen, Thomas Lin. 2024a. ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://doi.org/10.32614/CRAN.package.ggraph.\n\n\n———. 2024b. tidygraph: A Tidy API for Graph Manipulation. https://doi.org/10.32614/CRAN.package.tidygraph.\n\n\nQiu, Yixuan, and authors/contributors of the included software. See file AUTHORS for details. 2024. showtext: Using Fonts More Easily in r Graphs. https://doi.org/10.32614/CRAN.package.showtext.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. ggtext: Improved Text Rendering Support for “ggplot2”. https://doi.org/10.32614/CRAN.package.ggtext."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTipFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(children = n()) %&gt;%\n  # just to check\n  mutate(\n    No_of_families = as.integer(children / nkids),\n    # Why do we divide\n\n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)\n  )\nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;%\n  gf_col(No_of_families ~ nkids) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(No_of_families,\n    colorBy = \"data\",\n    legend = FALSE\n  ) %&gt;% # Or \"series\"\n\n  # https://echarts4r.john-coene.com/articles/grid.html\n  # echarts4r does not \"automatically\" name the axes!\n  # And look at the \"categorical\" x-axis below!\n\n  e_x_axis(\n    name = \"Family Size\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25, ) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;%\n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"Family Size (nkids)\", nameLocation = \"center\",\n    nameGap = 20, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 20,\n    nameTextStyle = list(align = \"center\"),\n    nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\nTo be coded up.\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(family, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(family) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 25, nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 5) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nNoteSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;%\n  e_charts() %&gt;%\n  e_histogram(height) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$height)),\n    label = list(\n      label = \"Mean Height\",\n      label.position = \"end\"\n    ),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;%\n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(height) %&gt;%\n  e_mark_line(\n    data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n    # This code colours both v-lines red...how?\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(\n    data = list(xAxis = measures %&gt;%\n      filter(sex == \"F\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n\n    # This piece of code has no effect...wonder why not?\n    # BOTH lines are in red ...why??\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;%\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(father) %&gt;%\n  e_density(mother) %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$mother)),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_mark_line(data = list(\n    xAxis = mean(Galton$father),\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  )) %&gt;%\n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  e_charts(height = 400) %&gt;%\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids, sex) %&gt;%\n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  e_charts(nkids, height = 300) %&gt;%\n  e_bar(mean_height, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\", nameGap = 25,\n    type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;%\n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;%\n  group_by(family, sex) %&gt;%\n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;%\n  select(family, sex, height, diff_height) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(diff_height, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  # Fit a trend line\n  e_lm(height ~ diff_height,\n    name = c(\"Female\", \"Male\")\n  ) %&gt;%\n  e_x_axis(\n    max = 18, min = -5,\n    name = \"Father - Mother Height\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50,\n    name = \"Children's Heights\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n())\nNHANES %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n()) %&gt;%\n  e_charts(Education, height = 300) %&gt;%\n  e_bar(count) %&gt;%\n  e_y_axis(max = 1750) %&gt;%\n  e_x_axis(type = \"category\") %&gt;%\n  e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip()\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(rows = 1, cols = 3) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n::: {.callout-note title=“Question”} Q.4. How are people Ages distributed across levels of Education?\n# Recall there are missing data\n# gf_boxplot(Age ~ Education,\n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;%\n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;%\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;%\n  group_by(Race1) %&gt;%\n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;%\n  group_by(Education, Race1) %&gt;%\n  summarize(n = n()) %&gt;%\n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;%\n  mutate(percapita_educated = (n / population) * 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;% # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;%\n  e_y_axis(max = 35) %&gt;%\n  e_facet(rows = 2, cols = 3) %&gt;%\n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(BMI)\nNHANES %&gt;%\n  group_by(Race1) %&gt;%\n  e_charts(height = 350) %&gt;%\n  e_density(BMI) %&gt;%\n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%\n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(\n  path = \"../data/banned.xlsx\",\n  sheet = \"Sorted by Author & Title\"\n)\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;-\n  banned %&gt;%\n  group_by(State) %&gt;%\n  summarise(total = n()) %&gt;%\n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;%\n  group_by(State, `Type of Ban`) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;%\n  #  pivot_wider(.,id_cols = State,\n  #              names_from = `Type of Ban`,\n  #              values_from = count) %&gt;% janitor::clean_names() %&gt;%\n  #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n  #                  banned_from_libraries = 0,\n  #                  banned_pending_investigation = 0,\n  #                  banned_from_classrooms = 0)) %&gt;%\n  # mutate(total = sum(across(where(is.integer)))) %&gt;%\n  gf_col(count ~ reorder(State, total),\n    fill = ~`Type of Ban`\n  ) %&gt;%\n  gf_labs(\n    x = \"Count of Banned Books\",\n    y = \"State\"\n  ) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html",
    "title": "\n Surveys",
    "section": "",
    "text": "Figure 1: Rural Survey",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#sec-setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#sec-setting-up-r-packages",
    "title": "\n Surveys",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic) # Our trusted friend\nlibrary(skimr)\nlibrary(vcd) # Michael Friendly's package, Visualizing Categorical Data\nlibrary(vcdExtra) # Categorical Data Sets\nlibrary(resampledata) # More datasets\n\nlibrary(ggstats) # Likert Scale Plots\nlibrary(labelled) # Creating Labelled Data for Likert Plots\nlibrary(sjPlot) # Another package for Likert Plots\n\n## Making Tables\nlibrary(kableExtra) # html styled tables\nlibrary(tinytable) # Elegant Tables for our data\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#inspiration",
    "title": "\n Surveys",
    "section": "\n Inspiration",
    "text": "Inspiration\nTo be Found !!\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#what-graphs-will-we-see-today",
    "title": "\n Surveys",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\n“Chart Shape”\n\n\nQual\nQual\nLikert Plots",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Surveys",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n3\n                  How, What Kind, What Sort\n                  A Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\n                  Qualitative/Ordinal\n                  Socioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\n                  Median,Percentile",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#how-do-these-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#how-do-these-charts-work",
    "title": "\n Surveys",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nIn many business and design situations, we perform say customer surveys to get Likert Scale data, where several respondents rate a product or a service on a scale of Very much like, somewhat like, neutral, Dislike and Very much dislike, for example. These are then plotted in a chart to get a distribution of opinions for each question in the survey. Some examples of Likert Scales are shown below.\n\n\n\n\n\nFigure 2: Likert Scale Questionnaire Samples\n\n\nAs seen, we can use Likert Scale based questionnaire for a variety of aspects in our survey instruments.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#plots-for-survey-data",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#plots-for-survey-data",
    "title": "\n Surveys",
    "section": "\n Plots for Survey Data",
    "text": "Plots for Survey Data\nHow does this data look like, and how does one plot it? Let us consider a fictitious example, followed by a real world dataset.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-1-a-fictitious-app-survey-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-1-a-fictitious-app-survey-dataset",
    "title": "\n Surveys",
    "section": "\n Case Study-1: A fictitious app Survey dataset",
    "text": "Case Study-1: A fictitious app Survey dataset\n\n\n\n\n\n\nNoteA fictitious QuickEZ app\n\n\n\nWe are a start-up that has an app called QuickEZ for delivery of groceries. We conduct a survey of 200 people at a local store, with the following questions,\n\n“Have your heard of the QuickEZ app?”\n“Do you use the QuickEZ app?”\n“Do you find it easy to use the QuickEZ app?”\n“Will you continue to use the QuickEZ app?”\n\nwhere each questions is to be answered on a scale of : “always”, “often”, “sometimes”,“rarely”, “never”.\n\n\nSuch data may look for example as follows:\n\n\n\n\nFirst 10 Responses\n\nq1\nq2\nq3\nq4\n\n\n\nnever\nrarely\nalways\nalways\n\n\nnever\nalways\nalways\nnever\n\n\noften\nrarely\nnever\nrarely\n\n\nnever\nalways\nalways\nnever\n\n\nalways\nnever\noften\nalways\n\n\nrarely\nalways\nalways\nalways\n\n\nalways\nrarely\nrarely\nrarely\n\n\noften\nalways\nalways\nnever\n\n\nalways\nnever\nalways\nalways\n\n\nalways\nnever\nalways\nalways\n\n\n\n\n\n \n\n\ntibble [200 × 4] (S3: tbl_df/tbl/data.frame)\n $ q1: Factor w/ 4 levels \"never\",\"rarely\",..: 1 1 3 1 4 2 4 3 4 4 ...\n  ..- attr(*, \"label\")= chr \"Have your heard of the QuickEZ app?\"\n $ q2: Factor w/ 4 levels \"never\",\"rarely\",..: 2 4 2 4 1 4 2 4 1 1 ...\n  ..- attr(*, \"label\")= chr \"Do you use the QuickEZ app?\"\n $ q3: Factor w/ 4 levels \"never\",\"rarely\",..: 4 4 1 4 3 4 2 4 4 4 ...\n  ..- attr(*, \"label\")= chr \"Do you find it easy to use the QuickEZ app?\"\n $ q4: Factor w/ 4 levels \"never\",\"rarely\",..: 4 1 2 1 4 4 2 1 4 4 ...\n  ..- attr(*, \"label\")= chr \"Will you continue to use the QuickEZ app?\"\n\n\n\nThe columns here correspond to the 4 questions (q1-q4) and the rows contain the 200 responses, which have been coded as (1:4). Such data is also a form of Categorical data and we need to count and plot counts for each of the survey questions. Such a plot is called a Likert plot and it looks like this:\n\n\n\n\n\n\n\n\nBased on this chart, since it looks like about 40% the survey respondents have not heard of our app, we need more publicity, and many do not find it easy to use 😿, so we have serious re-design and user testing to do !! But at least those who have managed to get past the hurdles are stating they will continue to use the app, so it does the job, but we can make it easier to use.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-2-eurofam-survey-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#case-study-2-eurofam-survey-dataset",
    "title": "\n Surveys",
    "section": "\n Case Study-2: EUROFAM Survey dataset",
    "text": "Case Study-2: EUROFAM Survey dataset\nHere is another example of Likert data from the healthcare industry.\nefc is a German data set from a European study titled EUROFAM study, on family care of older people. Following a common protocol, data were collected from national samples of approximately 1,000 family carers (i.e. caregivers) per country and clustered into comparable subgroups to facilitate cross-national analysis. The research questions in this EUROFAM study were:\n\n\nTo what extent do family carers of older people use support services or receive financial allowances across Europe? What kind of supports and allowances do they mainly use?\nWhat are the main difficulties carers experience accessing the services used? What prevents carers from accessing unused supports that they need? What causes them to stop using still-needed services?\nIn order to improve support provision, what can be understood about the service characteristics considered crucial by carers, and how far are these needs met? and,\nWhich channels or actors can provide the greatest help in underpinning future policy efforts to improve access to services/supports?\n\n\nWe will select the variables from the efc data set that related to coping (on part of care-givers) and plot their responses after inspecting them:\n\ndata(efc, package = \"sjPlot\")\nglimpse(efc)\n\nRows: 908\nColumns: 26\n$ c12hour  &lt;dbl&gt; 16, 148, 70, 168, 168, 16, 161, 110, 28, 40, 100, 25, 25, 24,…\n$ e15relat &lt;dbl&gt; 2, 2, 1, 1, 2, 2, 1, 4, 2, 2, 1, 8, 2, 1, 2, 2, 1, 1, 2, 1, 2…\n$ e16sex   &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2…\n$ e17age   &lt;dbl&gt; 83, 88, 82, 67, 84, 85, 74, 87, 79, 83, 68, 97, 80, 75, 82, 8…\n$ e42dep   &lt;dbl&gt; 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 3, 3, 1, 3, 3, 4, 4…\n$ c82cop1  &lt;dbl&gt; 3, 3, 2, 4, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 2, 4, 3, 4, 3, 3, 3…\n$ c83cop2  &lt;dbl&gt; 2, 3, 2, 1, 2, 2, 2, 2, 2, 2, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2…\n$ c84cop3  &lt;dbl&gt; 2, 3, 1, 3, 1, 3, 4, 2, 3, 1, 4, 3, 2, 4, 3, 1, 1, 1, 1, 4, 3…\n$ c85cop4  &lt;dbl&gt; 2, 3, 4, 1, 2, 3, 1, 1, 2, 2, 4, 1, 2, 4, 3, 3, 2, 2, 2, 2, 2…\n$ c86cop5  &lt;dbl&gt; 1, 4, 1, 1, 2, 3, 1, 1, 2, 1, 4, 3, 2, 1, 2, 3, 1, 1, 2, 1, 1…\n$ c87cop6  &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1…\n$ c88cop7  &lt;dbl&gt; 2, 3, 1, 1, 1, 2, 4, 2, 3, 1, 4, 4, 2, 2, 1, 2, 2, 2, 3, 3, 1…\n$ c89cop8  &lt;dbl&gt; 3, 2, 4, 2, 4, 1, 1, 3, 1, 1, 1, 3, 4, 4, 1, 1, 4, 3, 1, 2, 1…\n$ c90cop9  &lt;dbl&gt; 3, 2, 3, 4, 4, 1, 4, 3, 3, 3, 1, 1, 4, 4, 1, 3, 4, 3, 4, 2, 3…\n$ c160age  &lt;dbl&gt; 56, 54, 80, 69, 47, 56, 61, 67, 59, 49, 66, 47, 58, 75, 49, 5…\n$ c161sex  &lt;dbl&gt; 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2…\n$ c172code &lt;dbl&gt; 2, 2, 1, 2, 2, 2, 2, 2, NA, 2, 2, 2, 3, 1, 3, 2, 2, 2, 3, 3, …\n$ c175empl &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ barthtot &lt;dbl&gt; 75, 75, 35, 0, 25, 60, 5, 35, 15, 0, 25, 85, 15, 70, NA, 0, 9…\n$ neg_c_7  &lt;dbl&gt; 12, 20, 11, 10, 12, 19, 15, 11, 15, 10, 28, 18, 13, 18, 16, 1…\n$ pos_v_4  &lt;dbl&gt; 12, 11, 13, 15, 15, 9, 13, 14, 13, 13, 9, 8, 14, 14, 9, 14, 1…\n$ quol_5   &lt;dbl&gt; 14, 10, 7, 12, 19, 8, 20, 20, 8, 15, 1, 19, 12, 8, 8, 6, 16, …\n$ resttotn &lt;dbl&gt; 0, 4, 0, 2, 2, 1, 0, 0, 0, 1, 1, 1, 0, 0, 3, 0, 0, 0, 3, 0, 1…\n$ tot_sc_e &lt;dbl&gt; 4, 0, 1, 0, 1, 3, 0, 1, 2, 1, 1, 1, 3, 0, 3, 2, 2, 0, 1, 7, 1…\n$ n4pstu   &lt;dbl&gt; 0, 0, 2, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 1, 0, 0, 0, 2, 1, 2, 4…\n$ nur_pst  &lt;dbl&gt; NA, NA, 2, 3, 2, 2, 3, 1, 3, 3, 3, 1, 3, 1, NA, NA, NA, 2, 1,…\n\n\nefc %&gt;%\n  select(dplyr::contains(\"cop\")) %&gt;%\n  head(20)\n##\nefc %&gt;%\n  select(dplyr::contains(\"cop\")) %&gt;%\n  str()\n\n\n\n\n  \n\n\n\n\n\n'data.frame':   908 obs. of  9 variables:\n $ c82cop1: num  3 3 2 4 3 2 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel you cope well as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c83cop2: num  2 3 2 1 2 2 2 2 2 2 ...\n  ..- attr(*, \"label\")= chr \"do you find caregiving too demanding?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c84cop3: num  2 3 1 3 1 3 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your friends?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c85cop4: num  2 3 4 1 2 3 1 1 2 2 ...\n  ..- attr(*, \"label\")= chr \"does caregiving have negative effect on your physical health?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c86cop5: num  1 4 1 1 2 3 1 1 2 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause difficulties in your relationship with your family?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c87cop6: num  1 1 1 1 2 2 2 1 1 1 ...\n  ..- attr(*, \"label\")= chr \"does caregiving cause financial difficulties?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c88cop7: num  2 3 1 1 1 2 4 2 3 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel trapped in your role as caregiver?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"Never\" \"Sometimes\" \"Often\" \"Always\"\n $ c89cop8: num  3 2 4 2 4 1 1 3 1 1 ...\n  ..- attr(*, \"label\")= chr \"do you feel supported by friends/neighbours?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n $ c90cop9: num  3 2 3 4 4 1 4 3 3 3 ...\n  ..- attr(*, \"label\")= chr \"do you feel caregiving worthwhile?\"\n  ..- attr(*, \"labels\")= Named num [1:4] 1 2 3 4\n  .. ..- attr(*, \"names\")= chr [1:4] \"never\" \"sometimes\" \"often\" \"always\"\n\n\n\nThe coping related variables have responses on the Likert Scale (1,2,3,4) which correspond to (never, sometimes, often, always), and each variable also has a label defining each variable. The labels are actually ( and perhaps usually ) the questions in the survey.\nWe can plot this data using the gglikert function from package ggstats:\n\nefc %&gt;%\n  select(dplyr::contains(\"cop\")) %&gt;%\n  gglikert(labels_size = 3, width = 0.75) +\n  labs(title = \"Caregiver Survey from EUROFAM\") +\n  scale_fill_brewer(\n    name = \"Responses\",\n    labels = c(\"never\", \"sometimes\", \"often\", \"always\"),\n    palette = \"Set3\", direction = -1\n  ) +\n  theme(legend.position = \"bottom\") + theme_custom()\n\n\n\n\n\n\n\nMany questions here have strong negative responses. This may indicate that policy and publicity related efforts may be required.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#labelled-data",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#labelled-data",
    "title": "\n Surveys",
    "section": "\n Labelled Data",
    "text": "Labelled Data\nNote how the y-axis has been populated with Survey Questions: this is an example of a labelled dataset, where not only do the variables have names i.e. column names, but also have longish text labels that add information to the data variables. The data values ( i.e scores) in the columns is also labelled as per the the Likert scale (Like/Dislike/Strongly Dislike OR never/sometimes/often/always) etc. These Likert scores are usually a set of contiguous integers.\n\n\n\n\n\n\nNoteVariable Labels and Value Labels\n\n\n\nVariable label is human readable description of the variable. R supports rather long variable names and these names can contain even spaces and punctuation but short variables names make coding easier. Variable label can give a nice, long description of variable. With this description it is easier to remember what those variable names refer to.\nValue labels are similar to variable labels, but value labels are descriptions of the values a variable can take. Labeling values means we don’t have to remember if 1=Extremely poor and 7=Excellent or vice-versa. We can easily get dataset description and variables summary with info function.\n\n\nLet us manually create one such dataset, since this is a common-enough situation1 that we have survey data and then have to label the variables and the values before plotting. We will use the R package labelled to label our data.2.\nIt is also possible to label the tibble, the columns, and the values in similar fashion using the sjlabelled package3 and the labelr package4.\n\nset.seed(42)\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nvariable_labels &lt;- c(\n  \"Do you practice Analytics?\",\n  \"Do you code in R?\",\n  \"Have you published your R Code?\",\n  \"Do you use Quarto as your Workflow in R?\",\n  \"Will you use R at Work?\"\n)\n##\nvalue_labels &lt;- c(\"never\", \"sometimes\", \"often\", \"always\")\n##\nmy_survey_data &lt;-\n  # Create toy survey data\n  # 200 responses to 5 questions\n  # responses on Likert Scale\n  # 1:4 = \"never\", \"sometimes\",\"often\",\"always\")\n\n  tibble(\n    q1 = mosaic::sample(value_labels,\n      replace = TRUE, size = 200,\n      prob = c(0.2, 0.2, 0.5, 0.1)\n    ),\n    q2 = mosaic::sample(value_labels,\n      replace = TRUE, size = 200,\n      prob = c(0.3, 0.3, 0.3, 0.1)\n    ),\n    q3 = mosaic::sample(value_labels,\n      replace = TRUE, size = 200,\n      prob = c(0.2, 0.1, 0.1, 0.6)\n    ),\n    q4 = mosaic::sample(value_labels,\n      replace = TRUE, size = 200,\n      prob = c(0.4, 0.2, 0.1, 0.3)\n    ),\n    q5 = mosaic::sample(value_labels,\n      replace = TRUE, size = 200,\n      prob = c(0.1, 0.2, 0.5, 0.2)\n    )\n  ) %&gt;%\n  # Set VARIABLE labels\n  labelled::set_variable_labels(\n    .data = .,\n    q1 = variable_labels[1],\n    q2 = variable_labels[2],\n    q3 = variable_labels[3],\n    q4 = variable_labels[4],\n    q5 = variable_labels[5]\n  )\n# Values within the variables are already labelled\n###\nhead(my_survey_data, 6)\n\n\n  \n\n\n###\nstr(my_survey_data)\n\ntibble [200 × 5] (S3: tbl_df/tbl/data.frame)\n $ q1: chr [1:200] \"always\" \"always\" \"often\" \"sometimes\" ...\n  ..- attr(*, \"label\")= chr \"Do you practice Analytics?\"\n $ q2: chr [1:200] \"sometimes\" \"often\" \"sometimes\" \"often\" ...\n  ..- attr(*, \"label\")= chr \"Do you code in R?\"\n $ q3: chr [1:200] \"always\" \"always\" \"never\" \"always\" ...\n  ..- attr(*, \"label\")= chr \"Have you published your R Code?\"\n $ q4: chr [1:200] \"always\" \"never\" \"sometimes\" \"never\" ...\n  ..- attr(*, \"label\")= chr \"Do you use Quarto as your Workflow in R?\"\n $ q5: chr [1:200] \"never\" \"always\" \"often\" \"sometimes\" ...\n  ..- attr(*, \"label\")= chr \"Will you use R at Work?\"\n\n##\nmy_survey_data %&gt;%\n  gglikert(labels_size = 3, width = 0.5) +\n  labs(\n    title = \"Do you use R Survey\",\n    subtitle = \"Creating and Using Labelled Data\",\n    caption = \"Using gglikert from ggstats package\"\n  ) +\n  scale_fill_brewer(\n    palette = \"Spectral\",\n    name = \"Responses\",\n    labels = c(\"never\", \"sometimes\", \"often\", \"always\"),\n  ) +\n  geom_vline(xintercept = 0) +\n  theme_custom() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nIt seems many people in the survey plan to use R at work!! And have published R code as well. But Quarto seems to have mixed results! But of course this is a toy dataset!!\nSo there we are with Survey data analysis and plots!\nThere are a few other plots with this type of data, which are useful in very specialized circumstances. One example of this is the agreement plot5 which captures the agreement between two (sets) of evaluators, on ratings given on a shared ordinal scale to a set of items. An example from the field of medical diagnosis is the opinions of two specialists on a common set of patients. However, that is for a more advanced course!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#wait-but-why",
    "title": "\n Surveys",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nLikert plots are like stacked bar-charts aligned horizontally, back to back . They are useful to indicate aspects like opinion, belief, and habits.\nThe scale for Likert data is ordinal: it should not be assumed that the points on the Likert scale (“never”, “sometimes”, “often”, “always”) are separated by the same distance.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#conclusion",
    "title": "\n Surveys",
    "section": "\n Conclusion",
    "text": "Conclusion\nLikert Plots for Survey data are not too different from Bar Plots; we can view the Likert Charts as a set of stacked bar charts, based on Likert-scale response counts. At a pinch we can make a Likert Plot with vanilla bar graphs, but the elegance and power of the ggstat package is undeniable. The packages sjPlot and sjlabelled also feature a plot_likert graphing function which is very elegant too.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#your-turn",
    "title": "\n Surveys",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTake some of the categorical datasets from the vcd and vcdExtra packages and recreate the plots from this module. Go to https://vincentarelbundock.github.io/Rdatasets/articles/data.html and type “vcd” in the search box. You can directly load CSV files from there, using read_csv(\"url-to-csv\").\nIncluding Edible Insects in our Diet!\n\n Download the Edible Insects Dataset \nThere are several questions here for each “area” of preference for edible insects: experience, fear, concern for the environment, etc. Take all the columns marked as average as your data for your Likert Plot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#references",
    "title": "\n Surveys",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nShelomi. (2022). Dataset for: Factors Affecting Willingness and Future Intention to Eat Insects in Students of an Edible Insect Course. [Data set]. Zenodo. https://doi.org/10.5281/zenodo.7379294\n\nWhat is a Likert Scale? https://www.surveymonkey.com/mp/likert-scale/\n\nRickards, G., Magee, C., & Artino, A. R., Jr (2012). You Can’t Fix by Analysis What You’ve Spoiled by Design: Developing Survey Instruments and Collecting Validity Evidence. Journal of graduate medical education, 4(4), 407–410. https://doi.org/10.4300/JGME-D-12-00239.1\n\nJamieson, S. (2004). Likert scales: how to (ab)use them. Medical Education, 38(12), 1217–1218. https://doi:10.1111/j.1365-2929.2004.02012.x \nMark Bounthavong. (May 16, 2019). Communicating data effectively with data visualization – Part 15 (Diverging Stacked Bar Chart for Likert scales). https://mbounthavong.com/blog/2019/5/16/communicating-data-effectively-with-data-visualization-part-15-divergent-stacked-bar-chart-for-likert-scales\n\nAnthony R. Artino Jr., Jeffrey S. La Rochelle, Kent J. Dezee & Hunter Gehlbach (2014). Developing questionnaires for educational research: AMEE Guide. No. 87, Medical Teacher, 36:6, 463-474, DOI:10.3109/0142159X.2014.889814 To link to this article: https://doi.org/10.3109/0142159X.2014.889814\n\nNaomi B. Robbins, Richard M. Heiberger. Plotting Likert and Other Rating Scales. Section on Survey Research Methods – JSM 2011. PDF Available here",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#additional-readings",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#additional-readings",
    "title": "\n Surveys",
    "section": "\n Additional Readings",
    "text": "Additional Readings\n\nDaniel Lüdecke. (2024-05-13). Plotting Likert Scales with sjPlot. https://cran.r-project.org/web/packages/sjPlot/vignettes/plot_likert_scales.html\n\nJoseph Larmarange. Plot Likert-type items with gglikert(). https://cran.r-project.org/web/packages/ggstats/vignettes/gglikert.html\n\nPiping Hot Data. Leveraging Labelled Data in R. https://www.pipinghotdata.com/posts/2020-12-23-leveraging-labelled-data-in-r/\\\n\nBangdiwala, S.I., Shankar, V. The agreement chart. BMC Med Res Methodol 13, 97 (2013). https://doi.org/10.1186/1471-2288-13-97. Open Access.\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggstats\n0.10.0\nLarmarange (2025a)\n\n\nlabelled\n2.14.1\nLarmarange (2025b)\n\n\nsjPlot\n2.9.0\nLüdecke (2025)\n\n\n\n\n\n\nLarmarange, Joseph. 2025a. ggstats: Extension to “ggplot2” for Plotting Stats. https://doi.org/10.32614/CRAN.package.ggstats.\n\n\n———. 2025b. labelled: Manipulating Labelled Data. https://doi.org/10.32614/CRAN.package.labelled.\n\n\nLüdecke, Daniel. 2025. sjPlot: Data Visualization for Statistics in Social Science. https://CRAN.R-project.org/package=sjPlot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/45-SurveyData/index.html#footnotes",
    "title": "\n Surveys",
    "section": "Footnotes",
    "text": "Footnotes\n\nPiping Hot Data: Leveraging Labelled Data in R, https://www.pipinghotdata.com/posts/2020-12-23-leveraging-labelled-data-in-r/↩︎\nIntroduction to labelled:https://larmarange.github.io/labelled/articles/intro_labelled.html#using-labelled-with-dplyrmagrittr↩︎\nLabel Support in R:https://cran.r-project.org/web/packages/sjlabelled/index.html↩︎\nUsing the labelr package: https://cran.r-project.org/web/packages/labelr/vignettes/labelr-introduction.html↩︎\nBangdiwala, S.I., Shankar, V. The agreement chart. BMC Med Res Methodol 13, 97 (2013). https://doi.org/10.1186/1471-2288-13-97↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"wpf:survey\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Surveys"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html",
    "title": "\n Groups",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“In keeping silent about evil, in burying it so deep within us that no sign of it appears on the surface, we are implanting it, and it will rise up a thousand fold in the future.”\n— Aleksandr Solzhenitsyn",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#slides-and-tutorials",
    "title": "\n Groups",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“In keeping silent about evil, in burying it so deep within us that no sign of it appears on the surface, we are implanting it, and it will rise up a thousand fold in the future.”\n— Aleksandr Solzhenitsyn",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#setting-up-r-packages",
    "title": "\n Groups",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(skimr)\nlibrary(visStatistics) # All in one plot + stats test package\nlibrary(palmerpenguins) # Our new favourite dataset\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#what-graphs-will-we-see-today",
    "title": "\n Groups",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nQual\nBox Plot",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Groups",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n\n1\n                  How Many / Much / Heavy? Few? Seldom? Often? When?\n                  Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\n                  Quantitative/Ratio\n                  Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\n                  Correlation\n                \n\n4\n                  What, Who, Where, Whom, Which\n                  Name, Place, Animal, Thing\n                  Qualitative/Nominal\n                  Name\n                  Count no. of cases,Mode\n                \n\n\n\n\n\n\n\n Inspiration\n\n\n\n\n\nFigure 1: Box Plot Inspiration\n\n\nAlice said, “I say what I mean and I mean what I say!” Are the rest of us so sure? What do we mean when we use any of the phrases above? How definite are we? There is a range of “sureness” and “unsureness”…and this is where we can use box plots like Figure 1 to show that range of opinion.\nMaybe it is time for a box plot on uh, shades1 of meaning for Jane Austen Gen-Z phrases! Bah.\n\n How do these Chart(s) Work?\nBox Plots are an extremely useful data visualization that gives us an idea of the distribution of a Quant variable, for each level of another Qual variable.\n\n\n\n\n\n\n\nFigure 2: Box Plot Definitions\n\n\n\n\n\n\n\n\n\nFigure 3: Box Plot Definitions\n\n\n\n\nThe internal process of this plot is as follows:\n(Hat tip to student Tanya Michelle Justin for a good question on outlier calculation)\n\nMake groups of the Quant variable for each level of the Qual\nIn each group, rank the Quant variable values in increasing order\nCalculate:\n\nThe values for median = Q2, Q1, and Q3 based on rank!!\n\nValues for min, max, and then IQR = Q1 - Q3\n\nCalculate outlier limits:\n\n\\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\n\nWhiskers: All values within \\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\nOutliers: All values outside of \\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\n\nPlot these as a vertical or horizontal box structure, as shown.\n\nAs a result of this, while the box-part of the boxplot always shows 2 full quartiles, the whiskers may not stretch through their quartiles, since some values may be outliers on either side.\n\n\n\n\n\n\nNoteRanks and Values\n\n\n\nThe Quant variable is ordered based on the values from min to max. So you could imagine that each value has a rank or sequence number. The min value has \\(rank = 1\\) and the max value has \\(rank = length(var)\\).\n\n\n\n\n\n\n\n\nNoteHistograms and Box Plots\n\n\n\nNote how the histogram that dwells upon the mean and standard deviation, whereas the boxplot focuses on the median and quartiles. The former uses the values of the Quant variable, whereas the latter uses their sequence number or ranks.\n\n\nBox plots are often used for example in HR operations to understand Salary distributions across grades of employees. Marks of students in competitive exams are also declared using Quartiles.\nBox plots can show skew in distributions, with a large number of outliers on one side, as in Figure 4.\n\n\n\n\n\n\n\nFigure 4: Skewed Distributions and Boxplots\n\n\n\n\nIn other cases, there may be no ouliers, but the “bottom” and the “lid” of the box may not be the same size!\n\n\n\n\n\n\n\n\n\n(a) Box Plot and Skewness\n\n\n\n\n\n\n\n\n\n(b) Density and Skewness\n\n\n\n\n\n\nFigure 5: Box Plot Discussions\n\n\nIn the Figure 5, we see the difference between boxplots that show symmetric and skewed distributions. The “lid” and the “bottom” of the box are not of similar width in distributions with significant skewness.\nCompare these with the corresponding Figure 5 (b).",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#inspiration",
    "title": "\n Groups",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: Box Plot Inspiration\n\n\nAlice said, “I say what I mean and I mean what I say!” Are the rest of us so sure? What do we mean when we use any of the phrases above? How definite are we? There is a range of “sureness” and “unsureness”…and this is where we can use box plots like Figure 1 to show that range of opinion.\nMaybe it is time for a box plot on uh, shades1 of meaning for Jane Austen Gen-Z phrases! Bah.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#how-do-these-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#how-do-these-charts-work",
    "title": "\n Groups",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nBox Plots are an extremely useful data visualization that gives us an idea of the distribution of a Quant variable, for each level of another Qual variable.\n\n\n\n\n\n\n\nFigure 2: Box Plot Definitions\n\n\n\n\n\n\n\n\n\nFigure 3: Box Plot Definitions\n\n\n\n\nThe internal process of this plot is as follows:\n(Hat tip to student Tanya Michelle Justin for a good question on outlier calculation)\n\nMake groups of the Quant variable for each level of the Qual\nIn each group, rank the Quant variable values in increasing order\nCalculate:\n\nThe values for median = Q2, Q1, and Q3 based on rank!!\n\nValues for min, max, and then IQR = Q1 - Q3\n\nCalculate outlier limits:\n\n\\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\n\nWhiskers: All values within \\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\nOutliers: All values outside of \\([Q1 - 1.5*IQR, Q2 + 1.5*IQR]\\)\n\n\n\nPlot these as a vertical or horizontal box structure, as shown.\n\nAs a result of this, while the box-part of the boxplot always shows 2 full quartiles, the whiskers may not stretch through their quartiles, since some values may be outliers on either side.\n\n\n\n\n\n\nNoteRanks and Values\n\n\n\nThe Quant variable is ordered based on the values from min to max. So you could imagine that each value has a rank or sequence number. The min value has \\(rank = 1\\) and the max value has \\(rank = length(var)\\).\n\n\n\n\n\n\n\n\nNoteHistograms and Box Plots\n\n\n\nNote how the histogram that dwells upon the mean and standard deviation, whereas the boxplot focuses on the median and quartiles. The former uses the values of the Quant variable, whereas the latter uses their sequence number or ranks.\n\n\nBox plots are often used for example in HR operations to understand Salary distributions across grades of employees. Marks of students in competitive exams are also declared using Quartiles.\nBox plots can show skew in distributions, with a large number of outliers on one side, as in Figure 4.\n\n\n\n\n\n\n\nFigure 4: Skewed Distributions and Boxplots\n\n\n\n\nIn other cases, there may be no ouliers, but the “bottom” and the “lid” of the box may not be the same size!\n\n\n\n\n\n\n\n\n\n(a) Box Plot and Skewness\n\n\n\n\n\n\n\n\n\n(b) Density and Skewness\n\n\n\n\n\n\nFigure 5: Box Plot Discussions\n\n\nIn the Figure 5, we see the difference between boxplots that show symmetric and skewed distributions. The “lid” and the “bottom” of the box are not of similar width in distributions with significant skewness.\nCompare these with the corresponding Figure 5 (b).",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#case-study-1-gss_wages-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#case-study-1-gss_wages-dataset",
    "title": "\n Groups",
    "section": "\n Case Study-1: gss_wages dataset",
    "text": "Case Study-1: gss_wages dataset\nWe will first look at Wage data from the General Social Survey (1974-2018) conducted in the USA, which is used to illustrate wage discrepancies by gender (while also considering respondent occupation, age, and education). This is available on Vincent Arel-Bundock’s superb repository of datasets. Let us read into R directly from the website.\n\n R\n\n\n\nwages &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/stevedata/gss_wages.csv\")\n\nThe data has automatically been read into the webr session, so you can continue on to the next code chunk!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Examine the Data\nAs per our Workflow, we will look at the data using all the three methods we have seen.\n\n\n dplyr\n skimr\n mosaic\n web-r\n\n\n\n\nglimpse(wages)\n\nRows: 61,697\nColumns: 12\n$ rownames   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ year       &lt;dbl&gt; 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974, 1974,…\n$ realrinc   &lt;dbl&gt; 4935, 43178, NA, NA, 18505, 22206, 55515, NA, NA, 4935, NA,…\n$ age        &lt;dbl&gt; 21, 41, 83, 69, 58, 30, 48, 67, 51, 54, 89, 71, 27, 30, 22,…\n$ occ10      &lt;dbl&gt; 5620, 2040, NA, NA, 5820, 910, 230, 6355, 4720, 3940, 4810,…\n$ occrecode  &lt;chr&gt; \"Office and Administrative Support\", \"Professional\", NA, NA…\n$ prestg10   &lt;dbl&gt; 25, 66, NA, NA, 37, 45, 59, 49, 28, 38, 47, 45, 50, 29, 33,…\n$ childs     &lt;dbl&gt; 0, 3, 2, 2, 0, 0, 2, 1, 2, 2, 3, 1, 4, 3, 0, 1, 2, 3, 4, 8,…\n$ wrkstat    &lt;chr&gt; \"School\", \"Full-Time\", \"Housekeeper\", \"Housekeeper\", \"Full-…\n$ gender     &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Female\", \"Male\", \"Male…\n$ educcat    &lt;chr&gt; \"High School\", \"Bachelor\", \"Less Than High School\", \"Less T…\n$ maritalcat &lt;chr&gt; \"Married\", \"Married\", \"Widowed\", \"Widowed\", \"Never Married\"…\n\n\n\n\n\nskim(wages)\n\n\nData summary\n\n\nName\nwages\n\n\nNumber of rows\n61697\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\noccrecode\n3561\n0.94\n5\n37\n0\n11\n0\n\n\nwrkstat\n21\n1.00\n5\n23\n0\n8\n0\n\n\ngender\n0\n1.00\n4\n6\n0\n2\n0\n\n\neduccat\n135\n1.00\n8\n21\n0\n5\n0\n\n\nmaritalcat\n27\n1.00\n7\n13\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrownames\n0\n1.00\n30849.00\n17810.53\n1\n15425\n30849\n46273\n61697.0\n▇▇▇▇▇\n\n\nyear\n0\n1.00\n1996.07\n12.79\n1974\n1985\n1996\n2006\n2018.0\n▆▇▇▇▇\n\n\nrealrinc\n23810\n0.61\n22326.36\n28581.79\n227\n8156\n16563\n27171\n480144.5\n▇▁▁▁▁\n\n\nage\n219\n1.00\n46.18\n17.56\n18\n32\n44\n59\n89.0\n▇▇▆▅▂\n\n\nocc10\n3561\n0.94\n4695.77\n2627.72\n10\n2710\n4720\n6230\n9997.0\n▃▅▇▂▃\n\n\nprestg10\n4186\n0.93\n43.06\n12.99\n16\n33\n42\n50\n80.0\n▃▇▇▃▁\n\n\nchilds\n189\n1.00\n1.92\n1.76\n0\n0\n2\n3\n8.0\n▇▇▂▁▁\n\n\n\n\n\n\n\n\ninspect(wages)\n\n\ncategorical variables:  \n        name     class levels     n missing\n1  occrecode character     11 58136    3561\n2    wrkstat character      8 61676      21\n3     gender character      2 61697       0\n4    educcat character      5 61562     135\n5 maritalcat character      5 61670      27\n                                   distribution\n1 Professional (19%), Service (16.9%) ...      \n2 Full-Time (49.4%), Housekeeper (15.1%) ...   \n3 Female (56.1%), Male (43.9%)                 \n4 High School (51.5%) ...                      \n5 Married (51.7%), Never Married (21.8%) ...   \n\nquantitative variables:  \n      name   class  min    Q1 median    Q3      max         mean           sd\n1 rownames numeric    1 15425  30849 46273  61697.0 30849.000000 17810.534116\n2     year numeric 1974  1985   1996  2006   2018.0  1996.073715    12.794470\n3 realrinc numeric  227  8156  16563 27171 480144.5 22326.359234 28581.794499\n4      age numeric   18    32     44    59     89.0    46.176177    17.561065\n5    occ10 numeric   10  2710   4720  6230   9997.0  4695.774081  2627.724076\n6 prestg10 numeric   16    33     42    50     80.0    43.060701    12.987526\n7   childs numeric    0     0      2     3      8.0     1.923457     1.763569\n      n missing\n1 61697       0\n2 61697       0\n3 37887   23810\n4 61478     219\n5 58136    3561\n6 57511    4186\n7 61508     189\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\nFrom the dataset documentation page, we note that this is a large dataset (61K rows), with 11 variables:\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nyear(dbl): the survey year\n\nrealrinc(dbl): the respondent’s base income (in constant 1986 USD\n\nage(dbl): the respondent’s age in years\n\nocc10(dbl): respondent’s occupation code (2010)\n\nprestg10(dbl): respondent’s occupational prestige score (2010)\n\nchilds(dbl): number of children (0-8)\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\noccrecode(chr): recode of the occupation code into one of 11 main categories\n\n\nwrkstat(chr): the work status of the respondent (full-time, part-time, temporarily not working, unemployed (laid off), retired, school, housekeeper, other). 8 levels. \n\ngender(chr): respondent’s gender (male or female). 2 levels.\n\n\neduccat(chr): respondent’s degree level (Less Than High School, High School, Junior College, Bachelor, or Graduate). 5 levels.\n\n\nmaritalcat(chr): respondent’s marital status (Married, Widowed, Divorced, Separated, Never Married). 5 levels.\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights based on wages dataset\n\n\n\n\nFair amount of missing data; however with 61K rows, we can for the present simply neglect the missing data.\nGood mix of Qual and Quant variables\n\n\n\n\n Hypothesis and Research Questions\n\nThe target variable for an experiment that resulted in this data might be the realinc variable, the resultant income of the individual. Which is numerical variable.\n\n\n\n\n\n\n\nNoteResearch Questions:\n\n\n\n\nWhat is the basic distribution of realrinc?\nIs realrinc affected by gender?\nBy educcat? By maritalcat?\nIs realrinc affected by child?\nDo combinations of these factors have an effect on the target variable?\n\nThese should do for now! But we should make more questions when have seen some plots!\n\n\n\n Data Munging\nSince there are so many missing data in the target variable realinc and there is still enough data leftover, let us remove the rows containing missing data in that variable.\n\n\n\n\n\n\nImportant\n\n\n\nNOTE: This is not advised at all as a general procedure!! Data is valuable and there are better ways to manage this problem!\n\n\n\nwages_clean &lt;-\n  wages %&gt;%\n  tidyr::drop_na(realrinc) # choose column or leave blank to choose all\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#plotting-box-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#plotting-box-plots",
    "title": "\n Groups",
    "section": "\n Plotting Box Plots",
    "text": "Plotting Box Plots\n\n\n Question-1: What is the basic distribution of realrinc?\n\n\n\n\n\n\nNoteQuestion-1: What is the basic distribution of realrinc?\n\n\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(realrinc ~ \"Income\") %&gt;% # Dummy X-axis \"variable\"\n  gf_labs(\n    title = \"Plot 1A: Income has a skewed distribution\",\n    subtitle = \"Many outliers on the high side\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(y = realrinc, x = \"Income\")) + # Dummy X-axis \"variable\"\n  labs(\n    title = \"Plot 1A: Income has a skewed distribution\",\n    subtitle = \"Many outliers on the high side\"\n  )\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-1\n\nIncome is a very skewed distribution, as might be expected.\nPresence of many higher-side outliers is noted.\n\n\n\n\n Question-2: Is realrinc affected by gender?\n\n\n\n\n\n\nNoteQuestion-2: Is realrinc affected by gender?\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nweb-r\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 2A: Income by Gender\")\n\n\n\n\n\n\nSplit by Gender\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ log10(realrinc)) %&gt;%\n  gf_labs(title = \"Plot 2B: Log(Income) by Gender\")\n\n\n\n\n\n\nWith log income\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(gender ~ realrinc, fill = ~gender) %&gt;%\n  gf_refine(scale_x_log10(), scale_fill_brewer(palette = \"Set1\")) %&gt;%\n  gf_labs(title = \"Plot 2C: Income filled by Gender, log scale\")\n\n\n\n\n\n\nWith log scale\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(y = gender, x = realrinc)) +\n  labs(title = \"Plot 2A: Income by Gender\")\n##\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(y = gender, x = log10(realrinc))) +\n  labs(title = \"Plot 2B: Log(Income) by Gender\")\n##\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(y = gender, x = realrinc, fill = gender)) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"Plot 2C: Income filled by Gender, log scale\")\n\n\n\n\n\n\n\n\n\n(a) Split by Gender\n\n\n\n\n\n\n\n\n\n(b) With log income\n\n\n\n\n\n\n\n\n\n(c) With log scale\n\n\n\n\n\n\nFigure 6: Income by Gender\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-2\n\nEven when split by gender, realincome presents a skewed set of distributions.\nThe IQR for males is smaller than the IQR for females. There is less variation in the middle ranges of realrinc for men.\nlog10 transformation helps to view and understand the regions of low realrinc.\nThere are outliers on both sides, indicating that there may be many people who make very small amounts of money and large amounts of money in both genders.\n\n\n\n\n Question-3: Is realrinc affected by educcat?\n\n\n\n\n\n\nNoteQuestion-3: Is realrinc affected by educcat?\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nweb-r\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ realrinc) %&gt;%\n  gf_labs(title = \"Plot 3A: Income by Education Category\")\n\n\n\n\n\n\nSplit by Education Category\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nwages_clean %&gt;%\n  gf_boxplot(educcat ~ log10(realrinc)) %&gt;%\n  gf_labs(title = \"Plot 3B: Log(Income) by Education Category\")\n\n\n\n\n\n\nWith log income\n\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  gf_boxplot(\n    reorder(educcat, realrinc, FUN = median) ~ log(realrinc),\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 3C: Log(Income) by Education Category, sorted\",\n    x = \"Log Income\",\n    y = \"Education Category\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\nWith log income\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nwages_clean %&gt;%\n  gf_boxplot(reorder(educcat, realrinc, FUN = median) ~ realrinc,\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_refine(scale_x_log10()) %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Income by Education Category, sorted\",\n    subtitle = \"Log Income\",\n    x = \"Income\",\n    y = \"Education Category\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\nLog Income Scale\n\n\n\n\n\n\n\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(realrinc, educcat)) + # (x,y) format\n  labs(title = \"Plot 3A: Income by Education Category\")\n##\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(aes(log10(realrinc), educcat)) +\n  labs(title = \"Plot 3B: Log(Income) by Education Category\")\n##\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(\n    aes(log(realrinc),\n      reorder(educcat, realrinc, FUN = median),\n      fill = educcat\n    ),\n    alpha = 0.5\n  ) +\n  labs(\n    title = \"Plot 3C: Log(Income) by Education Category, sorted\",\n    x = \"Log Income\", y = \"Education Category\"\n  )\n##\nwages_clean %&gt;%\n  ggplot() +\n  geom_boxplot(\n    aes(realrinc,\n      reorder(educcat, realrinc, FUN = median),\n      fill = educcat\n    ),\n    alpha = 0.5\n  ) +\n  scale_x_log10() +\n  labs(\n    title = \"Plot 3D: Income by Education Category, sorted\",\n    subtitle = \"Log Income Scale\",\n    x = \"Income\", y = \"Education Category\"\n  )\n\n\n\n\n\n\n\n\n\n(a) Split by Education Category\n\n\n\n\n\n\n\n\n\n(b) With log income\n\n\n\n\n\n\n\n\n\n\n\n(c) With log scale\n\n\n\n\n\n\n\n\n\n(d) Split by Education Category\n\n\n\n\n\n\nFigure 7: Income by Education Category\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-3\n\n\nrealrinc rises with educcat, which is to be expected.\nHowever, there are people with very low and very high income in all categories of educcat\n\nHence educcat alone may not be a good predictor for realrinc.\n\n\n\nWe can do similar work with the other Qual variables. Let us now see how we can use more than one Qual variable and answer the last hypothesis, Question 4.\n\n Question-4: Is the target variable realrinc affected by combinations of Qual factors gender, educcat, maritalcat and childs?\n\n\n\n\n\n\nImportant\n\n\n\nThis is a rather complex question and could take us deep into Modelling. Ideally we ought to:\n\ntake each Qual variable, explain its effect on the target variable\n\n\nremove that effect and model the remainder ( i.e. residual) with the next Qual variable\n\nProceed in this way until we have a good model.\nif we are going to do this manually.\n\nThere are more modern Modelling Workflows, that can do things much faster and without such manual tweaking.\n\n\nSo will simply plot box plots showing effects on the target variable of combinations of Qual variables taken two at a time. (We will of course use facetted box plots!)\nWe will also drop NA values all around this time, to avoid seeing boxplots for undocumented categories.\n\n\n\n\n\n\nNoteQuestion-4: Is realrinc affected by combinations of factors?\n\n\n\n\n\nUsing ggformula\nUsing ggplot\nweb-r\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  gf_boxplot(reorder(educcat, realrinc) ~ log10(realrinc),\n    fill = ~educcat,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(childs)) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Dark2\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4A: Log Income by Education Category and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nFigure 8: Split by Education Category and Family Size\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(childs = as_factor(childs)) %&gt;%\n  gf_boxplot(childs ~ log10(realrinc),\n    group = ~childs,\n    fill = ~childs,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(~gender) %&gt;%\n  gf_refine(scale_fill_brewer(type = \"qual\", palette = \"Set3\")) %&gt;%\n  gf_labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\nFigure 9: Split by Gender and Family Size\n\n\n\n\n\n\n\n\nwages %&gt;%\n  drop_na() %&gt;%\n  ggplot() +\n  geom_boxplot(\n    aes(log10(realrinc), reorder(educcat, realrinc),\n      fill = educcat\n    ), # aes() closes here\n    alpha = 0.5\n  ) +\n  facet_wrap(vars(childs)) +\n  scale_fill_brewer(type = \"qual\", palette = \"Dark2\") +\n  labs(title = \"Plot 4A: Log Income by Education Category and Family Size\", x = \"Log income\", y = \"No. of Children\")\n##\nwages %&gt;%\n  drop_na() %&gt;%\n  mutate(childs = as_factor(childs)) %&gt;%\n  ggplot() +\n  geom_boxplot(\n    aes(log10(realrinc), childs,\n      group = childs,\n      fill = childs\n    ), # aes() closes here\n    alpha = 0.5\n  ) +\n  facet_wrap(vars(gender)) +\n  scale_fill_brewer(type = \"qual\", palette = \"Set3\") +\n  labs(\n    title = \"Plot 4B: Log Income by Gender and Family Size\",\n    x = \"Log income\",\n    y = \"No. of Children\"\n  )\n\n\n\n\n\n\n\n\n\n(a) Split by Education Category and Family Size\n\n\n\n\n\n\n\n\n\n(b) Split by Gender and Family Size\n\n\n\n\n\n\nFigure 10: Income and Other Qual Variables\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-4\n\nFrom Figure 8, we see that realrinc increases with educcat, across (almost) all family sizes childs.\nHowever, this trend breaks a little when family sizes childs is large, say &gt;= 7. Be aware that the data observations for such large families may be sparse and this inference may not be necessarily valid.\nFrom Figure 9, we see that the effect of childs on realrinc is different for each gender! For females, the income steadily drops with the number of children, whereas for males it actually increases up to a certain family size before decreasing again.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#are-the-differences-significant",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#are-the-differences-significant",
    "title": "\n Groups",
    "section": "\n Are the Differences Significant?",
    "text": "Are the Differences Significant?\n\n\n\n\n\n\nImportantHunches and Hypotheses\n\n\n\nIn data analysis, we always want to know2, as in life, how important things are, whether they matter. To do this, we make up hunches, or more precisely, Hypotheses. We make two in fact:\n\\(H_0\\): Nothing is happening;\\(H_a\\): (“a” for Alternate): Something is happening and it is important enough to pay attention to.\nWe then pretend that \\(H_0\\) is true and ask that our data prove us wrong; if it does, we reject \\(H_0\\) in favour of \\(H_a\\).\nThis is a very important idea of Hypothesis Testing which helps you justify your hunch. We will study this when we do Stats Tests for differences between two means(t-tests), and those between more than two means(ANOVA).",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#wait-but-why",
    "title": "\n Groups",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nBox plots are a powerful statistical graphic that give us a combined view of data ranges, quartiles, medians, and outliers.\nBox plots can compare groups within our Quant variable, based on levels of a Qual variable. This is a very common and important task in research!\nIn your design research, you would have numerical Quant data that is accompanied by categorical Qual data pertaining to groups within your target audience.\nAnalyzing for differences in the Quant across levels of the Qual (e.g household expenditure across groups of people) is a vital step in justifying time, effort, and money for further actions in your project. Don’t faff this.\n\nBox plots are ideal for visualizing statistical tests for difference in mean values across groups (t-test and ANOVA). (Even though they plot medians)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#conclusion",
    "title": "\n Groups",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nBox Plots “dwell upon” medians and Quartiles\n\nBox Plots can show distributions of a Quant variable over levels of a Qual variable\nThis allows a comparison of box plots side by side to visibly detect differences in medians and IQRs across such levels.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#your-turn",
    "title": "\n Groups",
    "section": "\n Your Turn",
    "text": "Your Turn\nHere are a couple of datasets that you might want to analyze with box plots:\n\n\n\n\n\n\nNoteInsurance Data\n\n\n\n Download the Insurance data \n\n\n\n\n\n\n\n\nNotePolitical Donations\n\n\n\n Download the Donations data \n\n\n\n\n\n\n\n\nNoteUFO Encounters\n\n\n\n\n\n UFO Sighting data\n\n\nThe data dictionary for this dataset is here at the TidyTuesday Website.. The TidyTuesday Website is a treasure trove of interesting datasets!\n\n\n\n\n\n\n\n\nNoteGPT-based Language detectors are biased against non-native English writers.\n\n\n\n\n\n AI Dectector data\n\n\nWhat story can you tell, and deduction can you make from Figure 11 below? How would you replicate it? What would you add?\n\n\n\n\n\nFigure 11: AI Detectors",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#ai-generated-summary-and-podcast",
    "title": "\n Groups",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis excerpt from “Groups – Applied Metaphors: Learning TRIZ, Complexity, Data/Stats/ML using Metaphors” provides a comprehensive guide to understanding and utilizing box plots for data visualization and analysis. The text explores the purpose, functionality, and application of box plots within the context of exploring relationships between quantitative and qualitative variables. The author illustrates these concepts using a case study of the “gss_wages” dataset, examining wage discrepancies by gender, occupation, age, and education. Through this analysis, the author highlights the effectiveness of box plots in visualizing distributions, identifying outliers, and comparing groups, providing valuable insights into the complexities of data. The text concludes with a call to action, encouraging readers to explore real-world datasets and apply these techniques to uncover hidden trends and patterns within data.\n\nWhat are the relationships between qualitative and quantitative variables in the gss_wages dataset?\nHow do box plots help visualize and understand the distribution of income across different groups?\nWhat insights can be gained by analyzing the impact of multiple qualitative factors on income distribution?\n\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#references",
    "title": "\n Groups",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nBevans, R. (2023, June 22). An Introduction to t Tests | Definitions, Formula and Examples. Scribbr. https://www.scribbr.com/statistics/t-test/\n\nBrown, Angus. (2008). The Strange Origins of the t-test. Physiology News | No. 71 | Summer 2008| https://static.physoc.org/app/uploads/2019/03/22194755/71-a.pdf\n\nStephen T. Ziliak.(2008). Guinnessometrics: The Economic Foundation of “Student’s” t. Journal of Economic Perspectives—Volume 22, Number 4—Fall 2008—Pages 199–216. https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.22.4.199\n\nhttps://quillette.com/2024/08/03/xy-athletes-in-womens-olympic-boxing-paris-2024-controversy-explained-khelif-yu-ting/\nSenefeld JW, Lambelet Coleman D, Johnson PW, Carter RE, Clayburn AJ, Joyner MJ. Divergence in Timing and Magnitude of Testosterone Levels Between Male and Female Youths. JAMA. 2020;324(1):99–101. doi:10.1001/jama.2020.5655. https://jamanetwork.com/journals/jama/fullarticle/2767852\n\nDoriane Lambelet Coleman.(2017) Sex in Sport, 80 Law and Contemporary Problems. Available at: https://scholarship.law.duke.edu/lcp/vol80/iss4/5\n\nDistributome - An Interactive Web-based Resource for Probability Distributions https://distributome.org\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nsn\n2.1.1\nAzzalini (2023)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\ntidyplots\n0.3.1\nEngler (2025)\n\n\ntinyplot\n0.4.2\nMcDermott, Arel-Bundock, and Zeileis (2025)\n\n\ntinytable\n0.10.0\nArel-Bundock (2025)\n\n\nvisStatistics\n0.1.7\nSchilling (2025)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nArel-Bundock, Vincent. 2025. tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,” “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats. https://doi.org/10.32614/CRAN.package.tinytable.\n\n\nAzzalini, Azzalini A. 2023. The R Package sn: The Skew-Normal and Related Distributions Such as the Skew-\\(t\\) and the SUN (Version 2.1.1). Università degli Studi di Padova, Italia. https://cran.r-project.org/package=sn.\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://doi.org/10.32614/CRAN.package.visualize.\n\n\nEngler, Jan Broder. 2025. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” iMeta, e70018. https://doi.org/10.1002/imt2.70018.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nMcDermott, Grant, Vincent Arel-Bundock, and Achim Zeileis. 2025. tinyplot: Lightweight Extension of the Base r Graphics System. https://doi.org/10.32614/CRAN.package.tinyplot.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSchilling, Sabine. 2025. visStatistics: Automated Selection and Visualisation of Statistical Hypothesis Tests. https://doi.org/10.32614/CRAN.package.visStatistics.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/index.html#footnotes",
    "title": "\n Groups",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe term throwing a shade can be found in Jane Austen’s novel Mansfield Park (1814). Young Edmund Bertram is displeased with a dinner guest’s disparagement of the uncle who took her in: “With such warm feelings and lively spirits it must be difficult to do justice to her affection for Mrs. Crawford, without throwing a shade on the Admiral.”↩︎\n“Ah, Misha, he has a stormy spirit. His mind is in bondage. He is haunted by a great, unsolved doubt. He is one of those who don’t want millions, but an answer to their questions.” ― Fyodor Dostoevsky, The Brothers Karamazov: A Novel in Four Parts With Epilogue↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"lucide:group\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html",
    "title": "Tutorial on Evolutions and Flow",
    "section": "",
    "text": "Tutorial Content to be written up when Arvind has time !!!\n\n\n\n\n\n Back to topCitationBibTeX citation:@online{venkatadri,\n  author = {Venkatadri, Arvind},\n  title = {Tutorial on {Evolutions} and {Flow}},\n  url = {https://madhatterguide.netlify.app/content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nVenkatadri, Arvind. n.d. “Tutorial on Evolutions and Flow.”\nhttps://madhatterguide.netlify.app/content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/files/evolutions.html."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html",
    "title": "\n Space",
    "section": "",
    "text": "Spatial Data\nStatic Maps\nInteractive Maps with Leaflet\nInteractive Maps with Mapview\n\n\nSpatialData \n\nStaticMaps \n Interactive Mapswith leaflet\n Interactive Maps with mapview\n\n\n\n\n\n“If we were to wake up some morning and find that everyone was the same race, creed, and color, we would find some other cause for prejudice by noon.”\n— George D. Aiken, US senator (20 Aug 1892-1984)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#slides-and-tutorials",
    "title": "\n Space",
    "section": "",
    "text": "Spatial Data\nStatic Maps\nInteractive Maps with Leaflet\nInteractive Maps with Mapview\n\n\nSpatialData \n\nStaticMaps \n Interactive Mapswith leaflet\n Interactive Maps with mapview\n\n\n\n\n\n“If we were to wake up some morning and find that everyone was the same race, creed, and color, we would find some other cause for prejudice by noon.”\n— George D. Aiken, US senator (20 Aug 1892-1984)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#setting-up-r-packages",
    "title": "\n Space",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(tmaptools)\n# install.packages(\"remotes\")\n# remotes::install_github(\"r-tmap/tmap.mapgl\")\nlibrary(tmap.mapgl) # Free mapgl maps\nlibrary(osmdata)\nlibrary(rnaturalearth)\n## Interactive Maps\nlibrary(leaflet)\nlibrary(leaflet.providers)\nlibrary(leaflet.extras)\n\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#what-graphs-will-we-see-today",
    "title": "\n Space",
    "section": "What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQual\nChoropleth and Symbols Maps, Cartograms\n\n{{&lt; iconify gis statistic-map size=4x &gt;}}",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#inspiration",
    "title": "\n Space",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n(a) Infosys in the EU\n\n\n\n\n\n\n\n\n\n(b) Population Cartogram\n\n\n\n\n\n\nFigure 1: Choropleth and Cartogram\n\n\n\n\n\n\n\n\n\n\n\n(a) Where’s the next Houthi attack?\n\n\n\n\n\n\n\n\n\n(b) US Cities\n\n\n\n\n\n\nFigure 2: Symbol Maps",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#how-do-these-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#how-do-these-charts-work",
    "title": "\n Space",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nIn Figure 1 (a), we have a choropleth map. What does choropleth1 mean? And what kind of information could this map represent? The idea is to colour a specific area of the map, a district or state, based on a Quant or a Qual variable.\nThe Figure 1 (b) deliberately distorts and scales portions of the map in proportion to a Quant variable, in this case, population in 2018.\nIn Figure 2 (a) and Figure 2 (b), symbols are used to indicate either the location/presence of an item of interest, or a quantity by scaling their size in proportion to a Quant variable",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#introduction",
    "title": "\n Space",
    "section": "\n Introduction",
    "text": "Introduction\nFirst; let us watch a short, noisy video on maps:",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#what-kind-of-visualizations-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#what-kind-of-visualizations-will-we-make",
    "title": "\n Space",
    "section": "\n What kind of visualizations will we make?",
    "text": "What kind of visualizations will we make?\nLet us first understand the idea of a Geographical Information System, GIS: \n\nWe will first understand the structure of spatial data and where to find it. For now, we will deal with vector spatial data; the discussion on raster data will be dealt with in another future module.\nWe will get hands-on with making maps, both static and interactive.\n\n Choropleth Map\n\n\n\n Bubble Map\nWhat information could this map below represent?\n\n\nLet us now look at the slides. Then we will understand how the R packages sf, tmap work to create maps, using data downloadable into R using osmdata and osmplotr. We will also make interactive maps with leaflet and mapview; tmap is also capable of creating interactive maps.\n    View slides in full screen",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#your-turn",
    "title": "\n Space",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n Animal and Bird Migration\n\nHead off to movebank.org. Look at a few species of interest and choose one.\nDownload the data ( ESRI Shapefile). Note: You will get a .zip file with a good many files in it. Save all of them, but read only the .shp file into R.\nImport that into R using sf_read()\n\nSee how you can plot locations, tracks and colour by species….based on the data you download.\nFor tutorial info: https://movebankworkshopraleighnc.netlify.app/\n\n\n UFO Sightings\nHere is a UFO Sighting dataset, containing location and text descriptions. https://github.com/planetsig/ufo-reports/blob/master/csv-data/ufo-scrubbed-geocoded-time-standardized.csv\n\n Sales Data from kaggle\nHead off to Kaggle and search for Geographical Sales related data. Make both static and interactive maps with this data. Justify your decisions for type of map.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#references",
    "title": "\n Space",
    "section": "\n References",
    "text": "References\n\nHadley Wickham, Danielle Navarro and Thomas Lin Pedersen. ggplot2: Elegant Graphics for Data Analysis, https://ggplot2-book.org/maps.html\n\nMartijn Tennekes and Jakub Nowosad (2025). Elegant and informative maps with tmap. https://tmap.geocompx.org\n\nRobin Lovelace, Jakub Nowosad, Jannes Muenchow. Geocomputation with R. https://r.geocompx.org/\n\nEmine Fidan. Guide to Creating Interactive Maps in R, https://bookdown.org/eneminef/DRR_Bookdown/\n\nNikita Voevodin. R, Not the Best Practices, https://bookdown.org/voevodin_nv/R_Not_the_Best_Practices/maps.html\n\nWant to make a cute logo-like map? Try https://prettymapp.streamlit.app\n\nFree Map Tile services. https://alexurquhart.github.io/free-tiles/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nleaflet\n2.2.2\nCheng et al. (2024)\n\n\nosmdata\n0.2.5\nMark Padgham et al. (2017)\n\n\nrnaturalearth\n1.0.1\nMassicotte and South (2023)\n\n\nsf\n1.0.21\n\nPebesma (2018); Pebesma and Bivand (2023)\n\n\n\ntmap\n4.1\nTennekes (2018)\n\n\n\n\n\n\nCheng, Joe, Barret Schloerke, Bhaskar Karambelkar, and Yihui Xie. 2024. leaflet: Create Interactive Web Maps with the JavaScript “Leaflet” Library. https://doi.org/10.32614/CRAN.package.leaflet.\n\n\nMark Padgham, Bob Rudis, Robin Lovelace, and Maëlle Salmon. 2017. “Osmdata.” Journal of Open Source Software 2 (14): 305. https://doi.org/10.21105/joss.00305.\n\n\nMassicotte, Philippe, and Andy South. 2023. rnaturalearth: World Map Data from Natural Earth. https://doi.org/10.32614/CRAN.package.rnaturalearth.\n\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With applications in R. Chapman and Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nTennekes, Martijn. 2018. “tmap: Thematic Maps in R.” Journal of Statistical Software 84 (6): 1–39. https://doi.org/10.18637/jss.v084.i06.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/90-Space/index.html#footnotes",
    "title": "\n Space",
    "section": "Footnotes",
    "text": "Footnotes\n\nEtymology. From Ancient Greek χώρα (khṓra, “location”) + πλῆθος (plêthos, “a great number”) + English map. First proposed in 1938 by American geographer John Kirtland Wright to mean “quantity in area,” although maps of the type have been used since the early 19th century.↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"gis:proj-geo\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Space"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html",
    "title": "\n Quantities",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“The fear of death follows from the fear of life. A man who lives fully is prepared to die at any time.”\n— Mark Twain",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#slides-and-tutorials",
    "title": "\n Quantities",
    "section": "",
    "text": "R (Static Viz)  \n\n  Radiant Tutorial \n  Datasets\n\n\n\n\n“The fear of death follows from the fear of life. A man who lives fully is prepared to die at any time.”\n— Mark Twain",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#setting-up-r-packages",
    "title": "\n Quantities",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(skimr)\n##\nlibrary(crosstable) # Fast stats for multiple variables in table form\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\n## ggplot theme\n# library(hrbrthemes)\n# hrbrthemes::import_roboto_condensed() # Import Roboto Condensed font for use in charts\n# hrbrthemes::update_geom_font_defaults() #Update matching font defaults for text geoms\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#what-graphs-will-we-see-today",
    "title": "\n Quantities",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Quantities",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n1\n                  How Many / Much / Heavy? Few? Seldom? Often? When?\n                  Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\n                  Quantitative/Ratio\n                  Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\n                  Correlation",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#inspiration",
    "title": "\n Quantities",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Golf Drive Distance over the years\n\n\nWhat do we see here? In about two-and-a-half decades, golf drive distances have increased, on the average, by 35 yards. The maximum distance has also gone up by 30 yards, and the minimum is now at 250 yards, which was close to average in 1983! What was a decent average in 1983 is just the bare minimum in 2017!!\nIs it the dimples that the golf balls have? But these have been around a long time…or is it the clubs, and the swing technique invented by more recent players?\nNow, let us listen to the late great Hans Rosling from the Gapminder Project, which aims at telling stories of the world with data, to remove systemic biases about poverty, income and gender related issues.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#how-do-histograms-work",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#how-do-histograms-work",
    "title": "\n Quantities",
    "section": "\n How do Histograms Work?",
    "text": "How do Histograms Work?\nHistograms are best to show the distribution of raw Quantitative data, by displaying the number of values that fall within defined ranges, often called buckets or bins. We use a Quant variable on the x-axis and the histogram shows us how frequently different values occur for that variable by showing counts/frequencies on the y-axis. The x-axis is typically broken up into “buckets” or ranges for the x-variable. And usually you can adjust the bucket ranges to explore frequency patterns. For example, you can widen histogram buckets from 0-1, 1-2, 2-3, etc. to 0-2, 2-4, etc.\nAlthough Bar Charts may look similar to Histograms, the two are different. Bar Charts show counts of observations with respect to a Qualitative variable. For instance, bar charts show categorical data with multiple levels, such as fruits, clothing, household products in an inventory. Each bar has a height proportional to the count per shirt-size, in this example.\nHistograms do not usually show spaces between buckets because the buckets represent contiguous ranges, while bar charts show spaces to separate each (unconnected) category/level within a Qual variable.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#case-study-1-diamonds-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#case-study-1-diamonds-dataset",
    "title": "\n Quantities",
    "section": "\n Case Study-1: diamonds dataset",
    "text": "Case Study-1: diamonds dataset\nWe will first look at at a dataset that is directly available in R, the diamonds dataset.\n\n Examine the Data\nAs per our Workflow, we will look at the data using all the three methods we have seen.\n\n\n dplyr::glimpse\n skimr::skim\n mosaic::inspect\n web-r\n\n\n\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\n\n\nskim(diamonds)\n\n\nData summary\n\n\nName\ndiamonds\n\n\nNumber of rows\n53940\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n7\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\ncut\n0\n1\nTRUE\n5\nIde: 21551, Pre: 13791, Ver: 12082, Goo: 4906\n\n\ncolor\n0\n1\nTRUE\n7\nG: 11292, E: 9797, F: 9542, H: 8304\n\n\nclarity\n0\n1\nTRUE\n8\nSI1: 13065, VS2: 12258, SI2: 9194, VS1: 8171\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ncarat\n0\n1\n0.80\n0.47\n0.2\n0.40\n0.70\n1.04\n5.01\n▇▂▁▁▁\n\n\ndepth\n0\n1\n61.75\n1.43\n43.0\n61.00\n61.80\n62.50\n79.00\n▁▁▇▁▁\n\n\ntable\n0\n1\n57.46\n2.23\n43.0\n56.00\n57.00\n59.00\n95.00\n▁▇▁▁▁\n\n\nprice\n0\n1\n3932.80\n3989.44\n326.0\n950.00\n2401.00\n5324.25\n18823.00\n▇▂▁▁▁\n\n\nx\n0\n1\n5.73\n1.12\n0.0\n4.71\n5.70\n6.54\n10.74\n▁▁▇▃▁\n\n\ny\n0\n1\n5.73\n1.14\n0.0\n4.72\n5.71\n6.54\n58.90\n▇▁▁▁▁\n\n\nz\n0\n1\n3.54\n0.71\n0.0\n2.91\n3.53\n4.04\n31.80\n▇▁▁▁▁\n\n\n\n\n\n\n\n\ninspect(diamonds)\n\n\ncategorical variables:  \n     name   class levels     n missing\n1     cut ordered      5 53940       0\n2   color ordered      7 53940       0\n3 clarity ordered      8 53940       0\n                                   distribution\n1 Ideal (40%), Premium (25.6%) ...             \n2 G (20.9%), E (18.2%), F (17.7%) ...          \n3 SI1 (24.2%), VS2 (22.7%), SI2 (17%) ...      \n\nquantitative variables:  \n   name   class   min     Q1  median      Q3      max         mean           sd\n1 carat numeric   0.2   0.40    0.70    1.04     5.01    0.7979397    0.4740112\n2 depth numeric  43.0  61.00   61.80   62.50    79.00   61.7494049    1.4326213\n3 table numeric  43.0  56.00   57.00   59.00    95.00   57.4571839    2.2344906\n4 price integer 326.0 950.00 2401.00 5324.25 18823.00 3932.7997219 3989.4397381\n5     x numeric   0.0   4.71    5.70    6.54    10.74    5.7311572    1.1217607\n6     y numeric   0.0   4.72    5.71    6.54    58.90    5.7345260    1.1421347\n7     z numeric   0.0   2.91    3.53    4.04    31.80    3.5387338    0.7056988\n      n missing\n1 53940       0\n2 53940       0\n3 53940       0\n4 53940       0\n5 53940       0\n6 53940       0\n7 53940       0\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\n\n\n\n\n\n\n\nFigure 2: Diamond Dimensions\n\n\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\ncarat(dbl): weight of the diamond 0.2-5.01\n\ndepth(dbl): depth total depth percentage 43-79\n\ntable(dbl): width of top of diamond relative to widest point 43-95\n\nprice(dbl): price in US dollars $326-$18,823\n\nx(dbl): length in mm 0-10.74\n\ny(dbl): width in mm 0-58.9\n\nz(dbl): depth in mm 0-31.8\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\ncut: diamond cut Fair, Good, Very Good, Premium, Ideal\n\n\ncolor: diamond color J (worst) to D (best). (7 levels)\n\n\nclarity. measurement of how clear the diamond is I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best).\n\nThese have 5, 7, and 8 levels respectively. The fact that the class for these is ordered suggests that these are factors and that the levels have a sequence/order.\n\n\n\n\n\n\n\n\nNoteBusiness Insights on Examining the diamonds dataset\n\n\n\n\nThis is a large dataset (54K rows).\nThere are several Qualitative variables:\n\ncarat, price, x, y, z, depth and table are Quantitative variables.\nThere are no missing values for any variable, all are complete with 54K entries.\n\n\n\n\n Data Munging\nWe will not do any data munging for this dataset, as it is already clean and ready to use.\n\n Hypothesis and Research Questions\nLet us formulate a few Questions about this dataset. At some point, we might develop a hunch or two, and these would become our hypotheses to investigate. This is an iterative process!\n\n\n\n\n\n\nNoteHypothesis and Research Questions\n\n\n\n\nThe target variable for an experiment that resulted in this data might be the price variable. Which is a numerical Quant variable.\n\nThere are also predictor variables such as carat (Quant), color(Qual), cut(Qual), and clarity(Qual).\n\nOther predictor variables might be x, y, depth, table(all Quant)\n\nResearch Questions:\n\nWhat is the distribution of the target variable price?\nWhat is the distribution of the predictor variable carat?\nDoes a price distribution vary based upon type of cut, clarity, and color?\n\n\n\nThese should do for now. Try and think of more Questions!\n\n\n\n Plotting Histograms\nLet’s plot some histograms to answer each of the Hypothesis questions above.\n\n\n Question-1: What is the distribution of the target variable price?\n\n\n\n\n\n\nNoteQuestion-1: What is the distribution of the target variable price?\n\n\n\n\n\nggformula-1\nggplot-1\n web-r-1\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\ngf_histogram(~price, data = diamonds) %&gt;%\n  gf_labs(\n    title = \"Plot 1A: Diamond Prices\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\n## More bins\ngf_histogram(~price,\n  data = diamonds,\n  bins = 100\n) %&gt;%\n  gf_labs(\n    title = \"Plot 1B: Diamond Prices\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\n\nggplot(data = diamonds) +\n  geom_histogram(aes(x = price)) +\n  labs(\n    title = \"Plot 1A: Diamond Prices\",\n    caption = \"ggplot\"\n  )\n\n\n\n\n\n\n## More bins\nggplot(data = diamonds) +\n  geom_histogram(aes(x = price), bins = 100) +\n  labs(\n    title = \"Plot 1B: Diamond Prices\",\n    caption = \"ggplot\"\n  )\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-1\n\nThe price distribution is heavily skewed to the right.\n\nThere are a great many diamonds at relatively low prices, but there are a good few diamonds at very high prices too.\n\nUsing a high number of bins does not materially change the view of the histogram.\n\n\n\n\n\n Question-2: What is the distribution of the predictor variable carat?\n\n\n\n\n\n\nNoteQuestion-1: Question-2: What is the distribution of the predictor variable carat?\n\n\n\n\n\nggformula-2\nggplot-2\n web-r-2\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\ndiamonds %&gt;%\n  gf_histogram(~carat) %&gt;%\n  gf_labs(\n    title = \"Plot 2A: Carats of Diamonds\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\n## More bins\ndiamonds %&gt;%\n  gf_histogram(~carat,\n    bins = 100\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 2B: Carats of Diamonds\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\ndiamonds %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = carat)) +\n  labs(\n    title = \"Plot 2A: Carats of Diamonds\",\n    caption = \"ggplot\"\n  )\n## More bins\ndiamonds %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = carat), bins = 100) +\n  labs(\n    title = \"Plot 2A: Carats of Diamonds\",\n    caption = \"ggplot\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-2\n\n\ncarat also has a heavily right-skewed distribution.\n\nHowever, there is a marked “discreteness” to the distribution. Some values of carat are far more common than others. For example, 1, 1.5, and 2 carat diamonds are large in number.\n\nWhy does the X-axis extend up to 5 carats? There must be some, very few, diamonds of very high carat value!\n\n\n\n\n Question-3: Does a price distribution vary based upon type of cut, clarity, and color?\n\n\n\n\n\n\nNoteDoes a price distribution vary based upon type of cut, clarity, and color?\n\n\n\n\n\nggformula-3\nggplot-3\n web-r-3\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\n##\ngf_histogram(~price, fill = ~cut, data = diamonds) %&gt;%\n  gf_labs(title = \"Plot 3A: Diamond Prices\", caption = \"ggformula\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~cut, color = \"black\", alpha = 0.3) %&gt;%\n  gf_labs(\n    title = \"Plot 3B: Prices by Cut\",\n    caption = \"ggformula\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~cut, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~cut) %&gt;%\n  gf_labs(\n    title = \"Plot 3C: Prices by Filled and Facetted by Cut\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x = element_text(\n      angle = 45,\n      hjust = 1\n    )\n  ))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\ndiamonds %&gt;%\n  gf_histogram(~price, fill = ~cut, color = \"black\", alpha = 0.3) %&gt;%\n  gf_facet_wrap(~cut, scales = \"free_y\", nrow = 2) %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Prices Filled and Facetted by Cut\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggformula\"\n  ) %&gt;%\n  gf_theme(theme(\n    axis.text.x =\n      element_text(\n        angle = 45,\n        hjust = 1\n      )\n  ))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\ndiamonds %&gt;% ggplot() +\n  geom_histogram(aes(x = price, fill = cut), alpha = 0.3) +\n  labs(title = \"Plot 3A: Prices by Cut\", caption = \"ggplot\")\n##\ndiamonds %&gt;%\n  ggplot() +\n  geom_histogram(aes(x = price, fill = cut),\n    colour = \"black\", alpha = 0.3\n  ) +\n  labs(title = \"Plot 3B: Prices filled by Cut\", caption = \"ggplot\")\n##\ndiamonds %&gt;% ggplot() +\n  geom_histogram(aes(price, fill = cut),\n    colour = \"black\", alpha = 0.3\n  ) +\n  facet_wrap(facets = vars(cut)) +\n  labs(\n    title = \"Plot 3C: Prices by Filled and Facetted by Cut\",\n    caption = \"ggplot\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n##\ndiamonds %&gt;% ggplot() +\n  geom_histogram(aes(price, fill = cut),\n    colour = \"black\", alpha = 0.3\n  ) +\n  facet_wrap(facets = vars(cut), scales = \"free_y\") +\n  labs(\n    title = \"Plot D: Prices by Filled and Facetted by Cut\",\n    subtitle = \"Free y-scale\",\n    caption = \"ggplot\"\n  ) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-3\n\nThe price distribution is heavily skewed to the right AND This long-tailed nature of the histogram holds true regardless of the cut of the diamond.\nSee the x-axis range for each plot in Plot D! Price ranges are the same regardless of cut !! Very surprising! So cut is perhaps not the only thing that determines price…\nFacetting the plot into small multiples helps look at patterns better: overlapping histograms are hard to decipher. Adding color defines the bars in the histogram very well.\n\n\n\n\n\n\n\n\n\nImportantA Hypothesis\n\n\n\nThe surprise insight above should lead you to make a Hypothesis! You should decide whether you want to investigate this question further, making more graphs, as we will see. Here, we are making a Hypothesis that more than just cut determines the price of a diamond.\n\n\n\n\nAn Interactive App for Histograms\nType in your Console:\n\n```{r}\n#| eval: false\ninstall.packages(\"shiny\")\nlibrary(shiny)\nrunExample(\"01_hello\") # an interactive histogram\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#case-study-2-race-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#case-study-2-race-dataset",
    "title": "\n Quantities",
    "section": "\n Case Study-2: race dataset",
    "text": "Case Study-2: race dataset\n\n Import data\nThese data come from the TidyTuesday, project, a weekly social learning project dedicated to gaining practical experience with R and data science. In this case the TidyTuesday data are based on International Trail Running Association (ITRA) data but inspired by Benjamin Nowak. We will use the TidyTuesday data that are on GitHub. Nowak’s data are also available on GitHub.\n\n R\n\n\n\nrace_df &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/race.csv\")\nrank_df &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-26/ultra_rankings.csv\")\n\nThe data has automatically been read into the webr session, so you can continue on to the next code chunk!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Examine the race Data\nLet us look at the dataset using all our three methods:\n\n\n dplyr\n skimr\n mosaic\n crosstable\n web-r\n\n\n\n\nglimpse(race_df)\n\nRows: 1,207\nColumns: 13\n$ race_year_id   &lt;dbl&gt; 68140, 72496, 69855, 67856, 70469, 66887, 67851, 68241,…\n$ event          &lt;chr&gt; \"Peak District Ultras\", \"UTMB®\", \"Grand Raid des Pyréné…\n$ race           &lt;chr&gt; \"Millstone 100\", \"UTMB®\", \"Ultra Tour 160\", \"PERSENK UL…\n$ city           &lt;chr&gt; \"Castleton\", \"Chamonix\", \"vielle-Aure\", \"Asenovgrad\", \"…\n$ country        &lt;chr&gt; \"United Kingdom\", \"France\", \"France\", \"Bulgaria\", \"Turk…\n$ date           &lt;date&gt; 2021-09-03, 2021-08-27, 2021-08-20, 2021-08-20, 2021-0…\n$ start_time     &lt;time&gt; 19:00:00, 17:00:00, 05:00:00, 18:00:00, 18:00:00, 17:0…\n$ participation  &lt;chr&gt; \"solo\", \"Solo\", \"solo\", \"solo\", \"solo\", \"solo\", \"solo\",…\n$ distance       &lt;dbl&gt; 166.9, 170.7, 167.0, 164.0, 159.9, 159.9, 163.8, 163.9,…\n$ elevation_gain &lt;dbl&gt; 4520, 9930, 9980, 7490, 100, 9850, 5460, 4630, 6410, 31…\n$ elevation_loss &lt;dbl&gt; -4520, -9930, -9980, -7500, -100, -9850, -5460, -4660, …\n$ aid_stations   &lt;dbl&gt; 10, 11, 13, 13, 12, 15, 5, 8, 13, 23, 13, 5, 12, 15, 0,…\n$ participants   &lt;dbl&gt; 150, 2300, 600, 150, 0, 300, 0, 200, 120, 100, 300, 50,…\n\nglimpse(rank_df)\n\nRows: 137,803\nColumns: 8\n$ race_year_id    &lt;dbl&gt; 68140, 68140, 68140, 68140, 68140, 68140, 68140, 68140…\n$ rank            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, NA, NA, NA,…\n$ runner          &lt;chr&gt; \"VERHEUL Jasper\", \"MOULDING JON\", \"RICHARDSON Phill\", …\n$ time            &lt;chr&gt; \"26H 35M 25S\", \"27H 0M 29S\", \"28H 49M 7S\", \"30H 53M 37…\n$ age             &lt;dbl&gt; 30, 43, 38, 55, 48, 31, 55, 40, 47, 29, 48, 47, 52, 49…\n$ gender          &lt;chr&gt; \"M\", \"M\", \"M\", \"W\", \"W\", \"M\", \"W\", \"W\", \"M\", \"M\", \"M\",…\n$ nationality     &lt;chr&gt; \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\", \"GBR\"…\n$ time_in_seconds &lt;dbl&gt; 95725, 97229, 103747, 111217, 117981, 118000, 120601, …\n\n\n\n\n\nskim(race_df)\n\n\nData summary\n\n\nName\nrace_df\n\n\nNumber of rows\n1207\n\n\nNumber of columns\n13\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nDate\n1\n\n\ndifftime\n1\n\n\nnumeric\n6\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nevent\n0\n1.00\n4\n57\n0\n435\n0\n\n\nrace\n0\n1.00\n3\n63\n0\n371\n0\n\n\ncity\n172\n0.86\n2\n30\n0\n308\n0\n\n\ncountry\n4\n1.00\n4\n17\n0\n60\n0\n\n\nparticipation\n0\n1.00\n4\n5\n0\n4\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\ndate\n0\n1\n2012-01-14\n2021-09-03\n2017-09-30\n711\n\n\nVariable type: difftime\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\nstart_time\n0\n1\n0 secs\n82800 secs\n05:00:00\n39\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrace_year_id\n0\n1\n27889.65\n20689.90\n2320\n9813.5\n23565.0\n42686.00\n72496.0\n▇▃▃▂▂\n\n\ndistance\n0\n1\n152.62\n39.88\n0\n160.1\n161.5\n165.15\n179.1\n▁▁▁▁▇\n\n\nelevation_gain\n0\n1\n5294.79\n2872.29\n0\n3210.0\n5420.0\n7145.00\n14430.0\n▅▇▇▂▁\n\n\nelevation_loss\n0\n1\n-5317.01\n2899.12\n-14440\n-7206.5\n-5420.0\n-3220.00\n0.0\n▁▂▇▇▅\n\n\naid_stations\n0\n1\n8.63\n7.63\n0\n0.0\n9.0\n14.00\n56.0\n▇▆▁▁▁\n\n\nparticipants\n0\n1\n120.49\n281.83\n0\n0.0\n21.0\n150.00\n2900.0\n▇▁▁▁▁\n\n\n\n\n\n\nskim(rank_df)\n\n\nData summary\n\n\nName\nrank_df\n\n\nNumber of rows\n137803\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nrunner\n0\n1.00\n3\n52\n0\n73629\n0\n\n\ntime\n17791\n0.87\n8\n11\n0\n72840\n0\n\n\ngender\n30\n1.00\n1\n1\n0\n2\n0\n\n\nnationality\n0\n1.00\n3\n3\n0\n133\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrace_year_id\n0\n1.00\n26678.70\n20156.18\n2320\n8670\n21795\n40621\n72496\n▇▃▃▂▂\n\n\nrank\n17791\n0.87\n253.56\n390.80\n1\n31\n87\n235\n1962\n▇▁▁▁▁\n\n\nage\n0\n1.00\n46.25\n10.11\n0\n40\n46\n53\n133\n▁▇▂▁▁\n\n\ntime_in_seconds\n17791\n0.87\n122358.26\n37234.38\n3600\n96566\n114167\n148020\n296806\n▁▇▆▁▁\n\n\n\n\n\n\n\nWe can also try our new friend mosaic::favstats:\n\nrace_df %&gt;%\n  favstats(~distance, data = .)\n\n\n  \n\n\n##\nrace_df %&gt;%\n  favstats(~participants, data = .)\n\n\n  \n\n\n##\nrank_df %&gt;%\n  drop_na() %&gt;%\n  favstats(time_in_seconds ~ gender, data = .)\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nNoteIntroducing crosstable\n\n\n\nmosaic::favstats allows to summarise just one variable at a time. On occasion we may need to see summaries of several Quant variables, over levels of Qual variables. This is where the package crosstable is so effective: note how crosstable also conveniently uses the formula interface that we are getting accustomed to.\nWe will find occasion to meet crosstable again when we do Inference.\n\n\n\n## library(crosstable)\ncrosstable(time_in_seconds + age ~ gender, data = rank_df) %&gt;%\n  crosstable::as_flextable()\n\n\n\n\n\n\nlabel\nvariable\ngender\n\n\nM\nW\nNA\n\n\n\n\ntime_in_seconds\nMin / Max\n3600.0 / 2.9e+05\n9191.0 / 3.0e+05\n8131.0 / 2.2e+05\n\n\nMed [IQR]\n1.2e+05 [9.7e+04;1.5e+05]\n1.1e+05 [9.7e+04;1.3e+05]\n1.2e+05 [9.9e+04;1.5e+05]\n\n\nMean (std)\n1.2e+05 (3.8e+04)\n1.2e+05 (3.5e+04)\n1.2e+05 (4.4e+04)\n\n\nN (NA)\n101643 (15073)\n18341 (2716)\n28 (2)\n\n\nage\nMin / Max\n0 / 133.0\n0 / 81.0\n29.0 / 59.0\n\n\nMed [IQR]\n47.0 [40.0;53.0]\n45.0 [39.0;52.0]\n40.5 [36.0;50.5]\n\n\nMean (std)\n46.4 (10.2)\n45.3 (9.7)\n41.7 (9.0)\n\n\nN (NA)\n116716 (0)\n21057 (0)\n30 (0)\n\n\n\n\n\n\n(The as_flextable command from the crosstable package helped to render this elegant HTML table we see. It should be possible to do Word/PDF also, which we might see later.)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\nFrom race_df, we have the following Quantitative variables:\n\n\nrace_year_id: A number uniquely identifying the race event\n\ndistance: Race Distance (miles?)\n\nelevation_gain: Gain in Elevation along the route (feet?)\n\nelevation_loss: Loss in Elevation along the route (feet?)\n\nparticants: No. of participants\n\naid_stations: No. of aid stations along the race route.\n\nAnd from rank_df we have the following Quantitative variables:\n\n\nrank: Placement Rank of the Athlete\n\ntime: Race completion time ( h:m:s)\n\ntime_in_seconds: Race Completion Time in seconds\n\nage: Age of the athlete in years;\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\ncountry: Country of the Race\n\ncity: Location\n\ngender: Of the Athlete\n\nparticipation: Solo / solo, relay, team. Badly coded?.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from race data\n\n\n\n\nWe have two datasets, one for races (race_df) and one for the ranking of athletes (rank_df).\nThere is atleast one common column between the two, the race_year_id variable.\nOverall, there are Qualitative variables such as country, city,gender, and participation. This last variables seems badly coded, with entries showing solo and Solo.\n\nQuantitative variables are rank, time,time_in_seconds, age from rank_df; and distance, elevation_gain, elevation_loss,particants, and aid_stations from race_df.\nWe have 1207 races and over 130K participants! But some races do show zero participants!! Is that an error in data entry?\n\n\n\n\n EDA with race datasets\nSince this dataset is somewhat complex, we may for now just have a detailed set of Questions, that helps us get better acquainted with it. On the way, we may get surprised by some finding then want to go deeper, with a hunch or hypothesis.\n\n Question-1: Max. Races and participants\n\n\n\n\n\n\nNoteQuestion #1\n\n\n\nWhich countries host the maximum number of races? Which countries send the maximum number of participants??\n\n\n R\n web-r\n\n\n\n\nrace_df %&gt;%\n  count(country) %&gt;%\n  arrange(desc(n)) %&gt;%\n  top_n(3, n)\n\n\n  \n\n\nrank_df %&gt;%\n  count(nationality) %&gt;%\n  arrange(desc(n)) %&gt;%\n  top_n(6, n)\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nThe top three locations for races were the USA, UK, and France. These are also the countries that send the maximum number of participants, naturally!\n\n\n\n Question-2: Max. Winners\n\n\n\n\n\n\nNoteQuestion #2\n\n\n\nWhich countries have the maximum number of winners (top 3 ranks)?\n\n\n R\n web-r\n\n\n\n\nrank_df %&gt;%\n  filter(rank %in% c(1, 2, 3)) %&gt;%\n  count(nationality) %&gt;%\n  arrange(desc(n)) %&gt;%\n  top_n(6, n)\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n1240 Participants from the USA have been top 3 finishers. Across all races…\n\n\n\n Question-3: Which countries have had the most top-3 finishes?\n\n\n\n\n\n\nNoteQuestion #3\n\n\n\nWhich countries have had the most top-3 finishes in the longest distance race?\nHere we see we have ranks in one dataset, and race details in another! How do we do this now? We have to join the two data frames into one data frame, using a common variable that uniquely identifies observations in both datasets.\n\n\n R\n web-r\n\n\n\nlongest_races &lt;- race_df %&gt;%\n  slice_max(n = 5, order_by = distance) %&gt;% # Longest distance races\n  select(race_year_id, country, distance) # Select only relevant columns)\nlongest_races\n### Now join this with the `rank_df` dataset\nlongest_races %&gt;%\n  left_join(., rank_df, by = \"race_year_id\") %&gt;% # total participants in longest 4 races\n  filter(rank %in% c(1:10)) %&gt;% # Top 10 ranks\n  count(nationality) %&gt;%\n  arrange(desc(n))\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWow….France has one the top 10 positions 26 times in the longest races… which take place in France, Thailand, Chad, Australia, and Portugal. So although the USA has the greatest number of top 10 finishes, when it comes to the longest races, it is 🇫🇷 vive la France!\n\n\n\n Question-4: What is the distribution of the finishing times?\n\n\n\n\n\n\nNoteQuestion #4\n\n\n\nWhat is the distribution of the finishing times, across all races and all ranks?\n\n\n R\n web-r\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\n##\n\nrank_df %&gt;%\n  gf_histogram(~time_in_seconds, bins = 75) %&gt;%\n  gf_labs(title = \"Histogram of Race Times\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nSo the distribution is (very) roughly bell-shaped, spread over a 2X range. And some people may have dropped out of the race very early and hence we have a small bump close to zero time! The histogram shows three bumps…at least one reason is that the distances to be covered are not the same…but could there be other reasons? Like altitude_gained for example?\n\n\n\n Question-5: What is the distribution of race distances?\n\n\n\n\n\n\nNoteQuestion #5\n\n\n\nWhat is the distribution of race distances?\n\n\n R\n web-r\n\n\n\n\n\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\nrace_df %&gt;%\n  gf_histogram(~distance, bins = 50) %&gt;%\n  gf_labs(title = \"Histogram of Race Distances\")\n\n\n\n\n\n\n\n\n\n\n\n\nHmm…a closely clumped set of race distances, with some entries in between [0-150], but some are zero? Which are these?\n\nrace_df %&gt;%\n  filter(distance == 0)\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nHmm…a closely clumped set of race distances, with some entries in between [0-150], but some are zero? Which are these?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nCurious…some of these zero-distance races have had participants too! Perhaps these were cancelled events…all of them are stated to be 100 mile events…\n\n\n\n Question-6: What is the distribution of finishing times for race distance around 150?\n\n\n\n\n\n\nNoteQuestion #6\n\n\n\nFor all races that have a distance around 150, what is the distribution of finishing times? Can these be split/facetted using start_time of the race (i.e. morning / evening) ?\n\n\n R\n web-r\n\n\n\nLet’s make a count of start times:\n\nrace_times &lt;- race_df %&gt;%\n  count(start_time) %&gt;%\n  arrange(desc(n))\nrace_times\n\n\n  \n\n\n\nLet’s convert start_time into a factor with levels: early_morning(0200:0600), late_morning(0600:1000), midday(1000:1400), afternoon(1400: 1800), evening(1800:2200), and night(2200:0200)\n\n# Demo purposes only!\n\n# ggplot2::theme_set(new = theme_classic(base_family = \"Roboto Condensed\")) # Set consistent graph theme\n\n\nrace_start_factor &lt;- race_df %&gt;%\n  filter(distance == 0) %&gt;% # Races that actually took place\n  mutate(\n    start_day_time =\n      case_when(\n        start_time &gt; hms(\"02:00:00\") &\n          start_time &lt;= hms(\"06:00:00\") ~ \"early_morning\",\n        start_time &gt; hms(\"06:00:01\") &\n          start_time &lt;= hms(\"10:00:00\") ~ \"late_morning\",\n        start_time &gt; hms(\"10:00:01\") &\n          start_time &lt;= hms(\"14:00:00\") ~ \"mid_day\",\n        start_time &gt; hms(\"14:00:01\") &\n          start_time &lt;= hms(\"18:00:00\") ~ \"afternoon\",\n        start_time &gt; hms(\"18:00:01\") &\n          start_time &lt;= hms(\"22:00:00\") ~ \"evening\",\n        start_time &gt; hms(\"22:00:01\") &\n          start_time &lt;= hms(\"23:59:59\") ~ \"night\",\n        start_time &gt;= hms(\"00:00:00\") &\n          start_time &lt;= hms(\"02:00:00\") ~ \"postmidnight\",\n        .default = \"other\"\n      )\n  ) %&gt;%\n  mutate(\n    start_day_time =\n      as_factor(start_day_time) %&gt;%\n        fct_collapse(\n          .f = .,\n          night = c(\"night\", \"postmidnight\")\n        )\n  )\n##\n# Join with rank_df\nrace_start_factor %&gt;%\n  left_join(rank_df, by = \"race_year_id\") %&gt;%\n  drop_na(time_in_seconds) %&gt;%\n  gf_histogram(\n    ~time_in_seconds,\n    bins = 75,\n    fill = ~start_day_time,\n    color = ~start_day_time,\n    alpha = 0.5\n  ) %&gt;%\n  gf_facet_wrap(vars(start_day_time), ncol = 2, scales = \"free_y\") %&gt;%\n  gf_labs(title = \"Race Times by Start-Time\")\n\n\n\n\n\n\n\n\n\nLet’s make a count of start times:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nLet’s convert start_time into a factor with levels: early_morning(0200:0600), late_morning(0600:1000), midday(1000:1400), afternoon(1400: 1800), evening(1800:2200), and night(2200:0200)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nWe see that finish times tend to be longer for afternoon and evening start races; these are lower for early morning and night time starts. Mid-day starts show a curious double hump in finish times that should be studied.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#distributions-and-densities-in-the-wild",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#distributions-and-densities-in-the-wild",
    "title": "\n Quantities",
    "section": "\n Distributions and Densities in the Wild",
    "text": "Distributions and Densities in the Wild\nBefore we conclude, let us look at a real world dataset: populations of countries. This dataset was taken from Kaggle https://www.kaggle.com/datasets/ulrikthygepedersen/populations. Click on the icon below to save the file into a subfolder called data in your project folder:\n Download the Populations data \npop &lt;- read_csv(\"data/populations.csv\")\npop\ninspect(pop)\n\n\n\n\n  \n\n\n\n\n\n\ncategorical variables:  \n          name     class levels     n missing\n1 country_code character    265 16400       0\n2 country_name character    265 16400       0\n                                   distribution\n1 ABW (0.4%), AFE (0.4%), AFG (0.4%) ...       \n2 Afghanistan (0.4%) ...                       \n\nquantitative variables:  \n   name   class  min       Q1  median       Q3        max         mean\n1  year numeric 1960   1975.0    1991     2006       2021 1.990529e+03\n2 value numeric 2646 986302.5 6731400 46024452 7888408686 2.140804e+08\n            sd     n missing\n1 1.789551e+01 16400       0\n2 7.040554e+08 16400       0\n\n\n\nLet us plot densities/histograms for value:\ngf_histogram(~value, data = pop, title = \"Long Tailed Histogram\")\ngf_density(~value, data = pop, title = \"Long Tailed Density\")\n\n\n\n\n\n\n\n\n\n\nThese graphs convey very little to us: the data is very heavily skewed to the right and much of the chart is empty. There are many countries with small populations and a few countries with very large populations. Such distributions are also called “long tailed” distributions. To develop better insights with this data, we should transform the variable concerned, using say a “log” transformation:\ngf_histogram(~ log10(value), data = pop, title = \"Histogram with Log transformed x-variable\")\ngf_density(~ log10(value), data = pop, title = \"Density with Log transformed x-variable\")\n\n\n\n\n\n\n\n\n\n\nBe prepared to transform your data with log or sqrt transformations when you see skewed distributions!\n\n Pareto, Power Laws, and Fat Tailed Distributions\nCity Populations, Sales across product categories, Salaries, Instagram connections, number of customers vs Companies, net worth / valuation of Companies, extreme events on stock markets….all of these could have highly skewed distributions. In such a case, the standard statistics of mean/median/sd may not convey too much information. With such distributions, one additional observation on say net worth, like say Mr Gates’, will change these measures completely. (More when we discuss Sampling)\nSince very large observations are indeed possible, if not highly probable, one needs to look at the result of such an observation and its impact on a situation rather than its (mere) probability. Classical statistical measures and analysis cannot apply with long-tailed distributions. More on this later in the Module on Statistical Inference, but for now, here is a video that talks in detail about fat-tailed distributions, and how one should use them and get used to them:\n\nSeveral distribution shapes exist, here is an illustration of the 6 most common ones:\n\n\n\n\nType_of_Distributions\n\n\n\nWhat insights could you develop based on these distribution shapes?\n\n\nBimodal: Maybe two different systems or phenomena or regimes under which the data unfolds. Like our geyser above. Or a machine that works differently when cold and when hot. Intermittent faulty behaviour…\n\n\nComb: Some specific Observations occur predominantly, in an otherwise even spread or observations. In a survey many respondents round off numbers to nearest 100 or 1000. Check the distribution of the diamonds dataset for carat values which are suspiciously integer numbers in too many cases.\n\n\nEdge Peak: Could even be a data entry artifact!! All unknown / unrecorded observations are recorded as \\(999\\) !!🙀\n\n\nNormal: Just what it says! Course Marks in a Univ cohort…\n\n\nSkewed: Income, or friends count in a set of people. Do UI/UX peasants have more followers on Insta than say CAP people?\n\n\nUniform: The World is not flat. Anything can happen within a range. But not much happens outside! Sharp limits…",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#z-scores",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#z-scores",
    "title": "\n Quantities",
    "section": "\n Z-scores",
    "text": "Z-scores\nOften when we compute wish to compare distributions with different values for means and standard deviations, we resort to a scaling of the variables that are plotted in the respective distributions.\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.9772499\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1] 0.7475075\n\n\n\nAlthough the densities all look the same, they are are quite different! The x-axis in each case has two scales: one is the actual value of the x-variable, and the other is the z-score which is calculated as:\n\\[\nz_x = \\frac{x - \\mu_{x}}{\\sigma_x}\n\\]\nWith similar distributions (i.e. normal distributions), we see that the variation in density is the same at the same values of z-score for each variable. However since the \\(\\mu_i\\) and \\(\\sigma_i\\) are different, the absolute value of the z-score is different for each variable. In the first plot (from the top left), \\(z = 1\\) corresponds to an absolute change of \\(5\\) units; it is \\(15\\) units in the plot directly below it.\nOur comparisons are done easily when we compare differences in probabilities at identical z-scores, or differences in z-scores at identical probabilities.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#wait-but-why",
    "title": "\n Quantities",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nHistograms are used to study the distribution of one or a few Quant variables.\n\nChecking the distribution of your variables one by one is probably the first task you should do when you get a new dataset.\n\nIt delivers a good quantity of information about spread, how frequent the observations are, and if there are some outlandish ones.\n\nComparing histograms side-by-side helps to provide insight about whether a Quant measurement varies with situation (a Qual variable). We will see this properly in a statistical way soon.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#conclusion",
    "title": "\n Quantities",
    "section": "\n Conclusion",
    "text": "Conclusion\nTo complicate matters: Having said all that, the histogram is really a bar chart in disguise! You probably suspect that the “bucketing” of the Quant variable is tantamount to creating a Qual variable! Each bucket is a level in this fictitious bucketed Quant variable.\n\nHistograms, Frequency Distributions, and Box Plots are used for Quantitative data variables\n\nHistograms “dwell upon” counts, ranges, means and standard deviations\n\nWe can split histograms on the basis of another Qualitative variable.\nLong tailed distributions need care in visualization and in inference making!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#your-turn",
    "title": "\n Quantities",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nOld Faithful Data in R (Find it!)\n\n\n\n\n\n\n\nNote2. Wage and Education Data from Canada\n\n\n\n Download the Wages/Education Dataset \n\n\n\n\n\n\n\n\nNote3. Time taken to Open or Close Packages\n\n\n\nSome students/HCD peasants tested Elderly people, some with and some without hand pain, and observed how long they took to open or close typical packages for milk, cheese, bottles etc.\n Download the Package Opening Times xlsx \n Download the Package Closing Times xlsx \n\n\ninspect the dataset in each case and develop a set of Questions, that can be answered by appropriate stat measures, or by using a chart to show the distribution.\n\n\n\n\n\n\nTip\n\n\n\nNote: reading xlsx files into R may need the the readxl package. Install it!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#ai-generated-summary-and-podcast",
    "title": "\n Quantities",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis module is an excerpt from a guide on learning statistical analysis using metaphors. The text focuses on the concept of quantitative variables and how histograms can be used to visualize their distribution. The author illustrates these concepts through real-world examples using datasets such as diamond prices, ultramarathon race times, and global population figures. By analyzing these datasets with histograms, the author explores various aspects of data distributions, including skewness, bimodality, and the presence of outliers. The guide also introduces additional tools like the crosstable package and z-scores to enhance data analysis. Finally, the author encourages readers to apply these concepts to real-world datasets, developing questions and insights through the use of histograms and statistical measures.\n\nWhat patterns emerge from the distributions of quantitative variables in each dataset, and what insights can we gain about the relationships between these variables?\nHow do different qualitative variables impact the distribution of quantitative variables in the datasets, and what are the implications of these findings for understanding the underlying phenomena?\nBased on the distributions and relationships between variables, what are the most relevant questions to ask about the datasets, and what further analyses could be conducted\n\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/index.html#references",
    "title": "\n Quantities",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\n\nMinimal R using mosaic.https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nSebastian Sauer, Plotting multiple plots using purrr::map and ggplot \n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncrosstable\n0.8.1\nChaltiel (2024)\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://doi.org/10.32614/CRAN.package.visualize.\n\n\nChaltiel, Dan. 2024. crosstable: Crosstables for Descriptive Analyses. https://doi.org/10.32614/CRAN.package.crosstable.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"fluent-mdl2:quantity\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Quantities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "",
    "text": "We will use the datasets that are part of the resampledata package.1\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#introduction",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "",
    "text": "We will use the datasets that are part of the resampledata package.1\n\nlibrary(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-study-1-verizon",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-study-1-verizon",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Case Study-1: Verizon",
    "text": "Case Study-1: Verizon\nDoes Verizon create a difference in Repair Times between ILEC and CLEC systems?\n\ndata(\"Verizon\")\ninspect(Verizon)\n\n\ncategorical variables:  \n   name  class levels    n missing\n1 Group factor      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\nDescribe the Variables!\nHypothesis Specification\nWrite the Null and Alternate hypotheses here.\nNull Distribution Computation\nVerizon Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-story-2-recidivism",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#case-story-2-recidivism",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Case Story-2: Recidivism",
    "text": "Case Story-2: Recidivism\nDo criminals released after a jail term commit crimes again? Does recidivism depend upon age?\n\ndata(\"Recidivism\")\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nDescribe the variables!\nHypothesis Specification\nLet us see if the incidence of recidivism is dependent upon whether a person is aged less than or more than 25 years. Write the Null and Alternate hypotheses here.\n\nRecidivism\n\n\n  \n\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We ought to convert it to a numeric variable of 1’s and 0’s. Why?\nNull Distribution for Recidivism\nRecidivism Conclusion\nCase Study #3: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\ndata(\"FlightDelays\")\ninspect(FlightDelays)\n\n\ncategorical variables:  \n         name  class levels    n missing\n1     Carrier factor      2 4029       0\n2 Destination factor      7 4029       0\n3  DepartTime factor      5 4029       0\n4         Day factor      7 4029       0\n5       Month factor      2 4029       0\n6   Delayed30 factor      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the FlightDelays dataset are:\nHypothesis Specification\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\nNull Distribution for FlightDelays\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#references",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#references",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/files/one-prop-tutorial.html#footnotes",
    "title": "Tutorial: Permutation Testing for One Proportion",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://github.com/rudeboybert/resampledata↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html",
    "title": "Permutation Tests for Two Means",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#setting-up-the-packages",
    "title": "Permutation Tests for Two Means",
    "section": "",
    "text": "library(ggplot2)\nlibrary(dplyr)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-study-1-verizon",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-study-1-verizon",
    "title": "Permutation Tests for Two Means",
    "section": "Case Study-1: Verizon",
    "text": "Case Study-1: Verizon\nDoes Verizon create a difference in Repair Times between ILEC and CLEC systems?\n\ndata(\"Verizon\")\ninspect(Verizon)\n\n\ncategorical variables:  \n   name  class levels    n missing\n1 Group factor      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\nDescribe the Variables!\nHypothesis Specification\nWrite the Null and Alternate hypotheses here.\nNull Distribution Computation\nVerizon Conclusion"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-story-2-recidivism",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/two-means-tutorial.html#case-story-2-recidivism",
    "title": "Permutation Tests for Two Means",
    "section": "Case Story-2: Recidivism",
    "text": "Case Story-2: Recidivism\nDo criminals released after a jail term commit crimes again? Does recidivism depend upon age?\n\ndata(\"Recidivism\")\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nDescribe the variables!\nHypothesis Specification\nLet us see if the indidence of recidivism is dependent upon whether a person is aged less than or more than 25 years. Write the Null and Alternate hypotheses here.\n\\[\nH_0: \\mu_{recid-age-25-minus}\\ = \\mu_{recid-age-25-plus}\\\\\n\\]\n\\[\nH_a:\\mu_{recid-age-25-minus}\\ \\ne\\mu_{recid-age-25-plus}\\\\\n\\]\n\nRecidivism\n\n\n  \n\n\ninspect(Recidivism)\n\n\ncategorical variables:  \n     name  class levels     n missing\n1  Gender factor      2 17019       3\n2     Age factor      5 17019       3\n3   Age25 factor      2 17019       3\n4    Race factor     10 16988      34\n5 Offense factor      2 17022       0\n6   Recid factor      2 17022       0\n7    Type factor      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 White-NonHispanic (67%) ...                  \n5 Felony (80.6%), Misdemeanor (19.4%)          \n6 No (68.4%), Yes (31.6%)                      \n7 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We ought to convert it to a numeric variable of 1’s and 0’s. Why?\nNull Distribution for Recidivism\nRecidivism Conclusion\nCase Study #3: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\ndata(\"FlightDelays\")\ninspect(FlightDelays)\n\n\ncategorical variables:  \n         name  class levels    n missing\n1     Carrier factor      2 4029       0\n2 Destination factor      7 4029       0\n3  DepartTime factor      5 4029       0\n4         Day factor      7 4029       0\n5       Month factor      2 4029       0\n6   Delayed30 factor      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the FlightDelays dataset are:\nHypothesis Specification\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\nNull Distribution for FlightDelays\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html",
    "title": "🃏 Inference for Two Independent Means",
    "section": "",
    "text": "To be nobody but myself – in a world which is doing its best, night and day, to make you everybody else – means to fight the hardest battle which any human being can fight, and never stop fighting.\n— E.E. Cummings, poet (14 Oct 1894-1962)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#setting-up-r-packages",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nShow the Codelibrary(tidyverse)\nlibrary(mosaic) # Our go-to package\nlibrary(ggformula)\nlibrary(infer) # An alternative package for inference using tidy data\nlibrary(broom) # Clean test results in tibble form\nlibrary(skimr) # data inspection\nlibrary(resampledata3) # Datasets from Chihara and Hesterberg's book\nlibrary(openintro) # datasets\nlibrary(gt) # for tables\n##\nlibrary(visStatistics) # All-in-one Stats test package\n\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#introduction",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Introduction",
    "text": "Introduction\n\n\n\n\n\nflowchart TD\n    A[Inference for Two Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both Parametric| D[t.test]\n    C --&gt;|Yes, but not variance Parametric| W[t.test with Welch Correction]\n    C --&gt;|No Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model with Signed-Ranks]\n    C --&gt;|No Non-Parametric| P[Bootstrap or Permutation]",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-1-a-simple-data-set-with-two-quant-variables",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-1-a-simple-data-set-with-two-quant-variables",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Case Study #1: A Simple Data set with Two Quant Variables",
    "text": "Case Study #1: A Simple Data set with Two Quant Variables\nLet us look at the MathAnxiety dataset from the package resampledata. Here we have “anxiety” scores for boys and girls, for different components of mathematics.\n\n Inspecting and Charting Data\n\nShow the Codedata(\"MathAnxiety\", package = \"resampledata\")\nMathAnxiety\nMathAnxiety_inspect &lt;- inspect(MathAnxiety)\nMathAnxiety_inspect$categorical\nMathAnxiety_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have ~600 data entries, and with 4 Quant variables; Age,AMAS, RCMAS, and AMAS; and two Qual variables, Gender and Grade. A simple dataset, with enough entries to make it worthwhile to explore as our first example.\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nIs there a difference between boy and girl “anxiety” levels for AMAS (test) in the population from which the MathAnxiety dataset is a sample?\n\n\nFirst, histograms, densities and counts of the variable we are interested in, after converting data into long format:\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nMathAnxiety %&gt;%\n  gf_density(\n    ~AMAS,\n    fill = ~Gender,\n    alpha = 0.5,\n    title = \"Math Anxiety Score Densities\",\n    subtitle = \"Boys vs Girls\"\n  )\n##\nMathAnxiety %&gt;%\n  pivot_longer(\n    cols = -c(Gender, Age, Grade),\n    names_to = \"type\",\n    values_to = \"value\"\n  ) %&gt;%\n  dplyr::filter(type == \"AMAS\") %&gt;%\n  gf_jitter(\n    value ~ Gender,\n    group = ~type, color = ~Gender,\n    width = 0.08, alpha = 0.3,\n    ylab = \"AMAS Anxiety Scores\",\n    title = \"Math Anxiety Score Jitter Plots\",\n    subtitle = \"Illustrating Difference in Means\"\n  ) %&gt;%\n  gf_summary(geom = \"point\", size = 3, colour = \"black\") %&gt;%\n  gf_line(\n    stat = \"summary\", linewidth = 1,\n    geom = \"line\", colour = ~\"MeanDifferenceLine\"\n  )\n##\nMathAnxiety %&gt;% count(Gender)\nMathAnxiety %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(mean = mean(AMAS))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nThe distributions for anxiety scores for boys and girls overlap considerably and are very similar, though the boxplot for boys shows a significant outlier. Are they close to being normal distributions too? We should check.\nA.  Check for Normality\nStatistical tests for means usually require a couple of checks1 2:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution. We will also look at Q-Q plots for both variables:\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nMathAnxiety %&gt;%\n  gf_density(~AMAS,\n    fill = ~Gender,\n    alpha = 0.5,\n    title = \"Math Anxiety scores for boys and girls\"\n  ) %&gt;%\n  gf_facet_grid(~Gender) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n##\nMathAnxiety %&gt;%\n  gf_qqline(~AMAS,\n    color = ~Gender,\n    title = \"Math Anxiety Score..are they Normally Distributed?\"\n  ) %&gt;%\n  gf_qq() %&gt;%\n  gf_facet_wrap(~Gender) # independent y-axis\n\n\n\n\n\n\n\n\n\n\nLet us split the dataset into subsets, to execute the normality check test (Shapiro-Wilk test):\n\nShow the Codeboys_AMAS &lt;- MathAnxiety %&gt;%\n  filter(Gender == \"Boy\") %&gt;%\n  select(AMAS)\n##\ngirls_AMAS &lt;- MathAnxiety %&gt;%\n  filter(Gender == \"Girl\") %&gt;%\n  select(AMAS)\n\n\n\n\n\nShow the Codeshapiro.test(boys_AMAS$AMAS)\nshapiro.test(girls_AMAS$AMAS)\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  boys_AMAS$AMAS\nW = 0.99043, p-value = 0.03343\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  girls_AMAS$AMAS\nW = 0.99074, p-value = 0.07835\n\n\n\n\nThe distributions for anxiety scores for boys and girls are almost normal, visually speaking. With the Shapiro-Wilk test we find that the scores for girls are normally distributed, but the boys scores are not so. Sigh.\n\n\n\n\n\n\nNote\n\n\n\nThe p.value obtained in the shapiro.wilk test suggests the chances of the data being so, given the Assumption that they are normally distributed.\n\n\nWe see that MathAnxiety contains discrete-level scores for anxiety for the two variables (for Boys and Girls) anxiety scores. The boys score has a significant outlier, which we saw earlier and perhaps that makes that variable lose out, perhaps narrowly.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nShow the Codevar.test(AMAS ~ Gender,\n  data = MathAnxiety,\n  conf.int = TRUE, conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\nShow the Code##\nqf(0.975, 275, 322)\n\n[1] 1.254823\n\n\nThe variances are quite similar as seen by the \\(p.value = 0.82\\). We also saw it visually when we plotted the overlapped distributions earlier.\n\n\n\n\n\n\nImportantConditions:\n\n\n\n\nThe two variables are not both normally distributed.\nThe two variances are significantly similar.\n\n\n\n\n Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is any difference in the mean anxiety score between Girls and Boys, in the population from which the dataset MathAnxiety has been drawn. So accordingly:\n\\[\nH_0: \\mu_{Boys} = \\mu_{Girls}\\\\\n\\]\n\\[\nH_a: \\mu_{Girls} \\ne \\mu_{Boys}\\\\\n\\]\n\n Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function:\n\nShow the Codeobs_diff_amas &lt;- diffmean(AMAS ~ Gender, data = MathAnxiety)\nobs_diff_amas\n\ndiffmean \n  1.7676 \n\n\nGirls’ AMAS anxiety scores are, on average, \\(1.76\\) points higher than those for Boys in the dataset/sample.\n\n\n\n\n\n\nNoteOn Observed Difference Estimates\n\n\n\nDifferent tests here will show the difference as positive or negative, but with the same value! This depends upon the way the factor variable Gender is used, i.e. Boy-Girl or Girl-Boy…\n\n\n\n Inference\n\n\nUsing the Parametric t.test\nUsing the Mann-Whitney Test\nUsing the Linear Model Interpretation\nUsing the Permutation Test\n\n\n\nSince the data are not both normally distributed, though the variances similar, we typically cannot use a parametric t.test. However, we can still examine the results:\n\nShow the Codemosaic::t_test(AMAS ~ Gender, data = MathAnxiety) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(0.001\\) ! And the Confidence Interval does not straddle \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar and that there is a significant difference between Boys and Girls when it comes to AMAS anxiety. But can we really believe this, given the non-normality of data?\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and though the variances are similar, we use the classical wilcox.test (Type help(wilcox.test) in your Console.) which implements what we need here: the Mann-Whitney U test:3\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\n\\[\nmean(rank(AMAS_{Girls})) - mean(rank(AMAS_{Boys})) = diff\n\\]\n\\[\nH_0: \\mu_{Boys} - \\mu_{Girls} = 0\n\\]\n\\[\nH_a: \\mu_{Boys} - \\mu_{Girls} \\ne 0\n\\]\n\n\n\n\n\n\n\n\n\nShow the Codewilcox.test(AMAS ~ Gender,\n  data = MathAnxiety,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is very similar, \\(0.00077\\), and again the Confidence Interval does not straddle \\(0\\), and we are hence able to reject the NULL hypothesis that the means are equal and accept the alternative hypothesis that there is a significant difference in mean anxiety scores between Boys and Girls.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(AMAS) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\]\n\\[\nH_0: \\beta_1 = 0\\\\\n\\]\n\\[\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n\n\n\n\n\n\n\n\nShow the Codelm(rank(AMAS) ~ Gender,\n  data = MathAnxiety\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nTipDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The Gender variable was treated as a binary “dummy” variable4.\n\n\nHere too we see that the p.value for the slope term (“GenderGirl”) is significant at \\(7.4*10^{-4}\\).\n\n\nWe pretend that Gender has no effect on the AMAS anxiety scores. If this is our position, then the Gender labels are essentially meaningless, and we can pretend that any AMAS score belongs to a Boy or a Girl. This means we can mosaic::shuffle (permute) the Gender labels and see how uncommon our real data is. And we do not have to really worry about whether the data are normally distributed, or if their variances are nearly equal.\n\n\n\n\n\n\nImportant\n\n\n\nThe “pretend” position is exactly the NULL Hypothesis!! The “uncommon” part is the p.value under NULL!!\n\n\n\nShow the Codenull_dist_amas &lt;-\n  do(4999) * diffmean(data = MathAnxiety, AMAS ~ shuffle(Gender))\nnull_dist_amas\n\n\n  \n\n\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_histogram(data = null_dist_amas, ~diffmean, bins = 25) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_amas,\n    colour = \"red\", linewidth = 1,\n    title = \"Null Distribution by Permutation\",\n    subtitle = \"Histogram\"\n  ) %&gt;%\n  gf_labs(x = \"Difference in Means\")\n###\ngf_ecdf(\n  data = null_dist_amas, ~diffmean,\n  linewidth = 1\n) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_amas,\n    colour = \"red\", linewidth = 1,\n    title = \"Null Distribution by Permutation\",\n    subtitle = \"Cumulative Density\"\n  ) %&gt;%\n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n\n\n\n\n\n\n\nShow the Code1 - prop1(~ diffmean &lt;= obs_diff_amas, data = null_dist_amas)\n\nprop_TRUE \n    6e-04 \n\n\nClearly the observed_diff_amas is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nShow the Codemosaic::t_test(AMAS ~ Gender,\n  data = MathAnxiety\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"t.test\") %&gt;%\n  tab_options(table.font.size = 10)\n\nwilcox.test(AMAS ~ Gender,\n  data = MathAnxiety\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"wilcox.test\") %&gt;%\n  tab_options(table.font.size = 10)\n\nlm(AMAS ~ Gender,\n  data = MathAnxiety\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  ) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with Original Data\") %&gt;%\n  tab_options(table.font.size = 10)\n\nlm(rank(AMAS) ~ Gender,\n  data = MathAnxiety\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  ) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with Ranked Data\") %&gt;%\n  tab_options(table.font.size = 10)\n\n\n\n\n\n\nt.test\n\n\nestimate\nestimate1\nestimate2\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-1.7676\n21.16718\n22.93478\n-3.291843\n0.001055808\n580.2004\n-2.822229\n-0.7129706\nWelch Two Sample t-test\ntwo.sided\n\n\n\n\n\n\n\n\nwilcox.test\n\n\nstatistic\np.value\nmethod\nalternative\n\n\n\n37483\n0.0007736219\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Original Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n21.16718\n0.3641315\n58.130602\n5.459145e-248\n20.4520482\n21.882317\n\n\nGenderGirl\n1.76760\n0.5364350\n3.295087\n1.042201e-03\n0.7140708\n2.821129\n\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n278.04644\n9.535561\n29.158898\n6.848992e-117\n259.31912\n296.7738\n\n\nGenderGirl\n47.64559\n14.047696\n3.391701\n7.405210e-04\n20.05668\n75.2345\n\n\n\n\n\n\nAs we can see, all tests are in agreement that there is a significant effect of Gender on the AMAS anxiety scores!\n\n One Test to Rule Them All: visStatistics\nWe can use the visStatistics package to run all the tests in one go, using the in-built decision tree. This is a very useful package for teaching statistics, and it can be used to run all the tests we have seen so far, and more. Here goes: we use the visstat function to run all the tests, and then we can summarize the results. The visstat function takes a dataset, a quantitative variable, a qualitative variable, and some options for the tests to run.\nFrom the visStatistics package documentation:\n\nvisStatistics automatically selects and visualises appropriate statistical hypothesis tests between two column vectors of type of class “numeric”, “integer”, or “factor”. The choice of test depends on the class, distribution, and sample size of the vectors, as well as the user-defined ‘conf.level’. The main function visstat() visualises the selected test with appropriate graphs (box plots, bar charts, regression lines with confidence bands, mosaic plots, residual plots, Q-Q plots), annotated with the main test results, including any assumption checks and post-hoc analyses.\n\n\nShow the Code# Generate the annotated plots and statistics\nvisstat(\n  x = MathAnxiety$Gender,\n  y = MathAnxiety$AMAS,\n  conf.level = 0.95, numbers = TRUE\n) %&gt;%\n  summary()\n\n\n\n\n\n\n\nSummary of visstat object\n\n--- Named components ---\n[1] \"dependent variable (response)\"    \"independent variables (features)\"\n[3] \"t-test-statistics\"                \"Shapiro-Wilk-test_sample1\"       \n[5] \"Shapiro-Wilk-test_sample2\"       \n\n--- Contents ---\n\n$dependent variable (response):\n[1] \"AMAS\"\n\n$independent variables (features):\n[1] Boy  Girl\nLevels: Boy Girl\n\n$t-test-statistics:\n\n    Welch Two Sample t-test\n\ndata:  x1 and x2\nt = -3.2918, df = 580.2, p-value = 0.001056\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.8222293 -0.7129706\nsample estimates:\nmean of x mean of y \n 21.16718  22.93478 \n\n\n$Shapiro-Wilk-test_sample1:\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99043, p-value = 0.03343\n\n\n$Shapiro-Wilk-test_sample2:\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99074, p-value = 0.07835\n\n\n--- Attributes ---\n$plot_paths\ncharacter(0)\n\n\nThe tool runs the Welch t-test and declares the p-value to be significant. The Shapiro-Wilk test results here also confirm what we had performed earlier. Hence we can say that we may reject the NULL Hypothesis and state that there is a statistically significant difference in AMAS anxiety scores between Boys and Girls.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-2-youth-risk-behavior-surveillance-system-yrbss-survey",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-2-youth-risk-behavior-surveillance-system-yrbss-survey",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Case Study #2: Youth Risk Behavior Surveillance System (YRBSS) survey",
    "text": "Case Study #2: Youth Risk Behavior Surveillance System (YRBSS) survey\nEvery two years, the Centers for Disease Control and Prevention in the USA conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from highschoolers (9th through 12th grade), to analyze health patterns. We will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.The yrbss dataset is part of the openintro package. Type this in your console: help(yrbss).\n\n Inspecting and Charting Data\n\nShow the Codedata(yrbss, package = \"openintro\")\nyrbss\nyrbss_inspect &lt;- inspect(yrbss)\nyrbss_inspect$categorical\nyrbss_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have 13K data entries, and with 13 different variables, some Qual and some Quant. Many entries are missing too, typical of real-world data and something we will have to account for in our computations. The meaning of each variable can be found by bringing up the help file.Type this in your console: help(yrbss)\nFirst, histograms, densities and counts of the variable we are interested in:\n\nShow the Codeyrbss_select_gender &lt;- yrbss %&gt;%\n  select(weight, gender) %&gt;%\n  mutate(gender = as_factor(gender)) %&gt;%\n  drop_na(weight) # Sadly dropping off NA data\n\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n##\nyrbss_select_gender %&gt;%\n  gf_density(~weight,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  )\n###\nyrbss_select_gender %&gt;%\n  gf_jitter(weight ~ gender,\n    color = ~gender,\n    show.legend = FALSE,\n    width = 0.05, alpha = 0.25,\n    ylab = \"Weight\",\n    title = \"Weights of Boys and Girls\"\n  ) %&gt;%\n  gf_summary(\n    group = ~1, # See the reference link above. Damn!!!\n    fun = \"mean\", geom = \"line\", colour = \"lightblue\",\n    lty = 1, linewidth = 2\n  ) %&gt;%\n  gf_summary(\n    fun = \"mean\", colour = \"firebrick\",\n    size = 4, geom = \"point\"\n  ) %&gt;%\n  gf_refine(scale_x_discrete(\n    breaks = c(\"male\", \"female\"),\n    labels = c(\"male\", \"female\"),\n    guide = \"prism_bracket\"\n  )) %&gt;%\n  gf_refine(\n    annotate(x = 0.75, y = 60, geom = \"text\", label = \"Mean\\n Girls Weights\"),\n    annotate(x = 2.25, y = 60, geom = \"text\", label = \"Mean\\n Boys Weights\"),\n    annotate(x = 1.5, y = 100, geom = \"label\", label = \"Slope indicates\\n differences in mean\", fill = \"moccasin\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nShow the Codeyrbss_select_gender %&gt;% count(gender)\n\n\n  \n\n\n\nOverlapped Distribution plot shows some difference in the means; and the Boxplots show visible difference in the medians. In this Case Study, our research question is:\n\n Hypothesis\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nDoes weight of highschoolers in this dataset vary with gender?\n\n\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across gender. So accordingly:\n\\[\nH_0: \\mu_{weight-male} = \\mu_{weight-female}\n\\]\n\\[\nH_a: \\mu_{weight-male} \\ne \\mu_{weight-female}\n\\]\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nWe will complete a visual check for normality with plots, and since we cannot do a shapiro.test (length(data) &gt;= 5000) we can use the Anderson-Darling test.\nLet us plot frequency distribution and Q-Q plots5 for both variables.\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nmale_student_weights &lt;- yrbss_select_gender %&gt;%\n  filter(gender == \"male\") %&gt;%\n  select(weight)\n##\nfemale_student_weights &lt;- yrbss_select_gender %&gt;%\n  filter(gender == \"female\") %&gt;%\n  select(weight)\n\n# shapiro.test(male_student_weights$weight)\n# shapiro.test(female_student_weights$weight)\n\nyrbss_select_gender %&gt;%\n  gf_density(~weight,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_facet_grid(~gender) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n##\nyrbss_select_gender %&gt;%\n  gf_qqline(~ weight | gender, ylab = \"scores\") %&gt;%\n  gf_qq()\n\n\n\n\n\n\n\n\n\n\nDistributions are not too close to normal…perhaps a hint of a rightward skew, suggesting that there are some obese students.\nNo real evidence (visually) of the variables being normally distributed.\n\nShow the Codelibrary(nortest)\nnortest::ad.test(male_student_weights$weight)\n\n\n    Anderson-Darling normality test\n\ndata:  male_student_weights$weight\nA = 113.23, p-value &lt; 2.2e-16\n\nShow the Codenortest::ad.test(female_student_weights$weight)\n\n\n    Anderson-Darling normality test\n\ndata:  female_student_weights$weight\nA = 157.17, p-value &lt; 2.2e-16\n\n\np-values are very low and there is no reason to think that the data is normal.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.testdoes this for us, with a NULL hypothesis that the variances are not significantly different:\n\nShow the Codevar.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n# qf(0.975,6164, 6413)\n\n\n  \n\n\n\nThe p.value being so small, we are able to reject the NULL Hypothesis that the variances of weight are nearly equal across the two exercise regimes.\n\n\n\n\n\n\nImportantConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nThis means that the parametric t.test must be eschewed in favour of the non-parametric wilcox.test. We will use that, and also attempt linear models with rank data, and a final permutation test.\n\n Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nShow the Codeobs_diff_gender &lt;- diffmean(weight ~ gender,\n  data = yrbss_select_gender\n)\n\nobs_diff_gender\n\n\n\ndiffmean \n11.70089 \n\n\n\n\n Inference\n\n\nUsing the Mann-Whitney test\nUsing the Linear Model\nUsing the Permutation Test\n\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and the variances are significantly different, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test,\nOur model would be:\n\\[\nmean(rank(Weight_{male})) - mean(rank(Weight_{female})) = \\beta_1;\n\\]\n\\[\nH_0: \\mu_{weight-male} = \\mu_{weight-female}\n\\]\n\\[\nH_a: \\mu_{weight-male} \\ne \\mu_{weight-female}\n\\]\nRecall the earlier graph showing ranks of anxiety-scores against Gender.\n\nShow the Codewilcox.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is negligible and we are able to reject the NULL hypothesis that the means are equal.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\]\n\\[\nH_0: \\beta_1 = 0\n\\]\n\\[\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\nShow the Code# Create a sign-rank function\n# signed_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(rank(weight) ~ gender,\n  data = yrbss_select_gender\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nTipDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression lm()! The gender variable was treated as a binary “dummy” variable6.\n\n\n\n\nFor the specific data at hand, we need to shuffle the gender and take the test statistic (difference in means) each time.\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nnull_dist_weight &lt;-\n  do(4999) * diffmean(\n    data = yrbss_select_gender,\n    weight ~ shuffle(gender)\n  )\nnull_dist_weight\n###\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_weight)\n###\ngf_histogram(\n  data = null_dist_weight, ~diffmean,\n  bins = 25\n) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_gender,\n    colour = \"red\", linewidth = 1,\n    title = \"Null Distribution by Permutation\",\n    subtitle = \"Histogram\"\n  ) %&gt;%\n  gf_labs(x = \"Difference in Means\")\n###\ngf_ecdf(\n  data = null_dist_weight, ~diffmean,\n  linewidth = 1\n) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_gender,\n    colour = \"red\", linewidth = 1,\n    title = \"Null Distribution by Permutation\",\n    subtitle = \"Cumulative Density\"\n  ) %&gt;%\n  gf_labs(x = \"Difference in Means\")\n\n\n\n\n  \n\n\n\nprop_TRUE \n        1 \n\n\n\n\n\n\n\n\n\n\n\nClearly the observed_diff_weight is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nShow the Codewilcox.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"wilcox.test\") %&gt;%\n  tab_options(table.font.size = 10)\n\nlm(rank(weight) ~ gender,\n  data = yrbss_select_gender\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  ) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with Ranked Data\") %&gt;%\n  tab_options(table.font.size = 10)\n\n\n\n\n\n\nwilcox.test\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-11.33999\n10808212\n0\n-11.34003\n-10.87994\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n4836.157\n42.52745\n113.71848\n0\n4752.797\n4919.517\n\n\ngendermale\n2851.246\n59.55633\n47.87478\n0\n2734.507\n2967.986\n\n\n\n\n\n\nThe wilcox.test and the linear model with rank data offer the same results. This is of course not surprising!\n\n One Test to Rule Them All: visStatistics again\nWe need to use a smaller sample of the dataset yrbss_select_gender, for the (same) reason: visstat() defaults to using the shapiro.wilk test internally:\n\nShow the Codeyrbss_select_gender_sample &lt;- yrbss_select_gender %&gt;%\n  slice_sample(n = 4999)\n\nvisstat(\n  x = yrbss_select_gender_sample$gender,\n  y = yrbss_select_gender_sample$weight,\n  conf.level = 0.95, numbers = TRUE\n) %&gt;%\n  summary()\n\n\n\n\n\n\n\nSummary of visstat object\n\n--- Named components ---\n[1] \"dependent variable (response)\"    \"independent variables (features)\"\n[3] \"t-test-statistics\"                \"Shapiro-Wilk-test_sample1\"       \n[5] \"Shapiro-Wilk-test_sample2\"       \n\n--- Contents ---\n\n$dependent variable (response):\n[1] \"weight\"\n\n$independent variables (features):\n[1] male   female\nLevels: female male\n\n$t-test-statistics:\n\n    Welch Two Sample t-test\n\ndata:  x1 and x2\nt = -26.238, df = 4920, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -12.87028 -11.08069\nsample estimates:\nmean of x mean of y \n 62.05522  74.03070 \n\n\n$Shapiro-Wilk-test_sample1:\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.88784, p-value &lt; 2.2e-16\n\n\n$Shapiro-Wilk-test_sample2:\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.93506, p-value &lt; 2.2e-16\n\n\n--- Attributes ---\n$plot_paths\ncharacter(0)\n\n\nCompare these results with those calculated earlier!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-3-weight-vs-exercise-in-the-yrbss-survey",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#case-study-3-weight-vs-exercise-in-the-yrbss-survey",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Case Study #3: Weight vs Exercise in the YRBSS Survey",
    "text": "Case Study #3: Weight vs Exercise in the YRBSS Survey\nFinally, consider the possible relationship between a highschooler’s weight and their physical activity.\nFirst, let’s create a new variable physical_3plus, which will be coded as either “yes” if the student is physically active for at least 3 days a week, and “no” if not. Recall that we have several missing data in that column, so we will (sadly) drop these before generating the new variable:\n\nShow the Codeyrbss_select_phy &lt;- yrbss %&gt;%\n  drop_na(physically_active_7d, weight) %&gt;%\n  ## add new variable physical_3plus\n  mutate(\n    physical_3plus = if_else(physically_active_7d &gt;= 3,\n      \"yes\", \"no\"\n    ),\n    # Convert it to a factor Y/N\n    physical_3plus = factor(physical_3plus,\n      labels = c(\"yes\", \"no\"),\n      levels = c(\"yes\", \"no\")\n    )\n  ) %&gt;%\n  select(weight, physical_3plus)\n\n# Let us check\nyrbss_select_phy %&gt;% count(physical_3plus)\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nDoes weight vary based on whether students exercise on more or less than 3 days a week? (physically_active_7d &gt;= 3 days)\n\n\n\n Inspecting and Charting Data\nWe can make distribution plots for weight by physical_3plus:\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n###\nyrbss_select_phy %&gt;%\n  gf_jitter(weight ~ physical_3plus,\n    group = ~physical_3plus,\n    width = 0.08, alpha = 0.08,\n    xlab = \"Days of Exercise &gt;=3\"\n  ) %&gt;%\n  gf_summary(\n    geom = \"point\", size = 3, group = ~physical_3plus,\n    colour = ~physical_3plus\n  ) %&gt;%\n  gf_line(\n    group = 1, # weird remedy to fix groups error message!\n    stat = \"summary\", linewidth = 1,\n    geom = \"line\", colour = ~\"MeanDifferenceLine\"\n  )\n###\ngf_density(~weight,\n  fill = ~physical_3plus,\n  data = yrbss_select_phy\n)\n\n\n\n\n\n\n\n\n\n\nThe jitter and density plots show the comparison between the two means. We can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nShow the Codeyrbss_select_phy %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test. But before that a few more checks on the data:\nA.  Check for Normality\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nyrbss_select_phy %&gt;%\n  gf_density(~weight,\n    fill = ~physical_3plus,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Exercise Frequency\"\n  ) %&gt;%\n  gf_facet_grid(~physical_3plus) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n##\nyrbss_select_phy %&gt;%\n  gf_qq(~ weight | physical_3plus,\n    color = ~physical_3plus\n  ) %&gt;%\n  gf_qqline(ylab = \"Weight\")\n\n\n\n\n\n\n\n\n\n\nAgain, not normally distributed…\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\nShow the Codevar.test(weight ~ physical_3plus,\n  data = yrbss_select_phy,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n# Critical F value\nqf(0.975, 4021, 8341)\n\n\n\n\n  \n\n\n\n[1] 1.054398\n\n\n\nThe p.value states the probability of the data being what it is, assuming the NULL hypothesis that variances were similar. It being so small, we are able to reject this NULL Hypothesis that the variances of weight are nearly equal across the two exercise frequencies. (Compare the statistic in the var.test with the critical F-value)\n\n\n\n\n\n\nImportantConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nHence we will have to use non-parametric tests to infer if the means are similar.\n\n Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across physical_3plus. So accordingly:\n\\[\nH_0: \\mu_{physical-3plus-Yes} = \\mu_{physical-3plus-No}\n\\]\n\\[\nH_a: \\mu_{physical-3plus-Yes} \\ne \\mu_{physical-3plus-No}\n\\]\n\n Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nShow the Codeobs_diff_phy &lt;- diffmean(weight ~ physical_3plus,\n  data = yrbss_select_phy\n)\n\nobs_diff_phy\n\n\n\n diffmean \n-1.774584 \n\n\n\n\n Inference\n\n\nUsing parametric t.test\nUsing non-parametric paired Wilcoxon test\nUsing the Linear Model Interpretation\n\n\n\nWell, the variables are not normally distributed, and the variances are significantly different so a standard t.test is not advised. We can still try:\n\nShow the Codemosaic::t_test(weight ~ physical_3plus,\n  var.equal = FALSE, # Welch Correction\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(8.9e-08\\) ! And the Confidence Interval is clear of \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\n\n\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\nShow the Code# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say,\n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(weight ~ physical_3plus,\n  conf.int = TRUE,\n  conf.level = 0.95,\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe nonparametric wilcox.test also suggests that the means for weight across physical_3plus are significantly different.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  physical.3plus) = \\beta_0 + \\beta_1 \\times physical.3plus\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\nShow the Codelm(rank(weight) ~ physical_3plus,\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\nHere too, the linear model using rank data arrives at a conclusion similar to that of the Mann-Whitney U test.\n\n\n\nUsing Permutation Tests\nFor this last Case Study, we will do this in two ways, just for fun: one using our familiar mosaic package, and the other using the package infer.\nBut first, we need to initialize the test, which we will save as obs_diff_**.\nShow the Codeobs_diff_infer &lt;- yrbss_select_phy %&gt;%\n  infer::specify(weight ~ physical_3plus) %&gt;%\n  infer::calculate(\n    stat = \"diff in means\",\n    order = c(\"yes\", \"no\")\n  )\nobs_diff_infer\n##\nobs_diff_mosaic &lt;-\n  mosaic::diffmean(~ weight | physical_3plus,\n    data = yrbss_select_phy\n  )\nobs_diff_mosaic\n##\nobs_diff_phy\n\n\n\n\n  \n\n\n\n diffmean \n-1.774584 \n\n\n diffmean \n-1.774584 \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\n\nUsing infer\nUsing mosaic\n\n\n\nNext, we will work through creating a permutation distribution using tools from the infer package.\nIn infer, the specify() function is used to specify the variables you are considering (notated y ~ x), and you can use the calculate() function to specify the statistic you want to calculate and the order of subtraction you want to use. For this hypothesis, the statistic you are searching for is the difference in means, with the order being yes - no.\nAfter you have calculated your observed statistic, you need to create a permutation distribution. This is the distribution that is created by shuffling the observed weights into new physical_3plus groups, labeled “yes” and “no”.\nWe will save the permutation distribution as null_dist.\nShow the Codenull_dist &lt;- yrbss_select_phy %&gt;%\n  specify(weight ~ physical_3plus) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 999, type = \"permute\") %&gt;%\n  calculate(\n    stat = \"diff in means\",\n    order = c(\"yes\", \"no\")\n  )\nnull_dist\n\n\n\n\n  \n\n\n\n\nThe hypothesize() function is used to declare what the null hypothesis is. Here, we are assuming that student’s weight is independent of whether they exercise at least 3 days or not.\nWe should also note that the type argument within generate() is set to \"permute\". This ensures that the statistics calculated by the calculate() function come from a reshuffling of the data (not a resampling of the data)! Finally, the specify() and calculate() steps should look familiar, since they are the same as what we used to find the observed difference in means!\nWe can visualize this null distribution with the following code:\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\nnull_dist %&gt;%\n  visualise() + # Note this plus sign!\n  shade_p_value(obs_diff_infer,\n    direction = \"two-sided\"\n  )\n\n\n\n\n\n\n\nNow that you have calculated the observed statistic and generated a permutation distribution, you can calculate the p-value for your hypothesis test using the function get_p_value() from the infer package.\n\nShow the Codenull_dist %&gt;%\n  get_p_value(\n    obs_stat = obs_diff_infer,\n    direction = \"two_sided\"\n  )\n\n\n  \n\n\n\nWhat warning message do you get? Why do you think you get this warning message? Let us construct and record a confidence interval for the difference between the weights of those who exercise at least three times a week and those who don’t, and interpret this interval in context of the data.\n\nShow the Codenull_dist %&gt;%\n  infer::get_confidence_interval(\n    point_estimate = obs_diff_infer,\n    level = 0.95\n  )\n\n\n  \n\n\n\nIt does look like the observed_diff_infer is too far away from this confidence interval. Hence if there was no difference in weight caused by physical_3plus, we would never have observed it! Hence the physical_3plus does have an effect on weight!\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nShow the Codenull_dist_mosaic &lt;- do(999) *\n  diffmean(~ weight | shuffle(physical_3plus),\n    data = yrbss_select_phy\n  )\n\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_histogram(~diffmean, data = null_dist_mosaic) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_mosaic,\n    colour = \"red\", linewidth = 2\n  )\n\n\n\n\n\n\nShow the Code# p-value\nprop(~ diffmean != obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\nShow the Code# Confidence Intervals for p = 0.95\nmosaic::cdata(~diffmean, p = 0.95, data = null_dist_mosaic)\n\n\n  \n\n\n\nAgain, it does look like the observed_diff_infer is too far away from this NULL distribution. Hence if there was no difference in weight caused by physical_3plus, we would never have observed it! Hence the physical_3plus does have an effect on weight!\n\n\n\nClearly there is a serious effect of Physical Exercise on the body weights of students in the population from which this dataset is drawn.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#wait-but-why",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nWe need often to infer differences in means between Quantitative variables in a Population\nWe treat our dataset as a sample from the population, which we cannot access\nWe can apply all the CLT ideas +t.test if the two variables in the dataset satisfy the conditions of normality, equal variance\nElse use non-parametric wilcox.test\n\nAnd by treating the two variables as one Quant in two groups, we can simply perform a Permutation test",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#conclusion",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nWe have learnt how to perform inference for independent means.\n\nWe have looked at the conditions that make the regular t.test possible, and learnt what to do if the conditions of normality and equal variance are not met.\n\nWe have also looked at how these tests can be understood as manifestations of the linear model, with data and sign-ranked data.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#your-turn",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry the SwimRecords dataset from the mosaicData package.\nTry some of the datasets in the moderndive package. Install it , peasants. And type in your Console data(package = \"moderndive\") to see what you have. Teacher evals might interest you!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#sec-references",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#sec-references",
    "title": "🃏 Inference for Two Independent Means",
    "section": "\n References",
    "text": "References\n\nRandall Pruim, Nicholas J. Horton, Daniel T. Kaplan, Start Teaching with R\n\nhttps://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307\nhttps://statsandr.com/blog/wilcoxon-test-in-r-how-to-compare-2-groups-under-the-non-normality-assumption/\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nexplore\n1.3.5\n@explore\n\n\ninfer\n1.0.9\n@infer\n\n\nopenintro\n2.5.0\n@openintro\n\n\nresampledata\n0.3.2\n@resampledata\n\n\nTeachHist\n0.2.1\n@TeachHist\n\n\nTeachingDemos\n2.13\n@TeachingDemos\n\n\nvisStatistics\n0.1.7\n@visStatistics",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/index.html#footnotes",
    "title": "🃏 Inference for Two Independent Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless↩︎\nhttps://www.allendowney.com/blog/2023/01/28/never-test-for-normality/↩︎\nhttps://stats.stackexchange.com/q/113337↩︎\nhttps://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/intro-linear-models.html#a-linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables↩︎\nhttps://stats.stackexchange.com/questions/92374/testing-large-dataset-for-normality-how-and-is-it-reliable↩︎\nhttps://en.wikipedia.org/wiki/Dummy_variable_(statistics)↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Two Independent Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "",
    "text": "R Tutorial  \n\n\n  Datasets\n\n\n\n\n“When Alexander the Great visited Diogenes and asked whether he could do anything for the famed teacher, Diogenes replied:”Only stand out of my light.” Perhaps some day we shall know how to heighten creativity. Until then, one of the best things we can do for creative men and women is to stand out of their light.”\n— John W. Gardner, author and educator (8 Oct 1912-2002)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#slides-and-tutorials",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "",
    "text": "R Tutorial  \n\n\n  Datasets\n\n\n\n\n“When Alexander the Great visited Diogenes and asked whether he could do anything for the famed teacher, Diogenes replied:”Only stand out of my light.” Perhaps some day we shall know how to heighten creativity. Until then, one of the best things we can do for creative men and women is to stand out of their light.”\n— John W. Gardner, author and educator (8 Oct 1912-2002)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#setting-up-r-packages",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # Data Processing in R\nlibrary(mosaic) # Our workhorse for stats, sampling\nlibrary(skimr) # Good to Examine data\nlibrary(ggformula) # Formula interface for graphs\n\n# load the NHANES data library\nlibrary(NHANES)\nlibrary(infer)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#what-is-a-population",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#what-is-a-population",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n What is a Population?",
    "text": "What is a Population?\nA population is a collection of individuals or observations we are interested in. This is also commonly denoted as a study population in science research, or a target audience in design. We mathematically denote the population’s size using upper-case N.\nA population parameter is some numerical summary about the population that is unknown but you wish you knew. For example, when this quantity is a mean like the mean height of all Bangaloreans, the population parameter of interest is the population mean \\(\\mu\\).\nA census is an exhaustive enumeration or counting of all N individuals in the population. We do this in order to compute the population parameter’s value exactly. Of note is that as the number N of individuals in our population increases, conducting a census gets more expensive (in terms of time, energy, and money).\n\n\n\n\n\n\nImportant Parameters\n\n\n\nPopulations Parameters are usually indicated by Greek Letters.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#what-is-a-sample",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#what-is-a-sample",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n What is a Sample?",
    "text": "What is a Sample?\nSampling is the act of collecting a small subset from the population, which we generally do when we can’t perform a census. We mathematically denote the sample size using lower case n, as opposed to upper case N which denotes the population’s size. Typically the sample size n is much smaller than the population size N. Thus sampling is a much cheaper alternative than performing a census.\nA sample statistic, also known as a point estimate, is a summary statistic like a mean or standard deviation that is computed from a sample.\n\n\n\n\n\n\nNoteWhy do we sample?\n\n\n\nBecause we cannot conduct a census ( not always ) — and sometimes we won’t even know how big the population is — we take samples. And we still want to do useful work for/with the population, after estimating its parameters, an act of generalizing from sample to population. So the question is, can we estimate useful parameters of the population, using just samples? Can point estimates serve as useful guides to population parameters?\nThis act of generalizing from sample to population is at the heart of statistical inference.\n\n\n\n\n\n\n\n\nImportantAn Alliterative Mnemonic\n\n\n\nNOTE: there is an alliterative mnemonic here: Samples have Statistics; Populations have Parameters.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#population-parameters-and-sample-statistics",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#population-parameters-and-sample-statistics",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Population Parameters and Sample Statistics",
    "text": "Population Parameters and Sample Statistics\n\nParameters and Statistics\n\n\nPopulation Parameter\nSample Statistic\n\n\n\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nStandard Deviation\n\\(\\sigma\\)\ns\n\n\nProportion\np\n\\(\\hat{p}\\)\n\n\nCorrelation\n\\(\\rho\\)\nr\n\n\nSlope (Regression)\n\\(\\beta_1\\)\n\\(b_1\\)\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1. What is the mean commute time for workers in a particular city?\nA.1. The parameter is the mean commute time \\(\\mu\\) for a population containing all workers who work in the city. We estimate it using \\(\\bar{x}\\), the mean of the random sample of people who work in the city.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the correlation between the size of dinner bills and the size of tips at a restaurant?\nA.2. The parameter is \\(\\rho\\) , the correlation between bill amount and tip size for a population of all dinner bills at that restaurant. We estimate it using r, the correlation from a random sample of dinner bills.\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. How much difference is there in the proportion of 30 to 39-year-old residents who have only a cell phone (no land line phone) compared to 50 to 59-year-olds in the country?\nA.3. The population is all citizens of the country, and the parameter is \\(p_1 - p_2\\), the difference in proportion of 30 to 39-year-old residents who have only a cell phone (\\(p_1\\)) and the proportion with the same property among all 50 to 59-year olds (\\(p_2\\)). We estimate it using (\\(\\hat{p_1} - \\hat{p_2}\\)), the difference in sample proportions computed from random samples taken from each group.\n\n\nSample statistics vary and in the following we will estimate this uncertainty and decide how reliable they might be as estimates of population parameters.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#case-study-1-sampling-the-nhanes-dataset",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#case-study-1-sampling-the-nhanes-dataset",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Case Study #1: Sampling the NHANES dataset",
    "text": "Case Study #1: Sampling the NHANES dataset\nWe will first execute some samples from a known dataset. We load up the NHANES dataset and glimpse it.\n\ndata(\"NHANES\")\nglimpse(NHANES)\n\nRows: 10,000\nColumns: 76\n$ ID               &lt;int&gt; 51624, 51624, 51624, 51625, 51630, 51638, 51646, 5164…\n$ SurveyYr         &lt;fct&gt; 2009_10, 2009_10, 2009_10, 2009_10, 2009_10, 2009_10,…\n$ Gender           &lt;fct&gt; male, male, male, male, female, male, male, female, f…\n$ Age              &lt;int&gt; 34, 34, 34, 4, 49, 9, 8, 45, 45, 45, 66, 58, 54, 10, …\n$ AgeDecade        &lt;fct&gt;  30-39,  30-39,  30-39,  0-9,  40-49,  0-9,  0-9,  40…\n$ AgeMonths        &lt;int&gt; 409, 409, 409, 49, 596, 115, 101, 541, 541, 541, 795,…\n$ Race1            &lt;fct&gt; White, White, White, Other, White, White, White, Whit…\n$ Race3            &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Education        &lt;fct&gt; High School, High School, High School, NA, Some Colle…\n$ MaritalStatus    &lt;fct&gt; Married, Married, Married, NA, LivePartner, NA, NA, M…\n$ HHIncome         &lt;fct&gt; 25000-34999, 25000-34999, 25000-34999, 20000-24999, 3…\n$ HHIncomeMid      &lt;int&gt; 30000, 30000, 30000, 22500, 40000, 87500, 60000, 8750…\n$ Poverty          &lt;dbl&gt; 1.36, 1.36, 1.36, 1.07, 1.91, 1.84, 2.33, 5.00, 5.00,…\n$ HomeRooms        &lt;int&gt; 6, 6, 6, 9, 5, 6, 7, 6, 6, 6, 5, 10, 6, 10, 10, 4, 3,…\n$ HomeOwn          &lt;fct&gt; Own, Own, Own, Own, Rent, Rent, Own, Own, Own, Own, O…\n$ Work             &lt;fct&gt; NotWorking, NotWorking, NotWorking, NA, NotWorking, N…\n$ Weight           &lt;dbl&gt; 87.4, 87.4, 87.4, 17.0, 86.7, 29.8, 35.2, 75.7, 75.7,…\n$ Length           &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HeadCirc         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Height           &lt;dbl&gt; 164.7, 164.7, 164.7, 105.4, 168.4, 133.1, 130.6, 166.…\n$ BMI              &lt;dbl&gt; 32.22, 32.22, 32.22, 15.30, 30.57, 16.82, 20.64, 27.2…\n$ BMICatUnder20yrs &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ BMI_WHO          &lt;fct&gt; 30.0_plus, 30.0_plus, 30.0_plus, 12.0_18.5, 30.0_plus…\n$ Pulse            &lt;int&gt; 70, 70, 70, NA, 86, 82, 72, 62, 62, 62, 60, 62, 76, 8…\n$ BPSysAve         &lt;int&gt; 113, 113, 113, NA, 112, 86, 107, 118, 118, 118, 111, …\n$ BPDiaAve         &lt;int&gt; 85, 85, 85, NA, 75, 47, 37, 64, 64, 64, 63, 74, 85, 6…\n$ BPSys1           &lt;int&gt; 114, 114, 114, NA, 118, 84, 114, 106, 106, 106, 124, …\n$ BPDia1           &lt;int&gt; 88, 88, 88, NA, 82, 50, 46, 62, 62, 62, 64, 76, 86, 6…\n$ BPSys2           &lt;int&gt; 114, 114, 114, NA, 108, 84, 108, 118, 118, 118, 108, …\n$ BPDia2           &lt;int&gt; 88, 88, 88, NA, 74, 50, 36, 68, 68, 68, 62, 72, 88, 6…\n$ BPSys3           &lt;int&gt; 112, 112, 112, NA, 116, 88, 106, 118, 118, 118, 114, …\n$ BPDia3           &lt;int&gt; 82, 82, 82, NA, 76, 44, 38, 60, 60, 60, 64, 76, 82, 7…\n$ Testosterone     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ DirectChol       &lt;dbl&gt; 1.29, 1.29, 1.29, NA, 1.16, 1.34, 1.55, 2.12, 2.12, 2…\n$ TotChol          &lt;dbl&gt; 3.49, 3.49, 3.49, NA, 6.70, 4.86, 4.09, 5.82, 5.82, 5…\n$ UrineVol1        &lt;int&gt; 352, 352, 352, NA, 77, 123, 238, 106, 106, 106, 113, …\n$ UrineFlow1       &lt;dbl&gt; NA, NA, NA, NA, 0.094, 1.538, 1.322, 1.116, 1.116, 1.…\n$ UrineVol2        &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ UrineFlow2       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ Diabetes         &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, N…\n$ DiabetesAge      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ HealthGen        &lt;fct&gt; Good, Good, Good, NA, Good, NA, NA, Vgood, Vgood, Vgo…\n$ DaysPhysHlthBad  &lt;int&gt; 0, 0, 0, NA, 0, NA, NA, 0, 0, 0, 10, 0, 4, NA, NA, 0,…\n$ DaysMentHlthBad  &lt;int&gt; 15, 15, 15, NA, 10, NA, NA, 3, 3, 3, 0, 0, 0, NA, NA,…\n$ LittleInterest   &lt;fct&gt; Most, Most, Most, NA, Several, NA, NA, None, None, No…\n$ Depressed        &lt;fct&gt; Several, Several, Several, NA, Several, NA, NA, None,…\n$ nPregnancies     &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 1, 1, 1, NA, NA, NA, NA, N…\n$ nBabies          &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ Age1stBaby       &lt;int&gt; NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ SleepHrsNight    &lt;int&gt; 4, 4, 4, NA, 8, NA, NA, 8, 8, 8, 7, 5, 4, NA, 5, 7, N…\n$ SleepTrouble     &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, No, Y…\n$ PhysActive       &lt;fct&gt; No, No, No, NA, No, NA, NA, Yes, Yes, Yes, Yes, Yes, …\n$ PhysActiveDays   &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, 5, 5, 5, 7, 5, 1, NA, 2, …\n$ TVHrsDay         &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CompHrsDay       &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ TVHrsDayChild    &lt;int&gt; NA, NA, NA, 4, NA, 5, 1, NA, NA, NA, NA, NA, NA, 4, N…\n$ CompHrsDayChild  &lt;int&gt; NA, NA, NA, 1, NA, 0, 6, NA, NA, NA, NA, NA, NA, 3, N…\n$ Alcohol12PlusYr  &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, Y…\n$ AlcoholDay       &lt;int&gt; NA, NA, NA, NA, 2, NA, NA, 3, 3, 3, 1, 2, 6, NA, NA, …\n$ AlcoholYear      &lt;int&gt; 0, 0, 0, NA, 20, NA, NA, 52, 52, 52, 100, 104, 364, N…\n$ SmokeNow         &lt;fct&gt; No, No, No, NA, Yes, NA, NA, NA, NA, NA, No, NA, NA, …\n$ Smoke100         &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, Yes, No, …\n$ Smoke100n        &lt;fct&gt; Smoker, Smoker, Smoker, NA, Smoker, NA, NA, Non-Smoke…\n$ SmokeAge         &lt;int&gt; 18, 18, 18, NA, 38, NA, NA, NA, NA, NA, 13, NA, NA, N…\n$ Marijuana        &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, NA, Ye…\n$ AgeFirstMarij    &lt;int&gt; 17, 17, 17, NA, 18, NA, NA, 13, 13, 13, NA, 19, 15, N…\n$ RegularMarij     &lt;fct&gt; No, No, No, NA, No, NA, NA, No, No, No, NA, Yes, Yes,…\n$ AgeRegMarij      &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 20, 15, N…\n$ HardDrugs        &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, No, No, No, No, Yes, …\n$ SexEver          &lt;fct&gt; Yes, Yes, Yes, NA, Yes, NA, NA, Yes, Yes, Yes, Yes, Y…\n$ SexAge           &lt;int&gt; 16, 16, 16, NA, 12, NA, NA, 13, 13, 13, 17, 22, 12, N…\n$ SexNumPartnLife  &lt;int&gt; 8, 8, 8, NA, 10, NA, NA, 20, 20, 20, 15, 7, 100, NA, …\n$ SexNumPartYear   &lt;int&gt; 1, 1, 1, NA, 1, NA, NA, 0, 0, 0, NA, 1, 1, NA, NA, 1,…\n$ SameSex          &lt;fct&gt; No, No, No, NA, Yes, NA, NA, Yes, Yes, Yes, No, No, N…\n$ SexOrientation   &lt;fct&gt; Heterosexual, Heterosexual, Heterosexual, NA, Heteros…\n$ PregnantNow      &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nLet us create a NHANES (sub)-dataset without duplicated IDs and only adults and select the Height variable:\n\nNHANES_adult &lt;-\n  NHANES %&gt;%\n  distinct(ID, .keep_all = TRUE) %&gt;%\n  filter(Age &gt;= 18) %&gt;%\n  select(Height) %&gt;%\n  drop_na(Height)\nNHANES_adult\n\n\n  \n\n\n\n\n An “Assumed” Population\n\n\n\n\n\n\nImportantAn “Assumed” Population\n\n\n\nNormally, we very rarely have access to a population. All we can do is sample it. However, for now, and in order to build up our intuition, we will treat this single-variable dataset as our Population. So this is our population, with appropriate population parameters such as pop_mean, and pop_sd.\n\n\nLet us calculate the population parameters for the Height data from our “assumed” population:\n\n# NHANES_adult is assumed population\npop_mean &lt;- mosaic::mean(~Height, data = NHANES_adult)\n\npop_sd &lt;- mosaic::sd(~Height, data = NHANES_adult)\n\npop_mean\n\n[1] 168.3497\n\npop_sd\n\n[1] 10.15705\n\n\n\n Sampling\nNow, we will sample ONCE from the NHANES Height variable. Let us take a sample of sample size 50. We will compare sample statistics with population parameters on the basis of this ONE sample of 50:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nsample_50 &lt;- mosaic::sample(NHANES_adult, size = 50) %&gt;%\n  select(Height)\nsample_50\nsample_mean_50 &lt;- mean(~Height, data = sample_50)\nsample_mean_50\n# Plotting the histogram of this sample\nsample_50 %&gt;%\n  gf_histogram(~Height, bins = 10) %&gt;%\n  gf_vline(\n    xintercept = ~sample_mean_50,\n    color = \"purple\"\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_mean,\n    colour = \"black\"\n  ) %&gt;%\n  gf_label(7 ~ (pop_mean + 8),\n    label = \"Population Mean\",\n    color = \"black\"\n  ) %&gt;%\n  gf_label(7 ~ (sample_mean_50 - 8),\n    label = \"Sample Mean\", color = \"purple\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Distribution and Mean of a Single Sample\",\n    subtitle = \"Sample Size = 50\"\n  )\n\n\n\n\n  \n\n\n\n[1] 169.002\n\n\n\n\n\n\n\n Repeated Samples and Sample Means\nOK, so the sample_mean_50 is not too far from the pop_mean. Is this always true?\nLet us check: we will create 500 repeated samples, each of size 50. And calculate their mean as the sample statistic, giving us a data frame containing 500 sample means. We will then see if these 500 means lie close to the pop_mean:\nsample_50_500 &lt;- do(500) * {\n  sample(NHANES_adult, size = 50) %&gt;%\n    select(Height) %&gt;% # drop sampling related column \"orig.id\"\n    summarise(\n      sample_mean = mean(Height),\n      sample_sd = sd(Height),\n      sample_min = min(Height),\n      sample_max = max(Height)\n    )\n}\nsample_50_500\ndim(sample_50_500)\n\n\n\n\n  \n\n\n\n[1] 500   6\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nsample_50_500 %&gt;%\n  gf_point(.index ~ sample_mean,\n    color = \"purple\",\n    title = \"Sample Means are close to the Population Mean\",\n    subtitle = \"Sample Means are Random!\",\n    caption = \"Grey lines represent our 500 samples\"\n  ) %&gt;%\n  gf_segment(\n    .index + .index ~ sample_min + sample_max,\n    color = \"grey\",\n    linewidth = 0.3,\n    alpha = 0.3,\n    ylab = \"Sample Index (1-500)\",\n    xlab = \"Sample Means\"\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_mean,\n    color = \"black\"\n  ) %&gt;%\n  gf_label(-25 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"black\"\n  )\n##\n\nsample_50_500 %&gt;%\n  gf_point(.index ~ sample_sd,\n    color = \"purple\",\n    title = \"Sample SDs are close to the Population Sd\",\n    subtitle = \"Sample SDs are Random!\",\n  ) %&gt;%\n  gf_vline(\n    xintercept = ~pop_sd,\n    color = \"black\"\n  ) %&gt;%\n  gf_label(-25 ~ pop_sd,\n    label = \"Population SD\",\n    color = \"black\"\n  ) %&gt;%\n  gf_refine(lims(x = c(4, 16)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Means and Sample SDs are a Random Variables!!1\n\n\n\nThe sample_means (purple dots in the two graphs), are themselves random because the samples are random, of course. It appears that they are generally in the vicinity of the pop_mean (vertical black line). And hence they will have a mean and sd too 😱. Do not get confused ;-D\nAnd the sample_sds are also random and have their own distribution, around the pop_sd.2\n\n\n\n Distribution of Sample-Means\nSince the sample-means are themselves random variables, let’s plot the distribution of these 500 sample-means themselves, called a distribution of sample-means. We will also plot the position of the population mean pop_mean parameter, the mean of the Height variable.\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nsample_50_500 %&gt;%\n  gf_dhistogram(~sample_mean, bins = 30, xlab = \"Height\") %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(0.01 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Sampling Mean Distribution\",\n    subtitle = \"500 means\"\n  )\n\n\n# How does this **distribution of sample-means** compare with the\n# overall distribution of the population?\n#\nsample_50_500 %&gt;%\n  gf_dhistogram(~sample_mean, bins = 30, xlab = \"Height\") %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(0.01 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  ## Add the population histogram\n  gf_histogram(~Height,\n    data = NHANES_adult,\n    alpha = 0.2, fill = \"blue\",\n    bins = 30\n  ) %&gt;%\n  gf_label(0.025 ~ (pop_mean + 20),\n    label = \"Population Distribution\", color = \"blue\"\n  ) %&gt;%\n  gf_labs(title = \"Sampling Mean Distribution\", subtitle = \"Original Population overlay\")\n\n\n\n\n\nSample Means\n\n\n\n\n\nSample Means and Population\n\n\n\n\n\nDistributions\n\n\n\n\n Deriving the Central Limit Theorem (CLT)\nWe see in the Figure above that\n\nthe distribution of sample-means is centered around the pop_mean. ( Mean of the sample means = pop_mean!! 😱)\nThat the “spread” of the distribution of sample means is less than pop_sd. Curiouser and curiouser! But exactly how much is it?\nAnd what is the kind of distribution?\n\nOne more experiment.\nNow let’s repeatedly sample Height and compute the sample-means, and look at the resulting histograms. (We will deal with sample-standard-deviations shortly.) We will also use sample sizes of c(8, 16, ,32, 64) and generate 1000 samples each time, take the means and plot these 4 * 1000 means:\n\n# set.seed(12345)\n\nsamples_08_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 08))\n\nsamples_16_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 16))\n\nsamples_32_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 32))\n\nsamples_64_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 64))\n\n# samples_128_1000 &lt;- do(1000) * mean(resample(NHANES_adult$Height, size = 128))\n\n# Quick Check\nhead(samples_08_1000)\n\n\n  \n\n\n\nLet us plot their individual histograms to compare them:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n# Let us overlay their individual histograms to compare them:\np5 &lt;- gf_dhistogram(~mean,\n  data = samples_08_1000,\n  color = \"grey\",\n  fill = \"dodgerblue\", title = \"N = 8\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean, inherit = FALSE,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-0.025 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n##\np6 &lt;- gf_dhistogram(~mean,\n  data = samples_16_1000,\n  color = \"grey\",\n  fill = \"sienna\", title = \"N = 16\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-.025 ~ pop_mean,\n    label = \"Population Mean\",\n    color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n##\np7 &lt;- gf_dhistogram(~mean,\n  data = samples_32_1000,\n  na.rm = TRUE,\n  color = \"grey\",\n  fill = \"palegreen\", title = \"N = 32\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-.025 ~ pop_mean,\n    label = \"Population Mean\", color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n\np8 &lt;- gf_dhistogram(~mean,\n  data = samples_64_1000,\n  na.rm = TRUE,\n  color = \"grey\",\n  fill = \"violetred\", title = \"N = 64\"\n) %&gt;%\n  gf_fitdistr(linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = pop_mean,\n    color = \"blue\"\n  ) %&gt;%\n  gf_label(-.025 ~ pop_mean,\n    label = \"Population Mean\", color = \"blue\"\n  ) %&gt;%\n  gf_theme(scale_y_continuous(expand = expansion(mult = c(0.08, 0.02))))\n\n# patchwork::wrap_plots(p5,p6,p7,p8)\np5\np6\np7\np8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if we overlay these histograms on top of one another:\n\n\n\n\n\n\n\n\nFrom the histograms we learn that the sample-means are normally distributed around the population mean. This feels intuitively right because when we sample from the population, many values will be close to the population mean, and values far away from the mean will be increasingly scarce.\nLet us calculate the means of the sample-distributions:\n\n\n\nmean(~mean, data = samples_08_1000) # Mean of means!!!;-0\nmean(~mean, data = samples_16_1000)\nmean(~mean, data = samples_32_1000)\nmean(~mean, data = samples_64_1000)\npop_mean\n\n\n\n\n[1] 168.4123\n\n\n[1] 168.2593\n\n\n[1] 168.3329\n\n\n[1] 168.3789\n\n\n[1] 168.3497\n\n\n\n\nAll are pretty close to the population mean !!!\nConsider the standard deviations of the sampling distributions:\n\\[\n\\Large{sd_i = \\sqrt{\\frac{\\Sigma[{x_i - \\bar{x_i}}]^2}{n_i}}}\n\\]\n\n\nWe take the sum of all squared differences from the mean in a sample. If we divide this sum-of-squared-differences by the sample length \\(n\\), we get a sort of average squared difference, or per-capita squared error from the mean, which is called the variance.\nTaking the square root gives us an average error, which we would call, of course, the standard deviation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npop_sd\nsd(~mean, data = samples_08_1000)\nsd(~mean, data = samples_16_1000)\nsd(~mean, data = samples_32_1000)\nsd(~mean, data = samples_64_1000)\n\n\n\n\n[1] 10.15705\n\n\n[1] 3.699512\n\n\n[1] 2.53183\n\n\n[1] 1.826739\n\n\n[1] 1.299786\n\n\n\n\nThese are also decreasing steadily with sample size. How are they related to the pop_sd?\n\n\n\npop_sd\npop_sd / sqrt(8)\npop_sd / sqrt(16)\npop_sd / sqrt(32)\npop_sd / sqrt(64)\n\n\n\n\n[1] 10.15705\n\n\n[1] 3.591058\n\n\n[1] 2.539262\n\n\n[1] 1.795529\n\n\n[1] 1.269631\n\n\n\n\nAs we can see, the standard deviations of the sampling-mean-distributions is inversely proportional to the squre root of lengths of the sample.\n\n\n\n\n\n\nNoteCentral Limit Theorem\n\n\n\nNow we have enough to state the Central Limit Theorem (CLT)\n\nthe sample-means are normally distributed around the population mean. So any mean of a single sample is a good, unbiased estimate for the pop_mean\n\nthe sample-mean distributions narrow with sample length, i.e their sd decreases with increasing sample size.\n\n\\(sd \\sim \\frac{1}{sqrt(n)}\\)\nThis is regardless of the distribution of the population itself.3\n\n\n\n\nThis theorem underlies all our procedures and techniques for statistical inference, as we shall see.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#the-standard-error",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#the-standard-error",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n The Standard Error",
    "text": "The Standard Error\nConsider once again the standard deviations in each of the sample-distributions that we have generated. As we saw these decrease with sample size.\nsd = pop_sd/sqrt(sample_size)where sample-size4 here is one of c(8, 16, 32, 64).\nWe reserve the term Standard Deviation for the population, and name this computed standard deviation of the sample-mean-distributions as the Standard Error. This statistic derived from the sample-mean-distribution will help us infer our population parameters with a precise estimate of the uncertainty involved.\n\\[\nStandard\\ Error\\ \\pmb {se} = \\frac{pop.sd}{\\sqrt[]{n}}\\\\\n\\] However, we don’t normally know the pop_sd!! So now what?? So is this chicken and egg??\nLet us take SINGLE SAMPLES of each size, as we normally would:\n\nsample_08 &lt;- mosaic::sample(NHANES_adult, size = 8) %&gt;%\n  select(Height)\nsample_16 &lt;- mosaic::sample(NHANES_adult, size = 16) %&gt;%\n  select(Height)\nsample_32 &lt;- mosaic::sample(NHANES_adult, size = 32) %&gt;%\n  select(Height)\nsample_64 &lt;- mosaic::sample(NHANES_adult, size = 64) %&gt;%\n  select(Height)\n##\nsd(~Height, data = sample_08)\n\n[1] 7.57062\n\nsd(~Height, data = sample_16)\n\n[1] 6.971893\n\nsd(~Height, data = sample_32)\n\n[1] 9.625545\n\nsd(~Height, data = sample_64)\n\n[1] 10.77258\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAs we saw at the start of this module, the sample-means are distributed around the pop_mean, AND it appears that the sample-sds are also distributed around the pop_sd!\nHowever, the sample-sd distribution is not Gaussian, and the sample_sd is also not an unbiased estimate for the pop_sd. However, when the sample size \\(n\\) is large (\\(n &gt;= 30\\)), the sample-sd distribution approximates the Gaussian, and the bias is small.\n\n\nAnd so, in the same way, we approximate the pop_sd with the sd of a single sample of the same length. Hence the Standard Error can be computed from a single sample as:\n\\[\nStandard\\ Error\\ \\pmb {se} = \\frac{sample.sd}{\\sqrt[]{n}}\n\\]\nWith these, the Standard Errors(SE) evaluate to:\n\n\n\npop_sd &lt;- sd(~Height, data = NHANES_adult)\npop_sd\nsd(~Height, data = sample_08) / sqrt(8)\nsd(~Height, data = sample_16) / sqrt(16)\nsd(~Height, data = sample_32) / sqrt(32)\nsd(~Height, data = sample_64) / sqrt(64)\n\n\n\n\n[1] 10.15705\n\n\n[1] 2.676618\n\n\n[1] 1.742973\n\n\n[1] 1.701572\n\n\n[1] 1.346573",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#confidence-intervals",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#confidence-intervals",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n Confidence intervals",
    "text": "Confidence intervals\nWhen we work with samples, we want to be able to speak with a certain degree of confidence about the population mean, based on the evaluation of one sample mean, not a large number of them. Given that sample-means are normally distributed around the pop_mean, we can say that \\(68\\%\\) of all possible sample-mean lie within \\(\\pm SE\\) of the population mean; and further that \\(95 \\%\\) of all possible sample-mean lie within \\(\\pm 2*SE\\) of the population mean. These two constants \\(\\pm 1\\) and \\(\\pm 2\\) are the z-scores from the z-distribution we saw earlier:\n\n\n\n\n\n\n\n\nThese two intervals \\(sample.mean \\pm SE\\) and \\(sample.mean \\pm 2*SE\\) are called the confidence intervals for the population mean, at levels \\(68\\%\\) and \\(95 \\%\\) probability respectively.\nHow do these vary with sample size \\(n\\)?f\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ntbl_1 &lt;- get_ci(samples_08_1000, level = 0.95)\ntbl_2 &lt;- get_ci(samples_16_1000, level = 0.95)\ntbl_3 &lt;- get_ci(samples_32_1000, level = 0.95)\ntbl_4 &lt;- get_ci(samples_64_1000, level = 0.95)\nrbind(tbl_1, tbl_2, tbl_3, tbl_4) %&gt;%\n  rownames_to_column(\"index\") %&gt;%\n  cbind(\"sample_size\" = c(8, 16, 32, 64)) %&gt;%\n  gf_segment(index + index ~ lower_ci + upper_ci) %&gt;%\n  gf_vline(xintercept = pop_mean) %&gt;%\n  gf_labs(\n    title = \"95% Confidence Intervals for the Mean\",\n    subtitle = \"Varying samples sizes 8-16-32-64\",\n    y = \"Sample Size\", x = \"Mean Ranges\"\n  ) %&gt;%\n  gf_refine(scale_y_discrete(labels = c(8, 16, 32, 64))) %&gt;%\n  gf_refine(annotate(geom = \"label\", x = pop_mean + 1.75, y = 1.5, label = \"Population Mean\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantConfidence Intervals Shrink!\n\n\n\nYes, with sample size! Why is that a good thing?\n\n\nSo finally, we can pretend that our sample is also distributed normally ( i.e Gaussian) and use its sample_sd as a substitute for pop_sd, plot the standard errors on the sample histogram, and see where our believed mean is.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nsample_mean &lt;- mean(~Height, data = sample_16)\nse &lt;- sd(~Height, data = sample_16) / sqrt(16)\n#\nxqnorm(\n  p = c(0.025, 0.975),\n  mean = sample_mean,\n  sd = sd(~Height, data = sample_16),\n  return = c(\"plot\"), verbose = F\n) %&gt;%\n  gf_vline(xintercept = ~pop_mean, colour = \"black\") %&gt;%\n  gf_vline(xintercept = mean(~Height, data = sample_16), colour = \"purple\") %&gt;%\n  gf_labs(\n    title = \"Confidence Intervals and the Bell Curve. N=16\",\n    subtitle = \"Sample is plotted as theoretical Gaussian Bell Curve\"\n  ) %&gt;%\n  gf_refine(\n    annotate(geom = \"label\", x = pop_mean + 15, y = 0.05, label = \"Population Mean\"),\n    annotate(geom = \"label\", x = sample_mean - 15, y = 0.05, label = \"Sample Mean\", colour = \"purple\")\n  )\n\n\n\n\n\n\n\n\npop_mean\nse &lt;- sd(~Height, data = sample_16) / sqrt(16)\nmean(~Height, data = sample_16) - 2.0 * se\nmean(~Height, data = sample_16) + 2.0 * se\n\n[1] 168.3497\n[1] 164.3203\n[1] 171.2922\n\n\nSo if our believed mean is within the Confidence Intervals, then OK, we can say our belief may be true. If it is way outside the confidence intervals, we have to think that our belief may be flawed and accept and alternative interpretation.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#clt-workflow",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#clt-workflow",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n CLT Workflow",
    "text": "CLT Workflow\nThus if we want to estimate a population mean:\n\nwe take one random sample from the population of length \\(n\\)\n\nwe calculate the mean from the sample sample-mean\n\nwe calculate the sample-sd\n\nwe calculate the Standard Error as \\(\\frac{sample-sd}{\\sqrt[]{n}}\\)\n\nwe calculate 95% confidence intervals for the population parameter based on the formula \\(CI_{95\\%}= sample.mean \\pm 2*SE\\).\nSince Standard Error decreases with sample size, we need to make our sample of adequate size.( \\(n=30\\) seems appropriate in most cases. Why?)\nAnd we do not have to worry about the distribution of the population. It need not be normal/Gaussian/Bell-shaped !!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#clt-assumptions",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#clt-assumptions",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n CLT Assumptions",
    "text": "CLT Assumptions\n\nSample is of “decent length” \\(n &gt;= 30\\);\nTherefore sample histogram is Gaussian",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#an-interactive-sampling-app",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#an-interactive-sampling-app",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n An interactive Sampling app",
    "text": "An interactive Sampling app\nHere below is an interactive sampling app. Play with the different settings, especially the distribution in the population to get a firm grasp on Sampling and the CLT.\nhttps://gallery.shinyapps.io/CLT_mean/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#references",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "\n References",
    "text": "References\n\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine, OpenIntro Statistics. https://www.openintro.org/book/os/\n\nRafael Irizzary. Introduction to Data Science. Chapter 15. Statistical Inference.https://rafalab.dfci.harvard.edu/dsbook/inference.html#populations-samples-parameters-and-estimates\n\nStats Test Wizard.https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx\n\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine: OpenIntro Statistics. Available online. https://www.openintro.org/book/os/\n\nMåns Thulin, Modern Statistics with R: From wrangling and exploring data to inference and predictive modelling. http://www.modernstatisticswithr.com/\n\nJonas Kristoffer Lindeløv, Common statistical tests are linear models (or: how to teach stats) https://lindeloev.github.io/tests-as-linear/\n\nCheatSheet https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf\n\nCommon statistical tests are linear models: a work through by Steve Doogue https://steverxd.github.io/Stat_tests/\n\nJeffrey Walker “Elements of Statistical Modeling for Experimental Biology”. https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/\n\nAdam Loy, Lendie Follett & Heike Hofmann (2016) Variations of Q–Q Plots: The Power of Our Eyes!, The American Statistician, 70:2, 202-214, DOI: 10.1080/00031305.2015.1077728\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nregressinator\n0.2.0\nReinhart (2024)\n\n\nsmovie\n1.1.6\nNorthrop (2023)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://doi.org/10.32614/CRAN.package.visualize.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nNorthrop, Paul J. 2023. smovie: Some Movies to Illustrate Concepts in Statistics. https://doi.org/10.32614/CRAN.package.smovie.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nReinhart, Alex. 2024. regressinator: Simulate and Diagnose (Generalized) Linear Models. https://doi.org/10.32614/CRAN.package.regressinator.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/index.html#footnotes",
    "title": "🎲 Samples, Populations, Statistics and Inference",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://stats.libretexts.org/Bookshelves/Introductory_Statistics/Introductory_Statistics_(Shafer_and_Zhang)/06%3A_Sampling_Distributions/6.01%3A_The_Mean_and_Standard_Deviation_of_the_Sample_Mean↩︎\nhttps://mathworld.wolfram.com/StandardDeviationDistribution.html↩︎\nThe `Height` variable seems to be normally distributed at population level. We will try other non-normal population variables as an exercise in the tutorials.↩︎\nOnce sample size = population, we have complete access to the population and there is no question of estimation error! So sample_sd = pop_sd!↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🎲 Samples, Populations, Statistics and Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html",
    "title": "Inference for numerical data",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using the tidyverse suite of packages, and perform statistical inference using infer. The data can be found in the companion package for OpenIntro resources, openintro.\nLet’s load the packages.\n\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\nlibrary(skimr)\n\n\nTo create your new lab report, in RStudio, go to New File -&gt; R Markdown… Then, choose From Template and then choose Lab Report for OpenIntro Statistics Labs from the list of templates.\n\nEvery two years, the Centers for Disease Control and Prevention conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\nLoad the yrbss data set into your workspace.\n\ndata(yrbss)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#getting-started",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#getting-started",
    "title": "Inference for numerical data",
    "section": "",
    "text": "In this lab, we will explore and visualize the data using the tidyverse suite of packages, and perform statistical inference using infer. The data can be found in the companion package for OpenIntro resources, openintro.\nLet’s load the packages.\n\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(openintro)\nlibrary(infer)\nlibrary(skimr)\n\n\nTo create your new lab report, in RStudio, go to New File -&gt; R Markdown… Then, choose From Template and then choose Lab Report for OpenIntro Statistics Labs from the list of templates.\n\nEvery two years, the Centers for Disease Control and Prevention conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from high schoolers (9th through 12th grade), to analyze health patterns. You will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\nLoad the yrbss data set into your workspace.\n\ndata(yrbss)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#exploratory-data-analysis",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#exploratory-data-analysis",
    "title": "Inference for numerical data",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nThere are observations on 13 different variables, some categorical and some numerical. The meaning of each variable can be found by bringing up the help file: type this in your console\n\n\n\n\n\n\nNote\n\n\n\nhelp(yrbss)\n\n\n\n\n\n\n\n\nNote\n\n\n\n1 . What are the cases in this data set? How many cases are there in our sample?\n\n\nYou will first start with analyzing the weight of the participants in kilograms: weight.\nUsing visualization and summary statistics, describe the distribution of weights. The inspect() function from the mosaic package produces nice summaries of the variables in the dataset, separating categorical (character) variables from quantitative variables.\n\nmosaic::inspect(yrbss)\n\n\ncategorical variables:  \n                      name     class levels     n missing\n1                   gender character      2 13571      12\n2                    grade character      5 13504      79\n3                 hispanic character      2 13352     231\n4                     race character      5 10778    2805\n5               helmet_12m character      6 13272     311\n6   text_while_driving_30d character      8 12665     918\n7  hours_tv_per_school_day character      7 13245     338\n8 school_night_hours_sleep character      7 12335    1248\n                                   distribution\n1 male (51.2%), female (48.8%)                 \n2 9 (26.6%), 12 (26.3%), 11 (23.6%) ...        \n3 not (74.4%), hispanic (25.6%)                \n4 White (59.5%) ...                            \n5 never (52.6%), did not ride (34.3%) ...      \n6 0 (37.8%), did not drive (36.7%) ...         \n7 2 (20.4%), &lt;1 (16.4%), 3 (16.1%) ...         \n8 7 (28.1%), 8 (21.8%), 6 (21.5%) ...          \n\nquantitative variables:  \n                  name   class   min    Q1 median    Q3    max      mean\n1                  age integer 12.00 15.00  16.00 17.00  18.00 16.157041\n2               height numeric  1.27  1.60   1.68  1.78   2.11  1.691241\n3               weight numeric 29.94 56.25  64.41 76.20 180.99 67.906503\n4 physically_active_7d integer  0.00  2.00   4.00  7.00   7.00  3.903005\n5 strength_training_7d integer  0.00  0.00   3.00  5.00   7.00  2.949948\n          sd     n missing\n1  1.2637373 13506      77\n2  0.1046973 12579    1004\n3 16.8982128 12579    1004\n4  2.5641046 13310     273\n5  2.5768522 12407    1176\n\n\nNext, consider the possible relationship between a high schooler’s weight and their physical activity. Plotting the data is a useful first step because it helps us quickly visualize trends, identify strong associations, and develop research questions.\nFirst, let’s create a new variable physical_3plus, which will be coded as either “yes” if the student is physically active for at least 3 days a week, and “no” if not. Recall that we have several missing data in that column, so we will (sadly) drop these before generating the new variable:\n\nyrbss &lt;- yrbss %&gt;%\n  drop_na() %&gt;%\n  mutate(\n    physical_3plus = if_else(physically_active_7d &gt;= 2, \"yes\", \"no\"),\n    physical_3plus = factor(physical_3plus,\n      labels = c(\"yes\", \"no\"),\n      levels = c(\"yes\", \"no\")\n    )\n  )\n# Let us check\nyrbss %&gt;% count(physical_3plus)\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nMake a side-by-side violin box plots of physical_3plus and weight.\nIs there a relationship between these two variables? What did you expect and why?\n\n\n\n\ngf_boxplot(weight ~ physical_3plus,\n  fill = ~physical_3plus,\n  data = yrbss,\n  draw_quantiles = TRUE\n)\n\n\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#inference",
    "title": "Inference for numerical data",
    "section": "Inference",
    "text": "Inference\n\n\n\n\n\n\nImportant\n\n\n\nAre all conditions necessary for inference satisfied? Comment on each. You can compute the group sizes with the summarize command above by defining a new variable with the definition n().\n\n\n\n\n\n\n\n\nNote\n\n\n\nWrite the hypotheses for testing if the average weights are different for those who exercise at least times a week and those who don’t.\nWrite here !\n\n\nWe will do this in two ways, just for fun: one using mosaic and the other using infer.\nBut first, we need to initialize the test, which we will save as obs_diff.\n\nobs_diff_infer &lt;- yrbss %&gt;%\n  specify(weight ~ physical_3plus) %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\n\n\n  \n\n\nobs_diff_mosaic &lt;- diffmean(~ weight | physical_3plus, data = yrbss)\nobs_diff_mosaic\n\n diffmean \n-1.694383 \n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\n\nInference Using `infer`\nInference Using `mosaic`\n\n\n\nNext, we will work through creating a permutation distribution using tools from the infer package.\nRecall that the specify() function is used to specify the variables you are considering (notated y ~x), and you can use the calculate() function to specify the statistic you want to calculate and the order of subtraction you want to use. For this hypothesis, the statistic you are searching for is the difference in means, with the order being yes - no.\nAfter you have calculated your observed statistic, you need to create a permutation distribution. This is the distribution that is created by shuffling the observed weights into new physical_3plus groups, labeled “yes” and “no”.\nWe will save the permutation distribution as null_dist.\n\nnull_dist &lt;- yrbss %&gt;%\n  specify(weight ~ physical_3plus) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\n\nThe hypothesize() function is used to declare what the null hypothesis is. Here, we are assuming that student’s weight is independent of whether they exercise at least 3 days or not.\nWe should also note that the type argument within generate() is set to \"permute\". This ensures that the statistics calculated by the calculate() function come from a reshuffling of the data (not a resampling of the data)! Finally, the specify() and calculate() steps should look familiar, since they are the same as what we used to find the observed difference in means!\nWe can visualize this null distribution with the following code:\n\ngf_histogram(data = null_dist, ~stat)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAdd a vertical red line to the plot above, demonstrating where the observed difference in means (obs_diff_mosaic) falls on the distribution.\nHow many of these null_dist permutations have a difference at least as large (or larger) as obs_diff_mosaic?\n\n\n\nNow that you have calculated the observed statistic and generated a permutation distribution, you can calculate the p-value for your hypothesis test using the function get_p_value() from the infer package.\n\nnull_dist %&gt;%\n  get_p_value(obs_stat = obs_diff_infer, direction = \"two_sided\")\n\n\n  \n\n\n\n\n\n\n\nWhat warning message do you get? Why do you think you get this warning message?\nConstruct and record a confidence interval for the difference between the weights of those who exercise at least three times a week and those who don’t, and interpret this interval in context of the data.\n\n\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic &lt;- do(1000) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\ngf_histogram(~diffmean, data = null_dist_mosaic) %&gt;%\n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n\n\n\n# p-value\nprop(~ diffmean &gt;= obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~diffmean, p = 0.95, data = null_dist_mosaic)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#more-practice",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/inf_for_numerical_data.html#more-practice",
    "title": "Inference for numerical data",
    "section": "More Practice",
    "text": "More Practice\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom) # Tidy Test data\nlibrary(resampledata3) # Datasets from Chihara and Hesterberg's book\nlibrary(gt) # for tables\n\n\n\nShow the Code# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n### https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# We have locally downloaded the `Alegreya` and `Roboto Condensed` fonts.\n# This ensures we are GDPR-compliant, and not using Google Fonts directly.\n# Let us import these local fonts into our session and use them to define our ggplot theme.\nlibrary(sysfonts)\n\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\n\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = \"Alegreya\"), # set default font family for all text\n\n      # text elements\n      plot.title = element_text( # title\n        family = \"Alegreya\", # set font family\n        size = 18, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        vjust = 2\n      ), # raise slightly\n\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = \"Alegreya\", # font family\n        size = 14\n      ), # font size\n\n      plot.caption = element_text( # caption\n        family = \"Alegreya\", # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 10\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#setting-up-r-packages",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom) # Tidy Test data\nlibrary(resampledata3) # Datasets from Chihara and Hesterberg's book\nlibrary(gt) # for tables\n\n\n\nShow the Code# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n### https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# We have locally downloaded the `Alegreya` and `Roboto Condensed` fonts.\n# This ensures we are GDPR-compliant, and not using Google Fonts directly.\n# Let us import these local fonts into our session and use them to define our ggplot theme.\nlibrary(sysfonts)\n\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\n\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = \"Alegreya\"), # set default font family for all text\n\n      # text elements\n      plot.title = element_text( # title\n        family = \"Alegreya\", # set font family\n        size = 18, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        vjust = 2\n      ), # raise slightly\n\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = \"Alegreya\", # font family\n        size = 14\n      ), # font size\n\n      plot.caption = element_text( # caption\n        family = \"Alegreya\", # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 10\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#introduction-to-inference-for-paired-data",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#introduction-to-inference-for-paired-data",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Introduction to Inference for Paired Data",
    "text": "Introduction to Inference for Paired Data\n\n What is Paired Data?\nSometimes the data is collected on the same set of individual categories, e.g. scores by sport persons in two separate tournaments, or sales of identical items in two separate locations of a chain store. Or say the number of customers in the morning and in the evening, at a set of store locations. In this case we treat the two sets of observations as paired, since they correspond to the same set of observable entities. This is how some experiments give us paired data.\nWe would naturally be interested in the differences in means across these two sets, which exploits this paired nature. In this module, we will examine tests for this purpose.\n\n Workflow for Inference for Paired Means\n\n\n\n\n\nflowchart TD\n    A[Inference for Paired Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test with paired=TRUE]\n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction, paired =TRUE]\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test with paired = TRUE]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n \n\n\n\n\n\n\nWe will now use a couple to case studies to traverse all the possible pathways in the Workflow above.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#case-study-1-results-from-a-diving-championship",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#case-study-1-results-from-a-diving-championship",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Case Study #1: Results from a Diving Championship",
    "text": "Case Study #1: Results from a Diving Championship\nHere we have swimming records across a Semi-Final and a Final:\n\n Inspecting and Charting Data\n\ndata(\"Diving2017\", package = \"resampledata3\")\nDiving2017\nDiving2017_inspect &lt;- inspect(Diving2017)\nDiving2017_inspect$categorical\nDiving2017_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nThe data is made up of paired observations per swimmer, one for the semi-final and one for the final race. There are 12 swimmers and therefore 12 paired records. How can we quickly visualize this data?\nLet us first make this data into long form:\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nDiving2017_long &lt;- Diving2017 %&gt;%\n  pivot_longer(\n    cols = c(Final, Semifinal),\n    names_to = \"race\",\n    values_to = \"scores\"\n  )\nDiving2017_long\n\n\n\n\n\n  \n\n\n\n\n\nNext, histograms and densities of the two variables at hand:\n\n\n\nDiving2017_long %&gt;%\n  gf_density(~scores,\n    fill = ~race,\n    alpha = 0.5,\n    title = \"Diving Scores\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n  gf_facet_grid(~race) %&gt;%\n  gf_fitdistr(dist = \"dnorm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiving2017_long %&gt;%\n  gf_col(\n    fct_reorder(Name, scores) ~ scores,\n    fill = ~ race,\n    alpha = 0.5,\n    position = \"dodge\",\n    xlab = \"Scores\",\n    ylab = \"Name\",\n    title = \"Diving Scores\"\n  )  %&gt;% \n  gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n\n\n\n\nError in parse(text = input): &lt;text&gt;:12:0: unexpected end of input\n10:   )  %&gt;% \n11:   gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n   ^\n\n\n\n\n\n\n\nDiving2017_long %&gt;%\n  gf_boxplot(\n    scores ~ race,\n    fill = ~ race,\n    alpha = 0.5,\n    xlab = \"Race\",\n    ylab = \"Scores\",\n    title = \"Diving Scores\"\n  ) %&gt;% \n    gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n\n\n\n\nError in parse(text = input): &lt;text&gt;:12:0: unexpected end of input\n10:     gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n11: \n   ^\n\n\n\n\nWe see that:\n\nThe data are not normally distributed. With just such few readings (n &lt; 30) it was just possible…more readings would have helped. We will verify this aspect formally very shortly.\n\nThere is no immediately identifiable trend in score changes from one race to the other.\n\nAlthough the two medians appear to be different, the box plots overlap considerably. So one cannot visually conclude that the two sets of race timings have different means.\nA.  Check for Normality\nLet us also complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution.\nshapiro.test(Diving2017$Final)\nshapiro.test(Diving2017$Semifinal)\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Diving2017$Final\nW = 0.9184, p-value = 0.273\n\n\n    Shapiro-Wilk normality test\n\ndata:  Diving2017$Semifinal\nW = 0.86554, p-value = 0.05738\n\n\n\nHmmm….the Shapiro-Wilk test suggests that both scores are normally distributed (!!!), though Semifinal is probably marginally so.\nCan we check this comparison also with plots? We can plot Q-Q plots for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nset.rseed(1234)\nDiving2017 %&gt;%\n  mutate(\n    Final_norm = rnorm(\n      n = 12,\n      mean = mean(Final),\n      sd = sd(Final)\n    ),\n    Semifinal_norm = rnorm(\n      n = 12,\n      mean = mean(Semifinal),\n      sd = sd(Semifinal)\n    )\n  ) %&gt;%\n  pivot_longer(\n    cols =\n      c(Semifinal, Final, Semifinal_norm, Final_norm),\n    names_to = \"score_type\", values_to = \"value\"\n  ) %&gt;%\n  gf_boxplot(value ~ score_type,\n    fill = ~score_type,\n    show.legend = FALSE\n  ) %&gt;%\n  gf_labs(title = \"Comparing Data and Normal Boxplots\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n###\nDiving2017_long %&gt;%\n  gf_qq(~ scores | race, size = 2) %&gt;%\n  gf_qqline(ylab = \"scores\", xlab = \"theoretical normal\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile the boxplots are not very evocative, we see in the QQ-plots that the Final scores are closer to the straight line than the Semifinal scores. But it is perhaps still hard to accept the data as normally distributed…hmm.\nB.  Check for Variances\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test(scores ~ race,\n  data = Diving2017_long,\n  ratio = 1, # What we believe\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe variances are not significantly different, as seen by the \\(p.value = 0.08\\).\nSo to summarise our data checks:\n\n\n\n\n\n\nNoteConditions\n\n\n\n\ndata are normally distributed\n\nvariances are not significantly different\n\n\n\n\n\n Hypothesis\nBased on the graph, how would we formulate our Hypothesis? We wish to infer whether there is any change in performance between the population of swimmers who might have participated in these two races. So accordingly:\n\\[\nH_0: \\mu_{semifinal} = \\mu_{final}\\\\\n\\]\n\\[\nH_a: \\mu_{semifinal} \\ne \\mu_{final}\\\n\\]\n\n Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, with the argument only.2 = FALSE to allow for paired data:\n\nobs_diff_swim &lt;- diffmean(scores ~ race,\n  data = Diving2017_long,\n  only.2 = FALSE\n) # paired data\n\n# Can use this also\n# formula method is better for permutation test!\n# obs_diff_swim &lt;- mean(~ (Final - Semifinal), data = Diving2017)\n\nobs_diff_swim\n\ndiffmean \n -11.975 \n\n\n\n Inference\n\n\nUsing the paired t.test\nUsing non-parametric paired Wilcoxon test\nUsing the Linear Model Method\nUsing Permutation Tests\n\n\n\nSince the data variables satisfy the assumption of being normally distributed, and the variances are not significantly different, we may attempt the classical t.test with paired data. (we will use the mosaic variant). Type help(t.test) in your Console. Our model would be:\n\\[\nmean(Final(i) - Semi\\_final(i)) = \\beta_0 \\\\\n\\]\nAnd that: \\[\nH_0: \\mu_{final} - \\mu_{semifinal} = 0;\n\\] \\[\nH_a: \\mu_{final} - \\mu_{semifinal} \\ne 0;\\\\\n\\]\n\nmosaic::t.test(\n  x = Diving2017$Semifinal,\n  y = Diving2017$Final,\n  paired = TRUE, var.equal = FALSE\n) %&gt;% broom::tidy()\n\n\n  \n\n\n\nThe confidence interval spans the zero value, and the p.value is a high \\(0.259\\), so there is no reason to accept alternative hypothesis that the means are different. Hence we say that there is no evidence of a difference between SemiFinal and Final scores.\n\n\nWell, we might consider ( based on knowledge of the sport ) that at least one of the variables does not meet the normality criteria, and though their variances are not significantly different. So we would attempt a non-parametric Wilcoxon test, that uses the signed-rank of the paired data differences, instead of the data variables. Our model would be:\n\\[\nmean(\\ sign.rank[\\ Final(i) - Semifinal(i)\\ ]\\ ) = \\beta_0 \\\\\n\\] \\[\nH_0: \\mu_{final} - \\mu_{semifinal} = 0;\n\\] \\[\nH_a: \\mu_{final} - \\mu_{semifinal} \\ne 0;\\\\\n\\]\n\nwilcox.test(\n  x = Diving2017$Semifinal,\n  y = Diving2017$Final,\n  mu = 0, # belief\n  alternative = \"two.sided\", # difference either way\n  paired = TRUE,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nHere also with the p.value being \\(0.3804\\), we have no reason to accept the Alternative Hypothesis. The parametric t.test and the non-parametric wilcox.test agree in their inferences.\n\n\nWe can apply the linear-model-as-inference interpretation both to the original data and to the sign.rank data: \n\\[\nlm(y_i - x_i \\sim 1) = \\beta_0\\\\\n\\\\ and\\\\\nlm(\\ sign.rank[\\ Final(i) - Semifinal(i)\\ ] \\sim 1) = \\beta_0\\\\\n\\]\nAnd the Hypothesis for both interpretations would be:\\[\nH_0: \\beta_0 = 0\\\\\n\\\\\\\nH_a: \\beta_0 \\ne 0\\\\\n\\]\n\nlm(Semifinal - Final ~ 1, data = Diving2017) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95)\n\n# Create a sign-rank function\nsigned_rank &lt;- function(x) {\n  sign(x) * rank(abs(x))\n}\n\nlm(signed_rank(Semifinal - Final) ~ 1,\n  data = Diving2017\n) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95)\n\n\n  \n\n\n  \n\n\n\nWe observe that using the linear model method for the original scores and the sign-rank scores both sdo not permit us to reject the \\(H_0\\) Null Hypothesis, since p.values are high, and the confidence.intervals straddle \\(0\\).\n\n\nFor the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis (paired data!!) and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\npolarity &lt;- c(rep(1, 6), rep(-1, 6))\n# 12 +/- 1s,\n# 6 each to make sure there is equal probability\npolarity\n##\nnull_dist_swim &lt;- do(4999) *\n  mean(\n    data = Diving2017,\n    ~ (Final - Semifinal) * # take (pairwise) differences\n      mosaic::resample(polarity, # Swap polarity randomly\n        replace = TRUE\n      )\n  )\n##\nnull_dist_swim\n\n\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\n\n\n  \n\n\n\n\nLet us plot the NULL distribution and compare it with the actual observed differences in the race times:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_histogram(data = null_dist_swim, ~mean) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_swim,\n    colour = \"red\",\n    linewidth = 1\n  )\n###\ngf_ecdf(data = null_dist_swim, ~mean, linewidth = 1) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_swim,\n    colour = \"red\",\n    linewidth = 1\n  )\n###\nprop1(~ mean &lt;= obs_diff_swim, data = null_dist_swim)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n   0.1272 \n\n\n\nHmm…so by generating 4999 shuffles of score-difference polarities, it does appear that we can not only obtain the current observed difference but even surpass it frequently. So it does seem that there is no difference in means between Semi-Final and Final swimming scores.\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nmosaic::t.test(\n  x = Diving2017$Semifinal,\n  y = Diving2017$Final,\n  paired = TRUE\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"t.test\")\n\nlm(Semifinal - Final ~ 1, data = Diving2017) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model\")\n\nwilcox.test(\n  x = Diving2017$Semifinal,\n  y = Diving2017$Final,\n  paired = TRUE\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"palegreen\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Wilcoxon test\")\n\nlm(signed_rank(Semifinal - Final) ~ 1,\n  data = Diving2017\n) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"palegreen\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with sign.rank\")\n\n\n\n\n\n\nt.test\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-11.975\n-1.190339\n0.2589684\n11\n-34.11726\n10.16726\nPaired t-test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n-11.975\n10.06016\n-1.190339\n0.2589684\n-34.11726\n10.16726\n\n\n\n\n\n\n\n\nWilcoxon test\n\n\nstatistic\np.value\nmethod\nalternative\n\n\n\n27\n0.3803711\nWilcoxon signed rank exact test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with sign.rank\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n-2\n2.135558\n-0.9365236\n0.3691097\n-6.70033\n2.70033\n\n\n\n\n\nThe linear model and the t.test are nearly identical in performance; the p.values are the same. The same is also true of the wilcox.test and the linear model with sign-rank data differences. This is of course not surprising!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#case-study-2-walmart-vs-target",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#case-study-2-walmart-vs-target",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Case Study #2: Walmart vs Target",
    "text": "Case Study #2: Walmart vs Target\nIs there a difference in the price of Groceries sold by the two retailers Target and Walmart? The data set Groceries contains a sample of grocery items and their prices advertised on their respective web sites on one specific day. We will:\n\nInspect the data set, then explain why this is an example of matched pairs data.\nCompute summary statistics of the prices for each store.\nConduct a permutation test to determine whether or not there is a difference in the mean prices.\nCreate a histogram bar-chart of the difference in prices. What is unusual about Quaker Oats Life cereal?\nRedo the hypothesis test without this observation. Would we reach the same conclusion?\n\n\n Inspecting and Charting Data\n\ndata(\"Groceries\")\nGroceries &lt;- Groceries %&gt;%\n  mutate(Product = stringr::str_squish(Product)) # Knock off extra spaces\nGroceries\nGroceries_inspect &lt;- inspect(Groceries)\nGroceries_inspect$categorical\nGroceries_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nThere are just 30 prices for each vendor….just barely enough to get an idea of what the distribution might be. Let us plot the prices for the products, as box plots after pivoting the data to long form, 1 and as bar charts:\n## Set graph theme\ntheme_set(new = theme_custom())\n##\nGroceries_long &lt;- Groceries %&gt;%\n  pivot_longer(\n    cols = c(Walmart, Target),\n    names_to = \"store\",\n    values_to = \"prices\"\n  ) %&gt;%\n  mutate(store = as_factor(store))\n\n\n\nLet us plot histograms/densities of the two variables that we wish to compare. We will also overlay a Gaussian distribution for comparison:\n\n\n\nGroceries_long %&gt;%\n  gf_dhistogram(~prices,\n    fill = ~store,\n    alpha = 0.5,\n    title = \"Grocery Costs\"\n  ) %&gt;%\n  gf_facet_grid(~store) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroceries_long %&gt;%\n  gf_density(~prices,\n    fill = ~store,\n    alpha = 0.5,\n    title = \"Grocery Costs\"\n  ) %&gt;%\n  gf_facet_grid(~store) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\nNot close to the Gaussian…there is clearly some skew to the right, with some items being very costly compared to the rest. More when we check the assumptions on data for the tests.\nHow about price differences, what we are interested in?\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\nGroceries_long %&gt;%\n  gf_boxplot(prices ~ store,\n    fill = ~store\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Set graph theme\ntheme_set(new = theme_custom())\n##\nGroceries_long %&gt;%\n  gf_col(fct_reorder(Product, prices) ~ prices,\n    fill = ~store,\n    alpha = 0.5,\n    position = \"dodge\",\n    xlab = \"Prices\",\n    ylab = \"\",\n    title = \"Groceries Costs\"\n  ) %&gt;%\n  gf_col(\n    data =\n      Groceries_long %&gt;%\n        filter(\n          Product == \"Quaker Oats Life Cereal Original\"\n        ),\n    fct_reorder(Product, prices) ~ prices,\n    fill = ~store,\n    position = \"dodge\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Apart from this Product, the rest have no discernible trend either way. Let us check observed statistic (the mean difference in prices)\n\n\nobs_diff_price &lt;- diffmean(prices ~ store,\n  data = Groceries_long,\n  only.2 = FALSE\n)\n# Can also use\n# obs_diff_price &lt;-  mean( ~ Walmart - Target, data = Groceries)\nobs_diff_price\n\n\n\n  diffmean \n0.05666667 \n\n\n\n\n Hypothesis\nBased on the graph, how would we formulate our Hypothesis? We wish to infer whether there is any change in prices, per product between the two Store chains. So accordingly:\n\\[\nH_0: \\mu_{Walmart} = \\mu_{Target}\\\\\n\\]\n\\[\nH_a: \\mu_{Walmart} \\ne \\mu_{Target}\\\n\\]\nTesting for Assumptions on the Data\nThere are a few checks we need to make of our data, to decide what test procedure to use.\nA.  Check for Normality\n\n\n\nshapiro.test(Groceries$Walmart)\nshapiro.test(Groceries$Target)\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Groceries$Walmart\nW = 0.78662, p-value = 3.774e-05\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Groceries$Target\nW = 0.79722, p-value = 5.836e-05\n\n\n\n\nFor both tests, we see that the p.value is very small, indicating that the data are unlikely to be normally distributed. This means we cannot apply a standard paired t.test and need to use the non-parametric wilcox.test, that does not rely on the assumption of normality.\nB.  Check for Variances\nLet us check if the two variables have similar variances:\n\nvar.test(Groceries$Walmart, Groceries$Target)\n\n\n    F test to compare two variances\n\ndata:  Groceries$Walmart and Groceries$Target\nF = 0.97249, num df = 29, denom df = 29, p-value = 0.9406\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4628695 2.0431908\nsample estimates:\nratio of variances \n         0.9724868 \n\n\nIt appears from the \\(p.value = 0.9\\) and the \\(Confidence Interval = [0.4629, 2.0432]\\), which includes \\(1\\), that we cannot reject the NULL Hypothesis that the variances are not significantly different.\n\n Inference\n\n\nUsing paired t.test\nUsing non-parametric paired Wilcoxon test\nUsing the Linear Model Method\nUsing Permutation Tests\n\n\n\nWell, the variables are not normally distributed, so a standard t.test is not advised, even if the variances are similar. We can still try:\n\nmosaic::t_test(Groceries$Walmart, Groceries$Target, paired = TRUE) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(0.64\\) ! And the Confidence Interval straddles \\(0\\). So the t.test gives us no reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\n\n\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\n# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say,\n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(Groceries$Walmart, Groceries$Target,\n  data = Groceries_long,\n  digits.rank = 7, paired = TRUE,\n  conf.int = TRUE, conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe Wilcoxon test result is very interesting: the p.value says there is a significant difference between the two store prices, and the confidence.interval also is unipolar…\n\n\nAs before we can do the linear model for both the original data and the sign.rank data. The test statistic is again the difference between thetwo variables:\n\nlm(Target - Walmart ~ 1, data = Groceries) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95)\n\n# Create a sign-rank function\nsigned_rank &lt;- function(x) {\n  sign(x) * rank(abs(x))\n}\n\nlm(signed_rank(Target - Walmart) ~ 1,\n  data = Groceries\n) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95)\n\n\n  \n\n\n  \n\n\n\nVery interesting results, but confirming what we saw earlier: The Linear Model with the original data reports no significant difference, but the linear model with sign-ranks, suggests there is a significant difference in means prices between stores!\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\n# | layout: [[15, 85, 15]]\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\npolarity &lt;- c(rep(1, 15), rep(-1, 15))\n##\nnull_dist_price &lt;- do(9999) *\n  mean(\n    data = Groceries,\n    ~ (Target - Walmart) *\n      resample(polarity, replace = TRUE)\n  )\nnull_dist_price\n\n\n  \n\n\n##\ngf_histogram(data = null_dist_price, ~mean) %&gt;%\n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n\n\n\n\n\n\nprop1(~mean, data = null_dist_price)\n\nprop_-0.292 \n      2e-04 \n\n\nDoes not seem to be any significant difference in prices…\n\n\n\nAll Tests Together\nWe can put all the test results together to get a few more insights about the tests:\n\nmosaic::t_test(Groceries$Walmart, Groceries$Target, paired = TRUE) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"cyan\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"t.test\")\n###\nlm(Target - Walmart ~ 1, data = Groceries) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"cyan\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model\")\n###\nwilcox.test(Groceries$Walmart, Groceries$Target,\n  digits.rank = 7,\n  paired = TRUE,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"palegreen\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Wilcoxon Test\")\n###\nlm(signed_rank(Target - Walmart) ~ 1,\n  data = Groceries\n) %&gt;%\n  broom::tidy(conf.int = TRUE, conf.level = 0.95) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"palegreen\"),\n      cell_text(weight = \"bold\")\n    ),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with Sign.Ranks\")\n\n\n\n\n\n\nt.test\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-0.05666667\n-0.4704556\n0.6415488\n29\n-0.3030159\n0.1896825\nPaired t-test\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n0.05666667\n0.1204506\n0.4704556\n0.6415488\n-0.1896825\n0.3030159\n\n\n\n\n\n\n\n\nWilcoxon Test\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-0.104966\n95\n0.01431746\n-0.1750051\n-0.03005987\nWilcoxon signed rank test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Sign.Ranks\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n(Intercept)\n8.533333\n2.888834\n2.953902\n0.006167464\n2.625004\n14.44166\n\n\n\n\n\nClearly, the parametric tests do not detect a significant difference in prices, whereas the non-parametric tests do.\nSuppose we knock off the Quaker Cereal data item…(note the spaces in the product name)\n## Set graph theme\ntheme_set(new = theme_custom())\n##\nset.seed(12345)\nGroceries_less &lt;- Groceries %&gt;%\n  filter(Product != \"Quaker Oats Life Cereal Original\")\n##\nGroceries_less_long &lt;- Groceries_less %&gt;%\n  pivot_longer(\n    cols = c(Target, Walmart),\n    names_to = \"store\",\n    values_to = \"prices\"\n  )\n##\nwilcox.test(Groceries_less$Walmart,\n  Groceries_less$Target,\n  paired = TRUE, digits.rank = 7,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n##\nobs_diff_price_less &lt;-\n  mean(~ (Target - Walmart), data = Groceries_less)\nobs_diff_price_less\npolarity_less &lt;- c(rep(1, 15), rep(-1, 14))\n# Due to resampling this small bias makes no difference\nnull_dist_price_less &lt;-\n  do(9999) * mean(\n    data = Groceries_less,\n    ~ (Target - Walmart) * resample(polarity_less, replace = TRUE)\n  )\n##\ngf_histogram(data = null_dist_price_less, ~mean) %&gt;%\n  gf_vline(\n    xintercept = obs_diff_price_less,\n    colour = \"red\"\n  )\n##\nmean(null_dist_price_less &gt;= obs_diff_price_less)\n\n\n\n\n  \n\n\n\n[1] 0.1558621\n\n\n\n\n\n\n\n\n\n\n[1] 0.01370137\n\n\n\nWe see that removing the Quaker Oats product item from the data does give a significant difference in mean prices !!! That one price difference was in the opposite direction compared to the general trend in differences, so when it was removed, we obtained a truer picture of price differences.\nTry to do a regular parametric t.test with this reduced data!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#wait-but-why",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nPaired data is a special case of dependent data, where the observations are paired in some way. This can be due to repeated measures on the same subjects, or matched subjects in a case-control study. The paired t-test and the Wilcoxon signed-rank test are both designed for paired data, and take into account the fact that the observations are not independent.\nCan you think of an underlying experiment where the data may be paired?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#conclusion",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have learnt how to perform inference for paired-means. We have looked at the conditions that make the regular t.test possible, and learnt what to do if the conditions of normality and equal variance are not met. We have also looked at how these tests can be understood as manifestations of the linear model, with data and sign-ranked data. It should also be fairly clear now that we can test for the equivalence of two paired means, using a very simple permutation tests. Given computing power, we can always mechanize this test very quickly to get our results. And that performing this test yields reliable results without having to rely on any assumption relating to underlying distributions and so on.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#your-turn",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry the datasets in the openintro package. Use data(package = \"openintro\") in your Console to list out the data packages. Then simply type the name of the dataset in a Quarto chunk ( e.g. babynames) to read it.\nSame with the resampledata and resampledata3 packages.\nTry the datasets in the PairedData package. Yes, install this too, peasants.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#references",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "\n References",
    "text": "References\n\nPaired Independence test with the package infer: https://infer.netlify.app/articles/paired\n\nRandall Pruim, Nicholas J. Horton, Daniel T. Kaplan, StartTeaching with R\n\nhttps://bcs.wiley.com/he-bcs/Books?action=index&itemId=111941654X&bcsId=11307\nhttps://statsandr.com/blog/wilcoxon-test-in-r-how-to-compare-2-groups-under-the-non-normality-assumption/\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ngt\n1.0.0\n@gt\n\n\ninfer\n1.0.9\n@infer\n\n\nMKinfer\n1.2\n@MKinfer\n\n\nopenintro\n2.5.0\n@openintro",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/index.html#footnotes",
    "title": "🃏 Inference for Comparing Two Paired Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://raw.githubusercontent.com/gadenbuie/tidyexplain/main/images/tidyr-pivoting.gif↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for Comparing Two Paired Means"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#introduction-the-lady-who-drank-tea",
    "href": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#introduction-the-lady-who-drank-tea",
    "title": "Basics of Randomization Tests",
    "section": "Introduction: The Lady who Drank Tea",
    "text": "Introduction: The Lady who Drank Tea\n\nThere is a famous story about a lady who claimed that tea with milk tasted different depending on whether the milk was added to the tea or the tea added to the milk. The story is famous because of the setting in which she made this claim. She was attending a party in Cambridge, England, in the 1920s. Also in attendance were a number of university dons and their wives. The scientists in attendance scoffed at the woman and her claim. What, after all, could be the difference?\nAll the scientists but one, that is. Rather than simply dismiss the woman’s claim, he proposed that they decide how one should test the claim. The tenor of the conversation changed at this suggestion, and the scientists began to discuss how the claim should be tested. Within a few minutes cups of tea with milk had been prepared and presented to the woman for tasting.\nAt this point, you may be wondering who the innovative scientist was and what the results of the experiment were. The scientist was R. A. Fisher, who first described this situation as a pedagogical example in his 1925 book on statistical methodology[^1]. Fisher developed statistical methods that are among the most important and widely used methods to this day, and most of his applications were biological.\n\nGame\nLet’s try an experiment. I’ll flip 10 coins. You guess which are heads and which are tails, and we’ll see how you do. Please write down a sequence of “H” or “T”. Comparing with your classmates, we will undoubtedly see that some of you did better and others worse.\nWhat would be your impression of one of you got 9 guesses correct? Is that SKILL or is that something else? What would be your immediate reaction and next move?\nAnalysis\nBack to the Lady who drank Tea !!\n\nLet’s suppose we decide to test the lady with ten cups of tea. We’ll flip a coin to decide which way to prepare the cups. If we flip a head, we will pour the milk in first; if tails, we put the tea in first. Then we present the ten cups to the lady and have her state which ones she thinks were prepared each way.\nIt is easy to give her a score (9 out of 10, or 7 out of 10, or whatever it happens to be). It is trickier to figure out what to do with her score. Even if she is just guessing and has no idea, she could get lucky and get quite a few correct – maybe even all 10. But how likely is that?\nNow let’s suppose the lady gets 9 out of 10 correct. That’s not perfect, but it is better than we would expect for someone who was just guessing. On the other hand, it is not impossible to get 9 out of 10 just by guessing.\nSo here is Fisher’s great idea: Let’s figure out how hard it is to get 9 out of 10 by guessing. If it’s not so hard to do, then perhaps that’s just what happened ( that she was guessing ), so we won’t be too impressed with the lady’s tea tasting ability. On the other hand, if it is really unusual to get 9 out of 10 correct by guessing, then we will have some evidence that she must be able to tell something ( and has an unusual Skill).\nBut how do we figure out how unusual it is to get 9 out of 10 just by guessing? Let’s just flip a bunch of coins and keep track. If the lady is just guessing, she might as well be flipping a coin.\nSo here’s the plan. We’ll flip 10 coins, and repeat that experiment 10000 times. We’ll call the heads correct guesses and the tails incorrect guesses.\n\n\n\nheads\n   0    1    2    3    4    5    6    7    8    9   10 \n   9  110  413 1145 2096 2446 2066 1141  469   97    8 \n\n\n\n\n\n\n\n\nSo what do we conclude? It is possible that the lady could get 9 or 10 correct just by guessing, but it is not very likely (it only happened in about \\(\\frac{97+8}{10000} = 1.05\\%\\) of our simulations). So one of two things must be true:\n• The lady got unusually “lucky”, by chance; OR\n• The lady is not just guessing and really has some ability in this regard.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Basics of Randomization Tests"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#commentary",
    "href": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#commentary",
    "title": "Basics of Randomization Tests",
    "section": "Commentary",
    "text": "Commentary\nFirst we realize something is surprising, and that we have a question or doubt. This is based on something we see, or measure, a test statistic. In our story, it is the score of \\(10/10\\) that the Lady was able to achieve about how the Tea was made.\nWe then assume the Lady is guessing and somehow by chance able to guess correctly. This would be our….NULL Hypothesis. This is our (conservative) belief about the Real World.\nWe then randomly generate many Parallel Counterfactual Worlds, where we repeat the experiment many many times, each time calculating the test statistic, under the assumption of the NULL Hypothesis is TRUE.\nWe see how often our Parallel Worlds can mimic or exceed Real World measurement of the the test statistic by comparison. If this is common (i.e. probability is high) we say we cannot reject the NULL Hypothesis (and the Lady is lucky). If the occurrence is rare, as in our case, we say we have reason to reject the NULL Hypothesis and reason to believe an underlying pattern (and Lady’s ability is beyond Question !)\nThis is the essence of the Simulation Method in statistical modelling. Take one more look at the picture from Allen Downey’s blog, below:\n\nFrom Reference #1:\n\nHypothesis testing can be thought of as a 4-step process:\n\nState the null and alternative hypotheses.\nCompute a test statistic.\nDetermine the p-value.\n\nDraw a conclusion.\nIn a traditional introductory statistics course, once this general framework has been mastered, the main work is in applying the correct formula to compute the standard test statistics in step 2 and using a table or computer to determine the p-value based on the known (usually approximate) theoretical distribution of the test statistic under the null hypothesis.\nIn a simulation-based approach, steps 2 and 3 change. In Step 2, it is no longer required that the test statistic be normalized to conform with a known, named distribution. Instead, natural test statistics, like the difference between two sample means \\(y1 − y2\\) can be used.\nIn Step 3, we use randomization to approximate the sampling distribution of the test statistic. Our lady tasting tea example demonstrates how this can be done from first principles. More typically, we will use randomization to create new simulated data sets ( “Parallel Worlds”) that are like our original data in some ways, but make the null hypothesis true. For each simulated data set, we calculate our test statistic, just as we did for the original sample. Together, this collection of test statistics computed from the simulated samples constitute our randomization distribution.\nWhen creating a randomization distribution, we will attempt to satisfy 3 guiding principles.\n\nBe consistent with the null hypothesis. We need to simulate a world in which the null hypothesis is true. If we don’t do this, we won’t be testing our null hypothesis.\nUse the data in the original sample. The original data should shed light on some aspects of the distribution that are not determined by null hypothesis. For example, a null hypothesis about a mean doesn’t tell us about the shape of the population distribution, but the data give us some indication.\nReflect the way the original data were collected.\n\n\nFrom Chihara and Hesterberg:\n\nThis is the core idea of statistical significance or classical hypothesis testing – to calculate how often pure random chance would give an effect as large as that observed in the data, in the absence of any real effect. If that probability is small enough, we conclude that the data provide convincing evidence of a real effect.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Basics of Randomization Tests"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/60-SimTest/index.html#references",
    "title": "Basics of Randomization Tests",
    "section": "References",
    "text": "References\n\nChapter 11, Hypothesis Testing with Randomization in Introduction to Modern Statistics (1st Ed) by Mine Çetinkaya-Rundel and Johanna Hardin.\nAllen Downey, There is still only one test\nR.A. Fisher. Statistical Methods for Research Workers. Oliver & Boyd, 1925\nhttps://timesofindia.indiatimes.com/sports/cricket/icc-mens-t20-world-cup/in-numbers-virat-kohli-and-his-strange-luck-with-the-coin-toss/articleshow/87538443.cms\nLaura Chihara, Tim Hesterberg, Mathematical Statistics with Resampling and R, Wiley, 2019.\nD. Salsburg. The Lady Tasting Tea: How statistics revolutionized science in the twentieth century. W.H. Freeman, New York, 2001\nDaniel Kaplan, Nicholas J. Horton, and Randall Pruim, Simulation-based inference with mosaic https://www.mosaic-web.org/mosaic/articles/Resampling.html",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Basics of Randomization Tests"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(explore)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#setting-up-the-packages",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#setting-up-the-packages",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(explore)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#introduction",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Introduction",
    "text": "Introduction\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe will use two packages in R, mosaic and the relatively new infer package, to develop our intuition for what are called permutation based statistical tests."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#testing-for-two-or-more-proportions",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#testing-for-two-or-more-proportions",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Testing for Two or More Proportions",
    "text": "Testing for Two or More Proportions\nLet us try a dataset with Qualitative / Categorical data. This is the General Social Survey GSS dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical ( both variables ), we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\ndata(GSS2002)\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name  class levels    n missing\n1         Region factor      7 2765       0\n2         Gender factor      2 2765       0\n3           Race factor      3 2765       0\n4      Education factor      5 2760       5\n5        Marital factor      5 2765       0\n6       Religion factor     13 2746      19\n7          Happy factor      3 1369    1396\n8         Income factor     24 1875     890\n9       PolParty factor      8 2729      36\n10      Politics factor      7 1331    1434\n11     Marijuana factor      2  851    1914\n12  DeathPenalty factor      2 1308    1457\n13        OwnGun factor      3  924    1841\n14        GunLaw factor      2  916    1849\n15 SpendMilitary factor      3 1324    1441\n16     SpendEduc factor      3 1343    1422\n17      SpendEnv factor      3 1322    1443\n18      SpendSci factor      3 1266    1499\n19        Pres00 factor      5 1749    1016\n20      Postlife factor      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nGSS2002 %&gt;% count(Education)\n\n\n  \n\n\nGSS2002 %&gt;% count(DeathPenalty)\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\ngss2002 &lt;- GSS2002 %&gt;%\n  dplyr::select(Education, DeathPenalty) %&gt;%\n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\ngss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n\n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  #\n  mutate(\n    edu_count = sum(count),\n    edu_prop = count / sum(count)\n  ) %&gt;%\n  ungroup()\n\ngss_summary"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#sec-table-plots",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#sec-table-plots",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Table Plots",
    "text": "Table Plots\nWe can plot a heatmap-like mosaic chart for this table.\nUsing ggplot\n\n\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes(x = Education, y = edu_prop)) +\n  geom_bar(aes(width = edu_count, fill = DeathPenalty),\n    stat = \"identity\",\n    position = \"fill\",\n    colour = \"black\"\n  ) +\n  geom_text(aes(label = scales::percent(edu_prop)),\n    position = position_stack(vjust = 0.5)\n  ) +\n\n\n  # if labels are desired\n  facet_grid(~Education, scales = \"free_x\", space = \"free_x\") +\n  theme(scale_fill_brewer(palette = \"RdYlGn\")) +\n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void()\n\n\n\n\n\n\n\nUsing ggmosaic\n\n\n# library(ggmosaic)\n\nggplot(data = gss2002) +\n  geom_mosaic(aes(x = product(DeathPenalty, Education), fill = DeathPenalty))"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#section",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#section",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "",
    "text": "Observed Statistic: the X^2 metric\nWhen there are multiple proportions involved, the X^2 test is what is used.\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\ngss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Left HS  HS Jr Col Bachelors Graduate\n      Favor      117 511     71       135       64\n      Oppose      72 200     16        71       50\n\n# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\n# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWhat would our Hypotheses be?\n$$ H_0: Education Does Not affect Votes on Death Penalty\\\nH_a: Education affects Votes on Death Penalty\n$$\nWe should now repeat the test with permutations on Education:\n\nnull_chisq &lt;- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n  \n\n\ngf_histogram(~X.squared, data = null_chisq) %&gt;%\n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\n\n\n\nprop1(~ X.squared &gt;= observedChi2, data = null_chisq)\n\nprop_TRUE \n9.999e-05 \n\n\nThe p-value is well below our threshold of \\(0.05%\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/files/one-mean-tutorial.html#conclusion",
    "title": "🃏 Permutation Test for Two Proportions",
    "section": "Conclusion",
    "text": "Conclusion\nSo, what do you think?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html",
    "title": "Inference for Correlation",
    "section": "",
    "text": "# CRAN Packages\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(mosaicCore)\nlibrary(mosaicData)\nlibrary(crosstable) # tabulated summary stats\n\nlibrary(openintro) # datasets and methods\nlibrary(resampledata3) # datasets\nlibrary(statsExpressions) # datasets and methods\nlibrary(ggstatsplot) # special stats plots\nlibrary(ggExtra)\n\n# Non-CRAN Packages\n# remotes::install_github(\"easystats/easystats\")\nlibrary(easystats)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#setting-up-r-packages",
    "title": "Inference for Correlation",
    "section": "",
    "text": "# CRAN Packages\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(mosaicCore)\nlibrary(mosaicData)\nlibrary(crosstable) # tabulated summary stats\n\nlibrary(openintro) # datasets and methods\nlibrary(resampledata3) # datasets\nlibrary(statsExpressions) # datasets and methods\nlibrary(ggstatsplot) # special stats plots\nlibrary(ggExtra)\n\n# Non-CRAN Packages\n# remotes::install_github(\"easystats/easystats\")\nlibrary(easystats)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#introduction",
    "title": "Inference for Correlation",
    "section": "\n Introduction",
    "text": "Introduction\nCorrelations define how one variables varies with another. One of the basic Questions we would have of our data is: Does some variable have a significant correlation score with another in some way? Does \\(y\\) vary with \\(x\\)? A Correlation Test is designed to answer exactly this question. The block diagram depicts the statistical procedures available to test for the significance of correlation scores between two variables.\nBefore we begin, let us recap a few basic definitions:\nWe have already encountered the variance of a variable:\n\\[\n\\begin{align*}\nvar_x &= \\frac{\\sum_{i=1}^{n}(x_i - \\mu_x)^2}{(n-1)}\\\\\nwhere ~ \\mu_x &= mean(x)\\\\\nn &= sample\\ size\n\\end{align*}\n\\] The standard deviation is:\n\\[\n\\sigma_x = \\sqrt{var_x}\\\\\n\\] The covariance of two variables is defined as:\n\\[\n\\begin{align}\ncov(x,y) &= \\frac{\\sum_{i = 1}^{n}(x_i - \\mu_x)*(y_i - \\mu_y)}{n-1}\\\\\n&= \\frac{\\sum{x_i *y_i}}{n-1} - \\frac{\\sum{x_i *\\mu_y}}{n-1} - \\frac{\\sum{y_i *\\mu_x}}{n-1} + \\frac{\\sum{\\mu_x *\\mu_y}}{n-1}\\\\\n&= \\frac{\\sum{x_i *y_i}}{n-1} - \\frac{\\sum{\\mu_x *\\mu_y}}{n-1}\\\\\n\\end{align}\n\\]\nHence covariance is the expectation of the product minus the product of the expectations of the two variables.\n\n\n\n\n\n\nTipCovariance uses z-scores!\n\n\n\nNote that in both cases we are dealing with z-scores: variable minus its mean, \\(x_i - \\mu_x\\), which we have seen when dealing with the CLT and the Gaussian Distribution.\n\n\nSo, finally, the coefficient of correlation between two variables is defined as:\n\\[\n\\begin{align}\ncorrelation ~ r &= \\frac{cov(x,y)}{\\sigma_x * \\sigma_y}\n\\\\\n&= \\frac{cov(x,y)}{\\sqrt{var_x} * \\sqrt{var_y}}\n\\end{align}\n\\tag{1}\\]\nThus correlation coefficient is the covariance scaled by the geometric mean of the variances.\n\n\n\n\n\nflowchart TD\n    A[Inference for Correlation] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Ranked Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Ranked Data\\n and Permutation]",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#case-study-1-galtons-famous-dataset",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#case-study-1-galtons-famous-dataset",
    "title": "Inference for Correlation",
    "section": "\n Case Study #1: Galton’s famous dataset",
    "text": "Case Study #1: Galton’s famous dataset\nHow can we start, except by using the famous Galton dataset, now part of the mosaicData package?\n\n Workflow: Read and Inspect the Data\n\ndata(\"Galton\", package = \"mosaicData\")\nGalton\n\n\n  \n\n\n\nThe variables in this dataset are:\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\n\n\nsex(char): sex of the child\n\nfamily(int): an ID for each family\n\n\n\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\n\n\nfather(dbl): father’s height in inches\n\nmother(dbl): mother’s height in inches\n\nheight(dbl): Child’s height in inches\n\nnkids(int): Number of children in each family\n\n\n\n\ninspect(Galton)\n\n\ncategorical variables:  \n    name  class levels   n missing\n1 family factor    197 898       0\n2    sex factor      2 898       0\n                                   distribution\n1 185 (1.7%), 166 (1.2%), 66 (1.2%) ...        \n2 M (51.8%), F (48.2%)                         \n\nquantitative variables:  \n    name   class min Q1 median   Q3  max      mean       sd   n missing\n1 father numeric  62 68   69.0 71.0 78.5 69.232851 2.470256 898       0\n2 mother numeric  58 63   64.0 65.5 70.5 64.084410 2.307025 898       0\n3 height numeric  56 64   66.5 69.7 79.0 66.760690 3.582918 898       0\n4  nkids integer   1  4    6.0  8.0 15.0  6.135857 2.685156 898       0\n\n\nSo there are several correlations we can explore here: Children’s height vs that of father or mother, based on sex. In essence we are replicating Francis Galton’s famous study.\n\n Workflow: Research Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\n\nBased on this sample, what can we say about the correlation between a son’s height and a father’s height in the population?\n\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\n\nBased on this sample, what can we say about the correlation between a daughter’s height and a father’s height in the population?\n\n\n\nOf course we can formulate more questions, but these are good for now! And since we are going to infer correlations by sex, let us split the dataset into two parts, one for the sons and one for the daughters, and quickly summarise them too:\n\nGalton_sons &lt;- Galton %&gt;%\n  dplyr::filter(sex == \"M\") %&gt;%\n  rename(\"son\" = height)\nGalton_daughters &lt;- Galton %&gt;%\n  dplyr::filter(sex == \"F\") %&gt;%\n  rename(\"daughter\" = height)\ndim(Galton_sons)\n\n[1] 465   6\n\ndim(Galton_daughters)\n\n[1] 433   6\n\n\n\nGalton_sons %&gt;%\n  summarize(across(\n    .cols = c(son, father),\n    .fns = list(mean = mean, sd = sd)\n  ))\n\n\n  \n\n\nGalton_daughters %&gt;%\n  summarize(across(\n    .cols = c(daughter, father),\n    .fns = list(mean = mean, sd = sd)\n  ))\n\n\n  \n\n\n\n\n Workflow: Visualization\nLet us first quickly plot a graph that is relevant to each of the two research questions.\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nGalton_sons %&gt;%\n  gf_point(son ~ father) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Height of Sons vs fathers\",\n    subtitle = \"Galton dataset\"\n  )\n##\nGalton_daughters %&gt;%\n  gf_point(daughter ~ father) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Height of Daughters vs Fathers\",\n    subtitle = \"Galton dataset\"\n  )\n\n\n\n\n\n\n\n\n\n\nWe might even plot the overall heights together and colour by sex of the child:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nGalton %&gt;%\n  gf_point(height ~ father,\n    group = ~sex, colour = ~sex\n  ) %&gt;%\n  gf_lm() %&gt;%\n  gf_refine(scale_color_brewer(palette = \"Set1\")) %&gt;%\n  gf_labs(\n    title = \"Height of sons vs fathers\",\n    subtitle = \"Galton dataset\"\n  )\n\n\n\n\n\n\n\nSo daughters are shorter than sons, generally speaking, and both heights seem related to that of the father.\n\n\n\n\n\n\nNoteWhat did filtering do?\n\n\n\nWhen we filtered the dataset into two, the filtering by sex of the child also effectively filtered the heights of the father (and mother). This is proper and desired; but think!\n\n\n\n Workflow: Assumptions\nFor the classical correlation tests, we need that the variables are normally distributed. As before we check this with the shapiro.test:\nshapiro.test(Galton_sons$father)\nshapiro.test(Galton_sons$son)\n##\nshapiro.test(Galton_daughters$father)\nshapiro.test(Galton_daughters$daughter)\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Galton_sons$father\nW = 0.98529, p-value = 0.0001191\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Galton_sons$son\nW = 0.99135, p-value = 0.008133\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Galton_daughters$father\nW = 0.98438, p-value = 0.0001297\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Galton_daughters$daughter\nW = 0.99113, p-value = 0.01071\n\n\n\nLet us also check the densities and quartile plots of the heights the dataset:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  gf_density(~height,\n    group = ~sex,\n    fill = ~sex\n  ) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\")) %&gt;%\n  gf_facet_grid(vars(sex)) %&gt;%\n  gf_labs(title = \"Facetted Density Plots\")\n##\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  gf_qq(~height,\n    group = ~sex,\n    colour = ~sex, size = 0.5\n  ) %&gt;%\n  gf_qqline(colour = \"black\") %&gt;%\n  gf_refine(scale_color_brewer(palette = \"Set1\")) %&gt;%\n  gf_facet_grid(vars(sex)) %&gt;%\n  gf_labs(\n    title = \"Facetted QQ Plots\",\n    x = \"Theoretical quartiles\",\n    y = \"Actual Data\"\n  )\n\n\n\n\n\n\n\n\n\n\nand the father’s heights:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n##\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  gf_density(~father,\n    group = ~sex, # no this is not weird\n    fill = ~sex\n  ) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_refine(scale_fill_brewer(name = \"Sex of Child\", palette = \"Set1\")) %&gt;%\n  gf_facet_grid(vars(sex)) %&gt;%\n  gf_labs(\n    title = \"Fathers: Facetted Density Plots\",\n    subtitle = \"By Sex of Child\"\n  )\n\n\n\n\n\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  gf_qq(~father,\n    group = ~sex, # no this is not weird\n    colour = ~sex, size = 0.5\n  ) %&gt;%\n  gf_qqline(colour = \"black\") %&gt;%\n  gf_facet_grid(vars(sex)) %&gt;%\n  gf_refine(scale_colour_brewer(name = \"Sex of Child\", palette = \"Set1\")) %&gt;%\n  gf_labs(\n    title = \"Fathers Heights: Facetted QQ Plots\",\n    subtitle = \"By Sex of Child\",\n    x = \"Theoretical quartiles\",\n    y = \"Actual Data\"\n  )\n\n\n\n\n\n\n\nThe shapiro.test informs us that the child-related height variables are not normally distributed; though visually there seems nothing much to complain about. Hmmm…\nDads are weird anyway, so we must not expect father heights to be normally distributed.\n\n Workflow: Inference\nLet us now see how Correlation Tests can be performed based on this dataset, to infer patterns in the population from which this dataset/sample was drawn.\nWe will go with classical tests first, and then set up a permutation test that does not need any assumptions.\n\n\nClassical Tests\nIntuitive\nPermutation Test\nThe Linear Model\n\n\n\nWe perform the Pearson correlation test first: the data is not normal so we cannot really use this. We should use a non-parametric correlation test as well, using a Spearman correlation.\n\n# Pearson (built-in test)\ncor_son_pearson &lt;- cor.test(son ~ father,\n  method = \"pearson\",\n  data = Galton_sons\n) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = \"Pearson Correlation r\")\ncor_son_pearson\n\n\n  \n\n\ncor_son_spearman &lt;- cor.test(son ~ father, method = \"spearman\", data = Galton_sons) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = \"Spearman Correlation r\")\ncor_son_spearman\n\n\n  \n\n\n\nBoth tests state that the correlation between son and father is significant.\n\n# Pearson (built-in test)\ncor_daughter_pearson &lt;- cor.test(daughter ~ father,\n  method = \"pearson\",\n  data = Galton_daughters\n) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = \"Pearson Correlation r\")\ncor_daughter_pearson\n\n\n  \n\n\n##\ncor_daughter_spearman &lt;- cor.test(daughter ~ father, method = \"spearman\", data = Galton_daughters) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = \"Spearman Correlation r\")\ncor_daughter_spearman\n\n\n  \n\n\n\nAgain both tests state that the correlation between daughter and father is significant.\n\n\nWhat is happening under the hood in cor.test?\nTo be Written Up! But when?\n\n\nWe can of course use a randomization based test for correlation. How would we mechanize this, what aspect would be randomize?\nCorrelation is calculated on a vector-basis: each individual observation of variable#1 is multiplied by the corresponding observation of variable#2. Look at Equation 1! So we might be able to randomize the order of this multiplication to see how uncommon this particular set of multiplications are. That would give us a p-value to decide if the observed correlation is close to the truth. So, onwards with our friend mosaic:\n\nobs_daughter_corr &lt;- cor(Galton_daughters$father, Galton_daughters$daughter)\nobs_daughter_corr\n\n[1] 0.4587605\n\n\n\ncorr_daughter_null &lt;- do(4999) * cor.test(daughter ~ shuffle(father),\n  data = Galton_daughters\n) %&gt;%\n  broom::tidy()\ncorr_daughter_null\n\n\n  \n\n\ncorr_daughter_null %&gt;%\n  gf_histogram(~estimate, bins = 50) %&gt;%\n  gf_vline(\n    xintercept = obs_daughter_corr,\n    color = \"red\", linewidth = 1\n  ) %&gt;%\n  gf_labs(\n    title = \"Permutation Null Distribution\",\n    subtitle = \"Daughter Heights vs Father Heights\"\n  )\n\n\n\n\n\n\n##\np_value_null &lt;- 2.0 * mean(corr_daughter_null$estimate &gt;= obs_daughter_corr)\np_value_null\n\n[1] 0\n\n\nWe see that will all permutations of father, we are never able to hit the actual obs_daughter_corr! Hence there is a definite correlation between father height and daughter height.\n\n\nThe premise here is that many common statistical tests are special cases of the linear model. A linear model estimates the relationship between dependent variable or\n“response” variable height and an explanatory variable or “predictor”, father. It is assumed that the relationship is linear. \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the linear fit, that predicts the value of height based the value of father.\n\\[\nheight = \\beta_0 + \\beta_1 \\times father\n\\] The model for Pearson Correlation tests is exactly the Linear Model:\n\\[\n\\begin{aligned}\nheight = \\beta_0 + \\beta_1 \\times father\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ =&gt; \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ =&gt; \\beta_1 \\ne 0\\\\\n\\end{aligned}\n\\]\nUsing the linear model method we get:\n\n# Linear Model\nlin_son &lt;- lm(son ~ father, data = Galton_sons) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;%\n  select(term, estimate, p.value)\nlin_son\n\n\n  \n\n\n##\nlin_daughter &lt;- lm(daughter ~ father, data = Galton_daughters) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;%\n  select(term, estimate, p.value)\nlin_daughter\n\n\n  \n\n\n\nWhy are the respective \\(r\\)-s and \\(\\beta_1\\)-s different, though the p-value-s is suspiciously the same!? Did we miss a factor of \\(\\frac{sd(son/daughter)}{sd(father)} = ??\\) somewhere…??\nLet us scale the variables to within {-1, +1} : (subtract the mean and divide by sd) and re-do the Linear Model with scaled versions of height and father:\n\n# Scaled linear model\nlin_scaled_galton_daughters &lt;- lm(scale(daughter) ~ 1 + scale(father), data = Galton_daughters) %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = c(\"beta_0\", \"beta_1\"))\nlin_scaled_galton_daughters\n\n\n  \n\n\n\nNow you’re talking!! The estimate is the same in both the classical test and the linear model! So we conclude:\n\nWhen both target and predictor have the same standard deviation, the slope from the linear model and the Pearson correlation are the same.\nThere is this relationship between the slope in the linear model and Pearson correlation:\n\n\\[\nSlope\\ \\beta_1 = \\frac{sd_y}{sd_x} * r\n\\]\nThe slope is usually much more interpretable and informative than the correlation coefficient.\n\nHence a linear model using scale() for both variables will show slope = r.\n\nSlope_Scaled: 0.4587605 = Correlation: 0.4587605\n\nFinally, the p-value for Pearson Correlation and that for the slope in the linear model is the same (\\(0.04280043\\)). Which means we cannot reject the NULL hypothesis of “no relationship” between daughter-s and father-s heights.\n\nCan you complete this for the sons?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#case-study-2-study-and-grades",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#case-study-2-study-and-grades",
    "title": "Inference for Correlation",
    "section": "\n Case Study #2: Study and Grades",
    "text": "Case Study #2: Study and Grades\nIn some cases the LINE assumptions may not hold.\nNonlinear relationships, non-normally distributed data ( with large outliers ) and working with ordinal rather than continuous data: these situations necessitate the use of Spearman’s ranked correlation scores. (Ranked, not sign-ranked.).\nSee the example below: We choose to look at the gpa_study_hours dataset. It has two numeric columns gpa and study_hours:\n\nglimpse(gpa_study_hours)\n\nRows: 193\nColumns: 2\n$ gpa         &lt;dbl&gt; 4.000, 3.800, 3.930, 3.400, 3.200, 3.520, 3.680, 3.400, 3.…\n$ study_hours &lt;dbl&gt; 10, 25, 45, 10, 4, 10, 24, 40, 10, 10, 30, 7, 15, 60, 10, …\n\n\nWe can plot this:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nggplot(gpa_study_hours, aes(x = study_hours, y = gpa)) +\n  geom_point() +\n  geom_smooth() +\n  labs(\n    title = \"GPA vs Study Hours\",\n    subtitle = \"Pearson Correlation Test\"\n  )\n\n\n\n\n\n\n\nHmm…not normally distributed, and there is a sort of increasing relationship, however is it linear? And there is some evidence of heteroscedasticity, so the LINE assumptions are clearly in violation. Pearson correlation would not be the best idea here.\nLet us quickly try it anyway, using a Linear Model for the scaled gpa and study_hours variables, from where we get:\n\n# Pearson Correlation as Linear Model\nmodel_gpa &lt;-\n  lm(scale(gpa) ~ 1 + scale(study_hours), data = gpa_study_hours)\n##\nmodel_gpa %&gt;%\n  broom::tidy() %&gt;%\n  mutate(term = c(\"beta_0\", \"beta_1\")) %&gt;%\n  cbind(confint(model_gpa) %&gt;% as_tibble()) %&gt;%\n  select(term, estimate, p.value, `2.5 %`, `97.5 %`)\n\n\n  \n\n\n\nThe correlation estimate is \\(0.133\\); the p-value is \\(0.065\\) (and the confidence interval includes \\(0\\)).\nHence we fail to reject the NULL hypothesis that study_hours and gpa have no relationship. But can this be right?\nShould we use another test, that does not need the LINE assumptions?\n“Signed Rank” Values\nMost statistical tests use the actual values of the data variables. However, in some non-parametric statistical tests, the data are used in rank-transformed sense/order. (In some cases the signed-rank of the data values is used instead of the data itself.)\nSigned Rank is calculated as follows:\n\nTake the absolute value of each observation in a sample\nPlace the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on. This gives is ranked data.\nGive each of the ranks the sign of the original observation ( + or -). This gives us signed ranked data.\n\n\nsigned_rank &lt;- function(x) {\n  sign(x) * rank(abs(x))\n}\n\nPlotting Original and Signed Rank Data\nLet us see how this might work by comparing data and its signed-rank version…A quick set of plots:\nSo the means of the ranks three separate variables seem to be in the same order as the means of the data variables themselves.\nHow about associations between data? Do ranks reflect well what the data might?\nThe slopes are almost identical, \\(0.25\\) for both original data and ranked data for \\(y1\\sim x\\). So maybe ranked and even sign_ranked data could work, and if it can work despite LINE assumptions not being satisfied, that would be nice!\nHow does Sign-Rank data work?\nTBD: need to add some explanation here.\nSpearman correlation = Pearson correlation using the rank of the data observations. Let’s check how this holds for a our x and y1 data:\nSo the Linear Model for the Ranked Data would be:\n\\[\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 \\times rank(x)\\\\\n\\\\\nH_0: Null\\ Hypothesis\\ =&gt; \\beta_1 = 0\\\\\\\nH_a: Alternate\\ Hypothesis\\ =&gt; \\beta_1 \\ne 0\\\\\n\\end{aligned}\n\\]\nCode\nNotes:\n\nWhen ranks are used, the slope of the linear model (\\(\\beta_1\\)) has the same value as the Spearman correlation coefficient ( \\(\\rho\\) ).\nNote that the slope from the linear model now has an intuitive interpretation: the number of ranks y changes for each change in rank of x. ( Ranks are “independent” of sd )\nExample\nWe examine the cars93 data, where the numeric variables of interest are weight and price.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\ncars93 %&gt;%\n  ggplot(aes(weight, price)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, lty = 2) +\n  labs(title = \"Car Weight and Car Price have a nonlinear relationship\") +\n  theme_classic()\n\n\n\n\n\n\n\nLet us try a Spearman Correlation score for these variables, since the data are not linearly related and the variance of price also is not constant over weight\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\ncor.test(cars93$price, cars93$weight, method = \"spearman\") %&gt;% broom::tidy()\n\n\n  \n\n\n# Using linear Model\nlm(rank(price) ~ rank(weight), data = cars93) %&gt;% summary()\n\n\nCall:\nlm(formula = rank(price) ~ rank(weight), data = cars93)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.0676  -3.0135   0.7815   3.6926  20.4099 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.22074    2.05894   1.564    0.124    \nrank(weight)  0.88288    0.06514  13.554   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.46 on 52 degrees of freedom\nMultiple R-squared:  0.7794,    Adjusted R-squared:  0.7751 \nF-statistic: 183.7 on 1 and 52 DF,  p-value: &lt; 2.2e-16\n\n# Stats Plot\nggstatsplot::ggscatterstats(\n  data = cars93, x = weight,\n  y = price,\n  type = \"nonparametric\",\n  title = \"Cars93: Weight vs Price\",\n  subtitle = \"Spearman Correlation\"\n)\n\n\n\n\n\n\n\nWe see that using ranks of the price variable, we obtain a Spearman’s \\(\\rho = 0.882\\) with a p-value that is very small. Hence we are able to reject the NULL hypothesis and state that there is a relationship between these two variables. The linear relationship is evaluated as a correlation of 0.882.\n\n# Other ways using other packages\nmosaic::cor_test(gpa ~ study_hours, data = gpa_study_hours) %&gt;%\n  broom::tidy() %&gt;%\n  select(estimate, p.value, conf.low, conf.high)\n\n\n  \n\n\n\n\nstatsExpressions::corr_test(\n  data = gpa_study_hours,\n  x = study_hours,\n  y = gpa\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#wait-but-why",
    "title": "Inference for Correlation",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\nCorrelation tests are useful to understand the relationship between two variables, but they do not imply causation. A high correlation does not mean that one variable causes the other to change. It is essential to consider the context and other factors that may influence the relationship.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#conclusion",
    "title": "Inference for Correlation",
    "section": "\n Conclusion",
    "text": "Conclusion\nCorrelation tests are a powerful way to understand the relationship between two variables. They can be performed using classical methods like Pearson and Spearman correlation, or using more robust methods like permutation tests. The linear model approach provides a deeper understanding of the relationship, especially when the assumptions of normality and homoscedasticity are met.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#your-turn",
    "title": "Inference for Correlation",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry the datasets in the infer package. Use data(package = \"infer\") in your Console to list out the data packages. Then simply type the name of the dataset in a Quarto chunk ( e.g. babynames) to read it.\nSame with the resampledata and resampledata3 packages.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/150-Correlation/index.html#references",
    "title": "Inference for Correlation",
    "section": "\n References",
    "text": "References\n\n\nCommon statistical tests are linear models (or: how to teach stats) by Jonas Kristoffer LindeløvCheatSheet\n\n\nCommon statistical tests are linear models: a work through by Steve Doogue\n\n\nJeffrey Walker “Elements of Statistical Modeling for Experimental Biology”\n\nDiez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine: OpenIntro Statistics\n\nModern Statistics with R: From wrangling and exploring data to inference and predictive modelling by Måns Thulin\n\nJeffrey Walker “A linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables”\n\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\neasystats\n0.7.5\nLüdecke et al. (2022)\n\n\nggExtra\n0.10.1\nAttali and Baker (2023)\n\n\nggstatsplot\n0.13.1\nPatil (2021b)\n\n\nopenintro\n2.5.0\nÇetinkaya-Rundel et al. (2024)\n\n\nresampledata3\n1.0\nChihara and Hesterberg (2022)\n\n\nstatsExpressions\n1.7.0\nPatil (2021a)\n\n\n\n\n\n\nAttali, Dean, and Christopher Baker. 2023. ggExtra: Add Marginal Histograms to “ggplot2,” and More “ggplot2” Enhancements. https://doi.org/10.32614/CRAN.package.ggExtra.\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2024. openintro: Datasets and Supplemental Functions from “OpenIntro” Textbooks and Labs. https://doi.org/10.32614/CRAN.package.openintro.\n\n\nChihara, Laura, and Tim Hesterberg. 2022. Resampledata3: Data Sets for “Mathematical Statistics with Resampling and R” (3rd Ed). https://doi.org/10.32614/CRAN.package.resampledata3.\n\n\nLüdecke, Daniel, Mattan S. Ben-Shachar, Indrajeet Patil, Brenton M. Wiernik, Etienne Bacher, Rémi Thériault, and Dominique Makowski. 2022. “easystats: Framework for Easy Statistical Modeling, Visualization, and Reporting.” CRAN. https://doi.org/10.32614/CRAN.package.easystats.\n\n\nPatil, Indrajeet. 2021a. “statsExpressions: R Package for Tidy Dataframes and Expressions with Statistical Details.” Journal of Open Source Software 6 (61): 3236. https://doi.org/10.21105/joss.03236.\n\n\n———. 2021b. “Visualizations with statistical details: The ‘ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Inference for Correlation"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/50-Bootstrap/files/bootstrap.html",
    "href": "content/courses/Analytics/Inference/Modules/50-Bootstrap/files/bootstrap.html",
    "title": "Bootstrap Case Studies",
    "section": "",
    "text": "Example 5.2\n\nShow the Codemy.sample &lt;- rgamma(16, 1, 1 / 2)\n\nN &lt;- 10^5\nmy.boot &lt;- numeric(N)\nfor (i in 1:N)\n{\n  x &lt;- sample(my.sample, 16, replace = TRUE) # draw resample\n  my.boot[i] &lt;- mean(x) # compute mean, store in my.boot\n}\n\nggplot() +\n  geom_histogram(aes(my.boot), bins = 15)\n\n\n\n\n\n\nShow the Codemean(my.boot) # mean\n\n[1] 2.543295\n\nShow the Codesd(my.boot) # bootstrap SE\n\n[1] 0.5302543\n\n\nExample 5.3\nArsenic in wells in Bangladesh\n\nShow the CodeBangladesh &lt;- read.csv(\"../../../../../../materials/data/resampling/Bangladesh.csv\")\n\nggplot(Bangladesh, aes(Arsenic)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\nShow the Codeggplot(Bangladesh, aes(sample = Arsenic)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the CodeArsenic &lt;- pull(Bangladesh, Arsenic)\n# Alternatively\n# Arsenic &lt;- Bangladesh$Arsenic\n\nn &lt;- length(Arsenic)\nN &lt;- 10^4\n\narsenic.mean &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  x &lt;- sample(Arsenic, n, replace = TRUE)\n  arsenic.mean[i] &lt;- mean(x)\n}\n\nggplot() +\n  geom_histogram(aes(arsenic.mean), bins = 15) +\n  labs(title = \"Bootstrap distribution of means\") +\n  geom_vline(xintercept = mean(Arsenic), colour = \"blue\")\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = arsenic.mean)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(arsenic.mean) # bootstrap mean\n\n[1] 125.3701\n\nShow the Codemean(arsenic.mean) - mean(Arsenic) # bias\n\n[1] 0.05015819\n\nShow the Codesd(arsenic.mean) # bootstrap SE\n\n[1] 18.13178\n\nShow the Codesum(arsenic.mean &gt; 161.3224) / N\n\n[1] 0.0301\n\nShow the Codesum(arsenic.mean &lt; 89.75262) / N\n\n[1] 0.0167\n\n\nExample 5.4 Skateboard\n\nShow the CodeSkateboard &lt;- read.csv(\"../../../../../../materials/data/resampling/Skateboard.csv\")\n\ntestF &lt;- Skateboard %&gt;%\n  filter(Experimenter == \"Female\") %&gt;%\n  pull(Testosterone)\ntestM &lt;- Skateboard %&gt;%\n  filter(Experimenter == \"Male\") %&gt;%\n  pull(Testosterone)\n\nobserved &lt;- mean(testF) - mean(testM)\n\nnf &lt;- length(testF)\nnm &lt;- length(testM)\n\nN &lt;- 10^4\n\nTestMean &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  sampleF &lt;- sample(testF, nf, replace = TRUE)\n  sampleM &lt;- sample(testM, nm, replace = TRUE)\n  TestMean[i] &lt;- mean(sampleF) - mean(sampleM)\n}\n\ndf &lt;- data.frame(TestMean)\nggplot(df) +\n  geom_histogram(aes(TestMean), bins = 15) +\n  labs(title = \"Bootstrap distribution of difference in means\", xlab = \"means\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\n\n\n\nShow the Codeggplot(df, aes(sample = TestMean)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(testF) - mean(testM)\n\n[1] 83.0692\n\nShow the Codemean(TestMean)\n\n[1] 82.78591\n\nShow the Codesd(TestMean)\n\n[1] 29.54104\n\nShow the Codequantile(TestMean, c(0.025, 0.975))\n\n     2.5%     97.5% \n 24.12827 139.53480 \n\nShow the Codemean(TestMean) - observed # bias\n\n[1] -0.283289\n\n\nPermutation test for Skateboard means\n\nShow the CodetestAll &lt;- pull(Skateboard, Testosterone)\n# testAll &lt;- Skateboard$Testosterone\n\nN &lt;- 10^4 - 1 # set number of times to repeat this process\n\n# set.seed(99)\nresult &lt;- numeric(N) # space to save the random differences\nfor (i in 1:N)\n{\n  index &lt;- sample(71, size = nf, replace = FALSE) # sample of numbers from 1:71\n  result[i] &lt;- mean(testAll[index]) - mean(testAll[-index])\n}\n\n(sum(result &gt;= observed) + 1) / (N + 1) # P-value\n\n[1] 0.0065\n\nShow the Codeggplot() +\n  geom_histogram(aes(result), bins = 15) +\n  labs(x = \"xbar1-xbar2\", title = \"Permutation distribution for testosterone levels\") +\n  geom_vline(xintercept = observed, colour = \"blue\")\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(result)\nggplot(df, aes(sample = result)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\n\nSection 5.4.1 Matched pairs for Diving data\n\nShow the CodeDiving2017 &lt;- read.csv(\"../../../../../../materials/data/resampling/Diving2017.csv\")\nDiff &lt;- Diving2017 %&gt;%\n  mutate(Diff = Final - Semifinal) %&gt;%\n  pull(Diff)\n# alternatively\n# Diff &lt;- Diving2017$Final - Diving2017$Semifinal\nn &lt;- length(Diff)\n\nN &lt;- 10^5\nresult &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  dive.sample &lt;- sample(Diff, n, replace = TRUE)\n  result[i] &lt;- mean(dive.sample)\n}\n\nggplot() +\n  geom_histogram(aes(result), bins = 15)\n\n\n\n\n\n\nShow the Codequantile(result, c(0.025, 0.975))\n\n    2.5%    97.5% \n-6.52500 30.99583 \n\n\nExample 5.5 Verizon cont. Bootstrap means for the ILEC data and for the CLEC data\nBootstrap difference of means.\n\nShow the CodeVerizon &lt;- read.csv(\"../../../../../../materials/data/resampling/Verizon.csv\")\n\nTime.ILEC &lt;- Verizon %&gt;%\n  filter(Group == \"ILEC\") %&gt;%\n  pull(Time)\nTime.CLEC &lt;- Verizon %&gt;%\n  filter(Group == \"CLEC\") %&gt;%\n  pull(Time)\n\nobserved &lt;- mean(Time.ILEC) - mean(Time.CLEC)\n\nn.ILEC &lt;- length(Time.ILEC)\nn.CLEC &lt;- length(Time.CLEC)\n\nN &lt;- 10^4\n\ntime.ILEC.boot &lt;- numeric(N)\ntime.CLEC.boot &lt;- numeric(N)\ntime.diff.mean &lt;- numeric(N)\n\nset.seed(100)\nfor (i in 1:N)\n{\n  ILEC.sample &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ILEC.boot[i] &lt;- mean(ILEC.sample)\n  time.CLEC.boot[i] &lt;- mean(CLEC.sample)\n  time.diff.mean[i] &lt;- mean(ILEC.sample) - mean(CLEC.sample)\n}\n\n# bootstrap for ILEC\nggplot() +\n  geom_histogram(aes(time.ILEC.boot), bins = 15) +\n  labs(title = \"Bootstrap distribution of ILEC means\", x = \"means\") +\n  geom_vline(xintercept = mean(Time.ILEC), colour = \"blue\") +\n  geom_vline(xintercept = mean(time.ILEC.boot), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codesummary(time.ILEC.boot)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.036   8.156   8.400   8.406   8.642   9.832 \n\nShow the Codedf &lt;- data.frame(x = time.ILEC.boot)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Code# bootstrap for CLEC\nggplot() +\n  geom_histogram(aes(time.CLEC.boot), bins = 15) +\n  labs(title = \"Bootstrap distribution of CLEC means\", x = \"means\") +\n  geom_vline(xintercept = mean(Time.CLEC), colour = \"blue\") +\n  geom_vline(xintercept = mean(time.CLEC.boot), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.CLEC.boot)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Code# Different in means\nggplot() +\n  geom_histogram(aes(time.diff.mean), bins = 15) +\n  labs(title = \"Bootstrap distribution of difference in means\", x = \"means\") +\n  geom_vline(xintercept = mean(time.diff.mean), colour = \"blue\") +\n  geom_vline(xintercept = mean(observed), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.diff.mean)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(time.diff.mean)\n\n[1] -8.096489\n\nShow the Codequantile(time.diff.mean, c(0.025, 0.975))\n\n      2.5%      97.5% \n-16.970068  -1.690859 \n\n\nSection 5.5 Verizon cont.\nBootstrap difference in trimmed means\n\nShow the CodeTime.ILEC &lt;- Verizon %&gt;%\n  filter(Group == \"ILEC\") %&gt;%\n  pull(Time)\nTime.CLEC &lt;- Verizon %&gt;%\n  filter(Group == \"CLEC\") %&gt;%\n  pull(Time)\nn.ILEC &lt;- length(Time.ILEC)\nn.CLEC &lt;- length(Time.CLEC)\n\nN &lt;- 10^4\ntime.diff.trim &lt;- numeric(N)\n\n# set.seed(100)\nfor (i in 1:N)\n{\n  x.ILEC &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  x.CLEC &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.diff.trim[i] &lt;- mean(x.ILEC, trim = .25) - mean(x.CLEC, trim = .25)\n}\n\nggplot() +\n  geom_histogram(aes(time.diff.trim), bins = 15) +\n  labs(x = \"difference in trimmed means\") +\n  geom_vline(xintercept = mean(time.diff.trim), colour = \"blue\") +\n  geom_vline(xintercept = mean(Time.ILEC, trim = .25) - mean(Time.CLEC, trim = .25), colour = \"red\", lty = 2)\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.diff.trim)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(time.diff.trim)\n\n[1] -10.32079\n\nShow the Codequantile(time.diff.trim, c(0.025, 0.975))\n\n     2.5%     97.5% \n-15.47049  -4.97130 \n\n\nSection 5.5 Other statistics Verizon cont:\nBootstrap of the ratio of means\nTime.ILEC and Time.CLEC created above.\nn.ILEC, n.CLEC created above\n\nShow the CodeN &lt;- 10^4\ntime.ratio.mean &lt;- numeric(N)\n\n# set.seed(100)\nfor (i in 1:N)\n{\n  ILEC.sample &lt;- sample(Time.ILEC, n.ILEC, replace = TRUE)\n  CLEC.sample &lt;- sample(Time.CLEC, n.CLEC, replace = TRUE)\n  time.ratio.mean[i] &lt;- mean(ILEC.sample) / mean(CLEC.sample)\n}\n\nggplot() +\n  geom_histogram(aes(time.ratio.mean), bins = 12) +\n  labs(title = \"bootstrap distribution of ratio of means\", x = \"ratio of means\") +\n  geom_vline(xintercept = mean(time.ratio.mean), colour = \"red\", lty = 2) +\n  geom_vline(xintercept = mean(Time.ILEC) / mean(Time.CLEC), col = \"blue\")\n\n\n\n\n\n\nShow the Codedf &lt;- data.frame(x = time.ratio.mean)\nggplot(df, aes(sample = x)) +\n  stat_qq() +\n  stat_qq_line()\n\n\n\n\n\n\nShow the Codemean(time.ratio.mean)\n\n[1] 0.5429164\n\nShow the Codesd(time.ratio.mean)\n\n[1] 0.1354238\n\nShow the Codequantile(time.ratio.mean, c(0.025, 0.975))\n\n     2.5%     97.5% \n0.3283862 0.8517156 \n\n\nExample 5.7 Relative risk example\n\nShow the Codehighbp &lt;- rep(c(1, 0), c(55, 3283)) # high blood pressure\nlowbp &lt;- rep(c(1, 0), c(21, 2655)) # low blood pressure\n\nN &lt;- 10^4\nboot.rr &lt;- numeric(N)\nhigh.prop &lt;- numeric(N)\nlow.prop &lt;- numeric(N)\n\nfor (i in 1:N)\n{\n  x.high &lt;- sample(highbp, 3338, replace = TRUE)\n  x.low &lt;- sample(lowbp, 2676, replace = TRUE)\n  high.prop[i] &lt;- sum(x.high) / 3338\n  low.prop[i] &lt;- sum(x.low) / 2676\n  boot.rr[i] &lt;- high.prop[i] / low.prop[i]\n}\n\nci &lt;- quantile(boot.rr, c(0.025, 0.975))\n\nggplot() +\n  geom_histogram(aes(boot.rr), bins = 15) +\n  labs(title = \"Bootstrap distribution of relative risk\", x = \"relative risk\") +\n  geom_vline(aes(xintercept = mean(boot.rr), colour = \"mean of bootstrap\")) +\n  geom_vline(aes(xintercept = 2.12, colour = \"observed rr\"), lty = 2) +\n  scale_colour_manual(name = \"\", values = c(\"mean of bootstrap\" = \"blue\", \"observed rr\" = \"red\"))\n\n\n\n\n\n\nShow the Codetemp &lt;- ifelse(high.prop &lt; 1.31775 * low.prop, 1, 0)\ntemp2 &lt;- ifelse(high.prop &gt; 3.687 * low.prop, 1, 0)\ntemp3 &lt;- temp + temp2\n\ndf &lt;- data.frame(y = high.prop, x = low.prop, temp, temp2, temp3)\ndf1 &lt;- df %&gt;% filter(temp == 1)\ndf2 &lt;- df %&gt;% filter(temp2 == 1)\ndf3 &lt;- df %&gt;% filter(temp3 == 0)\n\nggplot(df, aes(x = x, y = y)) +\n  geom_point(data = df1, aes(x = x, y = y), colour = \"green\") +\n  geom_point(data = df2, aes(x = x, y = y), colour = \"green\") +\n  geom_point(data = df3, aes(x = x, y = y), colour = \"red\") +\n  geom_vline(aes(xintercept = mean(low.prop)), colour = \"red\") +\n  geom_hline(yintercept = mean(high.prop), colour = \"red\") +\n  geom_abline(aes(intercept = 0, slope = 2.12, colour = \"observed rr\"), lty = 2, lwd = 1) +\n  geom_abline(aes(intercept = 0, slope = ci[1], colour = \"bootstrap CI\"), lty = 2, lwd = 1) +\n  geom_abline(intercept = 0, slope = ci[2], colour = \"blue\", lty = 2, lwd = 1) +\n  scale_colour_manual(name = \"\", values = c(\"observed rr\" = \"black\", \"bootstrap CI\" = \"blue\")) +\n  labs(x = \"Proportion in low blood pressure group\", y = \"Proportion in high blood pressure group\")\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html",
    "title": " I Publish, therefore I Am",
    "section": "",
    "text": "“Most institutions demand unqualified faith; but the institution of science makes skepticism a virtue.”\n— Robert King Merton, sociologist (4 Jul 1910-2003)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#setting-up-git-and-github",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#setting-up-git-and-github",
    "title": " I Publish, therefore I Am",
    "section": "Setting up Git and GitHub",
    "text": "Setting up Git and GitHub\n\nWindows: Head off https://gitforwindows.org and download and install the git app.\nMacOS: In a terminal type:\n\n\nxcode-select --install\n\nThis will also install git.\n\nHead off to https://github.com and register for an account. Make a note of your emailID and the password used.\nSetup 2FA Authentication for GitHub based on instructions here: https://docs.github.com/en/authentication/securing-your-account-with-two-factor-authentication-2fa",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#linking-rstudio-and-github-one-time-only",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#linking-rstudio-and-github-one-time-only",
    "title": " I Publish, therefore I Am",
    "section": "Linking RStudio and GitHub (One Time Only)",
    "text": "Linking RStudio and GitHub (One Time Only)\n\nIn RStudio, in your console:\n\n\ninstall.packages(usethis)\nlibrary(usethis)\nusethis::use_git_config(user.name = \"Your Name\", user.email = \"Your Email ID used on GitHub\")\n\n\nGenerate a Personal Access Token(PAT) (One Time Only, usually)\n\n\nusethis::create_github_token() This will open a browser window and ask you to log in to GitHub.\n\nIn the popup window, give your token a logical name you can remember(e.g. “For my Mac”), and remember why you created it. Check the scopes of the PAT; the defaults are OK. Make sure selecting repo, user, and workflow are checked. Save the PAT in your password manager. Default Expiry is 30days; we can set the PAT to not expire too. (I have.)\n\nLet RStudio have your PAT (One Time Only, usually)\n\n\ngitcreds::gitcreds_set() and paste the PAT when asked.\n\n\nCheck if all is OK using one or more of:\n\n\nusethis::gh_token_help()\nusethis::git_sitrep()\ngh::gh_whoami()\n\n\nCheck in RStudio if has automatically detected your Git installation.\n\n\nTools -&gt; Global Options -&gt; Git/SVN\n\n\n\n\n\n\n\nFigure 1: RStudio Project Options Window\n\n\n\nThe Git Executable field should be like:\n\nWindows: C:/Program Files/Git/bin/git.exe.\nMacOS:/usr/bin/git\n\n\nLet RStudio know which project branch to commit/push to GitHub. ( Default is “main”)\n\n\nusethis::git_default_branch_configure()",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#creating-your-blog-project-in-rstudio-one-time-only",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#creating-your-blog-project-in-rstudio-one-time-only",
    "title": " I Publish, therefore I Am",
    "section": "Creating your Blog Project in RStudio (One Time Only)",
    "text": "Creating your Blog Project in RStudio (One Time Only)\nIn the upper right corner of RStudio, click on:\nProject -&gt; New Project -&gt; New Directory -&gt; Quarto Blog\n\n\n\n\n\n\nFigure 2: New Quarto Blog Project\n\n\n\nName your Blog, browse to where you want the Blog Project Folder (default is fine). Check the Create a git repository. Click Create Project.\nRStudio will restart with a new Blog Project. The Files pane should like this:\n The posts folder contains dummy blog posts, which you can retain for now and delete once your own content has matured.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#making-your-blog-ready-to-publish-now-and-every-time-you-have-new-content",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#making-your-blog-ready-to-publish-now-and-every-time-you-have-new-content",
    "title": " I Publish, therefore I Am",
    "section": "Making your Blog ready to publish (Now, and every time you have new content)",
    "text": "Making your Blog ready to publish (Now, and every time you have new content)\nIn your Terminal:\n\nquarto render This will render all posts and prepare _site and _freeze folders in your Files tab. Check these.\nquarto preview This will pop up your browser and show you a preview of your Blog website. Check the look and feel, the typos.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#pushing-to-github-for-the-first-time",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#pushing-to-github-for-the-first-time",
    "title": " I Publish, therefore I Am",
    "section": "Pushing to GitHub for the First Time",
    "text": "Pushing to GitHub for the First Time\nIn your Console:\n\nusethis::use_git(): This will throw up a funky set of messages asking if you are ready to commit all files to push to Github. Choose the appropriate reply and enter.\nusethis::use_github(): This will create your new repository on Github and push all the committed files there. Your browser will open in Github and show you the contents of your new Blog repository.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#connecting-to-netlify",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#connecting-to-netlify",
    "title": " I Publish, therefore I Am",
    "section": "Connecting to Netlify",
    "text": "Connecting to Netlify\n\nHead off to https://www.netlify.com. Log in there with Github.\nClick Add new site -&gt; Import from existing project\n\n\n\n\n\n\n\nFigure 3: Netlify Window\n\n\n\n\nChoose Github, then point to your Blog Github repo.\nGive your Blog site a name: something-idiotic.netlify.app. Check Availability. (Note: netlify.app at the end is not removable, if you want free web-hosting at netlify)\nEnsure that the branch says main, and that the Publish directory says _site.\nHit Deploy. Netlify will ta ke a few minutes and then say the site is deployed. Click on the URL.\nThere!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#adding-content-and-updating",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#adding-content-and-updating",
    "title": " I Publish, therefore I Am",
    "section": "Adding Content and Updating",
    "text": "Adding Content and Updating\n\nCreate a new folder inside your posts folder. Name it something like new-post-todays-date. Hyphens only, no underscores.\nCreate a new Quarto Document: File -&gt; New-File -&gt; New Quarto Document. Save it as index.qmd inside your just-created new-post-todays-date folder.\nEdit / Write up your index.qmd Post. Add pictures, videos, code, visualizations, explanations. Use https://quarto.org to find other Markdown constructs ( tabs , asides, marginal content…) and complete your post.\nIn your Terminal, run quarto renderto update your _site and _freeze folders.\nIn your Terminal run quarto preview to check if your new post previews properly.\nIn your Git pane, you will see a list of changed files. Click on the Commit button, type in a commit message like Added new post on &lt;date&gt;, and then click Commit.\nIn the Git pane, click on the Push button to push your changes to Github.\nIn your browser, head off to your Github repo and check if the new post is there.\nIn your browser, head off to your Netlify site URL. You should see your new post there, in the list of posts.\nUsing GUI for git commit and push: Instead of using the Terminal, or the Git pane in Rstudio, there is a better way. This is a GUI, and one can see the files, the history of commits/pushes, the commit messages, revert back to a point in history, make brances…all that visually, without some tough-looking git commands:\n\n\nSo, install a git client, such as Gitkraken.\nIt offers a ready prompt to log in with Github, accept that, filling in ID and password.\nOpen File -&gt;New tab -&gt; Browse for Repo and browse to your Blog project folder (which is what you open in RStudio)\nYou will a list of previous commits, and a bunch of new edited / changed files which are default labelled WIP. (Work in Progress, peasants)\n\n\n\n\n\n\n\nFigure 4: Gitkraken Window\n\n\n\n\nClick on Stage All and type in a memorable commit message in the box below at right.\nThe WIP will be replaced by your commit message.\nOnce staging is done, hit the Push button on the main tool bar.\nAfter Gitkraken is done, a brief popup will show stating push was successful.\nYour website should be updated in a few minutes.\n\nYeah, peasants, you’re welcome as always.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#workflow-diagram",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#workflow-diagram",
    "title": " I Publish, therefore I Am",
    "section": "Workflow Diagram",
    "text": "Workflow Diagram",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#references",
    "href": "content/courses/Analytics/Workflow/Modules/30-BlogSite/index.html#references",
    "title": " I Publish, therefore I Am",
    "section": "References",
    "text": "References\n\nhttps://jcoliver.github.io/learn-r/010-github.html\nhttps://info5940.infosci.cornell.edu/setup/git/git-configure/\nhttps://posit-conf-2024.github.io/quarto-websites/\nAlice Bartlett. Git for Humans https://speakerdeck.com/alicebartlett/git-for-humans",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"entypo:publish\" width=\"1.2rem\" height=\"1.2rem\"></iconify-icon> I Publish, therefore I Am"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/20-Dashboards-Quarto/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/20-Dashboards-Quarto/index.html",
    "title": "The Mad Hatter's Guide to Data Viz and Stats in R",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/listing.html",
    "href": "content/courses/Analytics/Workflow/listing.html",
    "title": "Workflow",
    "section": "",
    "text": "Facing the Abyss: How to Probe Unknown Data. https://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/listing.html#references",
    "href": "content/courses/Analytics/Workflow/listing.html#references",
    "title": "Workflow",
    "section": "",
    "text": "Facing the Abyss: How to Probe Unknown Data. https://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html",
    "title": "Permutation Tests for Linear Regression",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html#linear-regression-using-permutation-tests",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/lin-perm.html#linear-regression-using-permutation-tests",
    "title": "Permutation Tests for Linear Regression",
    "section": "Linear Regression using Permutation Tests",
    "text": "Linear Regression using Permutation Tests\nWe wish to establish the significance of the effect size due to each of the levels in TempFac. From the normality tests conducted earlier we see that except at one level of TempFac, the times are are not normally distributed. Hence we opt for a Permutation Test to check for significance of effect.\nAs remarked in Ernst[^2], the non-parametric permutation test can be both exact and also intuitively easier for students to grasp. Permutations are easily executed in R, using packages such as mosaic[^3].\nWe proceed with a Permutation Test for TempFac. We shuffle the levels (13, 18, 25) randomly between the Times and repeat the ANOVA test each time and calculate the F-statistic. The Null distribution is the distribution of the F-statistic over the many permutations and the p-value is given by the proportion of times the F-statistic equals or exceeds that observed.\nRead the Data\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min          Q1     median          Q3       max\n1    tract integer   1.00000 1303.250000 3393.50000 3739.750000 5082.0000\n2      lon numeric -71.28950  -71.093225  -71.05290  -71.019625  -70.8100\n3      lat numeric  42.03000   42.180775   42.21810   42.252250   42.3810\n4     medv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n5    cmedv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n6     crim numeric   0.00632    0.082045    0.25651    3.677083   88.9762\n7       zn numeric   0.00000    0.000000    0.00000   12.500000  100.0000\n8    indus numeric   0.46000    5.190000    9.69000   18.100000   27.7400\n9      nox numeric   0.38500    0.449000    0.53800    0.624000    0.8710\n10      rm numeric   3.56100    5.885500    6.20850    6.623500    8.7800\n11     age numeric   2.90000   45.025000   77.50000   94.075000  100.0000\n12     dis numeric   1.12960    2.100175    3.20745    5.188425   12.1265\n13     rad integer   1.00000    4.000000    5.00000   24.000000   24.0000\n14     tax integer 187.00000  279.000000  330.00000  666.000000  711.0000\n15 ptratio numeric  12.60000   17.400000   19.05000   20.200000   22.0000\n16       b numeric   0.32000  375.377500  391.44000  396.225000  396.9000\n17   lstat numeric   1.73000    6.950000   11.36000   16.955000   37.9700\n           mean           sd   n missing\n1  2700.3557312 1.380037e+03 506       0\n2   -71.0563887 7.540535e-02 506       0\n3    42.2164403 6.177718e-02 506       0\n4    22.5328063 9.197104e+00 506       0\n5    22.5288538 9.182176e+00 506       0\n6     3.6135236 8.601545e+00 506       0\n7    11.3636364 2.332245e+01 506       0\n8    11.1367787 6.860353e+00 506       0\n9     0.5546951 1.158777e-01 506       0\n10    6.2846344 7.026171e-01 506       0\n11   68.5749012 2.814886e+01 506       0\n12    3.7950427 2.105710e+00 506       0\n13    9.5494071 8.707259e+00 506       0\n14  408.2371542 1.685371e+02 506       0\n15   18.4555336 2.164946e+00 506       0\n16  356.6740316 9.129486e+01 506       0\n17   12.6530632 7.141062e+00 506       0\n\n\nWe will use mosaic and also try with infer.\n\n\nUsing mosaic\nUsing infer\n\n\n\nmosaic offers an easy and intuitive way of doing a repeated permutation test, using the do() command. We will shuffle the TempFac factor to jumble up the Time observations, 10000 times. Each time we shuffle, we compute the F_statistic and record it. We then plot the 10000 F-statistics and compare that with the real-world observation of F-stat.\nThe Null distribution of the F_statistic under permutation shows it never crosses the real-world observed value, testifying the strength of the effect of TempFac on hatching Time. And the p-value is:\n\n\nWe calculate the observed F-stat with infer, which also has a very direct, if verbose, syntax for doing permutation tests:\nWe see that the observed F-Statistic is of course \\(385.8966\\) as before. Now we use infer to generate a NULL distribution using permutation of the factor TempFac:\nAs seen, the infer based permutation test also shows that the permutationally generated F-statistics are nowhere near that which was observed. The effect of TempFac is very strong."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html",
    "title": "Correlation and Regression Explorations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(ggformula)\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#packages",
    "title": "Correlation and Regression Explorations",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(ggformula)\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#intro",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#intro",
    "title": "Correlation and Regression Explorations",
    "section": "Intro",
    "text": "Intro\nI will work through and “unify” at least two things:\n\nHadley Wickham’s chapter on modelling and his analysis of the linear model for the diamonds dataset\nThe diagnostic aspects of Linear Regression as detailed in Crawley’s book"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#explorations-into-diagnostic-plots",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/corr-lm-explorations.html#explorations-into-diagnostic-plots",
    "title": "Correlation and Regression Explorations",
    "section": "Explorations into Diagnostic Plots",
    "text": "Explorations into Diagnostic Plots\nLet us create dependent y* variables with different sorts of errors:\n\nx &lt;- 0:300\nen &lt;- rnorm(301, mean = 0, sd = 5)\neu &lt;- (runif(n = 301) - 0.5) * 20\neb &lt;- rnbinom(n = 301, prob = 0.3, size = 2)\neg &lt;- rgamma(n = 301, shape = 1, rate = 1 / x)\nyn &lt;- x + 10 + en\nyu &lt;- x + 10 + eu\nyb &lt;- x + 10 + eb\nyg &lt;- x + 10 + eg\ndata &lt;- tibble(x, yn, yu, yb, yg)\ndata\n\n\n  \n\n\n\nNormal Errors\n\nlm_norm_aug &lt;- lm(yn ~ x, data = data) %&gt;%\n  augment()\nlm_norm_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_norm_aug %&gt;%\n  gf_qq(~.resid) %&gt;%\n  gf_qqline()\n\n\n\n\n\n\n\nUniform Errors\n\nlm_unif_aug &lt;- lm(yu ~ x, data = data) %&gt;%\n  augment()\nlm_unif_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_unif_aug %&gt;%\n  gf_qq(~.resid, distribution = stats::qnorm) %&gt;%\n  gf_qqline()\n\n\n\n\n\n\n\nNegative Binom Errors\n\nlm_nbinom_aug &lt;- lm(yb ~ x, data = data) %&gt;%\n  augment()\nlm_nbinom_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_nbinom_aug %&gt;%\n  gf_qq(~.resid, distribution = stats::qnorm) %&gt;%\n  gf_qqline()\n\n\n\n\n\n\n\nGamma Errors\n\nlm_gamm_aug &lt;- lm(yg ~ x, data = data) %&gt;%\n  augment()\nlm_gamm_aug %&gt;% gf_point(.resid ~ .fitted)\n\n\n\n\n\n\nlm_gamm_aug %&gt;%\n  gf_qq(~.resid, distribution = stats::qnorm) %&gt;%\n  gf_qqline()"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(timetk)\n###\nlibrary(tsibble)\nlibrary(fpp3)\nlibrary(sweep) # Tidy forecast Model objects\n###\nlibrary(forecast)\nlibrary(prophet)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#setup-the-packages",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#setup-the-packages",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(lubridate)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(timetk)\n###\nlibrary(tsibble)\nlibrary(fpp3)\nlibrary(sweep) # Tidy forecast Model objects\n###\nlibrary(forecast)\nlibrary(prophet)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#introduction",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nIn this module we will look at modelling of time series. We will start with the simplest of exponential models and go all the way through ARIMA and forecasting with Prophet.\nFirst, some terminology!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#additive-and-multiplicative-time-series-models",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "\n Additive and Multiplicative Time Series Models",
    "text": "Additive and Multiplicative Time Series Models\nAdditive Time Series can be represented as:\n\\[\nY_t = S_t + T_t + ϵ_t\n\\]\nMultiplicative Time Series can be described as:\n\\[\nY_t = S_t × T_t × ϵ_t\n\\]\nLet us consider a Multiplicative Time Series, pertaining to sales of souvenirs at beaches in Australia: The time series looks like this:\n\n\nError in scan(\"http://robjhyndman.com/tsdldata/data/fancy.dat\"): scan() expected 'a real', got '&lt;html&gt;&lt;head&gt;&lt;link'\n\n\nError: object 'souvenir' not found\n\n\nNote that along with the trend, the amplitude of both seasonal and noise components are also increasing in a multiplicative way here !! A multiplicative time series can be converted to additive by taking a log of the time series.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#stationarity",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#stationarity",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is said to be stationary if it holds the following conditions true:\n\nThe mean value of time-series is constant over time, which implies, the trend component is nullified/constant.\nThe variance does not increase over time.\nSeasonality effect is minimal.\n\nThis means it is devoid of trend or seasonal patterns, which makes it looks like a random white noise irrespective of the observed time interval , i.e. zooming in on the time axis. ( i.e. self-similar and fractal)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#a-bit-of-forecasting",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#a-bit-of-forecasting",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "A Bit of Forecasting?",
    "text": "A Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Autoregressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called prophet.\n\nForecasting using Exponential Smoothing\nFor example, the file contains total annual rainfall in inches for London, from 1813-1912 (original data from Hipel and McLeod, 1994).\n\nrain &lt;- scan(\"https://robjhyndman.com/tsdldata/hurst/precip1.dat\", skip = 2)\nrainseries &lt;- ts(rain, start = c(1813))\n\nError in ts(rain, start = c(1813)): 'ts' object must have one or more observations\n\nplot(rainseries)\n\nError: object 'rainseries' not found\n\n\nThere is a nearly constant value of about 25 around which there are random fluctuations and it seems to be an additive model. How can we make forecasts with this time series?\nA deliberate detour:\nLet’s see some quick notation to aid understanding: Much of smoothing is based on the high school concept of a straight line, \\(y = m*x + c\\).\nIn the following, we choose to describe the models with:\n\n\n\\(y\\) : the actual values in the time series\n\n\\(\\hat y\\) : our predictions from whichever model we create\n\n\\(l\\) : a level or mean as forecast;\n\n\\(b\\) : a trend variable; akin to the slope in the straight line equation;\n\n\\(s\\) : seasonal component of the time series. Note that this is a set of values that stretch over one cycle of the time series.\n\nIn Exponential Smoothing and Forecasting, we make three models of increasing complexity:\n\nSimple Exponential Model: Here we deal only with the mean or level aspect of the (decomposed) time series and make predictions with that.\nHolt Model: Here we use the level and the trend from the decomposed time series for predictions\nHolt-Winters Model: Here we use the level, the trend, and the seasonal component from the decomposed time series for predictions.\n\n\n[&lt;start&gt;st]-&gt;[&lt;input&gt;input]\n[&lt;input&gt; input]-&gt;[&lt;package&gt; Time  Series|Decomposition]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Mean/Level]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Slope/Trend]\n[&lt;package&gt; Time  Series|Decomposition]-&gt;[&lt;component&gt; Seasonal]\n\n//Simple Exponential Smoothing\n[&lt;component&gt; Mean/Level]-&gt;[Delay A1]\n[Delay A1]-&gt;[Delay A2]\n[Delay A2]-&gt;[Delay A3]\n[Delay A3]...-&gt;...[Delay AN]\n[Delay A1]-&gt;[&lt;state&gt; A1]\n[Delay A2]-&gt;[&lt;state&gt; A2]\n[Delay A3]-&gt;[&lt;state&gt; A3]\n[Delay AN]-&gt;[&lt;state&gt; AN]\n[&lt;state&gt; AN]---([&lt;note&gt; $$alpha(1-alpha)^i$$]\n\n[&lt;state&gt; A1]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A2]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; A3]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; AN]-&gt;[&lt;state&gt; Add1]\n[&lt;state&gt; Add1]-&gt;[&lt;end&gt; Output]\n\n//Holt \n[&lt;component&gt; Slope/Trend]-&gt;[Delay B1]\n[Delay B1]-&gt;[Delay B2]\n[Delay B2]-&gt;[Delay B3]\n[Delay B3]...-&gt;...[Delay BN]\n[Delay B1]-&gt;[&lt;state&gt; B1]\n[Delay B2]-&gt;[&lt;state&gt; B2]\n[Delay B3]-&gt;[&lt;state&gt; B3]\n[Delay BN]-&gt;[&lt;state&gt; BN]\n[&lt;state&gt; BN]---([&lt;note&gt; $$beta(1-beta)^i$$]\n[&lt;state&gt; B1]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B2]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; B3]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; BN]-&gt;[&lt;state&gt; Add2]\n[&lt;state&gt; Add2]-&gt;[&lt;end&gt; Output]\n\n// Holt Winters\n[&lt;component&gt; Seasonal]-&gt;[Delay C1]\n[Delay C1]-&gt;[Delay C2]\n[Delay C2]-&gt;[Delay C3]\n[Delay C3]...-&gt;...[Delay CN]\n[Delay C1]-&gt;[&lt;state&gt; C1]\n[Delay C2]-&gt;[&lt;state&gt; C2]\n[Delay C3]-&gt;[&lt;state&gt; C3]\n[Delay CN]-&gt;[&lt;state&gt; CN]\n[&lt;state&gt; CN]---([&lt;note&gt; $$gamma(1-gamma)^i$$]\n[&lt;state&gt; C1]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C2]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; C3]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; CN]-&gt;[&lt;state&gt; Add3]\n[&lt;state&gt; Add3]-&gt;[&lt;end&gt; Output]\n\n// Final Output\n[&lt;end&gt; Output]-&gt;[&lt;receiver&gt; Forecast]\n\n\n\n\n\nSimple Smoothing is smoothing based forecasting using just the level ( i.e. mean) of the Time Series to make forecasts.\nDouble exponential smoothing, or Holt Smoothing Model, is just exponential smoothing applied to both level and trend.\nThe idea behind triple exponential smoothing, or the Holt-Winters Smoothing Model, is to apply exponential smoothing to the seasonal components in addition to level and trend.\nWhat does “Exponential” mean?\nAll three models use memory: at each time instant in the Time Series, a set of past values, along with the present sample is used to make a prediction of the relevant parameter ( level / slope / seasonal). These are then added together to make the forecast.\nThe memory in each case controlled by a parameter: alpha for the estimate of the level beta for the slope estimate, and gamma for the seasonal component estimate at the current time point. All these parameters are between 0 and 1. The model takes a weighted average of past values of each parameter. The weights are derived in the form of \\(\\alpha(1-\\alpha)^i\\), where \\(i\\) defines how old the sample is compared to the present one, thus forming a set of weights that decrease exponentially with delay. Values of \\(\\alpha, \\beta. \\gamma\\) that are close to 0 mean that significant weightage is placed on observations in the past.(Memory is “stronger”). To express this in mathematical notation we now need three equations: one for level, one for the trend and one to combine the level and trend to get the expected \\(\\hat y\\).\nTo make forecasts using simple exponential smoothing in R, we can use the HoltWinters() function in R, or the forecast::ets() function from forecasts. This latter function is more powerful.\n\nargs(HoltWinters)\n\nfunction (x, alpha = NULL, beta = NULL, gamma = NULL, seasonal = c(\"additive\", \n    \"multiplicative\"), start.periods = 2, l.start = NULL, b.start = NULL, \n    s.start = NULL, optim.start = c(alpha = 0.3, beta = 0.1, \n        gamma = 0.1), optim.control = list()) \nNULL\n\nargs(forecast::ets)\n\nfunction (y, model = \"ZZZ\", damped = NULL, alpha = NULL, beta = NULL, \n    gamma = NULL, phi = NULL, additive.only = FALSE, lambda = NULL, \n    biasadj = FALSE, lower = c(rep(1e-04, 3), 0.8), upper = c(rep(0.9999, \n        3), 0.98), opt.crit = c(\"lik\", \"amse\", \"mse\", \"sigma\", \n        \"mae\"), nmse = 3, bounds = c(\"both\", \"usual\", \"admissible\"), \n    ic = c(\"aicc\", \"aic\", \"bic\"), restrict = TRUE, allow.multiplicative.trend = FALSE, \n    use.initial.values = FALSE, na.action = c(\"na.contiguous\", \n        \"na.interp\", \"na.fail\"), ...) \nNULL\n\n\nTo use HoltWinters() for simple exponential smoothing, we need to set the parameters beta=FALSE and gamma=FALSE in the HoltWinters() function (the beta and gamma parameters are used for double exponential smoothing, or triple exponential smoothing.\nTo use forecast::ets, we set the model argument to “ANN”, “AAN”, and “AAA” respectively for each of the three smoothing models.\nNote: The HoltWinters() function returns a list variable, that contains several named elements.\n\nrainseriesforecasts &lt;- forecast::ets(rainseries, model = \"ANN\")\n\nError: object 'rainseries' not found\n\n# class(rainseriesforecasts)\n# str(rainseriesforecasts)\nplot(rainseriesforecasts)\n\nError: object 'rainseriesforecasts' not found\n\nplot(forecast(rainseriesforecasts, 10))\n\nError: object 'rainseriesforecasts' not found\n\n\nARIMA\nWe can also use past trends and seasonality in the data to make predictions about the future using the forecast package. Here we use an auto ARIMA model to guess at the trend in the time series. Then we use that model to forecast a few periods into the future.\nMathematically an ARIMA model can be shown as follows:\n\n\n\n\n\n\nWe will use the familiar Walmart Sales dataset, and try to predict weekly sales for one of the Departments.\n\ndata(\"walmart_sales_weekly\")\nwalmart_wide &lt;- walmart_sales_weekly %&gt;%\n  pivot_wider(.,\n    id_cols = c(Date),\n    names_from = Dept,\n    values_from = Weekly_Sales,\n    names_prefix = \"Sales_\"\n  )\n\n## forecast::auto.arima needs a SINGLE time series, so we pick one, Dept95\nsales_95_ts &lt;- walmart_wide %&gt;%\n  select(Sales_95) %&gt;%\n  ts(start = c(2010, 1), end = c(2012, 52), frequency = 52)\nsales_95_ts\n\nTime Series:\nStart = c(2010, 1) \nEnd = c(2012, 52) \nFrequency = 52 \n  [1] 106690.06 111390.36 107952.07 103652.58 112807.75 112048.41 117716.13\n  [8] 113117.35 111466.37 116770.82 126341.84 110204.77 107648.14 125592.28\n [15] 120247.90 120036.99 121902.19 133056.97 131995.00 134118.05 120172.47\n [22] 124821.44 126241.20 121386.73 116256.35 108781.57 131128.96 131288.83\n [29] 124601.48 117929.58 124220.10 125027.49 124372.90 114702.69 113009.41\n [36] 120764.22 123510.99 110052.15 105793.40 110332.92 110209.31 107544.02\n [43] 106015.41 100834.31 111384.36 116521.67 121695.13  93676.95 107317.32\n [50] 109955.90 103724.16  99043.34 114270.08 117548.75 112165.80 107742.95\n [57] 116225.68 120621.32 123405.41 122280.13 112905.09 126746.25 126834.30\n [64] 118632.26 111764.31 120882.84 124953.94 112581.20 119815.67 135260.49\n [71] 136364.46 135197.63 121814.84 128054.88 133213.04 127906.50 121483.11\n [78] 117284.94 138538.47 138567.10 133260.84 122721.92 130446.34 133762.77\n [85] 133939.40 116165.28 115663.78 132805.42 125954.30 116931.34 108018.21\n [92] 114793.92 115047.16 113966.34 112688.97 102798.99 119053.80 120721.07\n [99] 125041.39  93358.91 116427.93 118685.12 113021.23 102202.04 115507.25\n[106] 125038.09 119807.63 110870.94 118406.27 125840.82 132318.50 117030.73\n[113] 127706.00 137958.76 129438.22 123172.79 118589.44 130920.36 131341.85\n[120] 129031.19 127603.00 130573.37 139857.10 140806.36 124594.40 131935.56\n[127] 148798.05 129724.74 126861.49 121030.79 134832.22 137408.20 136264.68\n[134] 118845.34 124741.33 140657.40 128542.73 119121.35 115326.47 127009.22\n[141] 124559.93 123346.24 117375.38 106690.06 111390.36 107952.07 103652.58\n[148] 112807.75 112048.41 117716.13 113117.35 111466.37 116770.82 126341.84\n[155] 110204.77 107648.14\n\narima_dept_95 &lt;- forecast::auto.arima(y = sales_95_ts)\narima_dept_95\n\nSeries: sales_95_ts \nARIMA(0,1,1)(0,1,0)[52] \n\nCoefficients:\n          ma1\n      -0.8842\ns.e.   0.0530\n\nsigma^2 = 29974424:  log likelihood = -1033.02\nAIC=2070.03   AICc=2070.15   BIC=2075.3\n\nplot(arima_dept_95)\n\n\n\n\n\n\n# Use the model to forecast 12 weeks into the future\nsales95_forecast &lt;- forecast(arima_dept_95, h = 12)\n\n# Plot the forecast. Again, we can use autoplot.\nautoplot(sales95_forecast) +\n  theme_minimal()\n\n\n\n\n\n\n\nWe’re fairly limited in what we can actually tweak when using autoplot(), so instead we can convert the forecast object to a data frame and use ggplot() like normal:\n\n# Get data out of this weird sales95_forecast object\nsales95_forecast\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2013.000       116571.1 109554.8 123587.5 105840.6 127301.7\n2013.019       126102.0 119038.7 133165.2 115299.7 136904.3\n2013.038       120871.5 113761.7 127981.4 109998.0 131745.1\n2013.058       111934.8 104778.7 119091.0 100990.5 122879.2\n2013.077       119470.2 112268.0 126672.3 108455.5 130484.9\n2013.096       126904.7 119656.9 134152.5 115820.1 137989.3\n2013.115       133382.4 126089.2 140675.6 122228.3 144536.5\n2013.135       118094.6 110756.3 125433.0 106871.6 129317.7\n2013.154       128769.9 121386.7 136153.1 117478.2 140061.6\n2013.173       139022.7 131594.8 146450.5 127662.8 150382.5\n2013.192       130502.1 123030.0 137974.3 119074.5 141929.8\n2013.212       124236.7 116720.5 131752.9 112741.7 135731.7\n\nsales95_forecast_tidy &lt;- sweep::sw_sweep(sales95_forecast,\n  fitted = TRUE,\n  timetk_idx = TRUE\n)\n\nsales95_forecast_tidy\n\n\n  \n\n\n# For whatever reason, the date column here is a special type of variable called\n# \"yearmon\", which ggplot doesn't know how to deal with (like, we can't zoom in\n# on the plot with coord_cartesian). We use zoo::as.Date() to convert the\n# yearmon variable into a regular date\nsales95_forecast_tidy_real_date &lt;-\n  sales95_forecast_tidy %&gt;%\n  mutate(actual_date = zoo::as.Date(index, frac = 1))\nsales95_forecast_tidy_real_date\n\n\n  \n\n\n# Plot this puppy!\nggplot(sales95_forecast_tidy, aes(x = index, y = value, color = key)) +\n  geom_ribbon(aes(ymin = lo.95, ymax = hi.95),\n    fill = \"#3182bd\", color = NA\n  ) +\n  geom_ribbon(aes(ymin = lo.80, ymax = hi.80, fill = key),\n    fill = \"#deebf7\", color = NA, alpha = 0.8\n  ) +\n  geom_line(size = 1) +\n  geom_point(size = 0.5) +\n  labs(x = NULL, y = \"sales95\") +\n  scale_y_continuous(labels = scales::comma) +\n  # Zoom in on 2012-2016\n  # coord_cartesian(xlim = ymd(c(\"2004-07-01\", \"2007-07-31\"))) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\nplot_time_series(.data = sales95_forecast_tidy, .date_var = index, .value = value, .color_var = key, .smooth = FALSE)\n\n\n\n\n\nA Bit of Forecasting?\nWe are always interested in the future. We will do this in three ways:\n\nuse Simple Exponential Smoothing\nuse a package called forecast to fit an ARIMA (Auto-regressive Moving Average Integrated Model) model to the data and make predictions for weekly sales;\nAnd do the same using a package called ’prophet`.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#conclusion",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "Conclusion",
    "text": "Conclusion",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/ModelTimeSeries/index.html#references",
    "title": "🕔 Modelling and Predicting Time Series",
    "section": "References",
    "text": "References\n1, Shampoo Dataset Brownlee: https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "🕔 Modelling and Predicting Time Series"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/listing.html",
    "href": "content/courses/Analytics/Modelling/listing.html",
    "title": "Inferential Modelling",
    "section": "",
    "text": "William G. Hunter, Six Statistical Tales. Journal of the Royal Statistical Society. Series D (The Statistician), Vol. 30, No. 2, Jun., 1981, pp. 107-117 https://sci-hub.ru/10.2307/2987563\nAndrew Gelman, Jennifer Hill, Aki Vehtari. Regression and Other Stories, Cambridge University Press, 2023.Available Online",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/listing.html#references",
    "href": "content/courses/Analytics/Modelling/listing.html#references",
    "title": "Inferential Modelling",
    "section": "",
    "text": "William G. Hunter, Six Statistical Tales. Journal of the Royal Statistical Society. Series D (The Statistician), Vol. 30, No. 2, Jun., 1981, pp. 107-117 https://sci-hub.ru/10.2307/2987563\nAndrew Gelman, Jennifer Hill, Aki Vehtari. Regression and Other Stories, Cambridge University Press, 2023.Available Online",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html",
    "title": "\n Introduction to R and RStudio",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio\nlearnt to use Quarto in R, which a document format for reproducible report generation",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#goals",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#goals",
    "title": "\n Introduction to R and RStudio",
    "section": "",
    "text": "At the end of this Lab, we will:\n\nhave installed R and RStudio on our machines\nunderstood how to add additional R-packages for specific features and graphic capability\nrun code within RStudio and interpret the results\nhave learnt to look for help within R and RStudio\nlearnt to use Quarto in R, which a document format for reproducible report generation",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#introduction-to-r-and-rstudio",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#introduction-to-r-and-rstudio",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Introduction to R and RStudio",
    "text": "Introduction to R and RStudio\nThis guide will lead you through the steps to install and use R, a free and open-source software environment for statistical computing and graphics.\nWhat is R?\n\n\nR is the name of the programming language itself, based off S from Bell Labs, which users access through a command-line interpreter (&gt;)\n\nWhat is RStudio?\n\n\nRStudio is a powerful and convenient user interface that allows you to access the R programming language along with a lot of other bells and whistles that enhance functionality (and sanity).",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-r",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-r",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Install R",
    "text": "Install R\nInstall R from CRAN, the Comprehensive R Archive Network. Please choose a precompiled binary distribution for your operating system.\n\n Check in\nLaunch R by clicking this logo  in your Applications. You should see one console with a command line interpreter. Try typing 2 + 2 and check !\nClose R.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-rstudio",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-rstudio",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Install RStudio",
    "text": "Install RStudio\nInstall the free, open-source edition of RStudio: https://posit.co/download/rstudio-desktop/\nRStudio provides a powerful user interface for R, called an integrated development environment. RStudio includes:\n\na console (the standard command line interface: &gt;),\na syntax-highlighting editor that supports direct code execution, and\ntools for plotting, history, debugging and work space management.\n\n\n Check in\nLaunch RStudio.You should get a window similar to the screenshot you see here:\n\n\n\n\n\nRStudio Default Window\n\nbut yours will be empty. Look at the bottom left pane: this is the same console window you saw when you opened R in step Section 3.1.\n\nPlace your cursor where you see &gt; and type x &lt;- 2 + 2 again hit enter or return, then type x, and hit enter/return again.\nIf [1] 4 prints to the screen, you have successfully installed R and RStudio, and you can move onto installing packages.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#installation-slides",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#installation-slides",
    "title": "\n Introduction to R and RStudio",
    "section": "Installation Slides",
    "text": "Installation Slides\n    View slides in full screen",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-packages",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#install-packages",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Install packages",
    "text": "Install packages\nThe version of R that you just downloaded is considered base R, which provides you with good but basic statistical computing and graphics powers. For analytical and graphical super-powers, you’ll need to install add-on packages, which are user-written, to extend/expand your R capabilities. Packages can live in one of two places:\n\nThey may be carefully curated by CRAN (which involves a thorough submission and review process), and thus are easy install using install.packages(\"name_of_package\", dependencies = TRUE) in your CONSOLE.\nPersonal repositories of packages created by practitioners, which are usually in Github.\n\nPlace your cursor in the CONSOLE again (where you last typed x and [4] printed on the screen). You can use the first method to install the following packages directly from CRAN, all of which we will use:\nType these commands in your CONSOLE:\n\ninstall.packages(\"knitr\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"ggformula\")\ninstall.packages(\"babynames\")\n\n\n\n\n\n\n\nImportantInstallation and Usage of R Packages!\n\n\n\n\nTo install a package, you put the name of the package in quotes as in install.packages(\"name_of_package\"). Mind your use of quotes carefully with packages.\nTo use an already installed package, you must load it first, as in library(name_of_package), leaving the name of the package bare. You only need to do this once per RStudio session.\n\n\n\n\nIf you want help, no quotes are needed: help(name_of_package) or ?name_of_package.\nIf you want the citation for a package (and you should give credit where credit is due), ask R as in citation(\"name_of_package\").",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#using-quarto",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#using-quarto",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Using Quarto",
    "text": "Using Quarto\nWe will get acquainted with the Quarto Document format, which allows us to mix text narrative, code, code-developed figures and items from the web in a seamless document. Quarto can be used to generate multiple formats such as HTML, Word, PDF from the same text/code file.\nSomething that can:\n\nprovide a visualization\nprovide insight\ntell a story\nis reproducible\nbe a call to action or a recommendation\n\nimpress colleagues, bosses, and faculty\n\n\n Setting up Quarto\nQuarto is already installed along with RStudio!! We can check if all is in order by running a check in the Terminal in RStudio.\n\nThe commands are:\n\nquarto check install\nquarto check knitr\n\nIf these come out with no errors then we are ready to fire up our first Quarto document.\n\n Practice\nLet us now create a brand new Quarto document, create some graphs in R and add some narrative text and see how we can generate our first report!\n\nFire up a new Quarto document by going to: File -&gt; New File -&gt; Quarto Document.\n\nGive a title to your document ( “My First Quarto Document”, for example.\nChange the author name to your own! Keep HTML as your output format\nSwitch to Visual mode, if it is not already there. Use the visual mode tool bar.\n\n\n\nClick on the various buttons to see what happens. Try to create Sections, code chunks, embedding images and tables.\n\n\n\n\n\n\n\nTipAdd Anything Shortcut\n\n\n\nTry the “add anything” shortcut! Type “/” anywhere in your Quarto Doc, while in Visual Mode, and choose what you want to add from the drop-down menu!\n\n\n\nCreate a code chunk as shown below. You can either use the visual tool bar to create it, or simply hit the copy button in the code chunk display on this website and paste the results into your Quarto document. Check every step!\n\n\n```{r}\n#| label: setup\n\n# library(knitr) # to use this….document! More later!!\nlibrary(tidyverse) # Data Management and Plotting!!\nlibrary(babynames) # A package containing, yes, Baby Names\nlibrary(ggformula)\n```\n\n\nHit the green “play” button to run this “setup” chunk to include in your R session all the installed packages you need.\nLet us greet our data first !!\n\n\n```{r}\n#| label: babynames-data\nglimpse(babynames)\nhead(babynames)\ntail(babynames)\nnames(babynames)\n```\n\nRows: 1,924,665\nColumns: 5\n$ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,…\n$ sex  &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", …\n$ name &lt;chr&gt; \"Mary\", \"Anna\", \"Emma\", \"Elizabeth\", \"Minnie\", \"Margaret\", \"Ida\",…\n$ n    &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258,…\n$ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.016…\n\n\n\n  \n\n\n  \n\n\n\n[1] \"year\" \"sex\"  \"name\" \"n\"    \"prop\"\n\n\n\nIf you have done the above and produced sane-looking output, you are ready for the next bit. Use the code below to create a new data frame called my_name_data.\n\n\n```{r}\n#| label: manipulate-name-data\nmy_name_data &lt;- babynames %&gt;%\n  filter(name == \"Arvind\" | name == \"Aravind\") %&gt;%\n  filter(sex == \"M\")\n```\n\n\nThe first bit makes a new dataset called my_name_data that is a copy of the babynames dataset\nthe %&gt;% (pipe) tells you we are doing some other stuff to it later.1\n\nThe second bit filters our babynames to only keep rows where the name is either Arvind or Aravind (read | as “or”.)\nThe third bit applies another filter to keep only those where sex is male.\n\nLet’s check out the data.\n\n```{r}\nmy_name_data\nglimpse(my_name_data)\n```\n\n\n  \n\n\n\nRows: 61\nColumns: 5\n$ year &lt;dbl&gt; 1970, 1972, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983,…\n$ sex  &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", …\n$ name &lt;chr&gt; \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvind\", \"Arvi…\n$ n    &lt;int&gt; 5, 8, 7, 5, 9, 6, 7, 6, 8, 6, 7, 7, 7, 13, 8, 11, 6, 8, 12, 10, 1…\n$ prop &lt;dbl&gt; 2.620e-06, 4.780e-06, 4.310e-06, 3.060e-06, 5.260e-06, 3.510e-06,…\n\n\n\nAgain, if you have sane-looking output here, move along to plotting the data!\n\n\n```{r}\n#| label:  plot-name-data\n\nplot &lt;- gf_line(prop ~ year,\n  color = ~name,\n  data = my_name_data\n)\n```\n\nNow if you did this right, you will not see your plot!\n\nBecause we saved the ggplot with a name (plot), R just saved the object for you. But check out the top right pane in RStudio again: under the Environment pane you should see plot, so it is there, you just have to ask for it. Here’s how:\n\n\n```{r}\nplot\n```\n\n\n\n\n\n\n\n\nNow hit the Render button on your Visual toolbar and see what happens!! Try to use the drop down menu next to it and see if there are more output file options!!\n\n Make a new name plot!\n\nEdit my code above to create a new dataset. Pick 2 names to compare how popular they each are (these could be different spellings of your own name, like I did, but you can choose any 2 names that are present in the dataset), and create a new data object with a new name.\nWrite narratives comments wherever suitable in your Quarto document. Make sure you don’t type inside your code chunks. See if you can write your comments in sections which you can create with your visual tool bar, or by using the “add anything” shortcut.\nSave your work ( your Quarto document itself) so you can share your favorite plot.\nShare your Plot: You will not like the looks of your plot if you mouse over to Export and save it. Instead, use ggplot2’s command for saving a plot with sensible defaults.\n\nType help(ggsave) in your Console.\n\n```{r}\n#| label: Saving\n\nggsave(\"file_name_here.pdf\", plot) # please make the filename unique!\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#conclusions",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#conclusions",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have installed R, RStudio and created our Quarto document, complete with graphs and narrative text. We also rendered our Quarto doc into HTML and other formats!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#references",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#references",
    "title": "\n Introduction to R and RStudio",
    "section": "\n References",
    "text": "References\n\nhttps://www.markdowntutorial.com\nhttps://ysc-rmarkdown.netlify.app/slides/01-basics.html Nice RMarkdown presentation and “code movies” !\nhttps://rmarkdown.rstudio.com/index.html\nSamantha Csik. Customizing Quarto websites. https://ucsb-meds.github.io/customizing-quarto-websites/#/title-slide\nReproducible Reporting with Quarto. https://book.rwithoutstatistics.com/quarto-chapter\nThomas Mock.(). Quarto in Two Hours https://jthomasmock.github.io/quarto-in-two-hours/\nhttps://quarto.org/docs/get-started/hello/rstudio.html\nhttps://quarto.org/docs/authoring/markdown-basics.html How to do more with Quarto HTML format\nhttps://apps.machlis.com/shiny/quartotips/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#assignments",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#assignments",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Assignment(s)",
    "text": "Assignment(s)\n\nComplete the markdown tutorial in [reference 1]\nLook through the Slides in [reference 2]\nCreate a fresh Quarto document and use as many as possible of the RMarkdown constructs from the Cheatsheet [reference 1]",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#readings",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#readings",
    "title": "\n Introduction to R and RStudio",
    "section": "\n Readings",
    "text": "Readings\n\nR for Data Science, Workflow: Basics Chapter: http://r4ds.had.co.nz/workflow-basics.html\nModern Dive, Getting Started Chapter: http://moderndive.com/2-getting-started.html\nR & RStudio Basics: https://bookdown.org/chesterismay/rbasics/3-rstudiobasics.html\nRStudio IDE Cheatsheet: https://github.com/rstudio/cheatsheets/blob/master/rstudio-ide.pdf",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#footnotes",
    "href": "content/courses/Analytics/Tools/10-Intro-to-R/index.html#footnotes",
    "title": "\n Introduction to R and RStudio",
    "section": "Footnotes",
    "text": "Footnotes\n\nInsert the pipe character using the keyboard shortcutCTRL + SHIFT + M.↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools",
      "<iconify-icon icon=\"fa-brands:r-project\"></iconify-icon> Introduction to R and RStudio"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html",
    "title": "\n Valentine’s Day Spending",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggprism)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#setting-up-r-packages",
    "title": "\n Valentine’s Day Spending",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggprism)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#introduction",
    "title": "\n Valentine’s Day Spending",
    "section": "Introduction",
    "text": "Introduction\nThis dataset pertains to spending on gifts by various people for Valentine’s Day. This was part of the TidyTuesday Project for February 2024!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#read-the-data",
    "title": "\n Valentine’s Day Spending",
    "section": "Read the Data",
    "text": "Read the Data\n\ngifts_age &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/gifts_age.csv\")\nglimpse(gifts_age)\n\nRows: 6\nColumns: 9\n$ Age                 &lt;chr&gt; \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65+\"\n$ SpendingCelebrating &lt;dbl&gt; 51, 40, 31, 19, 18, 13\n$ Candy               &lt;dbl&gt; 70, 62, 58, 60, 50, 42\n$ Flowers             &lt;dbl&gt; 50, 44, 41, 37, 32, 25\n$ Jewelry             &lt;dbl&gt; 33, 34, 29, 20, 13, 8\n$ GreetingCards       &lt;dbl&gt; 33, 33, 42, 42, 43, 44\n$ EveningOut          &lt;dbl&gt; 41, 37, 30, 31, 29, 24\n$ Clothing            &lt;dbl&gt; 33, 27, 26, 20, 19, 12\n$ GiftCards           &lt;dbl&gt; 23, 19, 22, 23, 20, 20",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#data-dictionary",
    "title": "\n Valentine’s Day Spending",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#analysetransform-the-data",
    "title": "\n Valentine’s Day Spending",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n# Change data to factors etc.\n# Set up Counts, histograms etc\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#research-question",
    "title": "\n Valentine’s Day Spending",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!! Look at the Chart!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#plot-the-data",
    "title": "\n Valentine’s Day Spending",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#task-and-discussion",
    "title": "\n Valentine’s Day Spending",
    "section": "Task and Discussion",
    "text": "Task and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat pre-processing of the data was required to create the chart?\nWhat might be the Hypothesis / Research Question, based on the Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#references",
    "href": "content/courses/Analytics/CaseStudies/Modules/90-ValentinesDay/index.html#references",
    "title": "\n Valentine’s Day Spending",
    "section": "References",
    "text": "References\nTo obtain that cool-looking X-axis in the chart, you need to use a new package called ggprism. Look at the vignette there and copy the code to make the X-axis like what you see here.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"mingcute:hand-heart-line\"></iconify-icon> Valentine's Day Spending"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html",
    "title": "\n Ikea Furniture",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggridges)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#setting-up-r-packages",
    "title": "\n Ikea Furniture",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggridges)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#introduction",
    "title": "\n Ikea Furniture",
    "section": "Introduction",
    "text": "Introduction\nThis is a dataset pertaining to furniture prices at IKEA, modified for ease of analysis and plotting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#data",
    "title": "\n Ikea Furniture",
    "section": "Data",
    "text": "Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#download-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#download-the-modified-data",
    "title": "\n Ikea Furniture",
    "section": "Download the Modified data",
    "text": "Download the Modified data\n\n\n Ikea furniture Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#data-dictionary",
    "title": "\n Ikea Furniture",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#plot-the-data",
    "title": "\n Ikea Furniture",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/10-IkeaFurniture/index.html#task-and-discussion",
    "title": "\n Ikea Furniture",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Create the graph shown and discuss the following questions:\n\nWhat is the kind of plot used in the chart?\nWhat variables have been used in the chart?\nWhat can you say about the scale on X-axis?\nWhat can you say about prices of items that are available in single colour versus those that are available in more than one colour?\nWhat is a good hypothesis to interpret the double-humped nature of some of the curves?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:ikea\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Ikea Furniture"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html",
    "title": "\n Children’s Games",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#setting-up-r-packages",
    "title": "\n Children’s Games",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#introduction",
    "title": "\n Children’s Games",
    "section": "Introduction",
    "text": "Introduction\nChildren in the ages of 6 to 7 years are asked if they want to play two games. This dataset pertains to their responses about the two games. The research is based on this paper:\nLin Bian et al. ,Gender stereotypes about intellectual ability emerge early and influence children’s interests. Science 355,389-391(2017).DOI:10.1126/science.aah6524. This very short and crisp paper is available here.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#read-the-data",
    "title": "\n Children’s Games",
    "section": "Read the Data",
    "text": "Read the Data\nThe data is part of the R package openintro. Yes, install it. From the help menu ?children_gender_stereo:\n\nThis data object is more unusual than most. It is a list of 4 data frames. The four data frames correspond to the data used in Studies 1-4 of the referenced paper, and these data frames each have variables (columns) that are explained below:\n\n  Download PDF File\n   \n    Unable to display PDF file. Download instead.\n  \n  \n\nlibrary(openintro)\ndata(\"children_gender_stereo\")\nglimpse(children_gender_stereo)\n\nList of 4\n $ 1:'data.frame':  192 obs. of  5 variables:\n  ..$ subject   : int [1:192] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ gender    : chr [1:192] \"female\" \"female\" \"female\" \"male\" ...\n  ..$ age       : int [1:192] 7 7 7 6 7 5 5 5 5 5 ...\n  ..$ trait     : chr [1:192] \"smart\" \"smart\" \"smart\" \"smart\" ...\n  ..$ stereotype: num [1:192] 0.611 0.278 0.722 0.556 1 ...\n $ 2:'data.frame':  576 obs. of  7 variables:\n  ..$ subject             : int [1:576] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ gender              : chr [1:576] \"male\" \"male\" \"male\" \"female\" ...\n  ..$ age                 : int [1:576] 6 5 7 5 5 7 6 6 5 5 ...\n  ..$ trait               : chr [1:576] \"smart\" \"smart\" \"smart\" \"smart\" ...\n  ..$ target              : chr [1:576] \"adults\" \"adults\" \"adults\" \"adults\" ...\n  ..$ stereotype          : num [1:576] 0.75 1 0.25 1 0.25 0.75 0 0.5 0.75 1 ...\n  ..$ high_achieve_caution: num [1:576] 0.25 1 0.25 1 0.75 0.5 0.5 0.75 0.5 0.5 ...\n $ 3:'data.frame':  128 obs. of  7 variables:\n  ..$ subject   : int [1:128] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ gender    : chr [1:128] \"female\" \"male\" \"female\" \"male\" ...\n  ..$ age       : int [1:128] 7 7 7 7 7 6 7 7 6 6 ...\n  ..$ game      : chr [1:128] \"smart\" \"smart\" \"smart\" \"smart\" ...\n  ..$ interest  : num [1:128] 0.328 0.781 0.781 -0.213 -2.304 ...\n  ..$ difference: num [1:128] 0.244 0.453 0.209 -0.577 -1.523 ...\n  ..$ stereotype: num [1:128] -1.7982 0.0866 -1.7982 0.7784 -0.6051 ...\n $ 4:'data.frame':  96 obs. of  4 variables:\n  ..$ subject : int [1:96] 1 2 3 4 5 6 7 8 9 10 ...\n  ..$ gender  : chr [1:96] \"female\" \"female\" \"female\" \"female\" ...\n  ..$ age     : int [1:96] 6 6 6 6 6 6 6 6 6 6 ...\n  ..$ interest: num [1:96] 0.3924 0.68 -0.7163 -0.4279 -0.0413 ...\n\n\nLet us choose, arbitrarily, the third study:\n\n## Choosing, arbitrarily, the third game/third study\nchildren_gender_stereo[[3]] -&gt; games3\nglimpse(games3)\n\nRows: 128\nColumns: 7\n$ subject    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ gender     &lt;chr&gt; \"female\", \"male\", \"female\", \"male\", \"female\", \"female\", \"ma…\n$ age        &lt;int&gt; 7, 7, 7, 7, 7, 6, 7, 7, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6,…\n$ game       &lt;chr&gt; \"smart\", \"smart\", \"smart\", \"smart\", \"smart\", \"smart\", \"smar…\n$ interest   &lt;dbl&gt; 0.328241235, 0.781351865, 0.781351865, -0.213178560, -2.303…\n$ difference &lt;dbl&gt; 0.24441071, 0.45311063, 0.20869992, -0.57713058, -1.5226918…\n$ stereotype &lt;dbl&gt; -1.79820307, 0.08662039, -1.79820307, 0.77835148, -0.605110…",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#data-dictionary",
    "title": "\n Children’s Games",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#analysetransform-the-data",
    "title": "\n Children’s Games",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n# Counts, histograms etc\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#research-question",
    "title": "\n Children’s Games",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nIs there a difference the average interest level between Boys and Girls for the two kinds of games, “Smart Game” and “Try Hard Game”? Does that lead to the inference of how children acquire gender stereotypes about play?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#plot-the-data",
    "title": "\n Children’s Games",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/80-ChildrensGames/index.html#task-and-discussion",
    "title": "\n Children’s Games",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Select and Transform the variables as shown. Create the graphs shown below and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nDoes the Chart answer the Hypothesis? Justify?\nWrite a 2-line story based on the chart, describing your inference/surprise.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:children\"></iconify-icon> <iconify-icon icon=\"icon-park-solid:play-basketball\"></iconify-icon> Children's Games"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html",
    "title": "\n Hearing Loss in Children",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#setting-up-r-packages",
    "title": "\n Hearing Loss in Children",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#introduction",
    "title": "\n Hearing Loss in Children",
    "section": "Introduction",
    "text": "Introduction\nChildren are monitored for OME (Otitis Media with Effusion, i.e. fluid in the middle ear) over time. It is believed that they later ( i.e. during aduldhood) suffer from “binaural hearing loss” (detecting sound amplitude and direction) after past episodes of OME during their childhood. The hearing-test is conducted multiple times, with a Test Signal embedded in noise played over audio loudspeakers. One loudspeaker has only Noise, and the other loudspeaker has the Test Signal in Noise. There are also two types of Test Signals: one is like noise itself and the other is distinct. In any test round, children are expected to orient themselves towards the appropriate loudspeaker and detect the presence of the Test Signal at varying levels of volume, with a passing success rate of 75% over multiple tests.\nThis dataset is available on Vincent Arel-Bundock’s dataset repository and is a part of the R package MASS.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#read-the-data",
    "title": "\n Hearing Loss in Children",
    "section": "Read the Data",
    "text": "Read the Data\n\nome &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/MASS/OME.csv\")\nglimpse(ome)\n\nRows: 1,097\nColumns: 8\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ ID       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3…\n$ Age      &lt;dbl&gt; 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 60, 60, 60, 60, 60, 6…\n$ OME      &lt;chr&gt; \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\"…\n$ Loud     &lt;dbl&gt; 35, 35, 40, 40, 45, 45, 50, 50, 55, 55, 35, 35, 40, 40, 45, 4…\n$ Noise    &lt;chr&gt; \"coherent\", \"incoherent\", \"coherent\", \"incoherent\", \"coherent…\n$ Correct  &lt;dbl&gt; 1, 4, 0, 1, 2, 2, 3, 4, 3, 2, 2, 3, 1, 1, 1, 5, 4, 2, 3, 4, 4…\n$ Trials   &lt;dbl&gt; 4, 5, 3, 1, 4, 2, 3, 4, 3, 2, 4, 4, 4, 1, 2, 5, 4, 2, 3, 4, 6…\n\nome",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#data-dictionary",
    "title": "\n Hearing Loss in Children",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#research-question",
    "title": "\n Hearing Loss in Children",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nIn hearing tests on people with varying levels of OME infection in their childhood, what is the effect of using distinct types of Test Signal on successful (face) orientation ?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#analysetransform-the-data",
    "title": "\n Hearing Loss in Children",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n# Rename Variables if needed\n# Change data to factors etc.\n# Set up Counts, histograms etc\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#plot-the-data",
    "title": "\n Hearing Loss in Children",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/110-ChildrenHearingLoss/index.html#task-and-discussion",
    "title": "\n Hearing Loss in Children",
    "section": "Task and Discussion",
    "text": "Task and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat pre-processing of the data was required to create the chart?\nWrite a 2-line story based on the chart, describing your inference/surprise. Is there something counter-intuitive (to a lay person) in the chart?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:ear-deaf\"></iconify-icon> <iconify-icon icon=\"fa-regular:sad-tear\"></iconify-icon> Hearing Loss in Children"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(GGally)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#setting-up-r-packages",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(GGally)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#introduction",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Introduction",
    "text": "Introduction\nJohn Snow’s contention that cholera was principally spread by water was not accepted in the 1850s by the medical elite. The consequence of rejection was that hundreds in the UK continued to die. William Farr, who founded the science of epidemiology, tried to examine if there were other causes that led to cholera. He had concluded that the available data not only supported miasma (spread via atmospheric vapours) but also indicated that there was an underlying ‘natural law’ linking infection with cholera inversely to elevation above high water. The data is available on Vincent Arel-Bundock’s website, and is part of the HistData package from Michael Friendly, UC Davis.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#read-the-data",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Read the Data",
    "text": "Read the Data\n\nCholera &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/HistData/Cholera.csv\")\nCholera",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#data-dictionary",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#research-question",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in! Look at the charts below!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#analysetransform-the-data",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#plot-the-data",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/450-CholeraInLondon/index.html#tasks-and-discussion",
    "title": "\n William Farr’s Observations on Cholera in London",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as needed.\nLook at Plot 1. Would you agree based on this chart that William Farr was right in believing that elevation was a good predictor for cholera deaths? Justify.\nWhat is the nature of the relationship between Cholera Deaths and Elevation?\nLook at Plot 2. What kind of plot is it? What is the relationship here between Elevation and Cholera Death Rate?\nBased on this graph, would you agree that Elevation is a predictor for Cholera Deaths? Justify.\nIs the relationship you found between Cholera Deaths and Elevation also found in Plot 1? Justify.\nLook at Plot 3. Would you guess that there could be another predictor for Cholera Deaths? What could that Predictor be? Justify.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"simple-icons:transportforlondon\"></iconify-icon> <iconify-icon icon=\"hugeicons:water-pump\"></iconify-icon>  William Farr's Observations on Cholera in London"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html",
    "title": "\n Women Live Longer?",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#setting-up-r-packages",
    "title": "\n Women Live Longer?",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#introduction",
    "title": "\n Women Live Longer?",
    "section": "Introduction",
    "text": "Introduction\nThis dataset pertains to survival ages in different countries across the world. Women survival ages are compared to those of men.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#read-the-data",
    "title": "\n Women Live Longer?",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download the data\n\n\nRows: 18,408\nColumns: 7\n$ Entity                                                               &lt;chr&gt; \"…\n$ Code                                                                 &lt;chr&gt; \"…\n$ Year                                                                 &lt;dbl&gt; 2…\n$ `Life expectancy - Sex: female - Age: at birth - Variant: estimates` &lt;dbl&gt; N…\n$ `Life expectancy - Sex: male - Age: at birth - Variant: estimates`   &lt;dbl&gt; N…\n$ `Population - Sex: all - Age: all - Variant: estimates`              &lt;dbl&gt; N…\n$ Continent                                                            &lt;chr&gt; \"…",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#data-dictionary",
    "title": "\n Women Live Longer?",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#analysetransform-the-data",
    "title": "\n Women Live Longer?",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n# Rename Variables if needed\n# Change data to factors etc.\n# Set up Counts, histograms etc\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#research-question",
    "title": "\n Women Live Longer?",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!! Look at the Chart!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#plot-the-data",
    "title": "\n Women Live Longer?",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#task-and-discussion",
    "title": "\n Women Live Longer?",
    "section": "Task and Discussion",
    "text": "Task and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat pre-processing of the data was required to create the chart?\nWhat might be the Hypothesis / Research Question, based on the Chart?\nWhat does the dashed line in the chart represent?\nWrite a 2-line story based on the chart, describing your inference/surprise.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#reference",
    "href": "content/courses/Analytics/CaseStudies/Modules/100-WomenLiveMen/index.html#reference",
    "title": "\n Women Live Longer?",
    "section": "Reference",
    "text": "Reference\nIn order to obtain that floating text note slope = 1 in the chart, you need to use gf_refine(annotate(....)). Look at the vignette/help here. https://ggplot2.tidyverse.org/reference/annotate.html",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:mage-light-skin-tone\"></iconify-icon> Women Live Longer?"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html",
    "title": "\n New York Dog Bites",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#setting-up-r-packages",
    "title": "\n New York Dog Bites",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#introduction",
    "title": "\n New York Dog Bites",
    "section": "Introduction",
    "text": "Introduction\nNine types of Seaweed were rated on different parameters and charted as shown below.\n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#read-the-data",
    "title": "\n New York Dog Bites",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download Seaweed Nutrition data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#inspect-the-data",
    "title": "\n New York Dog Bites",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\nRows: 10\nColumns: 18\n$ `common name`     &lt;chr&gt; \"RDA\", \"Norwegian Kelp\", \"Oarweed\", \"Thongweed\", \"Wa…\n$ `sci-name`        &lt;chr&gt; NA, \"-Ascophyllum nodosum\", \"-Laminaria digitata\", \"…\n$ `total fats`      &lt;chr&gt; NA, \"0.6\", \"-\", \"-\", \"0.6\", \"0.3\", \"-\", \"0.2\", \"-\", …\n$ `saturated fat`   &lt;chr&gt; NA, \"0.2\", \"-\", \"-\", \"0.1\", \"0.1\", \"-\", \"0\", \"-\", \"-\"\n$ cholesterol       &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"-\"\n$ protein           &lt;chr&gt; NA, \"1.7\", \"-\", \"-\", \"3\", \"5.8\", \"-\", \"1.5\", \"-\", \"-\"\n$ `Total fiber`     &lt;dbl&gt; NA, 8.8, 6.2, 9.8, 3.4, 3.8, 5.4, 1.3, 3.8, 4.9\n$ `Soluble fiber`   &lt;chr&gt; NA, \"7.5\", \"5.4\", \"7.7\", \"2.9\", \"3\", \"3\", \"-\", \"2.1\"…\n$ `Insoluble fiber` &lt;chr&gt; NA, \"1.3\", \"0.8\", \"2.1\", \"0.5\", \"1\", \"2.3\", \"-\", \"1.…\n$ Carbohydrates     &lt;dbl&gt; NA, 13.1, 9.9, 15.0, 4.6, 5.4, 10.6, 12.0, 4.1, 7.8\n$ Calcium           &lt;dbl&gt; NA, 575.0, 364.7, 30.0, 112.3, 34.2, 148.8, 373.8, 3…\n$ Potassium         &lt;dbl&gt; NA, 765.0, 2013.2, 1351.4, 62.4, 302.2, 1169.6, 827.…\n$ Magnesium         &lt;dbl&gt; NA, 225.0, 403.5, 90.1, 78.7, 108.3, 97.6, 573.8, 46…\n$ Sodium            &lt;dbl&gt; NA, 1173.8, 624.6, 600.6, 448.7, 119.7, 255.2, 1572.…\n$ Copper            &lt;dbl&gt; NA, 0.8, 0.3, 0.1, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1\n$ Iron              &lt;dbl&gt; NA, 14.9, 45.6, 5.0, 3.9, 5.2, 12.8, 6.6, 15.3, 22.2\n$ Iodine            &lt;dbl&gt; NA, 18.2, 70.0, 10.7, 3.9, 1.3, 10.2, 6.1, 1.6, 97.9\n$ Zinc              &lt;chr&gt; NA, \"-\", \"1.6\", \"1.7\", \"0.3\", \"0.7\", \"0.3\", \"-\", \"0.…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#data-dictionary",
    "title": "\n New York Dog Bites",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#research-question",
    "title": "\n New York Dog Bites",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#analysetransform-the-data",
    "title": "\n New York Dog Bites",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#plot-the-data",
    "title": "\n New York Dog Bites",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/510-NYDogBites/index.html#tasks-and-discussion",
    "title": "\n New York Dog Bites",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.\nBased on the diagram, discuss which one an elderly person might try if they are deficient in calcium. If you were trying to avoid carbs, which seaweed sushi would you try?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html",
    "title": "\n Seaweed Nutrients",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)\nlibrary(paletteer) # fancy colour palettes!\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#setting-up-r-packages",
    "title": "\n Seaweed Nutrients",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)\nlibrary(paletteer) # fancy colour palettes!\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#introduction",
    "title": "\n Seaweed Nutrients",
    "section": "Introduction",
    "text": "Introduction\nNine types of Seaweed were rated on different parameters and charted as shown below.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#read-the-data",
    "title": "\n Seaweed Nutrients",
    "section": "Read the Data",
    "text": "Read the Data\n Download the Seaweed data \n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R. NOTE: The sheet that contains our data of interest is titled “seaweed nutrition”, range = “A3:R13”.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#inspect-the-data",
    "title": "\n Seaweed Nutrients",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\nRows: 10\nColumns: 18\n$ `common name`     &lt;chr&gt; \"RDA\", \"Norwegian Kelp\", \"Oarweed\", \"Thongweed\", \"Wa…\n$ `sci-name`        &lt;chr&gt; NA, \"-Ascophyllum nodosum\", \"-Laminaria digitata\", \"…\n$ `total fats`      &lt;chr&gt; NA, \"0.6\", \"-\", \"-\", \"0.6\", \"0.3\", \"-\", \"0.2\", \"-\", …\n$ `saturated fat`   &lt;chr&gt; NA, \"0.2\", \"-\", \"-\", \"0.1\", \"0.1\", \"-\", \"0\", \"-\", \"-\"\n$ cholesterol       &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"-\"\n$ protein           &lt;chr&gt; NA, \"1.7\", \"-\", \"-\", \"3\", \"5.8\", \"-\", \"1.5\", \"-\", \"-\"\n$ `Total fiber`     &lt;dbl&gt; NA, 8.8, 6.2, 9.8, 3.4, 3.8, 5.4, 1.3, 3.8, 4.9\n$ `Soluble fiber`   &lt;chr&gt; NA, \"7.5\", \"5.4\", \"7.7\", \"2.9\", \"3\", \"3\", \"-\", \"2.1\"…\n$ `Insoluble fiber` &lt;chr&gt; NA, \"1.3\", \"0.8\", \"2.1\", \"0.5\", \"1\", \"2.3\", \"-\", \"1.…\n$ Carbohydrates     &lt;dbl&gt; NA, 13.1, 9.9, 15.0, 4.6, 5.4, 10.6, 12.0, 4.1, 7.8\n$ Calcium           &lt;dbl&gt; NA, 575.0, 364.7, 30.0, 112.3, 34.2, 148.8, 373.8, 3…\n$ Potassium         &lt;dbl&gt; NA, 765.0, 2013.2, 1351.4, 62.4, 302.2, 1169.6, 827.…\n$ Magnesium         &lt;dbl&gt; NA, 225.0, 403.5, 90.1, 78.7, 108.3, 97.6, 573.8, 46…\n$ Sodium            &lt;dbl&gt; NA, 1173.8, 624.6, 600.6, 448.7, 119.7, 255.2, 1572.…\n$ Copper            &lt;dbl&gt; NA, 0.8, 0.3, 0.1, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1\n$ Iron              &lt;dbl&gt; NA, 14.9, 45.6, 5.0, 3.9, 5.2, 12.8, 6.6, 15.3, 22.2\n$ Iodine            &lt;dbl&gt; NA, 18.2, 70.0, 10.7, 3.9, 1.3, 10.2, 6.1, 1.6, 97.9\n$ Zinc              &lt;chr&gt; NA, \"-\", \"1.6\", \"1.7\", \"0.3\", \"0.7\", \"0.3\", \"-\", \"0.…",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#data-dictionary",
    "title": "\n Seaweed Nutrients",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#research-question",
    "title": "\n Seaweed Nutrients",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in! First look at the chart below!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#analysetransform-the-data",
    "title": "\n Seaweed Nutrients",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#plot-the-data",
    "title": "\n Seaweed Nutrients",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#tasks-and-discussion",
    "title": "\n Seaweed Nutrients",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.\nBased on the diagram, discuss which one an elderly person might try if they are deficient in calcium. If you were trying to avoid carbs, which seaweed sushi would you try?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#references",
    "href": "content/courses/Analytics/CaseStudies/Modules/250-Seaweed/index.html#references",
    "title": "\n Seaweed Nutrients",
    "section": "References",
    "text": "References\nOver 2500 colour palettes are available in the paletteer package. Can you find tayloRswift? wesanderson? harrypotter? timburton?\nHere are the Qualitative Palettes:\n\n\n\n\n\n\nAnd the Quantitative/Continuous palettes:\n\n\n\n\n\n\nUse the commands:\n\n## For Qual variable-&gt; colour/fill:\nscale_colour_paletteer_d(\n  name = \"Legend Name\",\n  palette = \"package::palette\",\n  dynamic = TRUE / FALSE\n)\n\n## For Quant variable-&gt; colour/fill:\nscale_colour_paletteer_c(\n  name = \"Legend Name\",\n  palette = \"package::palette\",\n  dynamic = TRUE / FALSE\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"mdi:weed\"></iconify-icon> Seaweed Nutrients"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html",
    "title": "\n Elizabeth Bennett says No",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#setting-up-r-packages",
    "title": "\n Elizabeth Bennett says No",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#introduction",
    "title": "\n Elizabeth Bennett says No",
    "section": "Introduction",
    "text": "Introduction\nNine types of Seaweed were rated on different parameters and charted as shown below.\n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#read-the-data",
    "title": "\n Elizabeth Bennett says No",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download Seaweed Nutrition data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#inspect-the-data",
    "title": "\n Elizabeth Bennett says No",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\nRows: 10\nColumns: 18\n$ `common name`     &lt;chr&gt; \"RDA\", \"Norwegian Kelp\", \"Oarweed\", \"Thongweed\", \"Wa…\n$ `sci-name`        &lt;chr&gt; NA, \"-Ascophyllum nodosum\", \"-Laminaria digitata\", \"…\n$ `total fats`      &lt;chr&gt; NA, \"0.6\", \"-\", \"-\", \"0.6\", \"0.3\", \"-\", \"0.2\", \"-\", …\n$ `saturated fat`   &lt;chr&gt; NA, \"0.2\", \"-\", \"-\", \"0.1\", \"0.1\", \"-\", \"0\", \"-\", \"-\"\n$ cholesterol       &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"-\"\n$ protein           &lt;chr&gt; NA, \"1.7\", \"-\", \"-\", \"3\", \"5.8\", \"-\", \"1.5\", \"-\", \"-\"\n$ `Total fiber`     &lt;dbl&gt; NA, 8.8, 6.2, 9.8, 3.4, 3.8, 5.4, 1.3, 3.8, 4.9\n$ `Soluble fiber`   &lt;chr&gt; NA, \"7.5\", \"5.4\", \"7.7\", \"2.9\", \"3\", \"3\", \"-\", \"2.1\"…\n$ `Insoluble fiber` &lt;chr&gt; NA, \"1.3\", \"0.8\", \"2.1\", \"0.5\", \"1\", \"2.3\", \"-\", \"1.…\n$ Carbohydrates     &lt;dbl&gt; NA, 13.1, 9.9, 15.0, 4.6, 5.4, 10.6, 12.0, 4.1, 7.8\n$ Calcium           &lt;dbl&gt; NA, 575.0, 364.7, 30.0, 112.3, 34.2, 148.8, 373.8, 3…\n$ Potassium         &lt;dbl&gt; NA, 765.0, 2013.2, 1351.4, 62.4, 302.2, 1169.6, 827.…\n$ Magnesium         &lt;dbl&gt; NA, 225.0, 403.5, 90.1, 78.7, 108.3, 97.6, 573.8, 46…\n$ Sodium            &lt;dbl&gt; NA, 1173.8, 624.6, 600.6, 448.7, 119.7, 255.2, 1572.…\n$ Copper            &lt;dbl&gt; NA, 0.8, 0.3, 0.1, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1\n$ Iron              &lt;dbl&gt; NA, 14.9, 45.6, 5.0, 3.9, 5.2, 12.8, 6.6, 15.3, 22.2\n$ Iodine            &lt;dbl&gt; NA, 18.2, 70.0, 10.7, 3.9, 1.3, 10.2, 6.1, 1.6, 97.9\n$ Zinc              &lt;chr&gt; NA, \"-\", \"1.6\", \"1.7\", \"0.3\", \"0.7\", \"0.3\", \"-\", \"0.…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#data-dictionary",
    "title": "\n Elizabeth Bennett says No",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#research-question",
    "title": "\n Elizabeth Bennett says No",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#analysetransform-the-data",
    "title": "\n Elizabeth Bennett says No",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#plot-the-data",
    "title": "\n Elizabeth Bennett says No",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/520-JaneAustenNo/index.html#tasks-and-discussion",
    "title": "\n Elizabeth Bennett says No",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.\nBased on the diagram, discuss which one an elderly person might try if they are deficient in calcium. If you were trying to avoid carbs, which seaweed sushi would you try?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html",
    "title": "\n California Transit Payments",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(correlation)\n#\nlibrary(ggstats)\nlibrary(labelled)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#setting-up-r-packages",
    "title": "\n California Transit Payments",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(correlation)\n#\nlibrary(ggstats)\nlibrary(labelled)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#introduction",
    "title": "\n California Transit Payments",
    "section": "Introduction",
    "text": "Introduction\nThis dataset is the result of a research study on payment options for people using public transit in California.\nThe dataset is available on Dataset Dryad:\nPike, Susan (2022). Transit payment preferences of unbanked passengers. Dataset Dryad. https://doi.org/10.25338/B8R04T\nAnd a brief 2-pager on the research methodology is here.\nYes, peasants, you should read such stuff from other very different domains!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#read-the-data",
    "title": "\n California Transit Payments",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download the Cal Payment data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#data-dictionary",
    "title": "\n California Transit Payments",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#data-munging",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#data-munging",
    "title": "\n California Transit Payments",
    "section": "Data Munging",
    "text": "Data Munging\n\n\n Munged Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#summarize-and-prepare-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#summarize-and-prepare-the-data",
    "title": "\n California Transit Payments",
    "section": "Summarize and Prepare the Data",
    "text": "Summarize and Prepare the Data\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\nLet’s label the data variables…\n\n\ntibble [204 × 5] (S3: tbl_df/tbl/data.frame)\n $ phone.wifi    : num [1:204] 1 1 2 1 1 2 1 1 1 2 ...\n  ..- attr(*, \"label\")= Named chr \"Wi_Fi access?\"\n  .. ..- attr(*, \"names\")= chr \"phone.wifi\"\n  ..- attr(*, \"labels\")= Named num [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"No\" \"Yes\"\n $ phone.money   : num [1:204] 1 1 1 1 1 1 1 1 1 2 ...\n  ..- attr(*, \"label\")= Named chr \"Ways to add money?\"\n  .. ..- attr(*, \"names\")= chr \"phone.money\"\n  ..- attr(*, \"labels\")= Named num [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"No\" \"Yes\"\n $ phone.identity: num [1:204] 1 1 2 2 1 1 2 1 1 2 ...\n  ..- attr(*, \"label\")= Named chr \"Identity Concerns?\"\n  .. ..- attr(*, \"names\")= chr \"phone.identity\"\n  ..- attr(*, \"labels\")= Named num [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"No\" \"Yes\"\n $ phone.fees    : num [1:204] 1 2 1 1 1 1 1 1 1 1 ...\n  ..- attr(*, \"label\")= Named chr \"Monthly Fees?\"\n  .. ..- attr(*, \"names\")= chr \"phone.fees\"\n  ..- attr(*, \"labels\")= Named num [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"No\" \"Yes\"\n $ phone.balance : num [1:204] 1 2 1 1 1 1 1 2 1 2 ...\n  ..- attr(*, \"label\")= Named chr \"Knowing the balance?\"\n  .. ..- attr(*, \"names\")= chr \"phone.balance\"\n  ..- attr(*, \"labels\")= Named num [1:2] 1 2\n  .. ..- attr(*, \"names\")= chr [1:2] \"No\" \"Yes\"",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#plot-the-data",
    "title": "\n California Transit Payments",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/200-CaliforniaTransitPayments/index.html#task-and-discussion",
    "title": "\n California Transit Payments",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Select and Transform the variables as shown. Create the graph shown below and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat would be your recommendation to the Transport Company?\nTo the Phone Companies?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"openmoji:california-flag\"></iconify-icon> <iconify-icon icon=\"material-symbols:transit-ticket-outline-sharp\"></iconify-icon> California Transit Payments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggprism) # Interesting Categorical Axes\nlibrary(ggridges)\nlibrary(supernova)\n# devtools::install_github('cttobin/ggthemr')\nlibrary(ggthemr)\nlibrary(ggsci)\n\n\n\nShow the Code# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n##\n## https://stackoverflow.com/questions/36476751/associate-a-color-palette-with-ggplot2-theme\n##\nmy_colours &lt;- c(\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\")\nmy_pastels &lt;- c(\"#66C5CC\", \"#F6CF71\", \"#F89C74\", \"#DCB0F2\", \"#87C55F\", \"#9EB9F3\", \"#FE88B1\", \"#C9DB74\", \"#8BE0A4\", \"#B497E7\", \"#D3B484\", \"#B3B3B3\")\nmy_greys &lt;- c(\"#000000\", \"#333333\", \"#666666\", \"#999999\", \"#cccccc\")\nmy_vivids &lt;- c(\"#E58606\", \"#5D69B1\", \"#52BCA3\", \"#99C945\", \"#CC61B0\", \"#24796C\", \"#DAA51B\", \"#2F8AC4\", \"#764E9F\", \"#ED645A\", \"#CC3A8E\", \"#A5AA99\")\n\nmy_bolds &lt;- c(\"#7F3C8D\", \"#11A579\", \"#3969AC\", \"#F2B701\", \"#E73F74\", \"#80BA5A\", \"#E68310\", \"#008695\", \"#CF1C90\", \"#f97b72\", \"#4b4b8f\", \"#A5AA99\")\n\nfont &lt;- \"Roboto Condensed\"\nmytheme &lt;- theme_classic(base_size = 14) + ### %+replace%    #replace elements we want to change\n\n  theme(\n    text = element_text(family = font),\n    panel.grid.minor = element_blank(),\n    # text elements\n    plot.title = element_text(\n      family = font,\n      face = \"bold\",\n      hjust = 0, # left align\n      # vjust = 2 #raise slightly\n      margin = margin(0, 0, 10, 0)\n    ),\n    plot.subtitle = element_text(\n      family = font,\n      hjust = 0,\n      margin = margin(2, 0, 5, 0)\n    ),\n    plot.caption = element_text(\n      family = font,\n      size = 8,\n      hjust = 1\n    ),\n    # right align\n\n    axis.title = element_text( # axis titles\n      family = font, # font family\n      size = 10\n    ), # font size\n    axis.text = element_text( # axis text\n      family = font, # axis family\n      size = 8\n    ) # font size\n  )\ntheme_av &lt;- list(\n  mytheme,\n  scale_colour_manual(values = my_bolds, aesthetics = c(\"colour\", \"fill\"))\n)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#setting-up-r-packages",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggprism) # Interesting Categorical Axes\nlibrary(ggridges)\nlibrary(supernova)\n# devtools::install_github('cttobin/ggthemr')\nlibrary(ggthemr)\nlibrary(ggsci)\n\n\n\nShow the Code# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n##\n## https://stackoverflow.com/questions/36476751/associate-a-color-palette-with-ggplot2-theme\n##\nmy_colours &lt;- c(\"#fd7f6f\", \"#7eb0d5\", \"#b2e061\", \"#bd7ebe\", \"#ffb55a\", \"#ffee65\", \"#beb9db\", \"#fdcce5\", \"#8bd3c7\")\nmy_pastels &lt;- c(\"#66C5CC\", \"#F6CF71\", \"#F89C74\", \"#DCB0F2\", \"#87C55F\", \"#9EB9F3\", \"#FE88B1\", \"#C9DB74\", \"#8BE0A4\", \"#B497E7\", \"#D3B484\", \"#B3B3B3\")\nmy_greys &lt;- c(\"#000000\", \"#333333\", \"#666666\", \"#999999\", \"#cccccc\")\nmy_vivids &lt;- c(\"#E58606\", \"#5D69B1\", \"#52BCA3\", \"#99C945\", \"#CC61B0\", \"#24796C\", \"#DAA51B\", \"#2F8AC4\", \"#764E9F\", \"#ED645A\", \"#CC3A8E\", \"#A5AA99\")\n\nmy_bolds &lt;- c(\"#7F3C8D\", \"#11A579\", \"#3969AC\", \"#F2B701\", \"#E73F74\", \"#80BA5A\", \"#E68310\", \"#008695\", \"#CF1C90\", \"#f97b72\", \"#4b4b8f\", \"#A5AA99\")\n\nfont &lt;- \"Roboto Condensed\"\nmytheme &lt;- theme_classic(base_size = 14) + ### %+replace%    #replace elements we want to change\n\n  theme(\n    text = element_text(family = font),\n    panel.grid.minor = element_blank(),\n    # text elements\n    plot.title = element_text(\n      family = font,\n      face = \"bold\",\n      hjust = 0, # left align\n      # vjust = 2 #raise slightly\n      margin = margin(0, 0, 10, 0)\n    ),\n    plot.subtitle = element_text(\n      family = font,\n      hjust = 0,\n      margin = margin(2, 0, 5, 0)\n    ),\n    plot.caption = element_text(\n      family = font,\n      size = 8,\n      hjust = 1\n    ),\n    # right align\n\n    axis.title = element_text( # axis titles\n      family = font, # font family\n      size = 10\n    ), # font size\n    axis.text = element_text( # axis text\n      family = font, # axis family\n      size = 8\n    ) # font size\n  )\ntheme_av &lt;- list(\n  mytheme,\n  scale_colour_manual(values = my_bolds, aesthetics = c(\"colour\", \"fill\"))\n)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#introduction",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Introduction",
    "text": "Introduction\nThis is a dataset pertaining to tyres from different companies and their lifetime mileages."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#data",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Data",
    "text": "Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#download-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#download-the-modified-data",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Download the Modified data",
    "text": "Download the Modified data\n\n\n Tyre Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#data-dictionary",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#plot-the-data",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#task-and-discussion-anova",
    "href": "content/courses/Analytics/CaseStudies/Modules/325-ANOVA/index.html#task-and-discussion-anova",
    "title": "\n ANOVA - Tyre Brands and Mileage",
    "section": "Task and Discussion: ANOVA",
    "text": "Task and Discussion: ANOVA\n\nComplete the pre-analysis steps for ANOVA\n\nWrite in.\nModel + Table\n\nCreate the ANOVA model\nCreate the ANOVA table using the supernova package\n\n\n\nCall:\n   aov(formula = Mileage ~ Brands, data = tyre)\n\nTerms:\n                  Brands Residuals\nSum of Squares  256.2908  266.6495\nDeg. of Freedom        3        56\n\nResidual standard error: 2.182108\nEstimated effects may be unbalanced\n\n\n Analysis of Variance Table (Type III SS)\n Model: Mileage ~ Brands\n\n                              SS df     MS      F   PRE     p\n ----- --------------- | ------- -- ------ ------ ----- -----\n Model (error reduced) | 256.291  3 85.430 17.942 .4901 .0000\n Error (from model)    | 266.649 56  4.762                   \n ----- --------------- | ------- -- ------ ------ ----- -----\n Total (empty model)   | 522.940 59  8.863                   \n\n\nPost-hoc Analysis and Plots\n\nCompute the post-hoc differences in means and plot the pair-wise difference plots\n\n\n\n\n  group_1     group_2       diff pooled_se      q    df  lower  upper  p_adj\n  &lt;chr&gt;       &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Bridgestone Apollo      -3.019     0.563 -5.358    56 -5.129 -0.909  .0021\n2 CEAT        Apollo      -0.038     0.563 -0.067    56 -2.148  2.072 1.0000\n3 Falken      Apollo       2.826     0.563  5.015    56  0.716  4.935  .0043\n4 CEAT        Bridgestone  2.981     0.563  5.291    56  0.871  5.091  .0024\n5 Falken      Bridgestone  5.845     0.563 10.373    56  3.735  7.954  .0000\n6 Falken      CEAT         2.863     0.563  5.082    56  0.754  4.973  .0037\n\n\n\n\n\n\n\n\nConclusion\n\nState a conclusion about the effect of Brands on Mileage.\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/listing.html",
    "href": "content/courses/Analytics/CaseStudies/listing.html",
    "title": "Case Studies",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n Demo:Product Packaging and Elderly People\n\n\n22 min\n\n\n\n\n\n\n\n\n Ikea Furniture\n\n\n4 min\n\n\n\n\n\n\n\n\n ANOVA - Tyre Brands and Mileage\n\n\n4 min\n\n\n\n\n\n\n\n\n Movie Profits\n\n\n7 min\n\n\n\n\n\n\n\n\n Gender at the Work Place\n\n\n7 min\n\n\n\n\n\n\n\n\n Heptathlon\n\n\n8 min\n\n\n\n\n\n\n\n\n School Scores\n\n\n10 min\n\n\n\n\n\n\n\n\n Children’s Games\n\n\n8 min\n\n\n\n\n\n\n\n\n Valentine’s Day Spending\n\n\n8 min\n\n\n\n\n\n\n\n\n Women Live Longer?\n\n\n8 min\n\n\n\n\n\n\n\n\n Hearing Loss in Children\n\n\n8 min\n\n\n\n\n\n\n\n\nGrain Transportation Cartels\n\n\n12 min\n\n\n\n\n\n\n\n\n California Transit Payments\n\n\n7 min\n\n\n\n\n\n\n\n\n Seaweed Nutrients\n\n\n10 min\n\n\n\n\n\n\n\n\n Coffee Flavours\n\n\n13 min\n\n\n\n\n\n\n\n\n Legionnaire’s Disease in the USA\n\n\n13 min\n\n\n\n\n\n\n\n\n Antarctic Sea ice\n\n\n10 min\n\n\n\n\n\n\n\n\n William Farr’s Observations on Cholera in London\n\n\n9 min\n\n\n\n\n\n\n\n\n Satisfaction with AI Tools\n\n\n8 min\n\n\n\n\n\n\n\n\n New York Dog Bites\n\n\n13 min\n\n\n\n\n\n\n\n\n Elizabeth Bennett says No\n\n\n8 min\n\n\n\n\n\n\n\n\n Seattle Bicycle Zones\n\n\n8 min\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies"
    ]
  },
  {
    "objectID": "content/courses/listing.html",
    "href": "content/courses/listing.html",
    "title": "Teaching",
    "section": "",
    "text": "Here are the courses that I teach:\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\nData Analytics for Managers and Creators\n\n\nUsing R for Data Visualization, Analysis, and Inference\n\n\nNov 7, 2022\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html",
    "title": "\n Gender at the Work Place",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(scales)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        # size = 20,               #set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        # size = 14,                #font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#setting-up-r-packages",
    "title": "\n Gender at the Work Place",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(scales)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        # size = 20,               #set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        # size = 14,                #font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#introduction",
    "title": "\n Gender at the Work Place",
    "section": "Introduction",
    "text": "Introduction\nThis is a dataset pertaining to gender and compensation at the workplace, modified for ease of analysis and plotting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#data",
    "title": "\n Gender at the Work Place",
    "section": "Data",
    "text": "Data\n\n\n[1] 2088   12",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#download-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#download-the-modified-data",
    "title": "\n Gender at the Work Place",
    "section": "Download the Modified data",
    "text": "Download the Modified data\n\n\n Job and Gender Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#data-dictionary",
    "title": "\n Gender at the Work Place",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#plot-the-data",
    "title": "\n Gender at the Work Place",
    "section": "Plot the Data",
    "text": "Plot the Data\n\n\nError in `summarise()`:\nℹ In argument: `median_salary = median()`.\nℹ In group 1: `major_category = \"Computer, Engineering, and Science\"`.\nCaused by error in `median()`:\n! argument \"x\" is missing, with no default\n\n\nError: object 'wage_percent_of_male' not found",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/30-GenderWorkplace/index.html#task-and-discussion",
    "title": "\n Gender at the Work Place",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Create the graph shown and discuss the following questions:\n\nWhat kind of chart is used in the figure?\nWhat geometries have been used and to which variables have these geometries been mapped?\nBased on this graph, do you think gender plays a role in salaries? What is the trend you see?\nIf SALARY, NO_OF_WORKERS, GENDER, OCCUPATION were available in the original dataset, what pre-processing would have been necessary to obtain this plot?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icons8:gender\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Gender at the Work Place"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "",
    "text": "This is the YAML at the top of this Quarto document. Use order to sequence your blog posts, and df_print to tidily print your dataframes in the final HTML.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#setting-up-r-packages",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggridges)\n#\nlibrary(crosstable)\nlibrary(paletteer)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#setting-up-plot-theme",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#setting-up-plot-theme",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "Setting up plot theme",
    "text": "Setting up plot theme\n\nShow the Codeextrafont::loadfonts(quiet = TRUE)\nfont &lt;- \"Roboto Condensed\"\ntheme_set(new = theme_classic(base_size = 12))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  text = element_text(family = font),\n  # text elements\n  plot.title = element_text( # title\n    family = font, # set font family\n    size = 16, # set font size\n    face = \"bold\", # bold typeface\n    hjust = 0, # left align\n    # vjust = 2                #raise slightly\n    margin = margin(0, 0, 10, 0)\n  ),\n  plot.subtitle = element_text( # subtitle\n    family = font, # font family\n    size = 12, # font size\n    hjust = 0,\n    margin = margin(2, 0, 5, 0)\n  ),\n  plot.caption = element_text( # caption\n    family = font, # font family\n    size = 8, # font size\n    hjust = 1\n  ), # right align\n\n  axis.title = element_text( # axis titles\n    family = font, # font family\n    size = 10 # font size\n  ),\n  axis.text = element_text( # axis text\n    family = font, # axis family\n    size = 8\n  ) # font size\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#introduction",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Introduction",
    "text": "Introduction\nAs a demonstration Data Analysis flow, I will take a dataset and show the various steps involved in the workflow: data inspection, cleaning, setting up a hypothesis, plotting a chart, and responding to the hypothesis.\n\n\n\n\n\n\nNote\n\n\n\nI will repeat the entire code in every chunk, so that the whole process is visible in one shot at the end. This is not something you should do as a practice.\n\n\nThis is a dataset pertaining to packaging of groceries, and the difficulty that elderly people face with opening or closing those packages. The study also included people who were experiencing hand pain due to ailments such as arthritis.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#data",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Data",
    "text": "Data\nThe data is available here: Juliá-Nehme, Begoña (2023). Usability of Food Packaging in Older Adults. Figshare Dataset. https://doi.org/10.6084/m9.figshare.22637656.v1\nAnd, for you peasants, here too:\n\n\n Click me, peasants!\n\n\n Click me also, peasants!\n\n\n\nopening &lt;- opening %&gt;% janitor::clean_names()\nglimpse(opening)\ninspect(opening)\n\nRows: 17\nColumns: 17\n$ group          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2\n$ age            &lt;dbl&gt; 70, 71, 73, 73, 70, 67, 73, 72, 67, 66, 75, 81, 77, 78,…\n$ sex            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0\n$ hand_pain      &lt;dbl&gt; 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1\n$ hand_illness   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0\n$ hand_strength  &lt;dbl&gt; 21.0, 37.0, 21.0, 30.0, 21.0, 23.0, 18.0, 12.0, 20.0, 6…\n$ pinch_strength &lt;dbl&gt; 6.0, 6.5, 5.5, 7.5, 7.5, 6.3, 4.3, 2.5, 4.7, 1.7, 6.0, …\n$ time_jar       &lt;chr&gt; \"11\", \"9\", \"20\", \"7\", \"8\", \"62\", \"26\", \"25\", \"21\", \"5\",…\n$ time_beverage  &lt;dbl&gt; 5, 7, 21, 5, 4, 4, 3, 9, 9, 13, 3, 6, 6, 17, 6, 5, 2\n$ time_suace     &lt;chr&gt; \"17\", \"11\", \"14\", \"6\", \"7\", \"18\", \"8\", \"38\", \"11\", \"25\"…\n$ time_juice     &lt;dbl&gt; 15, 8, 13, 6, 7, 8, 6, 18, 10, 6, 9, 8, 17, 27, 11, 17,…\n$ time_milk      &lt;dbl&gt; 21, 3, 8, 5, 5, 52, 17, 48, 8, 2, 8, 6, 6, 34, 26, 14, …\n$ time_crackers  &lt;dbl&gt; 56, 46, 52, 7, 6, 47, 14, 49, 29, 25, 13, 19, 61, 33, 2…\n$ time_cheese    &lt;dbl&gt; 14, 35, 23, 29, 25, 9, 13, 12, 9, 36, 43, 19, 31, 18, 4…\n$ time_chickpeas &lt;dbl&gt; 35, 20, 69, 28, 29, 30, 29, 76, 20, 25, 27, 16, 39, 55,…\n$ time_bottle    &lt;dbl&gt; 10, 4, 18, 5, 4, 6, 6, 8, 11, 6, 8, 4, 6, 7, 8, 7, 6\n$ time_soup      &lt;dbl&gt; 6, 23, 23, 6, 3, 6, 2, 30, 23, 3, 4, 6, 9, 4, 6, 8, 12\n\ncategorical variables:  \n        name     class levels  n missing\n1   time_jar character     14 17       0\n2 time_suace character     14 17       0\n                                   distribution\n1 7 (17.6%), 11 (11.8%), 12 (5.9%) ...         \n2 11 (11.8%), 6 (11.8%), 8 (11.8%) ...         \n\nquantitative variables:  \n             name   class  min   Q1 median   Q3   max       mean         sd  n\n1           group numeric  1.0  1.0    1.0  2.0   2.0  1.4117647  0.5072997 17\n2             age numeric 66.0 70.0   73.0 78.0  91.0 74.5294118  6.6437720 17\n3             sex numeric  0.0  0.0    1.0  1.0   1.0  0.6470588  0.4925922 17\n4       hand_pain numeric  0.0  0.0    1.0  1.0   1.0  0.5294118  0.5144958 17\n5    hand_illness numeric  0.0  0.0    0.0  1.0   1.0  0.3529412  0.4925922 17\n6   hand_strength numeric  6.0 14.0   20.0 23.0  37.0 19.4529412  7.5407325 17\n7  pinch_strength numeric  1.7  4.5    5.5  6.5   9.5  5.4941176  2.0464819 17\n8   time_beverage numeric  2.0  4.0    6.0  9.0  21.0  7.3529412  5.1713293 17\n9      time_juice numeric  6.0  8.0   10.0 15.0  27.0 11.5294118  5.6802030 17\n10      time_milk numeric  2.0  6.0    8.0 21.0  52.0 16.2941176 15.3613342 17\n11  time_crackers numeric  6.0 19.0   31.0 47.0  61.0 32.5882353 17.2665096 17\n12    time_cheese numeric  9.0 14.0   25.0 35.0  80.0 28.0000000 17.8815547 17\n13 time_chickpeas numeric 16.0 27.0   30.0 62.0 128.0 45.8823529 30.6061316 17\n14    time_bottle numeric  4.0  6.0    6.0  8.0  18.0  7.2941176  3.3868257 17\n15      time_soup numeric  2.0  4.0    6.0 12.0  30.0 10.2352941  8.7644838 17\n   missing\n1        0\n2        0\n3        0\n4        0\n5        0\n6        0\n7        0\n8        0\n9        0\n10       0\n11       0\n12       0\n13       0\n14       0\n15       0\n\n\n\n## janitor is a good package to make clean names out of weird column names\n##\nclosing &lt;- closing %&gt;% janitor::clean_names()\nglimpse(closing)\ninspect(closing)\n\nRows: 17\nColumns: 17\n$ group          &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2\n$ age            &lt;dbl&gt; 70, 71, 73, 73, 70, 67, 73, 72, 67, 66, 75, 81, 77, 78,…\n$ sex            &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0\n$ hand_pain      &lt;dbl&gt; 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1\n$ hand_illness   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0\n$ hand_strength  &lt;dbl&gt; 21.0, 37.0, 21.0, 30.0, 21.0, 23.0, 18.0, 12.0, 20.0, 6…\n$ pinch_strength &lt;dbl&gt; 6.0, 6.5, 5.5, 7.5, 7.5, 6.3, 4.3, 2.5, 4.7, 1.7, 6.0, …\n$ time_jar       &lt;chr&gt; \"11\", \"9\", \"20\", \"7\", \"8\", \"62\", \"26\", \"25\", \"21\", \"5\",…\n$ time_beverage  &lt;dbl&gt; 5, 7, 21, 5, 4, 4, 3, 9, 9, 13, 3, 6, 6, 17, 6, 5, 2\n$ time_suace     &lt;chr&gt; \"17\", \"11\", \"14\", \"6\", \"7\", \"18\", \"8\", \"38\", \"11\", \"25\"…\n$ time_juice     &lt;dbl&gt; 15, 8, 13, 6, 7, 8, 6, 18, 10, 6, 9, 8, 17, 27, 11, 17,…\n$ time_milk      &lt;dbl&gt; 21, 3, 8, 5, 5, 52, 17, 48, 8, 2, 8, 6, 6, 34, 26, 14, …\n$ time_crackers  &lt;dbl&gt; 56, 46, 52, 7, 6, 47, 14, 49, 29, 25, 13, 19, 61, 33, 2…\n$ time_cheese    &lt;dbl&gt; 14, 35, 23, 29, 25, 9, 13, 12, 9, 36, 43, 19, 31, 18, 4…\n$ time_chickpeas &lt;dbl&gt; 35, 20, 69, 28, 29, 30, 29, 76, 20, 25, 27, 16, 39, 55,…\n$ time_bottle    &lt;dbl&gt; 10, 4, 18, 5, 4, 6, 6, 8, 11, 6, 8, 4, 6, 7, 8, 7, 6\n$ time_soup      &lt;dbl&gt; 6, 23, 23, 6, 3, 6, 2, 30, 23, 3, 4, 6, 9, 4, 6, 8, 12\n\ncategorical variables:  \n        name     class levels  n missing\n1   time_jar character     14 17       0\n2 time_suace character     14 17       0\n                                   distribution\n1 7 (17.6%), 11 (11.8%), 12 (5.9%) ...         \n2 11 (11.8%), 6 (11.8%), 8 (11.8%) ...         \n\nquantitative variables:  \n             name   class  min   Q1 median   Q3   max       mean         sd  n\n1           group numeric  1.0  1.0    1.0  2.0   2.0  1.4117647  0.5072997 17\n2             age numeric 66.0 70.0   73.0 78.0  91.0 74.5294118  6.6437720 17\n3             sex numeric  0.0  0.0    1.0  1.0   1.0  0.6470588  0.4925922 17\n4       hand_pain numeric  0.0  0.0    1.0  1.0   1.0  0.5294118  0.5144958 17\n5    hand_illness numeric  0.0  0.0    0.0  1.0   1.0  0.3529412  0.4925922 17\n6   hand_strength numeric  6.0 14.0   20.0 23.0  37.0 19.4529412  7.5407325 17\n7  pinch_strength numeric  1.7  4.5    5.5  6.5   9.5  5.4941176  2.0464819 17\n8   time_beverage numeric  2.0  4.0    6.0  9.0  21.0  7.3529412  5.1713293 17\n9      time_juice numeric  6.0  8.0   10.0 15.0  27.0 11.5294118  5.6802030 17\n10      time_milk numeric  2.0  6.0    8.0 21.0  52.0 16.2941176 15.3613342 17\n11  time_crackers numeric  6.0 19.0   31.0 47.0  61.0 32.5882353 17.2665096 17\n12    time_cheese numeric  9.0 14.0   25.0 35.0  80.0 28.0000000 17.8815547 17\n13 time_chickpeas numeric 16.0 27.0   30.0 62.0 128.0 45.8823529 30.6061316 17\n14    time_bottle numeric  4.0  6.0    6.0  8.0  18.0  7.2941176  3.3868257 17\n15      time_soup numeric  2.0  4.0    6.0 12.0  30.0 10.2352941  8.7644838 17\n   missing\n1        0\n2        0\n3        0\n4        0\n5        0\n6        0\n7        0\n8        0\n9        0\n10       0\n11       0\n12       0\n13       0\n14       0\n15       0",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#data-dictionary",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Data Dictionary",
    "text": "Data Dictionary\nSeveral variables are wrongly encoded here, as can be seen. For instance group, and sex are encoded as &lt;dbl&gt; and need to be converted to factors before analysis. We will write our Data Dictionary based on this understanding, and then convert the variables appropriately. (The full workflow will be shown here for the opening dataset; it follows in identical fashion for the closing dataset.)\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\n\n\nhand_strength(dbl): Hand Strength, numerical\n\npinch_strength(dbl): Pinch Strength, numerical\n\ntime_jar (chr) Time to open a jar. Needs to be (dbl)\n\n\ntime_beverage(dbl)Time to open a beverage\n\ntime_suace (chr) Time to open sauce\n\ntime_juice (dbl) Time to open juice\n\ntime_milk (dbl) Time to open milk carton\n\ntime_crackers(dbl)Time to open crackers pack\n\ntime_cheese (dbl) Time to open cheese packet\n\ntime_chickpeas (dbl) Time to open chickpeas packet\n\ntime_bottle (dbl) Time to open a bottle\n\ntime_soup(dbl) Time to open a a can of soup\n\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\n\n\ngroup(dbl): Groups in the study. Two. Make into (fct).\n\nsex(dbl): sex of the participant. Make into (fct).\n\nhand_pain: Did they suffer from hand pain or not? Binary. Make into (fct).\n\nhand_illness: How different from hand_pain? Make into (fct).\n\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nSmall dataset of 17 rows. Several time values have been measured across the same set of subjects, resulting is what is called a “repeat measures” experiment. Subjects seem to be in two groups, and with or without hand-pain. Are these the two groups? What is the difference between hand_pain and hand_illness?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#analyse-transform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#analyse-transform-the-data",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Analyse / Transform the Data",
    "text": "Analyse / Transform the Data\nWe need to first convert all the obvious Qual variables into, well, Qual factors! A few variables are also obviously Quant, and need to be transformed. We can also perform counts based on hand_pain and hand_illness to decide how to deal with them. And we will not modify the original data !!\n\nopening_modified &lt;- opening %&gt;%\n  # correct spelling mistake\n  rename(\"time_sauce\" = time_suace) %&gt;%\n  # If you want to do this fast!\n  mutate(across(contains(\"time\"), as.numeric)) %&gt;%\n  # Two \"NA\" entries exist\n\n  mutate(\n    hand_pain = as_factor(hand_pain),\n    hand_illness = as_factor(hand_illness),\n    group = as_factor(group),\n    sex = as_factor(sex)\n  )\n\nopening_modified\n\n\n  \n\n\nglimpse(opening_modified)\n\nRows: 17\nColumns: 17\n$ group          &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2\n$ age            &lt;dbl&gt; 70, 71, 73, 73, 70, 67, 73, 72, 67, 66, 75, 81, 77, 78,…\n$ sex            &lt;fct&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0\n$ hand_pain      &lt;fct&gt; 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1\n$ hand_illness   &lt;fct&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0\n$ hand_strength  &lt;dbl&gt; 21.0, 37.0, 21.0, 30.0, 21.0, 23.0, 18.0, 12.0, 20.0, 6…\n$ pinch_strength &lt;dbl&gt; 6.0, 6.5, 5.5, 7.5, 7.5, 6.3, 4.3, 2.5, 4.7, 1.7, 6.0, …\n$ time_jar       &lt;dbl&gt; 11, 9, 20, 7, 8, 62, 26, 25, 21, 5, 7, 7, 13, 11, 12, 2…\n$ time_beverage  &lt;dbl&gt; 5, 7, 21, 5, 4, 4, 3, 9, 9, 13, 3, 6, 6, 17, 6, 5, 2\n$ time_sauce     &lt;dbl&gt; 17, 11, 14, 6, 7, 18, 8, 38, 11, 25, 8, 6, 16, 29, 21, …\n$ time_juice     &lt;dbl&gt; 15, 8, 13, 6, 7, 8, 6, 18, 10, 6, 9, 8, 17, 27, 11, 17,…\n$ time_milk      &lt;dbl&gt; 21, 3, 8, 5, 5, 52, 17, 48, 8, 2, 8, 6, 6, 34, 26, 14, …\n$ time_crackers  &lt;dbl&gt; 56, 46, 52, 7, 6, 47, 14, 49, 29, 25, 13, 19, 61, 33, 2…\n$ time_cheese    &lt;dbl&gt; 14, 35, 23, 29, 25, 9, 13, 12, 9, 36, 43, 19, 31, 18, 4…\n$ time_chickpeas &lt;dbl&gt; 35, 20, 69, 28, 29, 30, 29, 76, 20, 25, 27, 16, 39, 55,…\n$ time_bottle    &lt;dbl&gt; 10, 4, 18, 5, 4, 6, 6, 8, 11, 6, 8, 4, 6, 7, 8, 7, 6\n$ time_soup      &lt;dbl&gt; 6, 23, 23, 6, 3, 6, 2, 30, 23, 3, 4, 6, 9, 4, 6, 8, 12\n\n\nAb theek hai. Haan, ab theek hai",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#analyse-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#analyse-the-data",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "Analyse the Data",
    "text": "Analyse the Data\nLet us make some counts wrt Qual variables, and histograms of Quant variables and get used to our data.\n\nopening_modified %&gt;% count(sex)\n\n\n  \n\n\nopening_modified %&gt;% count(hand_pain)\n\n\n  \n\n\nopening_modified %&gt;% count(hand_pain, hand_illness)\n\n\n  \n\n\n\nReasonably balanced groups. Hand_pain and Hand_illness are not the same thing, as seen from the 4-fold counts above.\n# Set graph theme\n# theme_set(new = theme_custom())\n#\nopening_modified %&gt;%\n  gf_histogram(~hand_strength)\nopening_modified %&gt;%\n  gf_histogram(~pinch_strength)\nopening_modified %&gt;%\n  gf_histogram(~time_jar)\nopening_modified %&gt;%\n  gf_histogram(~time_bottle)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHistograms do not look symmetric, but then we have only 17 observations anyway. Elderly people can’t very well be expected to be normal, bless them.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#research-questions",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#research-questions",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n Research Questions",
    "text": "Research Questions\nWe can create more than one too, and even iteratively, after we have answered the first one and so on. Let us write two:\n\n\n\n\n\n\nNote\n\n\n\nQ1. Do opening times for groceries vary between people with hand_pain and those without?\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ1. Do opening times for groceries vary between people of different sex?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#more-data-transformation",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#more-data-transformation",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "\n More data transformation",
    "text": "More data transformation\nAs seen, this data is in untidy form: there are several numerical columns that have some “Qual” information embedded in their column names, such as the the kind of package that is being opened. We should transform the data into long form so that all the time numbers are stacked up in one column, and the types of packages are in another column, called grocery. For more info see https://www.garrickadenbuie.com/project/tidyexplain/.\n\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7), # Choose columns to stack (by negation)\n    names_to = \"operation\", # Name of stack column\n    values_to = \"times\" # Name of values column\n  )\n\n\n  \n\n\n\nOnce we do this, we realize that the word “time” in the column operation adds no value, since we want only the grocery involved.\n\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7),\n    names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  # knock off that \"time\" word\n  tidyr::separate_wider_delim(\n    cols = operation,\n    delim = \"time_\",\n    # Rename \"operation\" column as \"grocery\", drop the silly column now containing only \"_time\"\n    names = c(NA, \"grocery\")\n  )\n\n\n  \n\n\n\nOK, looking better! Now if we plot times, we can colour or facet by grocery.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#first-plot",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#first-plot",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "First Plot",
    "text": "First Plot\n\n# Set graph theme\n# theme_set(new = theme_custom())\n#\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7),\n    names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  # knock off that \"time\" word\n  tidyr::separate_wider_delim(operation,\n    delim = \"time_\",\n    names = c(NA, \"grocery\")\n  ) %&gt;%\n  ## First Plot\n  gf_density_ridges(grocery ~ times,\n    fill = \"grey70\", scale = 0.75\n  ) %&gt;%\n  gf_facet_grid(hand_pain ~ .)\n\n\n\n\n\n\n\nOk! Not bad! We now want to label to do two things:\n\nlabel facets 0 and 1 as “Pain” and “No Pain”.\nReorder the groceries so that they are in decreasing order of me(di)an(times).\nIn two steps!!\n\n\n# Set graph theme\n# theme_set(new = theme_custom())\n#\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7),\n    names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  # knock off that \"time\" word\n  tidyr::separate_wider_delim(operation,\n    delim = \"time_\",\n    names = c(NA, \"grocery\")\n  ) %&gt;%\n  # Re-label the factor hand_pain\n  # Use base::factor() as this command is more clear to me\n  mutate(\n    hand_pain =\n      base::factor(hand_pain,\n        levels = c(0, 1),\n        labels = c(\"No Hand Pain\", \"Hand Pain\")\n      )\n  ) %&gt;%\n  ## First Plot\n  gf_density_ridges(grocery ~ times,\n    fill = \"grey70\", scale = 0.75\n  ) %&gt;%\n  gf_facet_grid(hand_pain ~ .)\n\n\n\n\n\n\n\nAnd to reorder the groceries in decreasing order of me(di)an (times):\n\n\n\n\n\n\nWarning\n\n\n\nThis does not seem to be happening at this time. Needs to be checked! Wonder what this chart is thinking…\n\n\n\n# Set graph theme\n# theme_set(new = theme_custom())\n#\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7),\n    names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  # knock off that \"time\" word\n  tidyr::separate_wider_delim(operation,\n    delim = \"time_\",\n    names = c(NA, \"grocery\")\n  ) %&gt;%\n  # Re-label the factor hand_pain\n  # Use base::factor() as this command is more clear to me\n  mutate(\n    hand_pain =\n      base::factor(hand_pain,\n        levels = c(0, 1),\n        labels = c(\"No Hand Pain\", \"Hand Pain\")\n      )\n  ) %&gt;%\n  ## First Plot modified\n\n  gf_density_ridges(\n    reorder(\n      grocery, # reorder the grocery var\n      times, # based on times variable\n      FUN = median\n    ) # taking the median times\n    ~ times,\n    fill = \"grey70\", scale = 0.75\n  ) %&gt;%\n  gf_facet_grid(hand_pain ~ .)\n\n\n\n\n\n\n\nAlmost done! We need to relabel the y-axis name and also add some title and subtitles to our plot. And maybe add a point on each sub-plot to show the median opening times?\n\nopening_modified %&gt;%\n  pivot_longer(\n    cols = -c(1:7),\n    names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  # knock off that \"time\" word\n  tidyr::separate_wider_delim(operation,\n    delim = \"time_\",\n    names = c(NA, \"grocery\")\n  ) %&gt;%\n  # Re-label the factor hand_pain\n  # Use base::factor() as this command is more clear to me\n  mutate(\n    hand_pain =\n      base::factor(hand_pain,\n        levels = c(0, 1),\n        labels = c(\"No Hand Pain\", \"Hand Pain\")\n      )\n  ) %&gt;%\n  group_by(grocery, hand_pain) %&gt;%\n  ## First Plot modified\n  gf_density_ridges(\n    reorder(\n      grocery, # reorder the grocery var\n      times, # based on times variable\n      FUN = median\n    ) # taking the median times\n    ~ times,\n    fill = \"grey70\", scale = 0.75\n  ) %&gt;%\n  ## Add the median points\n  gf_summary(\n    fun = \"median\", color = \"black\",\n    size = 1,\n    geom = \"point\"\n  ) %&gt;%\n  ## Facet by hand_pain\n  gf_facet_grid(hand_pain ~ .) %&gt;%\n  ## Add titles and labels\n  gf_labs(\n    title = \"Times take by Older People to Open Food Packages\",\n    x = \"Time in seconds\",\n    y = \"Type of Product\",\n    caption = \"Juliá-Nehme, Begoña (2023). Usability of Food Packaging in Older Adults.\\n figshare Dataset.\\n https://doi.org/10.6084/m9.figshare.22637656.v1\"\n  )\n\n\n\n\n\n\n# %&gt;% gf_theme(theme_custom())\n\nClosing Times Analysis\nSince the as.numeric did not work for us in the analysis of opening data, I have found and used another function as_numeric from the sjlabelled package. Sigh.\n\nclosing %&gt;%\n  mutate(across(starts_with(\"time\"), sjlabelled::as_numeric)) %&gt;%\n  pivot_longer(\n    cols = -c(1:7), names_to = \"operation\",\n    values_to = \"times\"\n  ) %&gt;%\n  separate_wider_delim(operation,\n    delim = \"time_\",\n    names = c(NA, \"operation\")\n  ) %&gt;%\n  mutate(\n    operation = str_replace(string = operation, pattern = \"suace\", replacement = \"sauce\"),\n    hand_pain = factor(hand_pain,\n      levels = c(0, 1),\n      labels = c(\"No Hand Pain\", \"Hand Pain\")\n    )\n  ) %&gt;%\n  group_by(operation, hand_pain) %&gt;%\n  gf_density_ridges(reorder(operation, times, FUN = mean) ~ times,\n    fill = \"grey70\", scale = 0.75\n  ) %&gt;%\n  gf_summary(fun = \"mean\", color = \"black\", size = 1, geom = \"point\") %&gt;%\n  gf_facet_grid(hand_pain ~ .) %&gt;%\n  gf_labs(\n    title = \"Times take by Older People to Close Food Packages\",\n    x = \"Time in seconds\", y = \"Type of Product\",\n    caption = \"Juliá-Nehme, Begoña (2023). Usability of Food Packaging in Older Adults.\\n figshare Dataset.\\n https://doi.org/10.6084/m9.figshare.22637656.v1\"\n  )\n\n\n\n\n\n\n# %&gt;%\n#   gf_theme(theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#task-and-discussion",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "Task and Discussion",
    "text": "Task and Discussion\n\nComplete the Data Dictionary.\nCreate the graph shown and discuss the following questions:\nWhat is the kind of plot used in the chart? A facetted ridge plot with medians marked using points\nWhat variables have been used in the chart?\n\nTime on X; Grocery Item on Y; Density on the ridges; Hand Pain for faceting\n\n\nQ1. Do opening times for groceries vary between people with hand_pain and those without?\n\nYes; the people with hand pain take longer to open the packages (meh, but all right!) While medians are not too different across the two groups, the distribution tails extend longer in the case of hand_pain = YES.\n\n\nWhy do that lines abruptly stop towards the right side of the upper half of the chart?\n\nBecause the extreme times are shorter across the board for closing, as compared to opening.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#references",
    "href": "content/courses/Analytics/CaseStudies/Modules/05-Demo/index.html#references",
    "title": "\n Demo:Product Packaging and Elderly People",
    "section": "References",
    "text": "References\n\nColour in R: https://r-for-artists.netlify.app/labs/04-graphics/04-colors\nThe paletteer package Over 2500 colour palettes are available in the paletteer package. Can you find tayloRswift? wesanderson? harrypotter? timburton?\n\nHere are the Qualitative Palettes (searchable):\n\n\n\n\n\n\nAnd the Quantitative/Continuous palettes (searchable):\n\n\n\n\n\n\nUse the commands:\n\nQual variable-&gt; colour/fill: scale_colour_paletteer_d(name = \"Legend Name\",                            palette = \"package::palette\",                           dynamic = TRUE/FALSE)\nQuant variable-&gt; colour/fill: scale_colour_paletteer_c(name = \"Legend Name\",                            palette = \"package::palette\",                           dynamic = TRUE/FALSE)\n\n\nIf you want those funky icons at the Section Headers, install this Quarto Extension, and then choosing the icons you want from https://iconify.design and using the iconify shortcode syntax shown below.\n\n\n\n{{&lt; iconify fluent-emoji exploding-head &gt;}}\n{{&lt; iconify fa6-brands apple width=50px height=10px rotate=90deg flip=vertical &gt;}}\n{{&lt; iconify simple-icons:quarto &gt;}}",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"healthicons:elderly-outline\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Demo:Product Packaging and Elderly People"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(patchwork)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#setting-up-r-packages",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(patchwork)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#introduction",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Introduction",
    "text": "Introduction\nLegionnaires’ disease (LD) is a severe form of pneumonia (∼10–25% fatality rate) caused by inhalation of aerosols containing Legionella, a pathogenic gram-negative bacteria. These bacteria can grow, spread, and aerosolize through building water systems. A recent dramatic increase in LD incidence has been observed globally, with a 9-fold increase in the United States from 2000 to 2018,\nRecords were also maintained of atmospheric Sulphur Dioxide (SO2) and the acidity i.e. pH of the atmosphere around building water systems such as Cooling Towers (CT) and in Rainwater.\nThis data is from this paper: Yu F, Nair AA, Lauper U (2024), https://doi.org/10.6084/m9.figshare.25157852.v2",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#read-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#read-the-modified-data",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Read the Modified Data",
    "text": "Read the Modified Data\n\n\n\n Download LD prevalence data\n\n\n\n\n\n Download SO2 in Nassau data\n\n\n\n\n\n Download SO2 in mainland US data\n\n\n\n\n\n Download pH Cooling Tower\n\n\n\n\n\n Download pH Rainwater",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#inspect-the-data",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n```{r}\n#| label: inspect-skim-glimpse\n\n# Write in\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#data-dictionary",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.\n\n\nDescribe how you may plan to transform the data.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#research-question",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in! Look first at the Charts below!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#join-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#join-the-data",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Join the Data",
    "text": "Join the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```\n\nHere is the plot-ready data:",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#plot-the-data",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Plot the Data",
    "text": "Plot the Data\nTwo plots were generated by the researchers with this data. Can you reproduce these? Do these graphs prove/disprove any of your hypotheses? What might have been the Hypotheses that led the creating of these graphs?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/350-LegionnairesDisease/index.html#tasks-and-discussion",
    "title": "\n Legionnaire’s Disease in the USA",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown. Combine the multiple datasets into one if needed!\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat is a peculiar feature of these graphs?\n\n\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWhat data gathering / research activity might have been carried out to obtain the data graphed here? Provide some details.\nWrite a short story based on the chart, describing your inference/surprise.\nIs there a paradox in this case study? Hint: SO2 is caused by cars/busses running on fossil fuels.\nWhat Statistical Tests might you run to confirm what the charts are saying?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa6-solid:disease\"></iconify-icon> Legionnaire's Disease in the USA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html",
    "title": "\n Movie Profits",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        # size = 20,               #set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        # size = 14,                #font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#setting-up-r-packages",
    "title": "\n Movie Profits",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        # size = 20,               #set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        # size = 14,                #font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#introduction",
    "title": "\n Movie Profits",
    "section": "Introduction",
    "text": "Introduction\nThis is a dataset pertaining to movies and genres, modified for ease of analysis and plotting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#data",
    "title": "\n Movie Profits",
    "section": "Data",
    "text": "Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#download-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#download-the-modified-data",
    "title": "\n Movie Profits",
    "section": "Download the Modified data",
    "text": "Download the Modified data\n\n\n Movie Profit Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#data-dictionary",
    "title": "\n Movie Profits",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#plot-the-data",
    "title": "\n Movie Profits",
    "section": "Plot the Data",
    "text": "Plot the Data\n\n\nUsing R\nUsing Observable",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/20-MovieProfits/index.html#task-and-discussion",
    "title": "\n Movie Profits",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Create the graph shown and discuss the following questions:\n\nIdentify the type of plot\nWhat are the variables used to plot this graph?\nIf you were to invest in movie production ventures, which are the two best genres that you might decide to invest in?\nWhich R command might have been used to obtain the separate plots for each distributor?\nIf the original dataset had BUDGETS and PROFITS in separate columns, what preprocessing might have been done to achieve this plot?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"noto:movie-camera\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Movie Profits"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html",
    "title": "\n Coffee Flavours",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\n\n\nShow the Codeextrafont::loadfonts(quiet = TRUE)\nfont &lt;- \"Roboto Condensed\"\ntheme_set(new = theme_classic(base_size = 14))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  text = element_text(family = font),\n  # text elements\n  plot.title = element_text( # title\n    family = font, # set font family\n    size = 20, # set font size\n    face = \"bold\", # bold typeface\n    hjust = 0, # left align\n    # vjust = 2                #raise slightly\n    margin = margin(0, 0, 10, 0)\n  ),\n  plot.subtitle = element_text( # subtitle\n    family = font, # font family\n    size = 14, # font size\n    hjust = 0,\n    margin = margin(2, 0, 5, 0)\n  ),\n  plot.caption = element_text( # caption\n    family = font, # font family\n    size = 8, # font size\n    hjust = 1\n  ), # right align\n\n  axis.title = element_text( # axis titles\n    family = font, # font family\n    size = 10 # font size\n  ),\n  axis.text = element_text( # axis text\n    family = font, # axis family\n    size = 8\n  ) # font size\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#setting-up-r-packages",
    "title": "\n Coffee Flavours",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\n\n\nShow the Codeextrafont::loadfonts(quiet = TRUE)\nfont &lt;- \"Roboto Condensed\"\ntheme_set(new = theme_classic(base_size = 14))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  text = element_text(family = font),\n  # text elements\n  plot.title = element_text( # title\n    family = font, # set font family\n    size = 20, # set font size\n    face = \"bold\", # bold typeface\n    hjust = 0, # left align\n    # vjust = 2                #raise slightly\n    margin = margin(0, 0, 10, 0)\n  ),\n  plot.subtitle = element_text( # subtitle\n    family = font, # font family\n    size = 14, # font size\n    hjust = 0,\n    margin = margin(2, 0, 5, 0)\n  ),\n  plot.caption = element_text( # caption\n    family = font, # font family\n    size = 8, # font size\n    hjust = 1\n  ), # right align\n\n  axis.title = element_text( # axis titles\n    family = font, # font family\n    size = 10 # font size\n  ),\n  axis.text = element_text( # axis text\n    family = font, # axis family\n    size = 8\n  ) # font size\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#introduction",
    "title": "\n Coffee Flavours",
    "section": "Introduction",
    "text": "Introduction\nThis dataset pertains to scores various types of coffees on parameters such as aroma, flavour, after-taste etc.\n\n\n\n\n\n\nNoteBreadcrumbs\n\n\n\nSince there are some interesting pre-processing actions required of data, and some choices to be made as well, I will leave some breadcrumbs, and some intermediate results, for you to look at and figure out the analysis/EDA path that you might take! You can then vary these at will after getting a measure of confidence!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#read-the-data",
    "title": "\n Coffee Flavours",
    "section": "Read the Data",
    "text": "Read the Data\n\n\nRows: 1,339\nColumns: 43\n$ total_cup_points      &lt;dbl&gt; 90.58, 89.92, 89.75, 89.00, 88.83, 88.83, 88.75,…\n$ species               &lt;chr&gt; \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica\", \"Ara…\n$ owner                 &lt;chr&gt; \"metad plc\", \"metad plc\", \"grounds for health ad…\n$ country_of_origin     &lt;chr&gt; \"Ethiopia\", \"Ethiopia\", \"Guatemala\", \"Ethiopia\",…\n$ farm_name             &lt;chr&gt; \"metad plc\", \"metad plc\", \"san marcos barrancas …\n$ lot_number            &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ mill                  &lt;chr&gt; \"metad plc\", \"metad plc\", NA, \"wolensu\", \"metad …\n$ ico_number            &lt;chr&gt; \"2014/2015\", \"2014/2015\", NA, NA, \"2014/2015\", N…\n$ company               &lt;chr&gt; \"metad agricultural developmet plc\", \"metad agri…\n$ altitude              &lt;chr&gt; \"1950-2200\", \"1950-2200\", \"1600 - 1800 m\", \"1800…\n$ region                &lt;chr&gt; \"guji-hambela\", \"guji-hambela\", NA, \"oromia\", \"g…\n$ producer              &lt;chr&gt; \"METAD PLC\", \"METAD PLC\", NA, \"Yidnekachew Dabes…\n$ number_of_bags        &lt;dbl&gt; 300, 300, 5, 320, 300, 100, 100, 300, 300, 50, 3…\n$ bag_weight            &lt;chr&gt; \"60 kg\", \"60 kg\", \"1\", \"60 kg\", \"60 kg\", \"30 kg\"…\n$ in_country_partner    &lt;chr&gt; \"METAD Agricultural Development plc\", \"METAD Agr…\n$ harvest_year          &lt;chr&gt; \"2014\", \"2014\", NA, \"2014\", \"2014\", \"2013\", \"201…\n$ grading_date          &lt;chr&gt; \"April 4th, 2015\", \"April 4th, 2015\", \"May 31st,…\n$ owner_1               &lt;chr&gt; \"metad plc\", \"metad plc\", \"Grounds for Health Ad…\n$ variety               &lt;chr&gt; NA, \"Other\", \"Bourbon\", NA, \"Other\", NA, \"Other\"…\n$ processing_method     &lt;chr&gt; \"Washed / Wet\", \"Washed / Wet\", NA, \"Natural / D…\n$ aroma                 &lt;dbl&gt; 8.67, 8.75, 8.42, 8.17, 8.25, 8.58, 8.42, 8.25, …\n$ flavor                &lt;dbl&gt; 8.83, 8.67, 8.50, 8.58, 8.50, 8.42, 8.50, 8.33, …\n$ aftertaste            &lt;dbl&gt; 8.67, 8.50, 8.42, 8.42, 8.25, 8.42, 8.33, 8.50, …\n$ acidity               &lt;dbl&gt; 8.75, 8.58, 8.42, 8.42, 8.50, 8.50, 8.50, 8.42, …\n$ body                  &lt;dbl&gt; 8.50, 8.42, 8.33, 8.50, 8.42, 8.25, 8.25, 8.33, …\n$ balance               &lt;dbl&gt; 8.42, 8.42, 8.42, 8.25, 8.33, 8.33, 8.25, 8.50, …\n$ uniformity            &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00,…\n$ clean_cup             &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, …\n$ sweetness             &lt;dbl&gt; 10.00, 10.00, 10.00, 10.00, 10.00, 10.00, 10.00,…\n$ cupper_points         &lt;dbl&gt; 8.75, 8.58, 9.25, 8.67, 8.58, 8.33, 8.50, 9.00, …\n$ moisture              &lt;dbl&gt; 0.12, 0.12, 0.00, 0.11, 0.12, 0.11, 0.11, 0.03, …\n$ category_one_defects  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ quakers               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ color                 &lt;chr&gt; \"Green\", \"Green\", NA, \"Green\", \"Green\", \"Bluish-…\n$ category_two_defects  &lt;dbl&gt; 0, 1, 0, 2, 2, 1, 0, 0, 0, 4, 1, 0, 0, 2, 2, 0, …\n$ expiration            &lt;chr&gt; \"April 3rd, 2016\", \"April 3rd, 2016\", \"May 31st,…\n$ certification_body    &lt;chr&gt; \"METAD Agricultural Development plc\", \"METAD Agr…\n$ certification_address &lt;chr&gt; \"309fcf77415a3661ae83e027f7e5f05dad786e44\", \"309…\n$ certification_contact &lt;chr&gt; \"19fef5a731de2db57d16da10287413f5f99bc2dd\", \"19f…\n$ unit_of_measurement   &lt;chr&gt; \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\"…\n$ altitude_low_meters   &lt;dbl&gt; 1950.0, 1950.0, 1600.0, 1800.0, 1950.0, NA, NA, …\n$ altitude_high_meters  &lt;dbl&gt; 2200.0, 2200.0, 1800.0, 2200.0, 2200.0, NA, NA, …\n$ altitude_mean_meters  &lt;dbl&gt; 2075.0, 2075.0, 1700.0, 2000.0, 2075.0, NA, NA, …\n\n\n Download Coffee data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#inspect-clean-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#inspect-clean-the-data",
    "title": "\n Coffee Flavours",
    "section": "Inspect, Clean the Data",
    "text": "Inspect, Clean the Data\nWhat are the non-numeric, or Qualitative variables here?\n\n\n\n  \n\n\n\nLook at the number of levels in those Qual variables!!Some are too many and some are so few… Suppose we count the data on the basis of a few?\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBreadcrumb 1\n\n\n\nWhy did I choose these Qual factors to count with?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#data-dictionary",
    "title": "\n Coffee Flavours",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#research-question",
    "title": "\n Coffee Flavours",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNoteBreadcrumb 1\n\n\n\nAmong the country_of_origin with the 5 highest average total_cup_points, how do the average ratings vary in ranks on the other coffee parameters?\nWhy this somewhat long-winded question? Why all this averagestuff??\nWhy did I choose country_of_origin?Are there any other options?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#analysetransform-the-data",
    "title": "\n Coffee Flavours",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBreadcrumb 3\n\n\n\nWe have too much coffee here! We need to compress this data!\nWhat??? Why? How? Where???\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBreadcrumb 4\n\n\n\nWhere did all that coffee go??? Why are there only 5 rows in the data? Why the names of the columns take on a surname, ’_mean`??\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBreadcrumb 5\n\n\n\nWhat just happened? How did we convert those mean numbers to ranks?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#plot-the-data",
    "title": "\n Coffee Flavours",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/300-Coffee/index.html#discussion",
    "title": "\n Coffee Flavours",
    "section": "Discussion",
    "text": "Discussion\nComplete the Data Dictionary. Select and Transform the variables as shown. Create the graphs shown below and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"fa:coffee\"></iconify-icon> Coffee Flavours"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html",
    "title": "\n Seattle Bicycle Zones",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#setting-up-r-packages",
    "title": "\n Seattle Bicycle Zones",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#introduction",
    "title": "\n Seattle Bicycle Zones",
    "section": "Introduction",
    "text": "Introduction\nNine types of Seaweed were rated on different parameters and charted as shown below.\n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#read-the-data",
    "title": "\n Seattle Bicycle Zones",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download Seaweed Nutrition data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#inspect-the-data",
    "title": "\n Seattle Bicycle Zones",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\nRows: 10\nColumns: 18\n$ `common name`     &lt;chr&gt; \"RDA\", \"Norwegian Kelp\", \"Oarweed\", \"Thongweed\", \"Wa…\n$ `sci-name`        &lt;chr&gt; NA, \"-Ascophyllum nodosum\", \"-Laminaria digitata\", \"…\n$ `total fats`      &lt;chr&gt; NA, \"0.6\", \"-\", \"-\", \"0.6\", \"0.3\", \"-\", \"0.2\", \"-\", …\n$ `saturated fat`   &lt;chr&gt; NA, \"0.2\", \"-\", \"-\", \"0.1\", \"0.1\", \"-\", \"0\", \"-\", \"-\"\n$ cholesterol       &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"-\"\n$ protein           &lt;chr&gt; NA, \"1.7\", \"-\", \"-\", \"3\", \"5.8\", \"-\", \"1.5\", \"-\", \"-\"\n$ `Total fiber`     &lt;dbl&gt; NA, 8.8, 6.2, 9.8, 3.4, 3.8, 5.4, 1.3, 3.8, 4.9\n$ `Soluble fiber`   &lt;chr&gt; NA, \"7.5\", \"5.4\", \"7.7\", \"2.9\", \"3\", \"3\", \"-\", \"2.1\"…\n$ `Insoluble fiber` &lt;chr&gt; NA, \"1.3\", \"0.8\", \"2.1\", \"0.5\", \"1\", \"2.3\", \"-\", \"1.…\n$ Carbohydrates     &lt;dbl&gt; NA, 13.1, 9.9, 15.0, 4.6, 5.4, 10.6, 12.0, 4.1, 7.8\n$ Calcium           &lt;dbl&gt; NA, 575.0, 364.7, 30.0, 112.3, 34.2, 148.8, 373.8, 3…\n$ Potassium         &lt;dbl&gt; NA, 765.0, 2013.2, 1351.4, 62.4, 302.2, 1169.6, 827.…\n$ Magnesium         &lt;dbl&gt; NA, 225.0, 403.5, 90.1, 78.7, 108.3, 97.6, 573.8, 46…\n$ Sodium            &lt;dbl&gt; NA, 1173.8, 624.6, 600.6, 448.7, 119.7, 255.2, 1572.…\n$ Copper            &lt;dbl&gt; NA, 0.8, 0.3, 0.1, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1\n$ Iron              &lt;dbl&gt; NA, 14.9, 45.6, 5.0, 3.9, 5.2, 12.8, 6.6, 15.3, 22.2\n$ Iodine            &lt;dbl&gt; NA, 18.2, 70.0, 10.7, 3.9, 1.3, 10.2, 6.1, 1.6, 97.9\n$ Zinc              &lt;chr&gt; NA, \"-\", \"1.6\", \"1.7\", \"0.3\", \"0.7\", \"0.3\", \"-\", \"0.…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#data-dictionary",
    "title": "\n Seattle Bicycle Zones",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#research-question",
    "title": "\n Seattle Bicycle Zones",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#analysetransform-the-data",
    "title": "\n Seattle Bicycle Zones",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#plot-the-data",
    "title": "\n Seattle Bicycle Zones",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/530-SeattleBicycles/index.html#tasks-and-discussion",
    "title": "\n Seattle Bicycle Zones",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.\nBased on the diagram, discuss which one an elderly person might try if they are deficient in calcium. If you were trying to avoid carbs, which seaweed sushi would you try?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html",
    "title": "\n School Scores",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(GGally)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#setting-up-r-packages",
    "title": "\n School Scores",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(GGally)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#introduction",
    "title": "\n School Scores",
    "section": "Introduction",
    "text": "Introduction\nThis dataset pertains to scores obtained by students in diverse subjects. Family Income is also part of this dataset.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#read-the-data",
    "title": "\n School Scores",
    "section": "Read the Data",
    "text": "Read the Data\n Download the School Scores Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#inspect-and-clean-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#inspect-and-clean-the-data",
    "title": "\n School Scores",
    "section": "Inspect and Clean the Data",
    "text": "Inspect and Clean the Data\nHint: Use the janitor package here to clean up the variable names. Try to use the big_camel case name format for variables.\n\n\nRows: 577\nColumns: 99\n$ Year                                              &lt;dbl&gt; 2005, 2005, 2005, 20…\n$ StateCode                                         &lt;chr&gt; \"AL\", \"AK\", \"AZ\", \"A…\n$ StateName                                         &lt;chr&gt; \"Alabama\", \"Alaska\",…\n$ TotalMath                                         &lt;dbl&gt; 559, 519, 530, 552, …\n$ TotalTestTakers                                   &lt;dbl&gt; 3985, 3996, 18184, 1…\n$ TotalVerbal                                       &lt;dbl&gt; 567, 523, 526, 563, …\n$ AcademicSubjectsArtsMusicAverageGpa               &lt;dbl&gt; 3.92, 3.76, 3.85, 3.…\n$ AcademicSubjectsArtsMusicAverageYears             &lt;dbl&gt; 2.2, 1.9, 2.1, 2.2, …\n$ AcademicSubjectsEnglishAverageGpa                 &lt;dbl&gt; 3.53, 3.35, 3.45, 3.…\n$ AcademicSubjectsEnglishAverageYears               &lt;dbl&gt; 3.9, 3.9, 3.9, 4.0, …\n$ AcademicSubjectsForeignLanguagesAverageGpa        &lt;dbl&gt; 3.54, 3.34, 3.41, 3.…\n$ AcademicSubjectsForeignLanguagesAverageYears      &lt;dbl&gt; 2.6, 2.1, 2.6, 2.6, …\n$ AcademicSubjectsMathematicsAverageGpa             &lt;dbl&gt; 3.41, 3.06, 3.25, 3.…\n$ AcademicSubjectsMathematicsAverageYears           &lt;dbl&gt; 4.0, 3.5, 3.9, 4.1, …\n$ AcademicSubjectsNaturalSciencesAverageGpa         &lt;dbl&gt; 3.52, 3.25, 3.43, 3.…\n$ AcademicSubjectsNaturalSciencesAverageYears       &lt;dbl&gt; 3.9, 3.2, 3.4, 3.7, …\n$ AcademicSubjectsSocialSciencesHistoryAverageGpa   &lt;dbl&gt; 3.59, 3.39, 3.55, 3.…\n$ AcademicSubjectsSocialSciencesHistoryAverageYears &lt;dbl&gt; 3.9, 3.4, 3.3, 3.6, …\n$ FamilyIncomeBetween20_40KMath                     &lt;dbl&gt; 513, 492, 498, 513, …\n$ FamilyIncomeBetween20_40KTestTakers               &lt;dbl&gt; 324, 401, 2121, 180,…\n$ FamilyIncomeBetween20_40KVerbal                   &lt;dbl&gt; 527, 500, 495, 526, …\n$ FamilyIncomeBetween40_60KMath                     &lt;dbl&gt; 539, 517, 520, 543, …\n$ FamilyIncomeBetween40_60KTestTakers               &lt;dbl&gt; 442, 539, 2270, 245,…\n$ FamilyIncomeBetween40_60KVerbal                   &lt;dbl&gt; 551, 522, 518, 555, …\n$ FamilyIncomeBetween60_80KMath                     &lt;dbl&gt; 550, 513, 524, 553, …\n$ FamilyIncomeBetween60_80KTestTakers               &lt;dbl&gt; 473, 603, 2372, 227,…\n$ FamilyIncomeBetween60_80KVerbal                   &lt;dbl&gt; 564, 519, 523, 570, …\n$ FamilyIncomeBetween80_100KMath                    &lt;dbl&gt; 566, 528, 534, 570, …\n$ FamilyIncomeBetween80_100KTestTakers              &lt;dbl&gt; 475, 444, 1866, 147,…\n$ FamilyIncomeBetween80_100KVerbal                  &lt;dbl&gt; 577, 534, 533, 580, …\n$ FamilyIncomeLessThan20KMath                       &lt;dbl&gt; 462, 464, 485, 489, …\n$ FamilyIncomeLessThan20KTestTakers                 &lt;dbl&gt; 175, 191, 891, 107, …\n$ FamilyIncomeLessThan20KVerbal                     &lt;dbl&gt; 474, 467, 474, 486, …\n$ FamilyIncomeMoreThan100KMath                      &lt;dbl&gt; 588, 541, 554, 572, …\n$ FamilyIncomeMoreThan100KTestTakers                &lt;dbl&gt; 980, 540, 3083, 314,…\n$ FamilyIncomeMoreThan100KVerbal                    &lt;dbl&gt; 590, 544, 546, 589, …\n$ GpaAMinusMath                                     &lt;dbl&gt; 569, 544, 541, 559, …\n$ GpaAMinusTestTakers                               &lt;dbl&gt; 724, 673, 3334, 298,…\n$ GpaAMinusVerbal                                   &lt;dbl&gt; 575, 546, 535, 572, …\n$ GpaAPlusMath                                      &lt;dbl&gt; 622, 600, 605, 629, …\n$ GpaAPlusTestTakers                                &lt;dbl&gt; 563, 173, 1684, 273,…\n$ GpaAPlusVerbal                                    &lt;dbl&gt; 623, 604, 593, 639, …\n$ GpaAMath                                          &lt;dbl&gt; 600, 580, 571, 579, …\n$ GpaATestTakers                                    &lt;dbl&gt; 1032, 671, 3854, 457…\n$ GpaAVerbal                                        &lt;dbl&gt; 608, 578, 563, 583, …\n$ GpaBMath                                          &lt;dbl&gt; 514, 492, 498, 492, …\n$ GpaBTestTakers                                    &lt;dbl&gt; 1253, 1622, 7193, 43…\n$ GpaBVerbal                                        &lt;dbl&gt; 525, 499, 499, 511, …\n$ GpaCMath                                          &lt;dbl&gt; 436, 466, 458, 419, …\n$ GpaCTestTakers                                    &lt;dbl&gt; 188, 418, 1184, 57, …\n$ GpaCVerbal                                        &lt;dbl&gt; 451, 472, 464, 436, …\n$ GpaDOrLowerMath                                   &lt;dbl&gt; 0, 424, 439, 0, 419,…\n$ GpaDOrLowerTestTakers                             &lt;dbl&gt; 0, 12, 16, 0, 240, 1…\n$ GpaDOrLowerVerbal                                 &lt;dbl&gt; 0, 466, 435, 0, 408,…\n$ GpaNoResponseMath                                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0,…\n$ GpaNoResponseTestTakers                           &lt;dbl&gt; 225, 427, 919, 78, 1…\n$ GpaNoResponseVerbal                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0,…\n$ GenderFemaleMath                                  &lt;dbl&gt; 538, 505, 513, 536, …\n$ GenderFemaleTestTakers                            &lt;dbl&gt; 2072, 2161, 9806, 85…\n$ GenderFemaleVerbal                                &lt;dbl&gt; 561, 521, 522, 558, …\n$ GenderMaleMath                                    &lt;dbl&gt; 582, 535, 549, 570, …\n$ GenderMaleTestTakers                              &lt;dbl&gt; 1913, 1835, 8378, 74…\n$ GenderMaleVerbal                                  &lt;dbl&gt; 574, 526, 531, 570, …\n$ ScoreRangesBetween200To300MathFemales             &lt;dbl&gt; 22, 30, 119, 12, 297…\n$ ScoreRangesBetween200To300MathMales               &lt;dbl&gt; 10, 20, 72, 7, 1453,…\n$ ScoreRangesBetween200To300MathTotal               &lt;dbl&gt; 32, 50, 191, 19, 443…\n$ ScoreRangesBetween200To300VerbalFemales           &lt;dbl&gt; 14, 26, 115, 9, 3382…\n$ ScoreRangesBetween200To300VerbalMales             &lt;dbl&gt; 17, 26, 86, 3, 2433,…\n$ ScoreRangesBetween200To300VerbalTotal             &lt;dbl&gt; 31, 52, 201, 12, 581…\n$ ScoreRangesBetween300To400MathFemales             &lt;dbl&gt; 173, 233, 881, 68, 1…\n$ ScoreRangesBetween300To400MathMales               &lt;dbl&gt; 93, 153, 450, 31, 71…\n$ ScoreRangesBetween300To400MathTotal               &lt;dbl&gt; 266, 386, 1331, 99, …\n$ ScoreRangesBetween300To400VerbalFemales           &lt;dbl&gt; 123, 218, 739, 46, 1…\n$ ScoreRangesBetween300To400VerbalMales             &lt;dbl&gt; 84, 171, 613, 42, 10…\n$ ScoreRangesBetween300To400VerbalTotal             &lt;dbl&gt; 207, 389, 1352, 88, …\n$ ScoreRangesBetween400To500MathFemales             &lt;dbl&gt; 514, 696, 3215, 210,…\n$ ScoreRangesBetween400To500MathMales               &lt;dbl&gt; 293, 485, 1948, 137,…\n$ ScoreRangesBetween400To500MathTotal               &lt;dbl&gt; 807, 1181, 5163, 347…\n$ ScoreRangesBetween400To500VerbalFemales           &lt;dbl&gt; 430, 656, 3048, 183,…\n$ ScoreRangesBetween400To500VerbalMales             &lt;dbl&gt; 332, 552, 2398, 141,…\n$ ScoreRangesBetween400To500VerbalTotal             &lt;dbl&gt; 762, 1208, 5446, 324…\n$ ScoreRangesBetween500To600MathFemales             &lt;dbl&gt; 722, 813, 3576, 316,…\n$ ScoreRangesBetween500To600MathMales               &lt;dbl&gt; 614, 616, 3152, 244,…\n$ ScoreRangesBetween500To600MathTotal               &lt;dbl&gt; 1336, 1429, 6728, 56…\n$ ScoreRangesBetween500To600VerbalFemales           &lt;dbl&gt; 690, 729, 3661, 302,…\n$ ScoreRangesBetween500To600VerbalMales             &lt;dbl&gt; 617, 596, 3101, 236,…\n$ ScoreRangesBetween500To600VerbalTotal             &lt;dbl&gt; 1307, 1325, 6762, 53…\n$ ScoreRangesBetween600To700MathFemales             &lt;dbl&gt; 485, 342, 1688, 204,…\n$ ScoreRangesBetween600To700MathMales               &lt;dbl&gt; 611, 445, 2126, 239,…\n$ ScoreRangesBetween600To700MathTotal               &lt;dbl&gt; 1096, 787, 3814, 443…\n$ ScoreRangesBetween600To700VerbalFemales           &lt;dbl&gt; 596, 423, 1831, 242,…\n$ ScoreRangesBetween600To700VerbalMales             &lt;dbl&gt; 613, 375, 1679, 226,…\n$ ScoreRangesBetween600To700VerbalTotal             &lt;dbl&gt; 1209, 798, 3510, 468…\n$ ScoreRangesBetween700To800MathFemales             &lt;dbl&gt; 156, 47, 327, 49, 54…\n$ ScoreRangesBetween700To800MathMales               &lt;dbl&gt; 292, 116, 630, 83, 8…\n$ ScoreRangesBetween700To800MathTotal               &lt;dbl&gt; 448, 163, 957, 132, …\n$ ScoreRangesBetween700To800VerbalFemales           &lt;dbl&gt; 219, 109, 412, 77, 5…\n$ ScoreRangesBetween700To800VerbalMales             &lt;dbl&gt; 250, 115, 501, 93, 4…\n$ ScoreRangesBetween700To800VerbalTotal             &lt;dbl&gt; 469, 224, 913, 170, …",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#data-dictionary",
    "title": "\n School Scores",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#analyse-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#analyse-the-data",
    "title": "\n School Scores",
    "section": "Analyse the Data",
    "text": "Analyse the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#plot-the-data-all-subjects",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#plot-the-data-all-subjects",
    "title": "\n School Scores",
    "section": "Plot the Data: All Subjects",
    "text": "Plot the Data: All Subjects",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#plot-the-data-maths-vs-family-income",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#plot-the-data-maths-vs-family-income",
    "title": "\n School Scores",
    "section": "Plot the Data: Maths vs Family Income",
    "text": "Plot the Data: Maths vs Family Income",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/60-SchoolScores/index.html#task-and-discussion",
    "title": "\n School Scores",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Select and Transform the variables as shown. Create the graphs shown below and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart #1?\nAnd Chart #2\nWrite a 2-line story based on each of the graphs, describing your inference/surprise.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"ic:twotone-school\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> School Scores"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html",
    "title": "\n Heptathlon",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(correlation)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#setting-up-r-packages",
    "title": "\n Heptathlon",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(correlation)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#introduction",
    "title": "\n Heptathlon",
    "section": "Introduction",
    "text": "Introduction\nThis is a dataset pertaining to scores of multiple athletes in the 7 events that make up the Heptathlon, modified for ease of analysis and plotting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#data",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#data",
    "title": "\n Heptathlon",
    "section": "Data",
    "text": "Data\n\nlibrary(HSAUR)\nheptathlon",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#download-the-modified-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#download-the-modified-data",
    "title": "\n Heptathlon",
    "section": "Download the Modified data",
    "text": "Download the Modified data\nNot Applicable!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#data-dictionary",
    "title": "\n Heptathlon",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#analyse-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#analyse-the-data",
    "title": "\n Heptathlon",
    "section": "Analyse the Data",
    "text": "Analyse the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#plot-the-data",
    "title": "\n Heptathlon",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/40-Heptathlon/index.html#task-and-discussion",
    "title": "\n Heptathlon",
    "section": "Task and Discussion",
    "text": "Task and Discussion\nComplete the Data Dictionary. Create the graph shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhich events in the 7-event heptathlon are most highly correlated with scores in hurdles?\nIf an athlete was a record holder in both high jump and hurdles, what would be your opinion about them? Justify based on the graph!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"icon-park-outline:sport\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Heptathlon"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html",
    "title": "\n Antarctic Sea ice",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#setting-up-r-packages",
    "title": "\n Antarctic Sea ice",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 20, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#introduction",
    "title": "\n Antarctic Sea ice",
    "section": "Introduction",
    "text": "Introduction\nThe extent of Antarctic Sea Ice over time is monitored by the National Snow and Ice Data Center https://nsidc.org/.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#read-the-data",
    "title": "\n Antarctic Sea ice",
    "section": "Read the Data",
    "text": "Read the Data\n Download the Sea Ice data \n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R. NOTE: The sheet that contains our data of interest is titled “SH-Daily-Extent”.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#inspect-the-data",
    "title": "\n Antarctic Sea ice",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\n\n  \n\n\n\nAppreciate the structure of this data. You may even want to open it in Excel for a closer look. List any imperfections in your Data Dictionary. Why do these matter now? Why might they not have mattered earlier, up to now?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#data-dictionary",
    "title": "\n Antarctic Sea ice",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#analysetransform-the-data",
    "title": "\n Antarctic Sea ice",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\nTry to figure what may be needed, based on the imperfections noted above, what you may attempt to clean the data. Refer to your “list of imperfections” in the data.\nThen look at the code below and execute line by line to get an idea.\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```\n\n\nShow the Codeice %&gt;%\n  # Select columns\n  # Rename some while selecting !!\n  select(\"month\" = ...1, \"day\" = ...2, c(4:49)) %&gt;%\n  # Fill the month column! Yes!!\n  tidyr::fill(month) %&gt;%\n  # Make Wide Data into Long\n  pivot_longer(\n    cols = -c(month, day),\n    names_to = \"series\",\n    values_to = \"values\"\n  ) %&gt;%\n  # Regular Munging\n  mutate(\n    series = as.integer(series),\n    month = factor(month,\n      levels = month.name,\n      labels = month.name,\n      ordered = TRUE\n    ),\n    # Note munging for date!!\n    # Using the lubridate package, part of tidyverse\n    date = lubridate::make_date(\n      year = series,\n      month = month,\n      day = day\n    )\n  ) -&gt; ice_prepared\n\nice_prepared",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#research-question",
    "title": "\n Antarctic Sea ice",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in! Look first at the graph!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#plot-the-data",
    "title": "\n Antarctic Sea ice",
    "section": "Plot the Data",
    "text": "Plot the Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/400-AntarcticSeaIce/index.html#tasks-and-discussion",
    "title": "\n Antarctic Sea ice",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWhat might the red points represent?\nWhat is perhaps a befuddling aspect of this graph until you…Ohhh!!!!!!\nDraw a sketch of a similar chart for ice extents in the Arctic.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Case Studies",
      "<iconify-icon icon=\"iconoir:sea-waves\"></iconify-icon> <iconify-icon icon=\"openmoji:floating-ice\"></iconify-icon>  Antarctic Sea ice"
    ]
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html",
    "title": "Grain Transportation Cartels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#setting-up-r-packages",
    "title": "Grain Transportation Cartels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\n\n\n\nShow the Code# https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Roboto Condensed\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      panel.grid.minor = element_blank(), # strip minor gridlines\n      text = element_text(family = font),\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 16, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        # vjust = 2                #raise slightly\n        margin = margin(0, 0, 10, 0)\n      ),\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0,\n        margin = margin(2, 0, 5, 0)\n      ),\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 8, # font size\n        hjust = 1\n      ), # right align\n\n      axis.title = element_text( # axis titles\n        family = font, # font family\n        size = 10 # font size\n      ),\n      axis.text = element_text( # axis text\n        family = font, # axis family\n        size = 8\n      ) # font size\n    )\n}\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#introduction",
    "title": "Grain Transportation Cartels",
    "section": "Introduction",
    "text": "Introduction\nFrom: Robert H. Porter (1983). A Study of Cartel Stability: The Joint Executive Committee, 1880-1886. The Bell Journal of Economics, Vol. 14, No. 2 (Autumn, 1983), pp. 301-314:\nThe Joint Executive Committee (JEC) was a cartel (of railroad firms) which controlled eastbound freight shipments from Chicago to the Atlantic seaboard in the 1880’s. While different railroad firms in the JEC shipped grain to different port cities (for example, Baltimore and New York), most of the wheat handled by the cartel was subsequently exported overseas, and the rates charged by different firms (were) adjusted to compensate for differences in ocean shipping rates.\nPrices, rather than quantity, has typically been thought to be the strategic variable of firms in the rail-freight industry. Total demand was quite variable, and so the actual market share of any particular railroad firm would depend on both the prices charged by all the firms as well as unpredictable (random) forces. Price wars were not random, but precipitated by periods of slackened demand, which were presumably unpredictable, at least to some extent.\nOn the other hand, the predictable fluctuations in demand that resulted from the annual opening and closing of the Great Lakes (Superior / Michigan / Huron / Ontario / Erie ) to shipping (because they were frozen in winter), which determined the degree of outside competition, did not disrupt industry conduct. Rather, rates adjusted systematically with the lake navigation season.\nThis dataset is available on Vincent Arel-Bundock’s dataset repository, and is part of the R package AER (Applied Econometrics in R)."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#read-the-data",
    "title": "Grain Transportation Cartels",
    "section": "Read the Data",
    "text": "Read the Data\n\ncartelstability &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/AER/CartelStability.csv\")\ncartelstability\n\n\n  \n\n\nglimpse(cartelstability)\n\nRows: 328\nColumns: 6\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ price    &lt;dbl&gt; 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.40, 0.35, 0…\n$ cartel   &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\"…\n$ quantity &lt;dbl&gt; 13632, 20035, 16319, 12603, 23079, 19652, 16211, 22914, 23710…\n$ season   &lt;chr&gt; \"Jan  1 - Jan 28\", \"Jan  1 - Jan 28\", \"Jan  1 - Jan 28\", \"Jan…\n$ ice      &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\"…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#data-dictionary",
    "title": "Grain Transportation Cartels",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#research-question",
    "title": "Grain Transportation Cartels",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nHow do prices for per-tonne grain transport vary based on whether the cartel is working or not? Does this depend upon whether it is summer time or winter time? Why?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#inspectanalysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#inspectanalysetransform-the-data",
    "title": "Grain Transportation Cartels",
    "section": "Inspect/Analyse/Transform the Data",
    "text": "Inspect/Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n# Rename Variables if needed\n# Change data to factors etc.\n# Set up Counts, histograms etc\n```\n\n\n\n\n  \n\n\n\nSome summarizing…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#plot-the-data",
    "title": "Grain Transportation Cartels",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#task-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/120-GrainCartels/index.html#task-and-discussion",
    "title": "Grain Transportation Cartels",
    "section": "Task and Discussion",
    "text": "Task and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat pre-processing of the data was required to create the chart?\nExplain what happens when it is stated “cartel is working” and “cartel is not working”.\nHow do prices for per-tonne grain transport vary based on whether the cartel is working or not? Does this depend upon whether it is summer time or winter time? Why?\nIs the cartel beneficial for customers of the JEC? What would be their behaviour based on whether the cartel was operational or not?"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html",
    "title": "\n Satisfaction with AI Tools",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#setting-up-r-packages",
    "title": "\n Satisfaction with AI Tools",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(ggformula)\nlibrary(ggbump)\nlibrary(ggprism)"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#introduction",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#introduction",
    "title": "\n Satisfaction with AI Tools",
    "section": "Introduction",
    "text": "Introduction\nNine types of Seaweed were rated on different parameters and charted as shown below.\n\n\n\n\n\n\nNoteExcel Data\n\n\n\nThe data is an excel sheet. Inspect it first in Excel and decide which sheet you need, and which part of the data you need. There are multiple sheets! Then use readxl::read_xlsx(..) to read it into R."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#read-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#read-the-data",
    "title": "\n Satisfaction with AI Tools",
    "section": "Read the Data",
    "text": "Read the Data\n\n\n Download Seaweed Nutrition data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#inspect-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#inspect-the-data",
    "title": "\n Satisfaction with AI Tools",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n\nRows: 10\nColumns: 18\n$ `common name`     &lt;chr&gt; \"RDA\", \"Norwegian Kelp\", \"Oarweed\", \"Thongweed\", \"Wa…\n$ `sci-name`        &lt;chr&gt; NA, \"-Ascophyllum nodosum\", \"-Laminaria digitata\", \"…\n$ `total fats`      &lt;chr&gt; NA, \"0.6\", \"-\", \"-\", \"0.6\", \"0.3\", \"-\", \"0.2\", \"-\", …\n$ `saturated fat`   &lt;chr&gt; NA, \"0.2\", \"-\", \"-\", \"0.1\", \"0.1\", \"-\", \"0\", \"-\", \"-\"\n$ cholesterol       &lt;chr&gt; NA, \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"-\"\n$ protein           &lt;chr&gt; NA, \"1.7\", \"-\", \"-\", \"3\", \"5.8\", \"-\", \"1.5\", \"-\", \"-\"\n$ `Total fiber`     &lt;dbl&gt; NA, 8.8, 6.2, 9.8, 3.4, 3.8, 5.4, 1.3, 3.8, 4.9\n$ `Soluble fiber`   &lt;chr&gt; NA, \"7.5\", \"5.4\", \"7.7\", \"2.9\", \"3\", \"3\", \"-\", \"2.1\"…\n$ `Insoluble fiber` &lt;chr&gt; NA, \"1.3\", \"0.8\", \"2.1\", \"0.5\", \"1\", \"2.3\", \"-\", \"1.…\n$ Carbohydrates     &lt;dbl&gt; NA, 13.1, 9.9, 15.0, 4.6, 5.4, 10.6, 12.0, 4.1, 7.8\n$ Calcium           &lt;dbl&gt; NA, 575.0, 364.7, 30.0, 112.3, 34.2, 148.8, 373.8, 3…\n$ Potassium         &lt;dbl&gt; NA, 765.0, 2013.2, 1351.4, 62.4, 302.2, 1169.6, 827.…\n$ Magnesium         &lt;dbl&gt; NA, 225.0, 403.5, 90.1, 78.7, 108.3, 97.6, 573.8, 46…\n$ Sodium            &lt;dbl&gt; NA, 1173.8, 624.6, 600.6, 448.7, 119.7, 255.2, 1572.…\n$ Copper            &lt;dbl&gt; NA, 0.8, 0.3, 0.1, 0.2, 0.1, 0.4, 0.1, 0.3, 0.1\n$ Iron              &lt;dbl&gt; NA, 14.9, 45.6, 5.0, 3.9, 5.2, 12.8, 6.6, 15.3, 22.2\n$ Iodine            &lt;dbl&gt; NA, 18.2, 70.0, 10.7, 3.9, 1.3, 10.2, 6.1, 1.6, 97.9\n$ Zinc              &lt;chr&gt; NA, \"-\", \"1.6\", \"1.7\", \"0.3\", \"0.7\", \"0.3\", \"-\", \"0.…"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#data-dictionary",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#data-dictionary",
    "title": "\n Satisfaction with AI Tools",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\nWrite in.\n\n\n\n\n\n\n\n\nNoteObservations\n\n\n\nWrite in."
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#research-question",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#research-question",
    "title": "\n Satisfaction with AI Tools",
    "section": "Research Question",
    "text": "Research Question\n\n\n\n\n\n\nNote\n\n\n\nWrite in!"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#analysetransform-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#analysetransform-the-data",
    "title": "\n Satisfaction with AI Tools",
    "section": "Analyse/Transform the Data",
    "text": "Analyse/Transform the Data\n\n```{r}\n#| label: data-preprocessing\n#\n# Write in your code here\n# to prepare this data as shown below\n# to generate the plot that follows\n```"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#plot-the-data",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#plot-the-data",
    "title": "\n Satisfaction with AI Tools",
    "section": "Plot the Data",
    "text": "Plot the Data"
  },
  {
    "objectID": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#tasks-and-discussion",
    "href": "content/courses/Analytics/CaseStudies/Modules/500-SatisfactionWithAI/index.html#tasks-and-discussion",
    "title": "\n Satisfaction with AI Tools",
    "section": "Tasks and Discussion",
    "text": "Tasks and Discussion\n\nComplete the Data Dictionary.\nSelect and Transform the variables as shown.\nCreate the graphs shown and discuss the following questions:\n\nIdentify the type of charts\nIdentify the variables used for various geometrical aspects (x, y, fill…). Name the variables appropriately.\nWhat research activity might have been carried out to obtain the data graphed here? Provide some details.\nWhat might have been the Hypothesis/Research Question to which the response was Chart?\nWrite a 2-line story based on the chart, describing your inference/surprise.\nBased on the diagram, discuss which one an elderly person might try if they are deficient in calcium. If you were trying to avoid carbs, which seaweed sushi would you try?"
  },
  {
    "objectID": "content/courses/Analytics/Tools/listing.html",
    "href": "content/courses/Analytics/Tools/listing.html",
    "title": "Tools and Software",
    "section": "",
    "text": "Title\n\n\n\nReading Time\n\n\n\n\n\n\n\n\n\n\n\n Introduction to R and RStudio\n\n\n10 min\n\n\n\n\n\n\nNo matching items\n Back to top",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Tools"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#abstract",
    "href": "content/courses/Analytics/listing.html#abstract",
    "title": "Data Analytics for Managers and Creators",
    "section": "Abstract",
    "text": "Abstract\nThis Course takes Business Practitioners and Design Peasants on a journey of Data Analytics: using data to derive insights, make predictions, and decide on plans of action that can be communicated and actualized in a Design and Business context.\n\n\n“Business analytics, or simply analytics, is the use of data, information technology, statistical analysis, quantitative methods, and mathematical or computer-based models to help managers gain improved insight about their business operations and make better, fact-based decisions. Business analytics is”a process of transforming data into actions through analysis and insights in the context of organizational decision making and problem solving”\n— Libertore and Luo, 2010\n\n\n The Course starts with Descriptive Analytics: Datasets from various domains of business, design, and scientific activity are introduced. The datasets are motivated from the point of view of the types of information they contain: students will relate the Data Variables (Qualitative and Quantitative) to various types of Data/Information Visualizations.\nStatistical Concepts such as Sampling, Hypothesis Tests, Simulation / Modelling, and Uncertainty will be introduced.\nPredictive Analytics will take us into looking at Data and training standard ML algorithms to make predictions with new Data. Regression, Clustering, and Classification will be covered.\nPrescriptive Analytics will deal with coming to terms with the uncertainty in Predictions, and using tools such as both ML, Linear/non-Linear Programming, and Decision-Making to make Business Decisions, with an assessment of the Risks involved.\nThe Course will include in a full Data Analytics Workflow that includes Data Gathering and Cleaning, Descriptive and Predictive Analytics, Prescriptive Analytics and Decision Making, and Communication resulting in a publication-worthy documents (HTML / PDF/ Word) and/or on your own website.\nA diagram from a very popular textbook by Ismay and Kim may be relevant here:",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#what-you-will-learn",
    "href": "content/courses/Analytics/listing.html#what-you-will-learn",
    "title": "Data Analytics for Managers and Creators",
    "section": "What you will learn",
    "text": "What you will learn\n\nData Basics: What does data look like and why should we care?\nUnderstand the R language, and appreciate how close it is t plain English, for the most part\nRapidly and intuitively creating Graphs and Data Visualizations using geometric metaphors to explore data for insights\nUse Statistical Tests, Procedures, Models, and Simulations and to answer Business and Design Questions\nUsing ML algorithms such Regression, Classification, and Clustering to develop Business Insights\nUse Linear Programming to make Business Decisions\nCreate crisp and readable Reports that can be shared in a Business Context",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#references",
    "href": "content/courses/Analytics/listing.html#references",
    "title": "Data Analytics for Managers and Creators",
    "section": "References",
    "text": "References\n\nVisualization and R language\n\nHadley Wickham, Mine Cetinkaya-Rundel, and Garett Grolemund. R for Data Science (2e). https://r4ds.hadley.nz. The most important reference for data visualization and analysis in R. Available free online.\nRobert Kabacoff. Modern Data Visualization with R. https://rkabacoff.github.io/datavis/. Available free online.\nJack Dougherty and Ilya Ilyankou, Hands-On Data Visualization: Interactive Storytelling from Spreadsheets to Code, https://handsondataviz.org/. Available free online.\nClaus O. Wilke, Fundamentals of Data Visualization, https://clauswilke.com/dataviz/. Available free online.\nJonathan Schwabish, Better Data Visualizations: A Guide for Scholars, Researchers, and Wonks, Columbia University Press, 2021.\nAlberto Cairo, The Functional Art: An introduction to information graphics and visualization, New Riders. 2013. ISBN-9780133041361. 1.. Cole Nussbaumer Knaflic, Storytelling With Data: A Data Visualization Guide for Business Professionals, Wiley 2015. ISBN-9781119002253.\n\n\n\nAnalytics\n\nJames R Evans, Business Analytics: Methods, Models, and Decisions, Pearson Education, 2021.\nJudd, C.M., McClelland, G.H., & Ryan, C.S. (2017). Data Analysis: A Model Comparison Approach To Regression, ANOVA, and Beyond, Third Edition (3rd ed.). Routledge. https://doi.org/10.4324/9781315744131\nThomas Maydon, The 4 Types of Data Analytics, https://www.kdnuggets.com/2017/07/4-types-data-analytics.html\nDimitris Bertsimas, Robert Freund, Data, Models, and Decisions: the Fundamentals of Management Science, Dynamic Ideas Press, 2004.\nCliff T. Ragsdale, Spreadsheet Modeling & Decision Analysis: A Practical Introduction to Management Science, South Western, Cengage Learning, Mason, OH, 2012.\nKeith McNulty. Handbook of Regression Modeling in People Analytics: With Examples in R, Python and Julia https://peopleanalytics-regression-book.org. Available free online.\n\n\n\nStatistics\n\nMine Cetinkaya-Rundel, Johanna Hardin. Introduction to Modern Statistics. https://openintro-ims2.netlify.app. Available free online.\nDaniel T. Kaplan. Statistical Models (second edition). https://dtkaplan.github.io/SM2-bookdown/. Available free online.\nDaniel T. Kaplan, Compact Introduction to Classical Inference, 2020. https://dtkaplan.github.io/CompactInference/. Available free online.\nDaniel T. Kaplan and Frank Shaw, Statistical Modeling: Computational Technique. https://www.mosaic-web.org/go/SM2-technique/. Available free online.\nJonas Kristoffer Lindeløv. Common statistical tests are linear models (or: how to teach stats). https://lindeloev.github.io/tests-as-linear/. Available free online.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#pedagogical-notes",
    "href": "content/courses/Analytics/listing.html#pedagogical-notes",
    "title": "Data Analytics for Managers and Creators",
    "section": "Pedagogical Notes",
    "text": "Pedagogical Notes\nWhy this course is what it is, and why it does what it does! Only if you are planning to be an educator yourself!!\n\nPRIMM\nThe method followed will be based on PRIMM:\n\nPREDICT: Inspect the code and guess at what the code might do, write predictions\nRUN: the code provided and check what happens\nINFER: what the parameters of the code do and write comments to explain. What bells and whistles can you see?\nMODIFY: the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\nMAKE: take an idea/concept of your own, and graph it.\n\n\n\n\nFrom https://primmportal.com, used without permission\n\n\nSo in this course, wherever you see “YOUR TURN”, please respond with questions of the data, explanations, more questions and if you are already confident, code chunks to create new calculations and graphs.\n\n\nAnd why teach R in this way?\nBecause we all know two things:\n\nPretty decent English\n\\(y = mx + c\\)\n\nLet us hear from Amelia McNamara:",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#our-tools",
    "href": "content/courses/Analytics/listing.html#our-tools",
    "title": "Data Analytics for Managers and Creators",
    "section": "Our Tools",
    "text": "Our Tools\nThis is eventually meant to be a three many-in-one course, based on the following free and open source tools:\n\nR https://cran.r-project.org/ and RStudio https://posit.co/\nR is a freely available language and environment for statistical computing and graphics which provides a wide variety of statistical and graphical techniques: linear and nonlinear modelling, statistical tests, time series analysis, classification, clustering,etc. RStudio is an integrated development environment (IDE) for R and Python.\nOrange Data Mining https://orangedatamining.com/\nOrange is also a FOSS visual point-and-click software for Data Mining and ML, developed at the University of Slovenia, in Ljubljana, Slovenia.\n\n\n\n\nRadiant – Business analytics using R and Shiny https://radiant-rstats.github.io/docs/index.html\nRadiant is a FOSS platform-independent browser-based interface for business analytics in R, developed at the University of San Diego. The application is based on the Shiny package and can be run using R, or in your browser with no installation required. The tool automatically installs a version of R and adds a Shiny-based GUI that removes the need to write R-code. Radiant can also be installed on top of an existing installation of R and invoked from within RStudio.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#learning-r-with-ai",
    "href": "content/courses/Analytics/listing.html#learning-r-with-ai",
    "title": "Data Analytics for Managers and Creators",
    "section": "Learning R with AI",
    "text": "Learning R with AI\nOf course.\n\nhttps://openai.com/index/improvements-to-data-analysis-in-chatgpt/\nhttps://rtutor.ai\nhttps://intro2r.library.duke.edu/ai.html\nhttps://chatlize.ai",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#business-analytics-courses-elsewhere",
    "href": "content/courses/Analytics/listing.html#business-analytics-courses-elsewhere",
    "title": "Data Analytics for Managers and Creators",
    "section": "Business Analytics Courses elsewhere",
    "text": "Business Analytics Courses elsewhere\n\nUniversity of San Diego, Rady School of Business. http://lab.rady.ucsd.edu/sawtooth/business_analytics_in_r/index.html",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats"
    ]
  },
  {
    "objectID": "content/courses/Analytics/listing.html#modules",
    "href": "content/courses/Analytics/listing.html#modules",
    "title": "Data Analytics for Managers and Creators",
    "section": "Modules",
    "text": "Modules",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html",
    "title": "Modelling with Logistic Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(infer)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#setting-up-r-packages",
    "title": "Modelling with Logistic Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(skimr)\nlibrary(GGally)\nlibrary(infer)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#introduction",
    "title": "Modelling with Logistic Regression",
    "section": "\n Introduction",
    "text": "Introduction\nSometimes the dependent variable is Qualitative: an either/or categorization. for example, or the variable we want to predict might be won or lost the contest, has an ailment or not, voted or not in the last election, or graduated from college or not. There might even be more than two categories such as voted for Congress, BJP, or Independent; or never smoker, former smoker, or current smoker.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#the-logistic-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#the-logistic-regression-model",
    "title": "Modelling with Logistic Regression",
    "section": "\n The Logistic Regression Model",
    "text": "The Logistic Regression Model\nWe saw with the General Linear Model that it models the mean of a target Quantitative variable as a linear weighted sum of the predictor variables:\n\\[\n\\Large{y \\sim N(x_i^T * \\beta, ~~\\sigma^2)}\n\\tag{1}\\]\nThis model is considered to be general because of the dependence on potentially more than one explanatory variable, v.s. the simple linear model:1 \\(y = \\beta_0 + \\beta_1*x_1 + \\epsilon\\). The general linear model gives us model “shapes” that start from a simple straight line to a p-dimensional hyperplane.\nAlthough a very useful framework, there are some situations where general linear models are not appropriate:\n\nthe range of Y is restricted (e.g. binary, count)\nthe variance of Y depends on the mean (Taylor’s Law)2\n\n\nHow do we use the familiar linear model framework when the target/dependent variable is Categorical?\nLinear Models for Categorical Targets?\nRecall that we spoke of dummy-encoded  Qualitative **predictor** variables for our linear models and how we would dummy encode them using numerical values, such as 0 and 1, or +1 and -1. Could we try the same way for a target categorical variable?\n\\[\nY_i = \\beta_0 + \\beta_1*X_i + \\epsilon_i\\\\ \\nonumber\n\\] \\[\nwhere\\\\\\\n\\]\n\\[\n\\begin{align}\nY_i &= 0 ~ if ~~~\"No\"\\\\ \\nonumber\n    &= 1 ~ if ~~~ \"Yes\"  \\nonumber\n\\end{align}\n\\]\nSadly this seems to not work for categorical dependent variables using a simple linear model as before. Consider the Credit Card Default data from the package ISLR.\n\n\n\n  \n\n\n\nWe see balance and income are quantitative predictors; student is a qualitative predictor, and default is a qualitative target variable. If we naively use a linear model equation as model = lm(default ~ balance, data = Default) and plot it, then…\n\n\n\n\n\n\n\n\n\nFigure 1: Naive Linear Model\n\n\n\n\n\n\n…it is pretty much clear from Figure 1 that something is very odd. (no pun intended! See below!) If the only possible values for default are \\(No = 0\\) and \\(Yes = 1\\), how could we interpret predicted value of, say, \\(Y_i = 0.25\\) or \\(Y_i = 1.55\\), or perhaps \\(Y_i = -0.22\\)? Anything other than Yes/No is hard to interpret!\n\n\n\n Problems…and Solutions\nWhere do we go from here?\nLet us state what we might desire of our model:\n\n\nModel Equation: Despite this setback, we would still like our model to be as close as possible to the familiar linear model equation.\n\\[\nY_i = \\beta_0 + \\beta_1*X_i + \\epsilon_i\\\\ \\nonumber\n\\]\n\\[\nwhere\\\\\\\n\\] \\[\n\\begin{align}\nY_i &= 0 ~ if ~~~\"No\"\\\\ \\nonumber\n&= 1 ~ if ~~~ \"Yes\"  \\nonumber\n\\end{align}\n\\tag{2}\\]\n\nPredictors and Weights: We have quantitative predictors so we still want to use a linear-weighted sum for the RHS (i.e predictor side) of the model equation. What can we try to make this work? Especially for the LHS (i.e the target side)?\nMaking the LHS continuous: What can we try? In dummy encoding our target variable, we found a range of [0,1], which is the same range for a probability value! Could we try to use probability of the outcome as our target, even though we are interested in binary outcomes? This would still leave us with a range of \\([0,1]\\) for the target variable, as before.\n\n\n\n\n\n\n\nNoteBinomially distributed target variable\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we map our Categorical/Qualitative target variable into a Quantitative probability, we need immediately to look at the LINE assumptions in linear regression.\n\n\nIn linear regression, we assume a normally distributed target variable, i.e. the residuals/errors around the predicted value are normally distributed. With a categorical target variable with two levels \\(0\\) and \\(1\\) it would be impossible for the errors \\(e_i = Y_i - \\hat{Y_i}\\) to have a normal distribution, as assumed for the statistical tests to be valid. The errors are bounded by \\([0,1]\\)! One candidate for the error distribution in this case is the binomial distribution, whose mean and variance are p and np(1-p) respectively.\nNote immediately that the binomial variance moves with the mean! The LINE assumption of normality is clearly violated. And from the figure above, extreme probabilities (near 1 or 0) are more stable (i.e., have less error variance) than middle probabilities. So the model has “built-in” heteroscedasticity, which we need to counter with transformations such as the \\(log()\\) function. More on this very shortly!\n\n\n\n\nOdds?: How would one “extend” the range of a target variable from [0,1] to \\([-\\infty, \\infty]\\) ? One step would be to try the odds of the outcome, instead of trying to predict the outcomes directly (Yes or No), or their probabilities \\([0,1]\\).\n\n\n\n\n\n\n\nNoteOdds\n\n\n\nOdds of an event with probability p of occurrence is defined as \\(Odds = p/(1-p)\\). As can be seen, the odds are the ratio of two probabilities, that of the event and its complement. In the Default dataset just considered, the odds of default and the odds of non-default can be calculated as:\n\n\n\n  \n\n\n\n\\[\n\\begin{align}\np(Default) &= 333/(333 + 9667)\\\\ \\nonumber\n           &= 0.333\\\\ \\nonumber\n\\end{align}\n\\]\ntherefore:\n\\[\n\\begin{align}\nOdds~of~Default &=p(Default)/(1-p(Default))\\\\ \\nonumber\n            &= 0.333/(1-0.333)\\\\ \\nonumber\n            &= 0.5\\\\\n\\end{align}\n\\]\nand OddsNoDefault = \\(0.9667/(1-0.9667) = 29\\).\nNow, odds cover half of real number line, i.e. \\([0, \\infty]\\) ! Clearly, when the probability p of an event is \\(0\\), the odds are \\(0\\)…and when it nears \\(1\\), the odds tend to \\(\\infty\\). So we have transformed a simple probability that lies between \\([0,1]\\) to odds lying between \\([0, \\infty]\\). That’s one step towards making a linear model possible; we have “removed” one of the limits on our linear model’s prediction range by using Odds as our target variable.\n\n\n\n\nTransformation using log()?: We need one more leap of faith: how do we convert a \\([0, \\infty]\\) range to a \\([-\\infty, \\infty]\\)? Can we try a log transformation?\n\n\\[\nlog([0, \\infty]) ~ = ~ [-\\infty, \\infty]\n\\]\nThis extends the range of our Qualitative target to the same as with a Quantitative target!\nThere is an additional benefit if this log() transformation: the Error Distributions with Odds targets. See the plot below. Odds are a necessarily nonlinear function of probability; the slope of Odds ~ probability also depends upon the probability itself, as we saw with the probability curve earlier.\n\n\n\n\n\n\n\n\n\n(a) Odds\n\n\n\n\n\n\n\n\n\n(b) Log Odds\n\n\n\n\n\n\nFigure 2: Odds Plot\n\n\nTo understand this issue intuitively, consider what happens to, say, a 5% change in the odds ratio near 1.0. If the odds ratio is \\(1.0\\), then the probabilities p and 1-p are \\(0.5\\), and \\(0.5\\). A 20% increase in the odds ratio to \\(1.20\\) would correspond to probabilities of \\(0.545\\) and \\(0.455\\). However, if the original probabilities were \\(0.9\\) and \\(0.1\\) for an odds ratio \\(9\\), then a 20% increase (in odds ratio) to \\(10.8\\) would correspond to probabilities of \\(0.915\\) and \\(0.085\\), a much smaller change in the probabilities. The basic curve is non-linear and the log transformation flattens this out to provide a more linear relationship, which is what we desire.\nSo in our model, instead of modeling odds as the dependent variable, we will use \\(log(odds)\\), also known as the logit, defined as:\n\\[\n\\begin{align}\nlog(odds_i) &= log\\bigg[p_i/(1-p_i)\\bigg]\\\\ \\nonumber\n            &= logit(p_i)\\\\\n\\end{align}\n\\tag{3}\\]\nThis is our Logistic Regression Model, which uses a Quantitative Predictor variable to predict a Categorical target variable. We write the model as ( for the Default dataset ) :\n\\[\n\\Large{logit(default) = \\beta_0 + \\beta_1 * balance}\n\\tag{4}\\]\nThis means that:\n\\[\nlog(p(default)/(1-p(default))) = \\beta_0+\\beta_1 * balance\n\\] and therefore:\n\\[\n\\begin{align}\np(default) &= \\frac{exp(\\beta_0 + \\beta_1 * balance)}{1 + exp(\\beta_0 + \\beta_1 * balance)}\\\\\n&= \\frac{1}{1 + exp^{-(\\beta_0 + \\beta_1 * balance)}}\n\\end{align}\n\\tag{5}\\]\nFrom the Equation 4 above it should be clear that a unit increase in balance should increase the odds of default by \\(\\beta_1\\) units. The RHS of Equation 5 is a sigmoid function of the weighted sum of predictors and is limited to the range [0,1].\n\n\n\n\n\n\n\n\n\n\n(a) naive linear regression model\n\n\n\n\n\n\n\n\n\n(b) logistic regression model\n\n\n\n\n\n\n\n\n\n(c) log odds gives linear models\n\n\n\n\n\n\nFigure 3: Model Plots\n\n\nIf we were to include income also as a predictor variable in the model, we might obtain something like:\n\n\\[\n\\begin{align}\np(default) &= \\frac{exp(\\beta_0 + \\beta_1 * balance + \\beta_2 * income)}{1 + exp(\\beta_0 + \\beta_1 * balance + \\beta_2 * income)}\\\\\n&= \\frac{1}{1 + exp^{-(\\beta_0 + \\beta_1 * balance + \\beta_2 * income)}}\n\\end{align}\n\\tag{6}\\]\nThis model Equation 6 is plotted a little differently, since it includes three variables. We’ll see this shortly, with code. The thing to note is that the formula inside the exp() is a linear combination of the predictors!\n\n\nEstimation of Model Parameters: The parameters \\(\\beta_i\\) now need to be estimated. How might we do that? This last problem is that because we have made so many transformations to get to the logits that we want to model, the logic of minimizing the sum of squared errors(SSE) is no longer appropriate.\n\n\n\n\n\n\n\nNoteInfinite SSE!!\n\n\n\nThe probabilities for default are \\(0\\) and \\(1\\). At these values the log(odds) will map respectively to \\(-\\infty\\) and \\(\\infty\\) 🙀. So if we naively try to take residuals, we will find that they are all \\(\\infty\\) !! Hence the Sum of Squared Errors \\(SSE\\) cannot be computed and we need another way to assess the quality of our model.\n\n\nInstead, we will have to use maximum likelihood estimation(MLE) to estimate the models. The maximum likelihood method maximizes the probability of obtaining the data at hand against every choice of model parameters \\(\\beta_i\\). (And compare that method with the \\(X^2\\) (“chi-squared”) test and statistic instead of t and F to evaluate the model comparisons)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-breast-cancer-dataset",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-breast-cancer-dataset",
    "title": "Modelling with Logistic Regression",
    "section": "\n Workflow: Breast Cancer Dataset",
    "text": "Workflow: Breast Cancer Dataset\nLet us proceed with the logistic regression workflow. We will use the well-known Wisconsin breast cancer dataset, readily available from Vincent Arel-Bundock’s website.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-read-the-data",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-read-the-data",
    "title": "Modelling with Logistic Regression",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\n\ncancer &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/dslabs/brca.csv\") %&gt;%\n  janitor::clean_names()\nglimpse(cancer)\n\nRows: 569\nColumns: 32\n$ rownames            &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n$ x_radius_mean       &lt;dbl&gt; 13.540, 13.080, 9.504, 13.030, 8.196, 12.050, 13.4…\n$ x_texture_mean      &lt;dbl&gt; 14.36, 15.71, 12.44, 18.42, 16.84, 14.63, 22.30, 2…\n$ x_perimeter_mean    &lt;dbl&gt; 87.46, 85.63, 60.34, 82.61, 51.71, 78.04, 86.91, 7…\n$ x_area_mean         &lt;dbl&gt; 566.3, 520.0, 273.9, 523.8, 201.9, 449.3, 561.0, 4…\n$ x_smoothness_mean   &lt;dbl&gt; 0.09779, 0.10750, 0.10240, 0.08983, 0.08600, 0.103…\n$ x_compactness_mean  &lt;dbl&gt; 0.08129, 0.12700, 0.06492, 0.03766, 0.05943, 0.090…\n$ x_concavity_mean    &lt;dbl&gt; 0.066640, 0.045680, 0.029560, 0.025620, 0.015880, …\n$ x_concave_pts_mean  &lt;dbl&gt; 0.047810, 0.031100, 0.020760, 0.029230, 0.005917, …\n$ x_symmetry_mean     &lt;dbl&gt; 0.1885, 0.1967, 0.1815, 0.1467, 0.1769, 0.1675, 0.…\n$ x_fractal_dim_mean  &lt;dbl&gt; 0.05766, 0.06811, 0.06905, 0.05863, 0.06503, 0.060…\n$ x_radius_se         &lt;dbl&gt; 0.2699, 0.1852, 0.2773, 0.1839, 0.1563, 0.2636, 0.…\n$ x_texture_se        &lt;dbl&gt; 0.7886, 0.7477, 0.9768, 2.3420, 0.9567, 0.7294, 1.…\n$ x_perimeter_se      &lt;dbl&gt; 2.058, 1.383, 1.909, 1.170, 1.094, 1.848, 1.735, 2…\n$ x_area_se           &lt;dbl&gt; 23.560, 14.670, 15.700, 14.160, 8.205, 19.870, 20.…\n$ x_smoothness_se     &lt;dbl&gt; 0.008462, 0.004097, 0.009606, 0.004352, 0.008968, …\n$ x_compactness_se    &lt;dbl&gt; 0.014600, 0.018980, 0.014320, 0.004899, 0.016460, …\n$ x_concavity_se      &lt;dbl&gt; 0.023870, 0.016980, 0.019850, 0.013430, 0.015880, …\n$ x_concave_pts_se    &lt;dbl&gt; 0.013150, 0.006490, 0.014210, 0.011640, 0.005917, …\n$ x_symmetry_se       &lt;dbl&gt; 0.01980, 0.01678, 0.02027, 0.02671, 0.02574, 0.014…\n$ x_fractal_dim_se    &lt;dbl&gt; 0.002300, 0.002425, 0.002968, 0.001777, 0.002582, …\n$ x_radius_worst      &lt;dbl&gt; 15.110, 14.500, 10.230, 13.300, 8.964, 13.760, 15.…\n$ x_texture_worst     &lt;dbl&gt; 19.26, 20.49, 15.66, 22.81, 21.96, 20.70, 31.82, 2…\n$ x_perimeter_worst   &lt;dbl&gt; 99.70, 96.09, 65.13, 84.46, 57.26, 89.88, 99.00, 8…\n$ x_area_worst        &lt;dbl&gt; 711.2, 630.5, 314.9, 545.9, 242.2, 582.6, 698.8, 5…\n$ x_smoothness_worst  &lt;dbl&gt; 0.14400, 0.13120, 0.13240, 0.09701, 0.12970, 0.149…\n$ x_compactness_worst &lt;dbl&gt; 0.17730, 0.27760, 0.11480, 0.04619, 0.13570, 0.215…\n$ x_concavity_worst   &lt;dbl&gt; 0.239000, 0.189000, 0.088670, 0.048330, 0.068800, …\n$ x_concave_pts_worst &lt;dbl&gt; 0.12880, 0.07283, 0.06227, 0.05013, 0.02564, 0.065…\n$ x_symmetry_worst    &lt;dbl&gt; 0.2977, 0.3184, 0.2450, 0.1987, 0.3105, 0.2747, 0.…\n$ x_fractal_dim_worst &lt;dbl&gt; 0.07259, 0.08183, 0.07773, 0.06169, 0.07409, 0.083…\n$ y                   &lt;chr&gt; \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\", …\n\nskim(cancer)\n\n\nData summary\n\n\nName\ncancer\n\n\nNumber of rows\n569\n\n\nNumber of columns\n32\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n31\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\ny\n0\n1\n1\n1\n0\n2\n0\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrownames\n0\n1\n285.00\n164.40\n1.00\n143.00\n285.00\n427.00\n569.00\n▇▇▇▇▇\n\n\nx_radius_mean\n0\n1\n14.13\n3.52\n6.98\n11.70\n13.37\n15.78\n28.11\n▂▇▃▁▁\n\n\nx_texture_mean\n0\n1\n19.29\n4.30\n9.71\n16.17\n18.84\n21.80\n39.28\n▃▇▃▁▁\n\n\nx_perimeter_mean\n0\n1\n91.97\n24.30\n43.79\n75.17\n86.24\n104.10\n188.50\n▃▇▃▁▁\n\n\nx_area_mean\n0\n1\n654.89\n351.91\n143.50\n420.30\n551.10\n782.70\n2501.00\n▇▃▂▁▁\n\n\nx_smoothness_mean\n0\n1\n0.10\n0.01\n0.05\n0.09\n0.10\n0.11\n0.16\n▁▇▇▁▁\n\n\nx_compactness_mean\n0\n1\n0.10\n0.05\n0.02\n0.06\n0.09\n0.13\n0.35\n▇▇▂▁▁\n\n\nx_concavity_mean\n0\n1\n0.09\n0.08\n0.00\n0.03\n0.06\n0.13\n0.43\n▇▃▂▁▁\n\n\nx_concave_pts_mean\n0\n1\n0.05\n0.04\n0.00\n0.02\n0.03\n0.07\n0.20\n▇▃▂▁▁\n\n\nx_symmetry_mean\n0\n1\n0.18\n0.03\n0.11\n0.16\n0.18\n0.20\n0.30\n▁▇▅▁▁\n\n\nx_fractal_dim_mean\n0\n1\n0.06\n0.01\n0.05\n0.06\n0.06\n0.07\n0.10\n▆▇▂▁▁\n\n\nx_radius_se\n0\n1\n0.41\n0.28\n0.11\n0.23\n0.32\n0.48\n2.87\n▇▁▁▁▁\n\n\nx_texture_se\n0\n1\n1.22\n0.55\n0.36\n0.83\n1.11\n1.47\n4.88\n▇▅▁▁▁\n\n\nx_perimeter_se\n0\n1\n2.87\n2.02\n0.76\n1.61\n2.29\n3.36\n21.98\n▇▁▁▁▁\n\n\nx_area_se\n0\n1\n40.34\n45.49\n6.80\n17.85\n24.53\n45.19\n542.20\n▇▁▁▁▁\n\n\nx_smoothness_se\n0\n1\n0.01\n0.00\n0.00\n0.01\n0.01\n0.01\n0.03\n▇▃▁▁▁\n\n\nx_compactness_se\n0\n1\n0.03\n0.02\n0.00\n0.01\n0.02\n0.03\n0.14\n▇▃▁▁▁\n\n\nx_concavity_se\n0\n1\n0.03\n0.03\n0.00\n0.02\n0.03\n0.04\n0.40\n▇▁▁▁▁\n\n\nx_concave_pts_se\n0\n1\n0.01\n0.01\n0.00\n0.01\n0.01\n0.01\n0.05\n▇▇▁▁▁\n\n\nx_symmetry_se\n0\n1\n0.02\n0.01\n0.01\n0.02\n0.02\n0.02\n0.08\n▇▃▁▁▁\n\n\nx_fractal_dim_se\n0\n1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.03\n▇▁▁▁▁\n\n\nx_radius_worst\n0\n1\n16.27\n4.83\n7.93\n13.01\n14.97\n18.79\n36.04\n▆▇▃▁▁\n\n\nx_texture_worst\n0\n1\n25.68\n6.15\n12.02\n21.08\n25.41\n29.72\n49.54\n▃▇▆▁▁\n\n\nx_perimeter_worst\n0\n1\n107.26\n33.60\n50.41\n84.11\n97.66\n125.40\n251.20\n▇▇▃▁▁\n\n\nx_area_worst\n0\n1\n880.58\n569.36\n185.20\n515.30\n686.50\n1084.00\n4254.00\n▇▂▁▁▁\n\n\nx_smoothness_worst\n0\n1\n0.13\n0.02\n0.07\n0.12\n0.13\n0.15\n0.22\n▂▇▇▂▁\n\n\nx_compactness_worst\n0\n1\n0.25\n0.16\n0.03\n0.15\n0.21\n0.34\n1.06\n▇▅▁▁▁\n\n\nx_concavity_worst\n0\n1\n0.27\n0.21\n0.00\n0.11\n0.23\n0.38\n1.25\n▇▅▂▁▁\n\n\nx_concave_pts_worst\n0\n1\n0.11\n0.07\n0.00\n0.06\n0.10\n0.16\n0.29\n▅▇▅▃▁\n\n\nx_symmetry_worst\n0\n1\n0.29\n0.06\n0.16\n0.25\n0.28\n0.32\n0.66\n▅▇▁▁▁\n\n\nx_fractal_dim_worst\n0\n1\n0.08\n0.02\n0.06\n0.07\n0.08\n0.09\n0.21\n▇▃▁▁▁\n\n\n\n\n\nWe see that there are 31 Quantitative variables, all named as x_***, and one Qualitative variable,y, which is a two-level target. (B = Benign, M = Malignant). The dataset has 569 observations, and no missing data.\n\n Workflow: Data Munging\nLet us rename y as diagnosis and take two other Quantitative parameters as predictors, suitably naming them too. We will also create a binary-valued variable called diagnosis_malignant (Binary, Malignant = 1, Benign = 0) for use as a target in our logistic regression model.\n\nShow the Codecancer_modified &lt;- cancer %&gt;%\n  rename(\n    \"diagnosis\" = y,\n    \"radius_mean\" = x_radius_mean,\n    \"concave_points_mean\" = x_concave_pts_mean\n  ) %&gt;%\n  ## Convert diagnosis to factor\n  mutate(diagnosis = factor(\n    diagnosis,\n    levels = c(\"B\", \"M\"),\n    labels = c(\"B\", \"M\")\n  )) %&gt;%\n  ## New Variable\n  mutate(diagnosis_malignant = if_else(diagnosis == \"M\", 1, 0)) %&gt;%\n  select(radius_mean, concave_points_mean, diagnosis, diagnosis_malignant)\n\ncancer_modified\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow can we predict whether a cancerous tumour is Benign or Malignant, based on the variable radius_mean alone, and with both radius_mean and concave_points_mean?\n\n\n\n Workflow: EDA\nLet us use GGally to plot a set of combo-plots for our modified dataset:\n\nShow the Codetheme_set(new = theme_custom())\n#\ncancer_modified %&gt;%\n  select(diagnosis, radius_mean, concave_points_mean) %&gt;%\n  GGally::ggpairs(\n    mapping = aes(colour = diagnosis),\n    switch = \"both\",\n    # axis labels in more traditional locations(left and bottom)\n\n    progress = FALSE,\n    # no compute progress messages needed\n\n    # Choose the diagonal graphs (always single variable! Think!)\n    diag = list(continuous = \"densityDiag\", alpha = 0.3),\n    # choosing density\n\n    # Choose lower triangle graphs, two-variable graphs\n    lower = list(continuous = wrap(\"points\", alpha = 0.3)),\n    title = \"Cancer Pairs Plot #1\"\n  ) +\n  scale_color_brewer(\n    palette = \"Set1\",\n    aesthetics = c(\"color\", \"fill\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from GGally::ggpairs\n\n\n\n\nThe counts for “B” and “M” are not terribly unbalanced; and both the radius_mean and concave_pts_mean appear to have well-separated box plot distributions for “B” and “M”.\nGiven the visible separation of the box-plots for both variables radius_mean and concave_pts_mean, we can believe that these will be good choices as predictors.\nInterestingly, radius_mean and concave_pts_mean are also mutually well-correlated, with a \\(\\rho = 0.823\\); we may wish (later) to choose (a pair of) predictor variables that are less strongly correlated.\n\n\n\n\n Workflow: Model Building\n\n\n Model Code\n Workflow: Model Checking and Diagnostics\n Workflow: Checks for Uncertainty\n Logistic Regression Models as Hypothesis Tests\n\n\n\nLet us code two models, using one and then both the predictor variables:\n\nShow the Codecancer_fit_1 &lt;- glm(diagnosis_malignant ~ radius_mean,\n  data = cancer_modified,\n  family = binomial(link = \"logit\")\n)\n\ncancer_fit_1 %&gt;% broom::tidy()\n\n\n\n\n  \n\n\n\n\nTable 1: Simple Model\n\n\n\nThe equation for the simple model is:\n\\[\n\\begin{aligned}\n\\operatorname{diagnosis\\_malignant} &\\sim Bernoulli\\left(\\operatorname{prob}_{\\operatorname{diagnosis\\_malignant} = \\operatorname{1}}= \\hat{P}\\right) \\\\\n\\log\\left[ \\frac { \\hat{P} }{ 1 - \\hat{P} } \\right]\n&= -15.25 + 1.03(\\operatorname{radius\\_mean})\n\\end{aligned}\n\\tag{7}\\]\nIncreasing radius_mean by one unit changes the log odds by \\(\\hat{\\beta_1} = 1.033\\) or equivalently it multiplies the odds by \\(exp(\\hat{\\beta_1}) =  2.809\\). We can plot the model as shown below:\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n##\nqthresh &lt;- c(0.2, 0.5, 0.8)\nbeta01 &lt;- coef(cancer_fit_1)[1]\nbeta11 &lt;- coef(cancer_fit_1)[2]\ndecision_point &lt;- (log(qthresh / (1 - qthresh)) - beta01) / beta11\n##\ncancer_modified %&gt;%\n  gf_point(\n    diagnosis_malignant ~ radius_mean,\n    colour = ~diagnosis,\n    title = \"diagnosis ~ radius_mean\",\n    xlab = \"Average radius\",\n    ylab = \"Diagnosis (1=malignant)\", size = 3, show.legend = F\n  ) %&gt;%\n  # gf_fun(exp(1.033 * radius_mean - 15.25) / (1 + exp(1.033 * radius_mean - 15.25)) ~ radius_mean, xlim = c(1, 30), linewidth = 3, colour = \"red\") %&gt;%\n  gf_smooth(\n    method = glm,\n    method.args = list(family = \"binomial\"),\n    se = FALSE,\n    color = \"black\"\n  ) %&gt;%\n  gf_vline(xintercept = decision_point, linetype = \"dashed\") %&gt;%\n  gf_refine(annotate(\n    \"text\",\n    label = paste0(\"q = \", qthresh),\n    x = decision_point + 0.45,\n    y = 0.4,\n    angle = -90\n  ), scale_color_brewer(palette = \"Set1\")) %&gt;%\n  gf_hline(yintercept = 0.5) %&gt;%\n  gf_theme(theme(plot.title.position = \"plot\")) %&gt;%\n  gf_refine(xlim(5, 30))\n\n\n\n\n\n\nFigure 4: Simple Model plot\n\n\n\n\nThe dotted lines show how the model can be used to classify the data in to two classes (“B” and “M”) depending upon the threshold probability \\(q\\).\nTaking both predictor variables, we obtain the model:\n\nShow the Codecancer_fit_2 &lt;- glm(diagnosis_malignant ~ radius_mean + concave_points_mean,\n  data = cancer_modified,\n  family = binomial(link = \"logit\")\n)\n\ncancer_fit_2 %&gt;% broom::tidy()\n\n\n\n\n  \n\n\n\n\nTable 2\n\n\n\nThe equation for the more complex model is:\n\\[\n\\begin{aligned}\n\\operatorname{diagnosis\\_malignant} &\\sim Bernoulli\\left(\\operatorname{prob}_{\\operatorname{diagnosis\\_malignant} = \\operatorname{1}}= \\hat{P}\\right) \\\\\n\\log\\left[ \\frac { \\hat{P} }{ 1 - \\hat{P} } \\right]\n&= -13.7 + 0.64(\\operatorname{radius\\_mean}) + 84.22(\\operatorname{concave\\_points\\_mean})\n\\end{aligned}\n\\tag{8}\\]\nIncreasing radius_mean by one unit changes the log odds by \\(\\hat{\\beta_1} = 0.6389\\) or equivalently it multiplies the odds by \\(exp(\\hat{\\beta_1}) =  1.894\\), provided concave_points_mean is held fixed.\nWe can plot the model as shown below: we create a scatter plot of the two predictor variables. The superimposed diagonal lines are lines for several constant values of threshold probability \\(q\\).\n\nShow the Code# Set graph theme\ntheme_set(new = theme_custom())\n##\nbeta02 &lt;- coef(cancer_fit_2)[1]\nbeta12 &lt;- coef(cancer_fit_2)[2]\nbeta22 &lt;- coef(cancer_fit_2)[3]\n##\ndecision_intercept &lt;- 1 / beta22 * (log(qthresh / (1 - qthresh)) - beta02)\ndecision_slope &lt;- -beta12 / beta22\n##\ncancer_modified %&gt;%\n  gf_point(concave_points_mean ~ radius_mean,\n    color = ~diagnosis, shape = ~diagnosis,\n    size = 3, alpha = 0.5\n  ) %&gt;%\n  gf_labs(\n    x = \"Average radius\",\n    y = \"Average concave\\nportions of the\\ncontours\",\n    color = \"Diagnosis\",\n    shape = \"Diagnosis\",\n    title = \"diagnosis ~ radius_mean + concave_points_mean\"\n  ) %&gt;%\n  gf_abline(\n    slope = decision_slope, intercept = decision_intercept,\n    linetype = \"dashed\"\n  ) %&gt;%\n  gf_refine(\n    scale_color_brewer(palette = \"Set1\"),\n    annotate(\"text\", label = paste0(\"q = \", qthresh), x = 10, y = c(0.08, 0.1, 0.115), angle = -17.155)\n  ) %&gt;%\n  gf_theme(theme(plot.title.position = \"plot\"))\n\n\n\n\n\n\nFigure 5: Complex Model plot\n\n\n\n\n\n\nTo Be Written Up.\n\n\nTo Be Written Up.\n\n\nTo Be Written Up.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-logistic-regression-internals",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#workflow-logistic-regression-internals",
    "title": "Modelling with Logistic Regression",
    "section": "Workflow: Logistic Regression Internals",
    "text": "Workflow: Logistic Regression Internals\nAll that is very well, but what is happening under the hood of the glm command? Consider the diagnosis (target) variable and say the average_radius feature/predictor variable. What we do is:\n\nPlot a scatter plot gf_point(diagnosis ~ average_radius, data = cancer_modified)\n\nStart with a sigmoid curve with some initial parameters \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_0}\\) that gives us some prediction of the probability of diagnosis for any given average_radius\n\nWe know the target labels for each data point ( i.e. “B” and “M”). We can calculate the likelihood of \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_0}\\), given the data.\nWe then change the values of \\(\\hat{\\beta_1}\\) and\\(\\hat{\\beta_0}\\) and calculate the likelihood again.\nThe set of parameters with the maximum likelihood(ML) for \\(\\hat{\\beta_1}\\) and \\(\\hat{\\beta_0}\\) gives us our logistic regression model.\nUse that model henceforth as a model for prediction.\n\nHow does one find out the “ML” parameters? There is clearly a two step procedure:\n\nFind the likelihood of the data for the parameters \\(\\beta_1\\)\n\nMaximize the likelihood by varying them. In practice, the changes to the parameters (step 5) are made in accordance with a method such as the Newton-Raphson method that can rapidly find the ML values for the parameters.\n\nLet us visualize the variations and computations from step(5). For the sake of clarity:\n\nwe will take a small sample of the original dataset\nwe take several different values for \\(\\beta_0\\) and \\(\\beta_1\\)\n\nUse these get a set of regression curves\nwhich we superimpose on the scatter plot of the sample\n\n\n\n\n\n\n\n\nFigure 6: Multiple Models\n\n\n\n\nIn Figure 6, we see three models: the “optimum” one in black, and two others in green and orange respectively.\nWe now project the actual points on to the regression curve, to obtain the predicted probability for each point.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe predicted probability \\(p_i\\) for each datum(radius_mean) is for the tumour being Malignant. If the datum corresponds to a tumour that is Benign, we must take \\(1-p_i\\). Each datum point is assumed to be independent, so we can calculate the likelihood as a product of probabilities, as follows: In this way, we calculate the likelihood of the data, give the model parameters as:\n\\[\n\\large{\n\\begin{equation}\n\\begin{aligned}\nlikelihood &=  \\prod_{Malignant}^{}p_i ~ \\times ~ \\prod_{Benign}^{}(1 - p_i)\\\\\n&= \\prod_{}^{}p_i^{y_{Malignant = 1}} ~ \\times ~ (1-p_i)^{y_{Benign = 1}}\\\\\n&= \\prod_{}^{}(p_i)^{y_i} ~\\times~ (1-p_i)^{1-y_i}//\n~since~labels~y_i~are~binary~1~or~0//\n\\end{aligned}\n\\end{equation}\n}\n\\] Lastly, since this is a product of small numbers, it can lead to inaccuracies, so we take the log of the whole thing to make it into an addition, obtaining the log-likelihood (LL):\n\\[\n\\large{\n\\begin{equation}\n\\begin{aligned}\nlog~likelihood ~~ ll(\\beta_i) &= log\\prod_{}^{}(p_i)^{y_i} * (1-p_i)^{1-y_i}\\\\\n&= \\sum_{}^{} y_i * log (p_i) + (1-y_i) * log(1 - p_i)\\\\\n\\end{aligned}\n\\end{equation}\n}\n\\tag{9}\\]\nWe now need to find the (global) maximum of this quantity and determine the \\(\\beta_i\\). Flipping this problem around, we find the maximum likelihood by minimizing the slope/gradient of of the LL!! And, to minimize the slope of the LL, we use the Newton-Raphson method or equivalent. Phew!\n\n\n\n\n\n\nNoteThe Newton-Raphson Method\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe black curve \\(y = fx)\\) is the function to be minimized, i.e. it is the gradient of the LL function.\nWe start with any arbitrary starting value of \\(x = x1, y1 = f(x1)\\) and calculate the tangent/slope/gradient equation \\(f'(x1)\\) at point \\((x1, y1) = (x1, f(x1))\\).\nThe tangent \\(f'(x1)\\) cuts the \\(x-axis\\) at \\(x2\\).(Grey line).\nRepeat.\nStop when the gradient becomes very small and \\(x_i\\) changes very in successive iterations.\n\n\n\nHow do we calculate the next value of x using the tangent?\n\nAt \\((x1,y1)\\), the tangent equation is: \\(y = y1 - slope1 * (x - x1)\\).\nThis equation applies at point \\((x2,0\\)), so \\(0 = y1 - slope1 *(x2 - x1)\\). (NOTE: Imagine that this is obtained by temporarily moving the y-axis to \\(x = x1\\) (dotted line), so \\(y1\\) in effect is the “c” in \\(y = mx + c\\))\nSolving for \\(x2\\), we get: \\(x2 = y1/slope1 - x1 = f(x1)/f'(x1)\\)\n\nSince \\(f(x)\\) is already the gradient of LL, we have: \\(x2 = x1 - ll'(x1)/ll''(x1)\\) !!\n\n\n\nTo be written up:\n\nFormula for gradient of LL\nConvergence of Newton- Raphson method for Maximum Likelihood\nHand Calculation of all steps (!!)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#conclusions",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#conclusions",
    "title": "Modelling with Logistic Regression",
    "section": "\n Conclusions",
    "text": "Conclusions\n\nLogistic Regression is a great ML algorithm for predicting Qualitative target variables.\nIt also works for multi-level/multi-valued Qual variables (multinomial logistic regression)\nThe internals of Logistic Regression are quite different compared to Linear Regression",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#sec-references",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#sec-references",
    "title": "Modelling with Logistic Regression",
    "section": "\n References",
    "text": "References\n\nJudd, Charles M. & McClelland, Gary H. & Ryan, Carey S. Data Analysis: A Model Comparison Approach to Regression, ANOVA, and Beyond. Routledge, Aug 2017. Chapter 14.\nEmi Tanaka.Logistic Regression https://emitanaka.org/iml/lectures/lecture-04A.html#/TOC. Course: ETC3250/5250, Monash University, Melbourne, Australia.\nGeeks for Geeks.Logistic Regression. https://www.geeksforgeeks.org/understanding-logistic-regression/\n\nGeeks for Geeks.Maximum Likelihood Estimation. https://www.geeksforgeeks.org/probability-density-estimation-maximum-likelihood-estimation/\n\nhttps://yury-zablotski.netlify.app/post/how-logistic-regression-works/\nhttps://uc-r.github.io/logistic_regression\nhttps://francisbach.com/self-concordant-analysis-for-logistic-regression/\nhttps://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf\nhttps://jasp-stats.org/2022/06/30/generalized-linear-models-glm-in-jasp/\nP. Bingham, N.Q. Verlander, M.J. Cheal (2004). John Snow, William Farr and the 1849 outbreak of cholera that affected London: a reworking of the data highlights the importance of the water supply. Public Health Volume 118, Issue 6, September 2004, Pages 387-394. Read the PDF.\n\nhttps://peopleanalytics-regression-book.org/bin-log-reg.html\nMcGill University. Epidemiology https://www.medicine.mcgill.ca/epidemiology/joseph/courses/epib-621/logfit.pdf\n\nhttps://arunaddagatla.medium.com/maximum-likelihood-estimation-in-logistic-regression-f86ff1627b67\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nequatiomatic\n0.3.7\nAnderson, Heiss, and Sumners (2025)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\n\n\n\n\nAnderson, Daniel, Andrew Heiss, and Jay Sumners. 2025. equatiomatic: Transform Models into “LaTeX” Equations. https://doi.org/10.32614/CRAN.package.equatiomatic.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://doi.org/10.32614/CRAN.package.ISLR.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LogReg/index.html#footnotes",
    "title": "Modelling with Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://statmath.wu.ac.at/courses/heather_turner/glmCourse_001.pdf↩︎\nhttps://en.wikipedia.org/wiki/Taylor%27s_law↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Logistic Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html",
    "title": "Modelling with Linear Regression",
    "section": "",
    "text": "Multiple Regression - Forward Selection  \n\n Multiple Regression - Backward Selection  \n\n  Permutation Test for Regression",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-linreg",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-linreg",
    "title": "Modelling with Linear Regression",
    "section": "",
    "text": "Multiple Regression - Forward Selection  \n\n Multiple Regression - Backward Selection  \n\n  Permutation Test for Regression",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#setting-up-r-packages",
    "title": "Modelling with Linear Regression",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(corrgram)\nlibrary(ggstatsplot)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#introduction",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#introduction",
    "title": "Modelling with Linear Regression",
    "section": "\n Introduction",
    "text": "Introduction\nOne of the most common problems in Prediction Analytics is that of predicting a Quantitative response variable, based on one or more Quantitative predictor variables or features. This is called Linear Regression. We will use the intuitions built up during our study of ANOVA to develop our ideas about Linear Regression.\nSuppose we have data on salaries in a Company, with years of study and previous experience. Would we be able to predict the prospective salary of a new candidate, based on their years of study and experience? Or based on the mileage done, could we predict the resale price of a used car? These are typical problems in Linear Regression.\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#the-linear-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#the-linear-regression-model",
    "title": "Modelling with Linear Regression",
    "section": "\n The Linear Regression Model",
    "text": "The Linear Regression Model\nThe premise here is that many common statistical tests are special cases of the linear model.\nA linear model estimates the relationship between one continuous or ordinal variable (dependent variable or “response”) and one or more other variables (explanatory variable or “predictors”). It is assumed that the relationship is linear:1\n\\[\n\\Large{y_i \\sim \\beta_1*x_i + \\beta_0\\\\}\n\\tag{1}\\]\nor\n\\[\n\\Large{y_1 \\sim exp(\\beta_1)*x_i + \\beta_0}\n\\tag{2}\\]\nbut not:\n\\[\n\\color{red}{y_i \\sim \\beta_1*exp(\\beta_2*x_i) + \\beta_0\\\\}\n\\]\nor\n\\[\n\\color{red}{y_i \\sim \\beta_1 *x^{\\beta_2} + \\beta_0}\n\\]\nIn Equation 1, \\(\\beta_0\\) is the intercept and \\(\\beta_1\\) is the slope of the linear fit, that predicts the value of y based the value of x. Each prediction leaves a small “residual” error between the actual and predicted values. \\(\\beta_0\\) and \\(\\beta_1\\) are calculated based on minimizing the sum of squares of these residuals, and hence this method is called “ordinary least squares” (OLS) regression.\n\n\n\n\n\nFigure 1: Least Squares\n\n\nThe net area of all the shaded squares is minimized in the calculation of \\(\\beta_0\\) and \\(\\beta_1\\). As per Lindoloev, many statistical tests, going from one-sample t-tests to two-way ANOVA, are special cases of this system. Also see Jeffrey Walker “A linear-model-can-be-fit-to-data-with-continuous-discrete-or-categorical-x-variables”.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#linear-models-as-hypothesis-tests",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#linear-models-as-hypothesis-tests",
    "title": "Modelling with Linear Regression",
    "section": "\n Linear Models as Hypothesis Tests",
    "text": "Linear Models as Hypothesis Tests\nUsing linear models is based on the idea of Testing of Hypotheses. The Hypothesis Testing method typically defines a NULL Hypothesis where the statements read as “there is no relationship” between the variables at hand, explanatory and responses. The Alternative Hypothesis typically states that there is a relationship between the variables.\nAccordingly, in fitting a linear model, we follow the process as follows:\n\n\n\n\n\n\nNoteModelling Process\n\n\n\nWith \\(y = \\beta_0 + \\beta_1 *x\\)\n\nMake the following hypotheses:\n\n\\[\n    NULL\\ Hypothesis\\ H_0 =&gt; x\\ and\\ y\\ are\\ unrelated.\\ (\\beta_1 = 0)\n\\]\n\\[\n    Alternate\\ Hypothesis\\ H_1 =&gt; x\\ and\\ y\\ are\\ linearly\\ related\\ (\\beta_1 \\ne 0)\n\\]\n\nWe “assume” that \\(H_0\\) is true.\nWe calculate \\(\\beta_1\\).\nWe then find probability p(\\(\\beta_1 = Estimated\\ Value\\) when the NULL Hypothesis is assumed TRUE). This is the p-value. If that probability is p&gt;=0.05, we say we “cannot reject” \\(H_0\\) and there is unlikely to be significant linear relationship.\nHowever, if p&lt;= 0.05 can we reject the NULL hypothesis, and say that there could be a significant linear relationship, because the probability p that \\(\\beta_1 = Estimated\\ Value\\) by mere chance under \\(H_0\\) is very small.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-assumptions-in-linear-models",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-assumptions-in-linear-models",
    "title": "Modelling with Linear Regression",
    "section": "\n Assumptions in Linear Models",
    "text": "Assumptions in Linear Models\nWhen does a Linear Model work? We can write the assumptions in Linear Regression Models as an acronym, LINE:\n1. L: \\(\\color{blue}{linear}\\) relationship between variables 2. I: Errors are independent (across observations)\n3. N: \\(y\\) is \\(\\color{red}{normally}\\) distributed at each “level” of \\(x\\).\n4. E: \\(y\\) has the same variance at all levels of \\(x\\). No heteroscedasticity.\n\n\n\n\n\nFigure 2: OLS Assumptions\n\n\nHence a very concise way of expressing the Linear Model is:\n\\[\n\\Large{y \\sim N(x_i^T * \\beta, ~~\\sigma^2)}\n\\]\n\n\n\n\n\n\nImportantGeneral Linear Models\n\n\n\nThe target variable \\(y\\) is modelled as a normally distribute variable whose mean depends upon a linear combination of predictor variables \\(x\\), and whose variance is \\(\\sigma^2\\).",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#linear-model-workflow",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#linear-model-workflow",
    "title": "Modelling with Linear Regression",
    "section": "\n Linear Model Workflow",
    "text": "Linear Model Workflow\nOK, on with the computation!\n\n Workflow: Read the Data\nLet us now read in the data and check for these assumptions as part of our Workflow.\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min          Q1     median          Q3       max\n1    tract integer   1.00000 1303.250000 3393.50000 3739.750000 5082.0000\n2      lon numeric -71.28950  -71.093225  -71.05290  -71.019625  -70.8100\n3      lat numeric  42.03000   42.180775   42.21810   42.252250   42.3810\n4     medv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n5    cmedv numeric   5.00000   17.025000   21.20000   25.000000   50.0000\n6     crim numeric   0.00632    0.082045    0.25651    3.677083   88.9762\n7       zn numeric   0.00000    0.000000    0.00000   12.500000  100.0000\n8    indus numeric   0.46000    5.190000    9.69000   18.100000   27.7400\n9      nox numeric   0.38500    0.449000    0.53800    0.624000    0.8710\n10      rm numeric   3.56100    5.885500    6.20850    6.623500    8.7800\n11     age numeric   2.90000   45.025000   77.50000   94.075000  100.0000\n12     dis numeric   1.12960    2.100175    3.20745    5.188425   12.1265\n13     rad integer   1.00000    4.000000    5.00000   24.000000   24.0000\n14     tax integer 187.00000  279.000000  330.00000  666.000000  711.0000\n15 ptratio numeric  12.60000   17.400000   19.05000   20.200000   22.0000\n16       b numeric   0.32000  375.377500  391.44000  396.225000  396.9000\n17   lstat numeric   1.73000    6.950000   11.36000   16.955000   37.9700\n           mean           sd   n missing\n1  2700.3557312 1.380037e+03 506       0\n2   -71.0563887 7.540535e-02 506       0\n3    42.2164403 6.177718e-02 506       0\n4    22.5328063 9.197104e+00 506       0\n5    22.5288538 9.182176e+00 506       0\n6     3.6135236 8.601545e+00 506       0\n7    11.3636364 2.332245e+01 506       0\n8    11.1367787 6.860353e+00 506       0\n9     0.5546951 1.158777e-01 506       0\n10    6.2846344 7.026171e-01 506       0\n11   68.5749012 2.814886e+01 506       0\n12    3.7950427 2.105710e+00 506       0\n13    9.5494071 8.707259e+00 506       0\n14  408.2371542 1.685371e+02 506       0\n15   18.4555336 2.164946e+00 506       0\n16  356.6740316 9.129486e+01 506       0\n17   12.6530632 7.141062e+00 506       0\n\n\nThe original data are 506 observations on 14 variables, medv being the target variable:\n\n\n\n\n\n\n\ncrim\nper capita crime rate by town\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft\n\n\nindus\nproportion of non-retail business acres per town\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nnox\nnitric oxides concentration (parts per 10 million)\n\n\nrm\naverage number of rooms per dwelling\n\n\nage\nproportion of owner-occupied units built prior to 1940\n\n\ndis\nweighted distances to five Boston employment centres\n\n\nrad\nindex of accessibility to radial highways\n\n\ntax\nfull-value property-tax rate per USD 10,000\n\n\nptratio\npupil-teacher ratio by town\n\n\nb\n\n\\(1000(B - 0.63)^2\\) where B is the proportion of Blacks by town\n\n\nlstat\npercentage of lower status of the population\n\n\nmedv\nmedian value of owner-occupied homes in USD 1000’s\n\n\n\nThe corrected data set has the following additional columns:\n\n\ncmedv\ncorrected median value of owner-occupied homes in USD 1000’s\n\n\ntown\nname of town\n\n\ntract\ncensus tract\n\n\nlon\nlongitude of census tract\n\n\nlat\nlatitude of census tract\n\n\nOur response variable is cmedv, the corrected median value of owner-occupied homes in USD 1000’s. Their are many Quantitative feature variables that we can use to predict cmedv. And there are two Qualitative features, chas and tax.\n\n Workflow: EDA\nIn order to fit the linear model, we need to choose predictor variables that have strong correlations with the target variable. We will first do this with GGally, and then with the tidyverse itself. Both give us a very unique view into the correlations that exist within this dataset.\n\n\n Workflow: Correlations with GGally\n Correlations using cor.test and purrr\n\n\n\nLet us select a few sets of Quantitative and Qualitative features, along with the target variable cmedv and do a pairs-plots with them:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors Rooms / Age / Distance to City Centres / Radial Highway Access\n  select(cmedv, rm, age, dis) %&gt;%\n  GGally::ggpairs(\n    title = \"Plot 1\",\n    progress = FALSE,\n    lower = list(continuous = wrap(\"smooth\",\n      alpha = 0.2\n    ))\n  )\n\n\n\n\n\n\n##\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors: Access to Radial Highways, / Resid. Land Proportion / proportion of non-retail business acres / full-value property-tax rate per USD 10,000\n  select(cmedv, rad, zn, indus, tax) %&gt;%\n  GGally::ggpairs(\n    title = \"Plot 2\",\n    progress = FALSE,\n    lower = list(continuous = wrap(\"smooth\",\n      alpha = 0.2\n    ))\n  )\n\n\n\n\n\n\n##\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictors Crime Rate / Nitrous Oxide / Black Population / Lower Status Population\n  select(cmedv, crim, nox, rad, b, lstat) %&gt;%\n  GGally::ggpairs(\n    title = \"Plot 3\",\n    progress = FALSE,\n    lower = list(continuous = wrap(\"smooth\",\n      alpha = 0.2\n    ))\n  )\n\n\n\n\n\n\n\nSee the top row of the pairs plots. Clearly, rm (avg. number of rooms) is a big determining feature for median price cmedv. This we infer based on the large correlation of rm withcmedv, \\(0.696\\). The variableage (proportion of owner-occupied units built prior to 1940) may also be a significant influence on cmedv, with a correlation of \\(-0.378\\).\nNone of the Quant variables rad, zn, indus, tax have a overly strong correlation with cmedv. .\nThe variable lstat (proportion of lower classes in the neighbourhood) as expected, has a strong (negative) correlation with cmedv; rad(index of accessibility to radial highways), nox(nitrous oxide) and crim(crime rate) also have fairly large correlations with cmedv, as seen from the pairs plots.\n\n\n\n\n\n\nImportantCorrelation Scores and Uncertainty\n\n\n\nRecall that cor.test reports a correlation score and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found.\nNote that GGally too reports the significance of the correlation scores using stars, *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor.test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\n\n\nLet us plot (again) scatter plots of Quant Variables that have strong correlation with cmedv:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ngf_point(\n  data = housing,\n  cmedv ~ age,\n  title = \"Price vs Proportion of houses older than 1940\",\n  ylab = \"Median Price\",\n  xlab = \"Proportion of older-than-1940 buildings\")\n\n##\ngf_point(\n  data = housing,\n  cmedv ~ lstat,\n  title = \"Price vs Proportion of lower classes...\"\n  subtitle = \"...In the neighbourhood\",\n  ylab = \"Median Price\",\n  xlab = \"proportion of lower classes in the neighbourhood\")\n##\ngf_point(\n  data = housing,\n  cmedv ~ rm,\n  title = \"Price vs Average no. of Rooms\",\n  ylab = \"(cmedv) Median Price\",\n  xlab = \"(rm) Avg. No. of Rooms\")\n\n\n\nError in parse(text = input): &lt;text&gt;:16:3: unexpected symbol\n15:   title = \"Price vs Proportion of lower classes...\"\n16:   subtitle\n      ^\n\n\n\nSo, rm does have a positive effect on cmedv, and age may have a (mild?) negative effect on cmedv; lstat seems to have a pronounced negative effet on cmedv. We have now managed to get a decent idea which Quant predictor variables might be useful in modelling cmedv: rm, lstat for starters, then perhapsage.\nLet us also check the Qualitative predictor variables: Access to the Charles river (chas) does seem to affect the prices somewhat.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nhousing %&gt;%\n  # Target variable cmedv\n  # Predictor Access to Charles River\n  select(cmedv, chas) %&gt;%\n  GGally::ggpairs(\n    title = \"Plot 4\",\n    progress = FALSE,\n    lower = list(continuous = wrap(\"smooth\",\n      alpha = 0.2\n    ))\n  )\n\n\n\n\n\n\n\nLook at the bar plot above. While not too many properties can be near the Charles River (for obvious reasons) the box plots do seem to show some dependency of cmedv on chas.\n\n\n\n\n\n\nNote\n\n\n\nQualitative predictors for a Quantitative target can be included in the model using what is called dummy variables, where each level of the Qualitative variable is given a one-hot kind of encoding. See for example https://www.statology.org/dummy-variables-regression/\n\n\n\n\nThis is somewhat advanced material: We will use the purrr package to develop all correlations with respect to our target variable in one shot and also plot these correlation test scores in an error-bar plot. See Tidy Modelling with R. This has the advantage of being able to depict all correlations in one plot. (We will use this approach again here when we trim our linear models down from the maximal one to a workable one of lesser complexity.). Let us do this.\nWe develop a list object containing all correlation test results with respect to cmedv, tidy these up using broom::tidy, and then plot these:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nall_corrs &lt;- housing %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off target variable cmedv and IDs\n  # get all the remaining ones\n  select(-cmedv, -medv) %&gt;%\n  purrr::map(\n    .x = ., # All numeric variables selected in the previous step\n    .f = \\(.x) cor.test(.x, housing$cmedv)\n  ) %&gt;% # Apply the cor.test with `cmedv`\n\n  # Tidy up the cor.test outputs into neat columns\n  # Need \".id\" column to keep track of predictor variable name\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nall_corrs\n\n\n  \n\n\nall_corrs %&gt;%\n  gf_hline(\n    yintercept = 0,\n    color = \"grey\",\n    linewidth = 2,\n    title = \"Correlations: Target Variable vs All Predictors\",\n    subtitle = \"Boston Housing Dataset\"\n  ) %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    colour = ~estimate,\n    width = 0.5,\n    linewidth = ~ -log10(p.value),\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  # Plot points(smallest geom) last!\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = \"Predictors\", y = \"Correlation with cmedv\") %&gt;%\n  # gf_theme(theme_minimal()) %&gt;%\n\n  # tilt the x-axis labels for readability\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %&gt;%\n  # Colour and linewidth scales + legends\n  gf_refine(\n    scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n    scale_linewidth_continuous(\"Significance\",\n      range = c(0.25, 3),\n\n      # guide_legend(reverse = TRUE): Fat Lines mean higher significance\n    )\n  ) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE)))\n\n\n\n\n\n\n\nWe can clearly see that rm and lstat have strong correlations with cmedv and should make good choices for setting up a minimal linear regression model. (medv is the older errored version of cmedv)\n\n\n\n\n Model Building\nWe will first execute the lm test with code and evaluate the results. Then we will do an intuitive walk through of the process and finally, hand-calculate entire analysis for clear understanding.\n\n\n Model Code\n Forecasting with the Linear Model\n Linear Model Intuitive\n Linear Models Manually Demonstrated (Apologies to Spinoza)\n Using Other Packages\n\n\n\nR offers a very simple command lm to execute an Linear Model: Note the familiar formula of stating the variables: ( \\(y \\sim x\\); where \\(y\\) = target, \\(x\\) = predictor)\n\nhousing_lm &lt;- lm(cmedv ~ rm, data = housing)\nsummary(housing_lm)\n\n\nCall:\nlm(formula = cmedv ~ rm, data = housing)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.336  -2.425   0.093   2.918  39.434 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -34.6592     2.6421  -13.12   &lt;2e-16 ***\nrm            9.0997     0.4178   21.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.597 on 504 degrees of freedom\nMultiple R-squared:  0.4848,    Adjusted R-squared:  0.4838 \nF-statistic: 474.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\nThe model for \\(\\widehat{cmedv}\\) , the prediction for cmedvcan be written in the form of \\(y = mx + c\\), as:\n\\[\n\\widehat{cmedv} \\sim -34.65924 + 9.09967* rm\n\\tag{3}\\]\n\n\n\n\n\n\nImportant\n\n\n\n\nThe effect size of rm on predicting cmedv a (slope) value of \\(9.09967\\) which is significant at p-value of \\(&lt;2.2e-16\\); for every one room increase in rm, we have a \\(USD~90997\\) increase in median price cmedv.\nThe F-statistic for the Linear Model is given by \\(F = 474.3\\), which is very high. (We will use the F-statistic again when we do Multiple Regression.)\nThe R-squared value is \\(R^2 = 0.48\\) which means that rm is able to explain about half of the trend in cmedv; there is substantial variation in cmedv that is still left to explain, an indication that we should perhaps use a richer model, with more predictors. These aspects are explored in the Tutorials.\n\n\n\nWe can plot the scatter plot of these two variables with the model also over-plotted.\n\n#| layout-ncol: 3\n#| fig-width: 5\n#| fig-height: 4\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n# Tidy Data frame for the model using `broom`\nhousing_lm_tidy &lt;-\n  housing_lm %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\nhousing_lm_tidy\n\n\n  \n\n\n##\nhousing_lm_augment &lt;-\n  housing_lm %&gt;%\n  broom::augment(\n    se_fit = TRUE,\n    interval = \"confidence\"\n  )\nhousing_lm_augment\n\n\n  \n\n\n##\nintercept &lt;-\n  housing_lm_tidy %&gt;%\n  filter(term == \"(Intercept)\") %&gt;%\n  select(estimate) %&gt;%\n  as.numeric()\n##\nslope &lt;-\n  housing_lm_tidy %&gt;%\n  filter(term == \"rm\") %&gt;%\n  select(estimate) %&gt;%\n  as.numeric()\n##\nhousing %&gt;%\n  drop_na() %&gt;%\n  gf_point(\n    cmedv ~ rm,\n    title = \"Price vs Average no. of Rooms\",\n    ylab = \"Median Price\",\n    xlab = \"Avg. No. of Rooms\",\n    alpha = 0.2\n  ) %&gt;%\n  # Plot the model equation\n  gf_abline(\n    slope = slope, intercept = intercept,\n    colour = \"lightcoral\",\n    linewidth = 2\n  ) %&gt;%\n  # Plot the model prediction points on the line\n  gf_smooth(\n    method = \"lm\", geom = \"point\",\n    color = \"grey30\",\n    size = 0.5\n  ) %&gt;%\n  gf_refine(\n    annotate(\n      geom = \"segment\",\n      y = 0, yend = 29, x = 7, xend = 7, # manually calculated\n      linetype = \"dashed\",\n      color = \"dodgerblue\",\n      arrow = arrow(\n        angle = 30,\n        length = unit(0.25, \"inches\"),\n        ends = \"last\",\n        type = \"closed\"\n      )\n    ),\n    annotate(\n      geom = \"segment\",\n      y = 29, yend = 29, x = 2.5, xend = 7, # manually calculated\n      linetype = \"dashed\",\n      arrow = arrow(\n        angle = 30,\n        length = unit(0.25, \"inches\"),\n        ends = \"first\",\n        type = \"closed\"\n      ),\n      color = \"dodgerblue\"\n    )\n  ) %&gt;%\n  gf_refine(\n    scale_x_continuous(\n      limits = c(2.5, 10),\n      expand = c(0, 0)\n    ),\n    # removes plot panel margins\n    scale_y_continuous(\n      limits = c(0, 55),\n      expand = c(0, 0)\n    )\n  ) %&gt;%\n  gf_theme(theme = theme_custom())\n\n\n\n\n\n\n\nFor any new value of rm, we go up to the vertical blue line and read off the predicted median price by following the horizontal blue line. That is how the model is used (by hand).\n\n\nIn practice, we use the broom package functions (tidy, glance and augment) to obtain a clear view of the model parameters and predictions of cmedv for all existing values of rm. We see estimates for the intercept and slope (rm) for the linear model, along with the standard errors and p.values for these estimated parameters. And we see the fitted values of cmedv for the existing rm; these values will naturally lie on the straight-line depicting the model. We will examine this augment-ed data more the section on Diagnostics.\nTo predict cmedv with new values of rm, we use predict. Let us now try to make predictions with some new data:\n\nnew &lt;- tibble(rm = seq(3, 10)) # must be named \"rm\"\nnew %&gt;% mutate(\n  predictions =\n    stats::predict(\n      object = housing_lm,\n      newdata = .,\n      se.fit = FALSE\n    )\n)\n\n\n  \n\n\n\nNote that “negative values” for predicted cmedv would have no meaning!\n\n\nAll that is very well, but what is happening under the hood of the lm command? Consider the cmedv (target) variable and the rm feature/predictor variable. What we do is:\n\nPlot a scatter plot gf_point(cmedv ~ rm, housing)\n\nFind a line that, in some way, gives us some prediction of cmedv for any given rm\n\nCalculate the errors in prediction and use those to find the “best” line.\nUse that “best” line henceforth as a model for prediction.\n\nHow does one fit the “best” line? Consider a choice of “lines” that we can use to fit to the data. Here are 6 lines of varying slopes (and intercepts ) that we can try as candidates for the best fit line:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt should be apparent that while we cannot determine which line may be the best, the worst line seems to be the one in the final plot, which ignores the x-variable rm altogether. This corresponds to the NULL Hypothesis, that there is no relationship between the two variables. Any of the other lines could be a decent candidate, so how do we decide?\n\n\n\n\n\n\n\n\n\n\nIn Fig A, the horizontal blue line is the overall mean of cmedv, denoted as \\(\\mu_{tot}\\). The vertical green lines to the points show the departures of each point from this overall mean, called residuals. The sum of squares of these residuals in Fig A is called the Total Sum of Squares (SST).\n\\[\nSST = \\Sigma (y - \\mu_{tot})^2\n\\tag{4}\\]\nIn Fig B, the vertical red lines are the residuals of each point from the potential line of fit. The sum of the squares of these lines is called the Total Error Sum of Squares (SSE).\n\\[\nSSE = \\Sigma [(y - a - b * rm)^2]\n\\tag{5}\\]\nIt should be apparent that if there is any positive linear relationship between cmedv and rm,then \\(SSE &lt; SST\\).\nHow do we get the optimum slope + intercept? If we plot the \\(SSE\\) as a function of varying slope, we get:\n\n#| echo: false\nsim_model &lt;- tibble(\n  b = slope + seq(-5, 5),\n  a = intercept,\n  dat = list(tibble(\n    cmedv = housing_sample$cmedv,\n    rm = housing_sample$rm\n  ))\n) %&gt;%\n  mutate(r_squared = pmap_dbl(\n    .l = list(a, b, dat),\n    .f = \\(a, b, dat) sum((dat$cmedv - (b * dat$rm + a))^2)\n  ))\nmin_r_squared &lt;- sim_model %&gt;%\n  select(r_squared) %&gt;%\n  min()\nmin_slope &lt;- sim_model %&gt;%\n  filter(r_squared == min_r_squared) %&gt;%\n  select(b) %&gt;%\n  as.numeric()\nsim_model %&gt;%\n  gf_point(r_squared ~ b, data = ., size = 2) %&gt;%\n  gf_line(ylab = \"SSE\", xlab = \"slope\", title = \"Error vs Slope\") %&gt;%\n  gf_hline(yintercept = min_r_squared, color = \"red\") %&gt;%\n  gf_segment(min_r_squared + 0 ~ min_slope + min_slope,\n    colour = \"red\",\n    arrow = arrow(ends = \"last\", length = unit(1, \"mm\"))\n  ) %&gt;%\n  gf_refine(\n    coord_cartesian(expand = FALSE),\n    expand_limits(y = c(0, 20000), x = c(3.5, 15))\n  )\n\n\n\n\n\n\n\nWe see that there is a quadratic minimum \\(SSE\\) at the optimum value of slope and at all other slopes, the \\(SSE\\) is higher. We can use this to find the optimum slope, which is what the function lm does.\n\n\nLet us hand-calculate the numbers so we know what the test is doing. Here is the SST: we pretend that there is no relationship between cmedv ans rm and compute a NULL model:\n\n# Calculate overall sum squares SST\n\nSST &lt;- deviance(lm(cmedv ~ 1, data = housing))\nSST\n\n[1] 42577.74\n\n\nAnd here is the SSE:\n\nSSE &lt;- deviance(housing_lm)\nSSE\n\n[1] 21934.39\n\n\nGiven that the model leaves unexplained variations in cmedv to the extent of \\(SSE\\), we can compute the \\(SSR\\), the Regression Sum of Squares, the amount of variation in cmedv that the linear model does explain:\n\nSSR &lt;- SST - SSE\nSSR\n\n[1] 20643.35\n\n\nWe have \\(SST = 42577.74\\), \\(SSE = 21934.39\\) and therefore \\(SSR = 20643.35\\).\nIn order to calculate the F-Statistic, we need to compute the variances, using these sum of squares. We obtain variances by dividing by their Degrees of Freedom:\n\\[\nF_{stat} = \\frac{SSR / df_{SSR}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSR}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in SSR and SSE.\nLet us calculate these Degrees of Freedom. If we have \\(n=\\) 506 observations of data, then:\n\n\n\\(SST\\) clearly has degree of freedom \\(n-1 = 505\\), since it uses all observations but loses one degree to calculate the global mean.\n\n\\(SSE\\) was computed using the slope and intercept, so it has \\((n-2) = 504\\) as degrees of freedom.\nAnd therefore \\(SSR\\) being their difference has just \\(1\\) degree of freedom.\n\nNow we are ready to compute the F-statistic:\n\nn &lt;- housing %&gt;%\n  count() %&gt;%\n  as.numeric()\ndf_SSR &lt;- 1\ndf_SSE &lt;- n - 2\nF_stat &lt;- (SSR / df_SSR) / (SSE / df_SSE)\nF_stat\n\n[1] 474.3349\n\n\nThe F-stat is compared with a critical value of the F-statistic, which is computed using the formula for the f-distribution in R. As with our hypothesis tests, we set the significance level to 0.95, and quote the two relevant degrees of freedom as parameters to qf() which computes the critical F value as a quartile:\n\nF_crit &lt;- qf(\n  p = 0.95, # Significance level is 5%\n  df1 = df_SSR, # Numerator degrees of freedom\n  df2 = df_SSE\n) # Denominator degrees of freedom\nF_crit\n\n[1] 3.859975\n\nF_stat\n\n[1] 474.3349\n\n\nThe F_crit value can also be seen in a plot2:\n\nmosaic::pdist(\n  dist = \"f\",\n  q = F_crit,\n  df1 = df_SSR, df2 = df_SSE\n)\n\n\n\n\n\n\n\n[1] 0.95\n\n\nAny value of F more than the \\(F_{crit}\\) occurs with smaller probability than 0.05. Our F_stat is much higher than \\(F_{crit}\\), by orders of magnitude! And so we can say with confidence that rm has a significant effect on cmedv.\nThe value of R.squared is also calculated from the previously computed sums of squares:\n\\[\nR.squared = \\frac{SSR}{SST} = \\frac{SSY-SSE}{SST}\n\\tag{6}\\]\n\nr_squared &lt;- (SST - SSE) / SST\nr_squared\n\n[1] 0.484839\n\n# Also computable by\n# mosaic::rsquared(housing_lm)\n\nSo R.squared = 0.484839\nThe value of Slope and Intercept are computed using a maximum likelihood derivation and the knowledge that the means square error is a minimum at the optimum slope: for a linear model \\(y \\sim mx + c\\)\n\\[\nslope = \\frac{\\Sigma[(y - y_{mean})*(x - x_{mean})]}{\\Sigma(x - x_{mean})^2}\n\\]\n\n\n\n\n\n\nTip\n\n\n\nNote that the slope is equal to the ratio of the covariance of x and y to the variance of x.\n\n\nand\n\\[\nIntercept = y_{mean} - slope * x_{mean}\n\\]\n\nslope &lt;- mosaic::cov(cmedv ~ rm, data = housing) / mosaic::var(~rm, data = housing)\nslope\n\n[1] 9.09967\n\n##\nintercept &lt;- mosaic::mean(~cmedv, data = housing) - slope * mosaic::mean(~rm, data = housing)\nintercept\n\n[1] -34.65924\n\n\nSo, there we are! All of this is done for us by one simple formula, lm()!\n\n\nThere is a very neat package called ggstatsplot3 that allows us to plot very comprehensive statistical graphs. Let us quickly do this:\n\nlibrary(ggstatsplot)\nhousing_lm %&gt;%\n  ggstatsplot::ggcoefstats(\n    title = \"Linear Model for Boston Housing\",\n    subtitle = \"Using ggstatsplot\"\n  )\n\n\n\n\n\n\n\nThis chart shows the estimates for the intercept and rm along with their error bars, the t-statistic, degrees of freedom, and the p-value.\nWe can also obtain crisp-looking model tables from the new supernova package 4, which is based on the methods discussed in Judd et al.\nlibrary(supernova)\nsupernova::supernova(housing_lm)\n\n\n\n Analysis of Variance Table (Type III SS)\n Model: cmedv ~ rm\n\n                                SS  df        MS       F   PRE     p\n ----- --------------- | --------- --- --------- ------- ----- -----\n Model (error reduced) | 20643.347   1 20643.347 474.335 .4848 .0000\n Error (from model)    | 21934.392 504    43.521                    \n ----- --------------- | --------- --- --------- ------- ----- -----\n Total (empty model)   | 42577.739 505    84.312                    \n\n\n\nThis table is very neat in that it gives the Sums of Squares for both the NULL (empty) model, and the current model for comparison. The PRE entry is the Proportional Reduction in Error, a measure that is identical with r.squared, which shows how much the model reduces the error compared to the NULL model(48%). The PRE idea is nicely discussed in Judd et al Section 10.\n\n\n\n\n Workflow: Model Checking and Diagnostics\nWe will follow much of the treatment on Linear Model diagnostics, given here on the STHDA website.\n\nA first step of this regression diagnostic is to inspect the significance of the regression beta coefficients, as well as, the R.square that tells us how well the linear regression model fits to the data.\nFor example, the linear regression model makes the assumption that the relationship between the predictors (x) and the outcome variable is linear. This might not be true. The relationship could be polynomial or logarithmic.\nAdditionally, the data might contain some influential observations, such as outliers (or extreme values), that can affect the result of the regression.\nTherefore, the regression model must be closely diagnosed in order to detect potential problems and to check whether the assumptions made by the linear regression model are met or not. To do so, we generally examine the distribution of residuals errors, that can tell us more about our data.\n\n\n Workflow: Checks for Uncertainty\nLet us first look at the uncertainties in the estimates of slope and intercept. These are most easily read off from the broom::tidy-ed model:\n\n# housing_lm_tidy &lt;-  housing_lm %&gt;% broom::tidy()\nhousing_lm_tidy\n\n\n  \n\n\n\nPlotting this is simple too:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nhousing_lm_tidy %&gt;%\n  gf_col(estimate ~ term, fill = ~term, width = 0.25) %&gt;%\n  gf_hline(yintercept = 0) %&gt;%\n  gf_errorbar(conf.low + conf.high ~ term,\n    width = 0.1,\n    title = \"Model Bar Plot for Estimates with Confidence Intervals\"\n  ) %&gt;%\n  gf_theme(theme = theme_custom())\n##\nhousing_lm_tidy %&gt;%\n  gf_pointrange(estimate + conf.low + conf.high ~ term,\n    title = \"Model Point-Range Plot for Estimates with Confidence Intervals\"\n  ) %&gt;%\n  gf_hline(yintercept = 0) %&gt;%\n  gf_theme(theme = theme_custom())\n\n\n\n\n\n\n\n\n\n\nThe point-range plot helps to avoid what has been called “within-the-bar bias”. The estimate is just a value, which we might plot as a bar or as a point, with uncertainty error-bars.\nValues within the bar are not more likely!! This is the bias that the point-range plot avoids.\n\n Checks for Constant Variance/Heteroscedasticity\nLinear Modelling makes 4 fundamental assumptions:(“LINE”)\n\n\nLinear relationship between y and x\nObservations are independent.\nResiduals are normally distributed\nVariance of the y variable is equal at all values of x.\n\nWe can check these using checks and graphs: Here we plot the residuals against the independent/feature variable and see if there is a gross variation in their range\nhousing_lm_augment %&gt;%\n  gf_point(.resid ~ .fitted, title = \"Residuals vs Fitted\") %&gt;%\n  gf_smooth(method = \"loess\")\nhousing_lm_augment %&gt;%\n  gf_hline(yintercept = 0, colour = \"grey\", linewidth = 2) %&gt;%\n  gf_point(.resid ~ cmedv, title = \"Residuals vs Target Variable\")\nhousing_lm_augment %&gt;%\n  gf_dhistogram(~.resid, title = \"Histogram of Residuals\") %&gt;%\n  gf_fitdistr()\nhousing_lm_augment %&gt;%\n  gf_qq(~.resid, title = \"Q-Q Residuals\") %&gt;%\n  gf_qqline()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Q-Q plot of residuals also has significant deviations from the normal quartiles. The residuals are not quite “like the night sky”, i.e. random enough. These point to the need for a richer model, with more predictors. The “trend line” of residuals vs predictors show a U-shaped pattern, indicating significant nonlinearity: there is a curved relationship in the graph. The solution can be a nonlinear transformation of the predictor variables, such as \\(\\sqrt(X)\\), \\(log(X)\\), or even \\(X^2\\). For instance, we might try a model for cmedv using \\(rm^2\\) instead of just rm as we have done. This will still be a linear model!\n\n\n\n\n\n\nTip\n\n\n\nBase R has a crisp command to plot these diagnostic graphs. But we will continue to use ggformula.\nplot(housing_lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne of the ggplot extension packages named lindia also has a crisp command to plot these diagnostic graphs.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nlibrary(lindia)\ngg_diagnose(housing_lm,\n  mode = \"base_r\", # plots like those with base-r\n  theme = theme(\n    axis.title = element_text(size = 6, face = \"bold\"),\n    title = element_text(size = 8)\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\nThe r-squared for a model lm(cmedv ~ rm^2) shows some improvement:\n\n\n[1] 0.5501221",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#extras",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#extras",
    "title": "Modelling with Linear Regression",
    "section": "Extras",
    "text": "Extras\n\n\n\n\n\n\nNoteMultiple Regression\n\n\n\nIt is also possible that there is more than one explanatory variable: this is multiple regression.\n\\[\ny = \\beta_0 + \\beta_1*x_1 + \\beta_2*x_2 ...+ \\beta_n*x_n\n\\tag{7}\\]\nwhere each of the \\(\\beta_i\\) are slopes defining the relationship between y and \\(x_i\\). Note that this is a vector dot-product, or inner-product, taken with a vector of input variables \\(x_i\\) and a vector of weights, \\(\\beta_i\\). Together, the RHS of that equation defines an n-dimensional hyperplane. The model is linear in the parameters \\(\\beta_i\\), e.g. these are OK:\n\\[\n\\color{black}{\n\\begin{cases}\n& y_i = \\pmb\\beta_0 + \\pmb\\beta_1x_1 + \\pmb\\beta_2x_1^2 + \\epsilon_i\\\\\n& y_1 = \\pmb\\beta_0 + \\pmb\\gamma_1\\pmb\\delta_1x_1 + exp(\\pmb\\beta_2)x_2+ \\epsilon_i\\\\\n\\end{cases}\n}\n\\]\nbut not, for example, these:\n\\[\n\\color{red}{\n\\begin{cases}\n& y_i = \\pmb\\beta_0 + \\pmb\\beta_1x_1^{\\beta_2} + \\epsilon_i\\\\\n& y_i = \\pmb\\beta_0 + exp(\\pmb\\beta_1x_1) + \\epsilon_i\\\\\n\\end{cases}\n}\n\\]\n\n\nThere are three ways5 to include more predictors:\n\n\nBackward Selection: We would typically start with a maximal model6 and progressively simplify the model by knocking off predictors that have the least impact on model accuracy.\n\nForward Selection: Start with no predictors and systematically add them one by one to increase the quality of the model\n\nMixed Selection: Wherein we start with no predictors and add them to gain improvement, or remove them at as their significance changes based on other predictors that have been added.\n\nThe first two are covered in the other tutorials above; Mixed Selection we will leave for a more advanced course. But for now we will first use just one predictor rm(Avg. no. of Rooms) to model housing prices.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#conclusions",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#conclusions",
    "title": "Modelling with Linear Regression",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have seen how starting from a basic EDA of the data, we have been able to choose a single Quantitative predictor variable to model a Quantitative target variable, using Linear Regression. As stated earlier, we may have wish to use more than one predictor variables, to build more sophisticated models with improved prediction capability. And there is more than one way of selecting these predictor variables, which we will examine in the Tutorials.\nSecondly, sometimes it may be necessary to mathematically transform the variables in the dataset to enable the construction of better models, something that was not needed here.\nWe may also encounter cases where the predictor variables seem to work together; one predictor may influence “how well” another predictor works, something called an interaction effect or a synergy effect. We might then have to modify our formula to include interaction terms that look like \\(predictor1 \\times predictor2\\).\nSo our Linear Modelling workflow might look like this: we have not seen all stages yet, but that is for another course module or tutorial!\n\n\n\nOur Linear Regression WorkflowDataEDACheck RelationshipsBuild ModelTransform VariablesTry Multiple Regression and/or interaction effectsCheck Model DiagnosticsCheck R^2Interpret ModelApply ModelSimple orComplex Model DecisionIs the Model Possible?inspectggformulaglimpseskimcorrplotcorrgramggformula + purrrcor.testAll GoodInadequate11 Still Inadequate22Check R^2",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#sec-references",
    "title": "Modelling with Linear Regression",
    "section": "\n References",
    "text": "References\n\n\nhttps://mlu-explain.github.io/linear-regression/\n\nThe Boston Housing Dataset, corrected version. StatLib @ CMU, lib.stat.cmu.edu/datasets/boston_corrected.txt\n\n\nhttps://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R\n\nAndrew Gelman, Jennifer Hill, Aki Vehtari. Regression and Other Stories, Cambridge University Press, 2023.Available Online\n\nMichael Crawley.(2013). The R Book,second edition. Chapter 11.\n\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Introduction to Statistical Learning, Springer, 2021. Chapter 3. https://www.statlearning.com/\n\nDavid C Howell, Permutation Tests for Factorial ANOVA Designs\n\nMarti Anderson, Permutation tests for univariate or multivariate analysis of variance and regression\n\n\nhttp://r-statistics.co/Assumptions-of-Linear-Regression.html\n\nJudd, Charles M., Gary H. McClelland, and Carey S. Ryan. 2017. “Introduction to Data Analysis.” In, 1–9. Routledge. https://doi.org/10.4324/9781315744131-1. Also see http://www.dataanalysisbook.com/index.html\n\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167,https://doi:10.21105/joss.03167\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nbroom\n1.0.8\nRobinson, Hayes, and Couch (2025)\n\n\ncorrgram\n1.14\nWright (2021)\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\ngeomtextpath\n0.1.5\nCameron and van den Brand (2025)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggstatsplot\n0.13.1\nPatil (2021)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\njanitor\n2.2.1\nFirke (2024)\n\n\nlindia\n0.10\nLee and Ventura (2023)\n\n\nreghelper\n1.1.2\nHughes and Beiner (2023)\n\n\nsupernova\n3.0.0\nBlake et al. (2024)\n\n\n\n\n\n\nBlake, Adam, Jeff Chrabaszcz, Ji Son, and Jim Stigler. 2024. supernova: Judd, McClelland, & Ryan Formatting for ANOVA Output. https://doi.org/10.32614/CRAN.package.supernova.\n\n\nCameron, Allan, and Teun van den Brand. 2025. geomtextpath: Curved Text in “ggplot2”. https://doi.org/10.32614/CRAN.package.geomtextpath.\n\n\nFirke, Sam. 2024. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://doi.org/10.32614/CRAN.package.janitor.\n\n\nHughes, Jeffrey, and David Beiner. 2023. reghelper: Helper Functions for Regression Analysis. https://doi.org/10.32614/CRAN.package.reghelper.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://doi.org/10.32614/CRAN.package.ISLR.\n\n\nLee, Yeuk Yu, and Samuel Ventura. 2023. lindia: Automated Linear Regression Diagnostic. https://doi.org/10.32614/CRAN.package.lindia.\n\n\nPatil, Indrajeet. 2021. “Visualizations with statistical details: The ‘ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.\n\n\nRobinson, David, Alex Hayes, and Simon Couch. 2025. broom: Convert Statistical Objects into Tidy Tibbles. https://doi.org/10.32614/CRAN.package.broom.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.\n\n\nWright, Kevin. 2021. corrgram: Plot a Correlogram. https://doi.org/10.32614/CRAN.package.corrgram.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/index.html#footnotes",
    "title": "Modelling with Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe model is linear in the parameters \\(\\beta_i\\), e.g. We can have this:↩︎\nMichael Crawley, The R Book, Third Edition 2023. Chapter 9. Statistical Modelling↩︎\nhttps://indrajeetpatil.github.io/ggstatsplot/reference/ggcoefstats.html↩︎\nhttps://github.com/UCLATALL/supernova↩︎\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Introduction to Statistical Learning, Springer, 2021. Chapter 3. Linear Regression. Available Online↩︎\nMichael Crawley, The R Book, Third Edition 2023. Chapter 9. Statistical Modelling↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Inferential Modelling",
      "Modelling with Linear Regression"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) # set to three decimal\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrgram)\nlibrary(corrplot)\nlibrary(broom)\n\n# datasets\nlibrary(ISLR)\n\n\n# Let us set a plot theme for Data visualization\n\ntheme_set(theme_light(base_size = 11, base_family = \"Roboto Condensed\"))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  plot.title = element_text(face = \"bold\"),\n  plot.title.position = \"plot\"\n)\n\nIn this tutorial, we will use the Boston housing Hitters dataset(s) from the ISLR package. Our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow do we predict the Salary of baseball players based on other Quantitative parameters such as Hits, HmRun AtBat?\nAnd how do we choose the “best” model, based on a trade-off between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#setting-up-r-packages",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) # set to three decimal\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(GGally)\nlibrary(corrgram)\nlibrary(corrplot)\nlibrary(broom)\n\n# datasets\nlibrary(ISLR)\n\n\n# Let us set a plot theme for Data visualization\n\ntheme_set(theme_light(base_size = 11, base_family = \"Roboto Condensed\"))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  plot.title = element_text(face = \"bold\"),\n  plot.title.position = \"plot\"\n)\n\nIn this tutorial, we will use the Boston housing Hitters dataset(s) from the ISLR package. Our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow do we predict the Salary of baseball players based on other Quantitative parameters such as Hits, HmRun AtBat?\nAnd how do we choose the “best” model, based on a trade-off between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-plan",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-plan",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow Plan",
    "text": "Workflow Plan\nOur target variable is Salary.\nWe will start with an examination of correlations between Salary and other Quant predictors.\nWe will use a null model for our Linear Regression at first, keeping just an intercept term. Based on the examination of the r-square improvement offered by each predictor individually, we will add another quantitative predictor. We will follow this process through up to a point where the gains in model accuracy are good enough to justify the additional model complexity.\n\n\n\n\n\n\nNote\n\n\n\nThis approach is the exact opposite of the earlier tutorial on multiple linear regression, where we started with a maximal model and trimmed it down based on an assessment of r.squared."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-eda",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-eda",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nThe Hitters dataset has the following variables:\n\ndata(\"Hitters\")\ninspect(Hitters)\n\n\ncategorical variables:  \n       name  class levels   n missing\n1    League factor      2 322       0\n2  Division factor      2 322       0\n3 NewLeague factor      2 322       0\n                                   distribution\n1 A (54.3%), N (45.7%)                         \n2 W (51.2%), E (48.8%)                         \n3 A (54.7%), N (45.3%)                         \n\nquantitative variables:  \n      name   class  min    Q1 median     Q3   max    mean      sd   n missing\n1    AtBat integer 16.0 255.2  379.5  512.0   687  380.93  153.40 322       0\n2     Hits integer  1.0  64.0   96.0  137.0   238  101.02   46.45 322       0\n3    HmRun integer  0.0   4.0    8.0   16.0    40   10.77    8.71 322       0\n4     Runs integer  0.0  30.2   48.0   69.0   130   50.91   26.02 322       0\n5      RBI integer  0.0  28.0   44.0   64.8   121   48.03   26.17 322       0\n6    Walks integer  0.0  22.0   35.0   53.0   105   38.74   21.64 322       0\n7    Years integer  1.0   4.0    6.0   11.0    24    7.44    4.93 322       0\n8   CAtBat integer 19.0 816.8 1928.0 3924.2 14053 2648.68 2324.21 322       0\n9    CHits integer  4.0 209.0  508.0 1059.2  4256  717.57  654.47 322       0\n10  CHmRun integer  0.0  14.0   37.5   90.0   548   69.49   86.27 322       0\n11   CRuns integer  1.0 100.2  247.0  526.2  2165  358.80  334.11 322       0\n12    CRBI integer  0.0  88.8  220.5  426.2  1659  330.12  333.22 322       0\n13  CWalks integer  0.0  67.2  170.5  339.2  1566  260.24  267.06 322       0\n14 PutOuts integer  0.0 109.2  212.0  325.0  1378  288.94  280.70 322       0\n15 Assists integer  0.0   7.0   39.5  166.0   492  106.91  136.85 322       0\n16  Errors integer  0.0   3.0    6.0   11.0    32    8.04    6.37 322       0\n17  Salary numeric 67.5 190.0  425.0  750.0  2460  535.93  451.12 263      59\n\n\n\n Scatter Plots and Correlations\nWe should examine scatter plots and Correlations of Salary against these variables. Let us select a few sets of Quantitative and Qualitative features, along with the target variable Salary and do a pairs-plots with them:\nHitters %&gt;%\n  select(Salary, AtBat, Hits, HmRun) %&gt;%\n  GGally::ggpairs(title = \"Plot 1\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;%\n  select(Salary, Runs, RBI, Walks, Years) %&gt;%\n  GGally::ggpairs(title = \"Plot 2\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;%\n  select(Salary, CRBI, CAtBat, CHits, CHmRun, CRuns, CWalks) %&gt;%\n  GGally::ggpairs(title = \"Plot 3\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;%\n  select(Salary, PutOuts, Assists, Errors) %&gt;%\n  GGally::ggpairs(title = \"Plot 4\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\nHitters %&gt;%\n  select(Salary, League, Division, NewLeague) %&gt;%\n  GGally::ggpairs(title = \"Plot 5\", lower = list(continuous = wrap(\"smooth\", alpha = 0.2)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAtBat and Hits seem relevant predictors for Salary. So are Runs, RBI,Walks, and Years. From Plot 2, both RBI and Walks are also inter-correlated with Runs. All the C* variables are well correlated with Salary and also among one another. (Plot3). Plot 4 has no significant correlations at all. Plot 5 shows Salary nearly equally distributed across League, Division, and NewLeague.\n\n Correlation Error-Bars\nWe can also plot all correlations in one graph using cor.test and purrr:\nall_corrs &lt;-\n  Hitters %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off Salary and year to get all the remaining ones\n  select(-Salary) %&gt;%\n  # perform a cor.test for all variables against Salary\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, Hitters$Salary)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\") %&gt;%\n  arrange(desc(estimate))\n\nall_corrs\nall_corrs %&gt;%\n  gf_hline(\n    yintercept = 0,\n    linewidth = 2,\n    color = \"grey\"\n  ) %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~estimate,\n    linewidth = ~ -log10(p.value),\n    width = 0.5,\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = NULL, y = \"Correlation with Salary\") %&gt;%\n  # gf_theme(theme = my_theme()) %&gt;%\n  gf_refine(\n    scale_colour_distiller(\"Correlation\",\n      type = \"div\",\n      palette = \"RdBu\"\n    ),\n    scale_linewidth_continuous(\"Significance\", range = c(0.25, 3))\n  ) %&gt;%\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    theme(axis.text.x = element_text(hjust = 1))\n  ) %&gt;%\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    coord_flip()\n  )\n\n\n\n\n  \n\n\n\n\n\n\n\nThere are a good many predictors which have statistically significant correlations with Salary, such as CRuns , CHmRun. The darker the colour, the higher is the correlation score; the fatter the bar, the higher is the significance of the correlation.\nWe now start with setting up simple Linear Regressions with no predictors, only an intercept. We then fit separate Linear Models using each predictor individually. Then based on the the improvement in r.squared offered by each predictor, we progressively add it to the model, until we are “satisfied” with the quality of the model ( using rsquared and other means).\nLet us now do this."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-minimal-multiple-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-minimal-multiple-regression-model",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Minimal Multiple Regression Model",
    "text": "Workflow: Minimal Multiple Regression Model\nNote the formula structure here: we want just and intercept.\n\nlm_min &lt;- lm(data = Hitters, Salary ~ 1)\nsummary(lm_min)\n\n\nCall:\nlm(formula = Salary ~ 1, data = Hitters)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -468   -346   -111    214   1924 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    535.9       27.8    19.3   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 451 on 262 degrees of freedom\n  (59 observations deleted due to missingness)\n\n\nlm_min %&gt;% broom::tidy()\nlm_min %&gt;% broom::glance()\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nOK, so the intercept is highly significant, the t-statistic is also high, but the intercept contributes nothing to the r.squared!! It is of no use at all!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round1",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round1",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Predictor Addition (Round#1)",
    "text": "Workflow: Predictor Addition (Round#1)\nWe will now set up individual models for each predictor and look at the p.value and r.squared offered by each separate model:\nnames &lt;- names(Hitters %&gt;%\n  select(\n    where(is.numeric),\n    -c(Salary)\n  ))\n\nn_vars &lt;- length(names)\n\nHitters_model_set &lt;- tibble(\n  all_vars = list(names),\n  keep_vars = seq(1, n_vars),\n  data = list(Hitters)\n)\n\n# Unleash purrr in a series of mutates\nHitters_model_set &lt;- Hitters_model_set %&gt;%\n  # Select Single Predictor for each Simple Model\n  mutate(\n    mod_vars =\n      pmap(\n        .l = list(all_vars, keep_vars, data),\n        .f = \\(all_vars, keep_vars, data) all_vars[keep_vars]\n      )\n  ) %&gt;%\n  # build formulae with these for linear regression\n  mutate(formula = map(\n    .x = mod_vars,\n    .f = \\(mod_vars) as.formula(paste(\n      \"Salary ~\", paste(mod_vars, collapse = \"+\")\n    ))\n  )) %&gt;%\n  # use the formulae to build multiple linear models\n  mutate(\n    models =\n      pmap(\n        .l = list(data, formula),\n        .f = \\(data, formula) lm(formula, data = data)\n      )\n  )\n\n\n# Tidy up the models using broom to expose their metrics\nHitters_model_set &lt;-\n  Hitters_model_set %&gt;%\n  mutate(\n    tidy_models =\n      map(\n        .x = models,\n        .f = \\(x) broom::glance(x,\n          conf.int = TRUE,\n          conf.lvel = 0.95\n        )\n      ),\n    predictor_name = names[keep_vars]\n  ) %&gt;%\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, predictor_name, tidy_models) %&gt;%\n  unnest(tidy_models) %&gt;%\n  arrange(desc(r.squared))\n\n# Check everything after the operation\nHitters_model_set\n# Plot r.squared vs predictor count\nHitters_model_set %&gt;%\n  gf_point(r.squared ~ reorder(predictor_name, r.squared),\n    size = 3.5,\n    color = \"black\",\n    ylab = \"R.Squared\",\n    xlab = \"Params in the Linear Model\", data = .\n  ) %&gt;%\n  # gf_theme(my_theme()) %&gt;%\n  gf_refine(theme(axis.text.x = element_text(\n    angle = 30,\n    hjust = 1\n  )))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n# Which is the winning Predictor?\nwinner &lt;- Hitters_model_set %&gt;%\n  arrange(desc(r.squared)) %&gt;%\n  select(predictor_name) %&gt;%\n  head(1) %&gt;%\n  as.character()\nwinner\n\n[1] \"CRBI\"\n\n\n# Here is the Round 1 Model\n# Minimal model updated to included winning predictor\nlm_round1 &lt;- update(lm_min, ~ . + CRBI)\nlm_round1 %&gt;% broom::tidy()\nlm_round1 %&gt;% broom::glance()\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nSo we can add CRBI as a predictor to our model as a predictor gives us an improved r.squared of \\(0.321\\), which is the square of the correlation between Salary and CRBI, \\(.567\\).\nAnd the model itself is: \\[\nSalary \\sim 274.580 + 0.791 \\times CRBI\n\\tag{1}\\]\nLet’s press on to Round 2."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round-2",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-predictor-addition-round-2",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Predictor Addition (Round #2)",
    "text": "Workflow: Predictor Addition (Round #2)\nWe will set up a round-2 model using CRBI as the predictor, and then proceed to add each of the other predictors as an update to the model.\n# Preliminaries\nnames &lt;- names(Hitters %&gt;%\n  select(where(is.numeric), -c(Salary, winner)))\n# names\n\nn_vars &lt;- length(names)\n# n_vars\n# names &lt;- names %&gt;% str_remove(winner)\n# names\n# n_vars &lt;- n_vars-1\n\n\n# Round 2 Iteration\nHitters_model_set &lt;- tibble(\n  all_vars = list(names),\n  keep_vars = seq(1, n_vars),\n  data = list(Hitters)\n)\n# Hitters_model_set\n\n# Unleash purrr in a series of mutates\nHitters_model_set &lt;- Hitters_model_set %&gt;%\n  # list of predictor variables for each model\n  mutate(\n    mod_vars =\n      pmap(\n        .l = list(all_vars, keep_vars, data),\n        .f = \\(all_vars, keep_vars, data) all_vars[keep_vars]\n      )\n  ) %&gt;%\n  # build formulae with these for linear regression\n  mutate(formula = map(\n    .x = mod_vars,\n    .f = \\(mod_vars) as.formula(paste(\n      \"Salary ~ CRBI +\", paste(mod_vars, collapse = \"+\")\n    ))\n  )) %&gt;%\n  # use the formulae to build multiple linear models\n  mutate(\n    models =\n      pmap(\n        .l = list(data, formula),\n        .f = \\(data, formula) lm(formula, data = data)\n      )\n  )\n# Check everything after the operation\n# Hitters_model_set\n\n# Tidy up the models using broom to expose their metrics\nHitters_model_set &lt;-\n  Hitters_model_set %&gt;%\n  mutate(\n    tidy_models =\n      map(\n        .x = models,\n        .f = \\(x) broom::glance(x,\n          conf.int = TRUE,\n          conf.lvel = 0.95\n        )\n      ),\n    predictor_name = names[keep_vars]\n  ) %&gt;%\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, predictor_name, tidy_models) %&gt;%\n  unnest(tidy_models) %&gt;%\n  arrange(desc(r.squared))\n\nHitters_model_set\n# Plot r.squared vs predictor count\nHitters_model_set %&gt;%\n  gf_point(r.squared ~ reorder(predictor_name, r.squared),\n    size = 3.5,\n    ylab = \"R.Squared\",\n    xlab = \"Param in the Linear Model\"\n  ) %&gt;%\n  # gf_theme(my_theme()) %&gt;%\n  gf_refine(theme(axis.text.x = element_text(\n    angle = 30,\n    hjust = 1\n  )))\n\n\n\n\n  \n\n\n\n\n\n\n\n# Which is the winning Predictor?\n#\nwinner &lt;- Hitters_model_set %&gt;%\n  arrange(desc(r.squared)) %&gt;%\n  select(predictor_name) %&gt;%\n  head(1) %&gt;%\n  as.character()\nwinner\n# Here is the Round 1 Model\nlm_round2 &lt;- update(lm_round1, ~ . + Hits)\nlm_round2 %&gt;% broom::tidy()\nlm_round2 %&gt;% broom::glance()\n\n\n\n[1] \"Hits\"\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nAnd now the model itself is: \\[\nSalary \\sim -47.96 + 0.691 \\times CRBI + 3.30 \\times Hits\n\\tag{2}\\]\nNote the change in both intercept and the slope for CRBI when the new predictor Hits is added!!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-visualization",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#workflow-visualization",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Workflow: Visualization",
    "text": "Workflow: Visualization\nLet us quickly see how this model might look. We know that with simple regression, we obtain a straight line as our model. Here, with two (or more) predictors, we should obtain a ….(hyper)plane! Play with the interactive plot below!"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#discussion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#discussion",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Discussion",
    "text": "Discussion\nIt is interesting that the second variable to be added was Hits which has a lower correlation of \\(r = 0.439\\) with Salary compared to some other Quant predictors such as Chits( \\(r = 0.525\\) ). This is because CRBI is hugely correlated with all of these predictors, so CRBI effectively acts as a proxy for all of these. See Plot 3.\nWe see that adding Hits to the model gives us an improved r.squared of \\(0.425\\)."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#conclusion",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe can proceed in this way to subsequent rounds and decide to stop when the model complexity (no. of predictors ) and the resulting gain in r.squared does not seem worth it.\n\n\n\n\n\n\nNoteAutomatic Iteration Method\n\n\n\nWe ought to convert the above code into an R function and run it that way for a specific number of rounds to see how things pan out. That is in the next version of this Tutorial! It appears that there is, what else, an R Package, called reghelper that allows us to do this! 😇 The reghelper::build_model() function can be used to:\n\nStart with only an intercept\n\nSequentially add each of the other predictor variables into the model “blocks”\n\nBlocks will be added in the order they are passed to the function, and variables from previous blocks will be included with each subsequent block, so they do not need to be repeated.\n\n\nType help(rehelper) in your Console.\nlibrary(reghelper)\n\nbig_model &lt;- build_model(\n  dv = Salary,\n  # Start with only an intercept lm(Salary ~ 1, data = .)\n\n  # Sequentially add each of the other predictor variables\n  # Pass through variable names (or interaction terms) to add for each block.\n  # To add one term to a block, just pass it through directly;\n  # to add multiple terms at a time to a block, pass it through in a vector or list.\n  # Interaction Terms can be specified using the vector/list\n  # Blocks will be added in the order they are passed to the function\n  # Variables from previous blocks will be included with each subsequent block,  so they do not need to be repeated.\n\n  1, AtBat, Hits, HmRun, Runs, RBI, Walks, Years, CAtBat, CHits,\n  CHmRun, CRuns, CRBI, CWalks, PutOuts, Assists, Errors,\n  data = Hitters,\n  model = \"lm\"\n)\n\n\n\nThis multiple model is a list object with 4 items. Type summary(big_model) in your Console.\nWe can clean it up a wee bit:\n\nlibrary(gt)\n# big_model has 4 parts: formulas, residuals, coefficients, overall\n\noverall_clean &lt;- summary(big_model)$overall %&gt;%\n  as_tibble() %&gt;%\n  janitor::clean_names()\n\nformulas_clean &lt;- summary(big_model)$formulas %&gt;%\n  as.character() %&gt;%\n  as_tibble() %&gt;%\n  rename(\"model_formula\" = value)\n\nall_models &lt;- cbind(formulas_clean, overall_clean) %&gt;%\n  dplyr::select(1, 2, 8)\nall_models %&gt;%\n  gt::gt() %&gt;%\n  tab_style(\n    style = cell_fill(color = \"grey\"),\n    locations = cells_body(rows = seq(1, 18, 2))\n  )\n\n\n\n\n\nmodel_formula\nr_squared\ndelta_r_sq\n\n\n\nSalary ~ 1\nNA\nNA\n\n\nSalary ~ 1 + AtBat\n0.156\nNA\n\n\nSalary ~ 1 + AtBat + Hits\n0.204\n0.047748\n\n\nSalary ~ 1 + AtBat + Hits + HmRun\n0.227\n0.023530\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs\n0.227\n0.000137\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI\n0.244\n0.016807\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks\n0.307\n0.062553\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years\n0.409\n0.101919\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat\n0.455\n0.046369\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits\n0.471\n0.016234\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun\n0.488\n0.016771\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns\n0.488\n0.000333\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI\n0.489\n0.001157\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks\n0.498\n0.008575\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + PutOuts\n0.522\n0.023739\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + PutOuts + Assists\n0.527\n0.005401\n\n\nSalary ~ 1 + AtBat + Hits + HmRun + Runs + RBI + Walks + Years + CAtBat + CHits + CHmRun + CRuns + CRBI + CWalks + PutOuts + Assists + Errors\n0.528\n0.000814\n\n\n\n\n\n\nSo we have a list of all models with main effects only. We could play with the build_model function to develop interaction models too! Slightly weird that the NULL model of Salary~1 does not show an r.squared value with build_model…??"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/forward-selection-1.html#references",
    "title": "Tutorial: Multiple Linear Regression with Forward Selection",
    "section": "\n References",
    "text": "References\n\nhttps://ethanwicker.com/2021-01-11-multiple-linear-regression-002/\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrgram\n1.14\nWright (2021)\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\ngt\n1.0.0\nIannone et al. (2025)\n\n\ninfer\n1.0.9\nCouch et al. (2021)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\njanitor\n2.2.1\nFirke (2024)\n\n\nreghelper\n1.1.2\nHughes and Beiner (2023)\n\n\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for Tidyverse-Friendly Statistical Inference.” Journal of Open Source Software 6 (65): 3661. https://doi.org/10.21105/joss.03661.\n\n\nFirke, Sam. 2024. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://doi.org/10.32614/CRAN.package.janitor.\n\n\nHughes, Jeffrey, and David Beiner. 2023. reghelper: Helper Functions for Regression Analysis. https://doi.org/10.32614/CRAN.package.reghelper.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, JooYoung Seo, Ken Brevoort, and Olivier Roy. 2025. gt: Easily Create Presentation-Ready Display Tables. https://doi.org/10.32614/CRAN.package.gt.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://doi.org/10.32614/CRAN.package.ISLR.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.\n\n\nWright, Kevin. 2021. corrgram: Plot a Correlogram. https://doi.org/10.32614/CRAN.package.corrgram."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) # set to three decimal\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\n\n\n# Let us set a plot theme for Data visualisation\n\n# my_theme &lt;- function(){  # Creating a function\n#   theme_classic() +  # Using pre-defined theme as base\n#   theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text\n#         axis.text.y = element_text(size = 12, face = \"bold\"),\n#         axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n#         panel.grid = element_blank(),  # Taking off the default grid\n#         plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n#         legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n#         legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n#         legend.position = \"right\",  # Customizing legend position\n#         plot.caption = element_text(size = 12))  # Customizing plot caption\n# }\n\nmy_theme &lt;- function() { # Creating a function\n  theme_classic() + # Using pre-defined theme as base\n    theme(\n      plot.title = element_text(face = \"bold\", size = 14),\n      axis.text.x = element_text(size = 10, face = \"bold\"),\n      # Customizing axes text\n      axis.text.y = element_text(size = 10, face = \"bold\"),\n      axis.title = element_text(size = 12, face = \"bold\"),\n      # Customizing axis title\n      panel.grid = element_blank(), # Taking off the default grid\n      plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n      legend.text = element_text(size = 8, face = \"italic\"),\n      # Customizing legend text\n      legend.title = element_text(size = 10, face = \"bold\"),\n      # Customizing legend title\n      legend.position = \"right\", # Customizing legend position\n      plot.caption = element_text(size = 8)\n    ) # Customizing plot caption\n}\n\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?\nAnd how do we choose the “best” model, based on a tradeoff between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#setting-up-r-packages",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "",
    "text": "options(scipen = 1, digits = 3) # set to three decimal\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(infer)\n\n\n# Let us set a plot theme for Data visualisation\n\n# my_theme &lt;- function(){  # Creating a function\n#   theme_classic() +  # Using pre-defined theme as base\n#   theme(axis.text.x = element_text(size = 12, face = \"bold\"),  # Customizing axes text\n#         axis.text.y = element_text(size = 12, face = \"bold\"),\n#         axis.title = element_text(size = 14, face = \"bold\"),  # Customizing axis title\n#         panel.grid = element_blank(),  # Taking off the default grid\n#         plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n#         legend.text = element_text(size = 12, face = \"italic\"),  # Customizing legend text\n#         legend.title = element_text(size = 12, face = \"bold\"),  # Customizing legend title\n#         legend.position = \"right\",  # Customizing legend position\n#         plot.caption = element_text(size = 12))  # Customizing plot caption\n# }\n\nmy_theme &lt;- function() { # Creating a function\n  theme_classic() + # Using pre-defined theme as base\n    theme(\n      plot.title = element_text(face = \"bold\", size = 14),\n      axis.text.x = element_text(size = 10, face = \"bold\"),\n      # Customizing axes text\n      axis.text.y = element_text(size = 10, face = \"bold\"),\n      axis.title = element_text(size = 12, face = \"bold\"),\n      # Customizing axis title\n      panel.grid = element_blank(), # Taking off the default grid\n      plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), units = , \"cm\"),\n      legend.text = element_text(size = 8, face = \"italic\"),\n      # Customizing legend text\n      legend.title = element_text(size = 10, face = \"bold\"),\n      # Customizing legend title\n      legend.position = \"right\", # Customizing legend position\n      plot.caption = element_text(size = 8)\n    ) # Customizing plot caption\n}\n\nIn this tutorial, we will use the Boston housing dataset. Our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nHow do we predict the price of a house in Boston, based on other parameters Quantitative parameters such as area, location, rooms, and crime-rate in the neighbourhood?\nAnd how do we choose the “best” model, based on a tradeoff between Model Complexity and Model Accuracy?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-read-the-data",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-read-the-data",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\n\ndata(\"BostonHousing2\", package = \"mlbench\")\nhousing &lt;- BostonHousing2\ninspect(housing)\n\n\ncategorical variables:  \n  name  class levels   n missing                                  distribution\n1 town factor     92 506       0 Cambridge (5.9%) ...                         \n2 chas factor      2 506       0 0 (93.1%), 1 (6.9%)                          \n\nquantitative variables:  \n      name   class       min       Q1   median       Q3      max     mean\n1    tract integer   1.00000 1303.250 3393.500 3739.750 5082.000 2700.356\n2      lon numeric -71.28950  -71.093  -71.053  -71.020  -70.810  -71.056\n3      lat numeric  42.03000   42.181   42.218   42.252   42.381   42.216\n4     medv numeric   5.00000   17.025   21.200   25.000   50.000   22.533\n5    cmedv numeric   5.00000   17.025   21.200   25.000   50.000   22.529\n6     crim numeric   0.00632    0.082    0.257    3.677   88.976    3.614\n7       zn numeric   0.00000    0.000    0.000   12.500  100.000   11.364\n8    indus numeric   0.46000    5.190    9.690   18.100   27.740   11.137\n9      nox numeric   0.38500    0.449    0.538    0.624    0.871    0.555\n10      rm numeric   3.56100    5.886    6.208    6.623    8.780    6.285\n11     age numeric   2.90000   45.025   77.500   94.075  100.000   68.575\n12     dis numeric   1.12960    2.100    3.207    5.188   12.127    3.795\n13     rad integer   1.00000    4.000    5.000   24.000   24.000    9.549\n14     tax integer 187.00000  279.000  330.000  666.000  711.000  408.237\n15 ptratio numeric  12.60000   17.400   19.050   20.200   22.000   18.456\n16       b numeric   0.32000  375.377  391.440  396.225  396.900  356.674\n17   lstat numeric   1.73000    6.950   11.360   16.955   37.970   12.653\n          sd   n missing\n1  1380.0368 506       0\n2     0.0754 506       0\n3     0.0618 506       0\n4     9.1971 506       0\n5     9.1822 506       0\n6     8.6015 506       0\n7    23.3225 506       0\n8     6.8604 506       0\n9     0.1159 506       0\n10    0.7026 506       0\n11   28.1489 506       0\n12    2.1057 506       0\n13    8.7073 506       0\n14  168.5371 506       0\n15    2.1649 506       0\n16   91.2949 506       0\n17    7.1411 506       0\n\n\nThe original data are 506 observations on 14 variables, medv being the target variable:\n\n\n\n\n\n\n\ncrim\nper capita crime rate by town\n\n\nzn\nproportion of residential land zoned for lots over 25,000 sq.ft\n\n\nindus\nproportion of non-retail business acres per town\n\n\nchas\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n\nnox\nnitric oxides concentration (parts per 10 million)\n\n\nrm\naverage number of rooms per dwelling\n\n\nage\nproportion of owner-occupied units built prior to 1940\n\n\ndis\nweighted distances to five Boston employment centres\n\n\nrad\nindex of accessibility to radial highways\n\n\ntax\nfull-value property-tax rate per USD 10,000\n\n\nptratio\npupil-teacher ratio by town\n\n\nb\n\n\\(1000(B - 0.63)^2\\) where B is the proportion of Blacks by town\n\n\nlstat\npercentage of lower status of the population\n\n\nmedv\nmedian value of owner-occupied homes in USD 1000’s\n\n\n\nThe corrected data set has the following additional columns:\n\n\ncmedv\ncorrected median value of owner-occupied homes in USD 1000’s\n\n\ntown\nname of town\n\n\ntract\ncensus tract\n\n\nlon\nlongitude of census tract\n\n\nlat\nlatitude of census tract\n\n\nOur response variable is cmedv, the corrected median value of owner-occupied homes in USD 1000’s. Their are many Quantitative feature variables that we can use to predict cmedv. And there are two Qualitative features, chas and tax."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-correlations",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-correlations",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Correlations",
    "text": "Workflow: Correlations\nWe can use purrr to evaluate all pair-wise correlations in one shot:\n\nall_corrs &lt;- housing %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off cmedv/medv to get all the remaining ones\n  select(-cmedv, -medv) %&gt;%\n  # perform a cor.test for all variables against cmedv\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, housing$cmedv)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nall_corrs\n\n\n  \n\n\nall_corrs %&gt;%\n  gf_hline(\n    yintercept = 0,\n    color = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    colour = ~estimate,\n    width = 0.5,\n    linewidth = ~ -log10(p.value),\n    caption = \"Significance = -log10(p.value)\"\n  ) %&gt;%\n  gf_point(estimate ~ reorder(predictor, estimate)) %&gt;%\n  gf_labs(x = \"Predictors\", y = \"Correlation with Median House Price\") %&gt;%\n  gf_theme(my_theme()) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1))) %&gt;%\n  gf_refine(\n    scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n    scale_linewidth_continuous(\"Significance\", range = c(0.25, 3)),\n    guides(linewidth = guide_legend(reverse = TRUE))\n  )\n\n\n\n\n\n\n\nThe variables rm, lstat seem to have high correlations with cmedv which are also statistically significant."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-maximal-multiple-regression-model",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-maximal-multiple-regression-model",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Maximal Multiple Regression Model",
    "text": "Workflow: Maximal Multiple Regression Model\nWe will create a regression model for cmedv using all the other numerical predictor features in the dataset.\n\nhousing_numeric &lt;- housing %&gt;% select(\n  where(is.numeric),\n\n  # remove medv\n  # an older version of cmedv\n  -c(medv)\n)\nnames(housing_numeric) # 16 variables, one target, 15 predictors\n\n [1] \"tract\"   \"lon\"     \"lat\"     \"cmedv\"   \"crim\"    \"zn\"      \"indus\"  \n [8] \"nox\"     \"rm\"      \"age\"     \"dis\"     \"rad\"     \"tax\"     \"ptratio\"\n[15] \"b\"       \"lstat\"  \n\nhousing_maximal &lt;- lm(cmedv ~ ., data = housing_numeric)\nsummary(housing_maximal)\n\n\nCall:\nlm(formula = cmedv ~ ., data = housing_numeric)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.934  -2.752  -0.619   1.711  26.120 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -3.45e+02   3.23e+02   -1.07  0.28734    \ntract       -7.52e-04   4.46e-04   -1.69  0.09231 .  \nlon         -6.79e+00   3.44e+00   -1.98  0.04870 *  \nlat         -2.35e+00   5.36e+00   -0.44  0.66074    \ncrim        -1.09e-01   3.28e-02   -3.32  0.00097 ***\nzn           4.40e-02   1.39e-02    3.17  0.00164 ** \nindus        2.75e-02   6.20e-02    0.44  0.65692    \nnox         -1.55e+01   4.03e+00   -3.85  0.00014 ***\nrm           3.81e+00   4.20e-01    9.07  &lt; 2e-16 ***\nage          5.82e-03   1.34e-02    0.43  0.66416    \ndis         -1.38e+00   2.10e-01   -6.59  1.1e-10 ***\nrad          2.36e-01   8.47e-02    2.78  0.00558 ** \ntax         -1.48e-02   3.74e-03   -3.96  8.5e-05 ***\nptratio     -9.49e-01   1.41e-01   -6.73  4.7e-11 ***\nb            9.55e-03   2.67e-03    3.57  0.00039 ***\nlstat       -5.46e-01   5.06e-02  -10.80  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.73 on 490 degrees of freedom\nMultiple R-squared:  0.743, Adjusted R-squared:  0.735 \nF-statistic: 94.3 on 15 and 490 DF,  p-value: &lt;2e-16\n\n\nThe maximal model has an R.squared of \\(0.7426\\) which is much better than that we obtained for a simple model based on rm alone. How much can we simplify this maximal model, without losing out on R.squared?"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-model-reduction",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-model-reduction",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Model Reduction",
    "text": "Workflow: Model Reduction\nWe now proceed naively by removing one predictor after another. We will resort to what may amount to p-hacking by sorting the predictors based on their p-value1 in the maximal model and removing them in decreasing order of their p-value.\nWe will also use some powerful features from the purrr package (also part of the tidyverse), to create all these models all at once. Then we will be able to plot their R.squared values together and decide where we wish to trade off Explainability vs Complexity for our model.\n\n# No of Quant predictor variables in the dataset\nn_vars &lt;- housing %&gt;%\n  select(where(is.numeric), -c(cmedv, medv)) %&gt;%\n  length()\n\n# Maximal Model, now tidied\nhousing_maximal_tidy &lt;-\n  housing_maximal %&gt;%\n  broom::tidy() %&gt;%\n  # Obviously remove \"Intercept\" ;-D\n  filter(term != \"(Intercept)\") %&gt;%\n  # And horrors! Sort variables by p.value\n  arrange(p.value)\n\nhousing_maximal_tidy\n\n\n  \n\n\n\nThe last 5 variables are clearly statistically insignificant.\nAnd now we unleash the purrr package to create all the simplified models at once. We will construct a dataset containing three columns:\n\nA list of all quantitative predictor variables\nA sequence of numbers from 1 to N(predictor)\n\nA “list” column containing the housing data frame itself\n\nWe will use the iteration capability of purrr to sequentially drop one variable at a time from the maximal(15 predictor) model, build a new reduced model each time, and compute the r.squared:\nhousing_model_set &lt;- tibble(\n  all_vars =\n    list(housing_maximal_tidy$term), # p-hacked order!!\n  keep_vars = seq(1, n_vars),\n  data = list(housing_numeric)\n)\nhousing_model_set\n# Unleash purrr in a series of mutates\nhousing_model_set &lt;- housing_model_set %&gt;%\n  # list of predictor variables for each model\n  mutate(\n    mod_vars =\n      pmap(\n        .l = list(all_vars, keep_vars, data),\n        .f = \\(all_vars, keep_vars, data) all_vars[1:keep_vars]\n      )\n  ) %&gt;%\n  # build formulae with these for linear regression\n  mutate(\n    formula =\n      map(\n        .x = mod_vars,\n        .f = \\(mod_vars) as.formula(paste(\n          \"cmedv ~\", paste(mod_vars, collapse = \"+\")\n        ))\n      )\n  ) %&gt;%\n  # use the formulae to build multiple linear models\n  mutate(\n    models =\n      pmap(\n        .l = list(data, formula),\n        .f = \\(data, formula) lm(formula, data = data)\n      )\n  )\n# Check everything after the operation\nhousing_model_set\n# Tidy up the models using broom to expose their metrics\nhousing_models_tidy &lt;- housing_model_set %&gt;%\n  mutate(\n    tidy_models =\n      map(\n        .x = models,\n        .f = \\(x) broom::glance(x,\n          conf.int = TRUE,\n          conf.lvel = 0.95\n        )\n      )\n  ) %&gt;%\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, tidy_models) %&gt;%\n  unnest(tidy_models)\n\nhousing_models_tidy %&gt;%\n  gf_line(\n    r.squared ~ keep_vars,\n    ylab = \"R.Squared\",\n    xlab = \"No. params in the Linear Model\",\n    title = \"Model Explainability vs Complexity\",\n    subtitle = \"Model r.squared vs No. of Predictors\",\n    data = .\n  ) %&gt;%\n  # Plot r.squared vs predictor count\n  gf_point(r.squared ~ keep_vars,\n    size = 3.5, color = \"grey\"\n  ) %&gt;%\n  # Show off the selected best model\n  gf_point(\n    r.squared ~ keep_vars,\n    size = 3.5,\n    color = \"red\",\n    data = housing_models_tidy %&gt;% filter(keep_vars == 4)\n  ) %&gt;%\n  gf_hline(yintercept = 0.7, linetype = \"dashed\") %&gt;%\n  gf_theme(my_theme())\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nAt the loss of some 5% in the r.squared, we can drop our model complexity from 15 predictors to say 4! Our final model will then be:\n\nhousing_model_final &lt;-\n  housing_model_set %&gt;%\n  # filter for best model, with 4 variables\n  filter(keep_vars == 4) %&gt;%\n  # tidy up the model\n  mutate(\n    tidy_models =\n      map(\n        .x = models,\n        .f = \\(x) broom::tidy(x,\n          conf.int = TRUE,\n          conf.lvel = 0.95\n        )\n      )\n  ) %&gt;%\n  # Remove unwanted columns, keep model and predictor count\n  select(keep_vars, models, tidy_models) %&gt;%\n  unnest(tidy_models)\n\nhousing_model_final\n\n\n  \n\n\nhousing_model_final %&gt;%\n  # And plot the model\n  # Remove the intercept term\n  filter(term != \"(Intercept)\") %&gt;%\n  gf_col(estimate ~ term, fill = ~term, width = 0.25) %&gt;%\n  gf_hline(yintercept = 0) %&gt;%\n  gf_errorbar(conf.low + conf.high ~ term,\n    width = 0.1,\n    title = \"Multiple Regression\",\n    subtitle = \"Model Estimates with Confidence Intervals\"\n  ) %&gt;%\n  gf_theme(my_theme())\n\n\n\n\n\n\n\nOur current best model can be stated as:\n\\[\n\\widehat{cmedv} \\sim 24.459 - 0.563 * dis - 0.673 * lstat - 0.957 * ptratio  + 4.199 * rm\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-diagnostics",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#workflow-diagnostics",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Workflow: Diagnostics",
    "text": "Workflow: Diagnostics\nLet us use broom::augment to calculate residuals and predictions to arrive at a quick set of diagnostic plots.\n\nhousing_model_final_augment &lt;-\n  housing_model_set %&gt;%\n  filter(keep_vars == 4) %&gt;%\n  # augment the model\n  mutate(\n    augment_models =\n      map(\n        .x = models,\n        .f = \\(x) broom::augment(x)\n      )\n  ) %&gt;%\n  unnest(augment_models) %&gt;%\n  select(cmedv:last_col())\n\nhousing_model_final_augment\n\n\n  \n\n\n\nhousing_model_final_augment %&gt;%\n  gf_point(.resid ~ .fitted, title = \"Residuals vs Fitted\") %&gt;%\n  gf_smooth() %&gt;%\n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;%\n  gf_qq(~.std.resid, title = \"Q-Q Residuals\") %&gt;%\n  gf_qqline() %&gt;%\n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;%\n  gf_point(sqrt(.std.resid) ~ .fitted,\n    title = \"Scale-Location Plot\"\n  ) %&gt;%\n  gf_smooth() %&gt;%\n  gf_theme(my_theme)\nhousing_model_final_augment %&gt;%\n  gf_point(.std.resid ~ .hat,\n    title = \"Residuals vs Leverage\"\n  ) %&gt;%\n  gf_smooth() %&gt;%\n  gf_theme(my_theme)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe residuals plot shows a curved trend, and certainly does not resemble the stars at night, so it is possible that we have left out some possible richness in our model-making, a “systemic inadequacy”.\nThe Q-Q plot of residuals also shows a J-shape which indicates a non-normal distribution of residuals.\nThese could indicate that more complex model ( e.g. linear model with interactions between variables ( i.e. product terms ) may be necessary."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#conclusion",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#conclusion",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have used a multiple-regression workflow that takes all predictor variables into account in a linear model, and then systematically simplified that model such that the performance was just adequate.\nThe models we chose were all linear of course, but without interaction terms : each predictor was used only for its main effect. When the diagnostic plots were examined, we did see some shortcomings in the model. This could be overcome with a more complex model. These might include selected interactions, transformations of target(\\(cmedv^2\\), or \\(sqrt(cmedv)\\)) and some selected predictors."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#references",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#references",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "\n References",
    "text": "References\n\nJames, Witten, Hastie, Tibshirani, An Introduction to Statistical Learning. Chapter 3. Linear Regression. https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrgram\n1.14\nWright (2021)\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\ngt\n1.0.0\nIannone et al. (2025)\n\n\ninfer\n1.0.9\nCouch et al. (2021)\n\n\nISLR\n1.4\nJames et al. (2021)\n\n\njanitor\n2.2.1\nFirke (2024)\n\n\nreghelper\n1.1.2\nHughes and Beiner (2023)\n\n\n\n\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for Tidyverse-Friendly Statistical Inference.” Journal of Open Source Software 6 (65): 3661. https://doi.org/10.21105/joss.03661.\n\n\nFirke, Sam. 2024. janitor: Simple Tools for Examining and Cleaning Dirty Data. https://doi.org/10.32614/CRAN.package.janitor.\n\n\nHughes, Jeffrey, and David Beiner. 2023. reghelper: Helper Functions for Regression Analysis. https://doi.org/10.32614/CRAN.package.reghelper.\n\n\nIannone, Richard, Joe Cheng, Barret Schloerke, Ellis Hughes, Alexandra Lauer, JooYoung Seo, Ken Brevoort, and Olivier Roy. 2025. gt: Easily Create Presentation-Ready Display Tables. https://doi.org/10.32614/CRAN.package.gt.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Rob Tibshirani. 2021. ISLR: Data for an Introduction to Statistical Learning with Applications in r. https://doi.org/10.32614/CRAN.package.ISLR.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.\n\n\nWright, Kevin. 2021. corrgram: Plot a Correlogram. https://doi.org/10.32614/CRAN.package.corrgram."
  },
  {
    "objectID": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#footnotes",
    "href": "content/courses/Analytics/Modelling/Modules/LinReg/files/backward-selection-1.html#footnotes",
    "title": "Tutorial: Multiple Linear Regression with Backward Selection",
    "section": "Footnotes",
    "text": "Footnotes\n\nJames, Witten, Hastie, Tibshirani,An Introduction to Statistical Learning. Chapter 3. Linear Regression https://hastie.su.domains/ISLR2/Labs/Rmarkdown_Notebooks/Ch3-linreg-lab.html↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html",
    "title": "Using FlexDashboard in R",
    "section": "",
    "text": "R Tutorial"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html#references",
    "href": "content/courses/Analytics/Workflow/Modules/10-Reports-flextable/index.html#references",
    "title": "Using FlexDashboard in R",
    "section": "References",
    "text": "References\n\nFlexdashboard Basics https://rstudio.github.io/flexdashboard/articles/flexdashboard.html\nFlexdashboard Examples https://rstudio.github.io/flexdashboard/articles/examples.html\nShannon Haymond,Create laboratory business intelligence dashboards for free using R: A tutorial using the flexdashboard package, Journal of Mass Spectrometry and Advances in the Clinical Lab, Volume 23, 2022,Pages 39-43, ISSN 2667-145X, https://doi.org/10.1016/j.jmsacl.2021.12.002.\nhttps://posit.co/blog/flexdashboard-easy-interactive-dashboards-for-r/"
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html",
    "title": "\n Facing the Abyss",
    "section": "",
    "text": "So you have your shiny new R skills and you’ve successfully loaded a cool dataframe into R… Now what?\nThe best charts come from understanding your data, asking good questions from it, and displaying the answers to those questions as clearly as possible.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#a-data-analytics-process",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#a-data-analytics-process",
    "title": "\n Facing the Abyss",
    "section": "",
    "text": "So you have your shiny new R skills and you’ve successfully loaded a cool dataframe into R… Now what?\nThe best charts come from understanding your data, asking good questions from it, and displaying the answers to those questions as clearly as possible.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#set-up-your-project",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#set-up-your-project",
    "title": "\n Facing the Abyss",
    "section": "\n Set up your Project",
    "text": "Set up your Project\n\nCreate a new Project in RStudio. File -&gt; New Project -&gt; Quarto Blog\nCreate a new Quarto document: all your Quarto documents should be in the posts/ folder. See the samples therein to get an idea.\nSave the document with a meaningful name, e.g. EDA-Workflow.qmd\n\nCreate a new folder in the Project for your data files, e.g. data/. This can be at the inside the posts/ folder.\nStore all datasets within this folder, and refer to them with relative paths, e.g. ../data/mydata.csv in any other Quarto document in the Project. (../ means “go up one level from the current folder”.)\n\nNow edit the `.qmd file which you are editing for this report to include the following sections, YAML, code chunks, and text as needed.\n\n\n\n\n\n\nNoteDownload this document as a Work Template\n\n\n\nHit the &lt;/&gt;Code button at upper right to copy/save this very document as a Quarto Markdown template for your work. Delete the text that you don’t need, but keep most of the Sections as they are!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#setting-up-r-packages",
    "title": "\n Facing the Abyss",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nInstall packages using install.packages() in your Console.\nLoad up your libraries in a setup chunk:\nAdd knitr options to your YAML header, so that all your plots are rendered in high quality PNG format.\n\ntitle: \"My Document\"\nformat: html\nknitr:\n  opts_chunk:\n    dev: \"ragg_png\"\n    \n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(ggridges)\nlibrary(skimr)\n##\nlibrary(GGally)\nlibrary(corrplot)\nlibrary(corrgram)\nlibrary(crosstable) # Summary stats tables\nlibrary(kableExtra)\n##\nlibrary(paletteer) # Colour Palettes for Peasants\n##\n## Add other packages here as needed, e.g.:\n## scales/ggprism;\n## ggstats/correlation;\n## vcd/vcdExtra/ggalluvial/ggpubr;\n## sf/tmap/osmplotr/rnaturalearth;\n## igraph/tidygraph/ggraph/graphlayouts;\n\n\n Themes and Fonts\nSet up a theme for your plots. This is a good time to set up your own theme, or use an existing one, e.g. ggprism, ggthemes, ggpubr, etc. If you have a Company logo, you can use that as a theme too.\n\nShow the Code# Chunk options\nknitr::opts_chunk$set(\n  fig.width = 7,\n  fig.asp = 0.618, # Golden Ratio\n  # out.width = \"80%\",\n  fig.align = \"center\"\n)\n### Ggplot Theme\n### https://rpubs.com/mclaire19/ggplot2-custom-themes\n### https://stackoverflow.com/questions/74491138/ggplot-custom-fonts-not-working-in-quarto\n\n# We have locally downloaded the `Alegreya` and `Roboto Condensed` fonts.\n# This ensures we are GDPR-compliant, and not using Google Fonts directly.\n# Let us import these local fonts into our session and use them to define our ggplot theme.\nlibrary(sysfonts)\n\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\n\n\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = \"Alegreya\"), # set default font family for all text\n\n      # text elements\n      plot.title = element_text( # title\n        family = \"Alegreya\", # set font family\n        size = 18, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        vjust = 2\n      ), # raise slightly\n\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = \"Alegreya\", # font family\n        size = 14\n      ), # font size\n\n      plot.caption = element_text( # caption\n        family = \"Alegreya\", # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 10\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())\n\n\nUse Namespace based Code\n\n\n\n\n\n\nWarning\n\n\n\nTry always to name your code-command with the package from whence it came! So use dplyr::filter() / dplyr::summarize() and not just filter() or summarize(), since these commands could exist across multiple packages, which you may have loaded last.\n(One can also use the conflicted package to set this up, but this is simpler for beginners like us. )",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#read-data",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#read-data",
    "title": "\n Facing the Abyss",
    "section": "\n Read Data",
    "text": "Read Data\n\nUse readr::read_csv(); or data(...) if the data is in a package\n\n\ndata(penguins, package = \"palmerpenguins\")",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#examine-data",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#examine-data",
    "title": "\n Facing the Abyss",
    "section": "\n Examine Data",
    "text": "Examine Data\n\nUse dplyr::glimpse()\n\nUse mosaic::inspect() or skimr::skim()\n\nUse dplyr::summarise() and crosstable::crosstable()\n\nFormat your tables with knitr::kable()\n\nHighlight any interesting summary stats or data imbalances\n\n\ndplyr::glimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\nskimr::skim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#data-dictionary-and-experiment-description",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#data-dictionary-and-experiment-description",
    "title": "\n Facing the Abyss",
    "section": "\n Data Dictionary and Experiment Description",
    "text": "Data Dictionary and Experiment Description\n\n\nData Dictionary: A table containing the variable names, their interpretation, and their nature(Qual/Quant/Ord…)\nIf there are wrongly coded variables in the original data, state them in their correct form, so you can munge the in the next step\nDeclare what might be target and predictor variables, based on available information of the experiment, or a description of the data.\n\n\n\n\n\n\n\nNoteQualitative Variables\n\n\n\n\nCategorical variables, e.g. species, island, sex\n\nUse dplyr::count() to get counts of each category\n\n\n\n\n\n\n\n\n\nNoteQuantitative Variables\n\n\n\n\nContinuous variables, e.g. body_mass_g, flipper_length_mm, bill_length_mm\n\nUse dplyr::summarise() to get summary statistics of each variable",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#data-munging",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#data-munging",
    "title": "\n Facing the Abyss",
    "section": "\n Data Munging",
    "text": "Data Munging\n\nConvert variables to factors as needed\nReformat / Rename other variables as needed\nClean badly formatted columns (e.g. text + numbers) using tidyr::separate_**_**()\n\nSave the data as a modified file\nDo not mess up the original data file\n\n\n```{r}\n#| label: data-munging\n#| eval: false\n\ndataset_modified &lt;- data %&gt;%\n  dplyr::mutate(across(where(is.character), as.factor))\n# And so on\n```\n\nMunge the variables separately if you need to specify factor labels and levels for each variable.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#form-hypotheses",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#form-hypotheses",
    "title": "\n Facing the Abyss",
    "section": "\n Form Hypotheses",
    "text": "Form Hypotheses\nQuestion-1\n\nState the Question or Hypothesis\n(Temporarily) Drop variables using dplyr::select()\n\nCreate new variables if needed with dplyr::mutate()\n\nFilter the data set using dplyr::filter()\n\nReformat data if needed with tidyr::pivot_longer() or tidyr::pivot_wider()\n\nAnswer the Question with a Table, a Chart, a Test, using an appropriate Model for Statistical Inference\nUse title, subtitle, legend and scales appropriately in your chart\nPrefer ggformula unless you are using a chart that is not yet supported therein (eg. ggbump() or plot_likert())\n\n\n## Set graph theme\n## Idiotic that we have to repeat this every chunk\n## Open issue in Quarto\ntheme_set(new = theme_custom())\n\npenguins %&gt;%\n  tidyr::drop_na() %&gt;%\n  gf_point(body_mass_g ~ flipper_length_mm,\n    colour = ~species\n  ) %&gt;%\n  gf_labs(\n    title = \"My First Penguins Plot\",\n    subtitle = \"Using ggformula with fonts\",\n    x = \"Flipper Length mm\", y = \"Body Mass gms\",\n    caption = \"I love penguins, and R\"\n  )\n\n\n\n\n\n\n\nInference-1\n. . . .\nQuestion-n\n….\nInference-n\n….",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#one-most-interesting-graph",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#one-most-interesting-graph",
    "title": "\n Facing the Abyss",
    "section": "\n One Most Interesting Graph",
    "text": "One Most Interesting Graph",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#conclusion",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#conclusion",
    "title": "\n Facing the Abyss",
    "section": "\n Conclusion",
    "text": "Conclusion\nDescribe what the graph shows and why it so interesting. What could be done next?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#references",
    "href": "content/courses/Analytics/Workflow/Modules/200-EDA-Workflow/index.html#references",
    "title": "\n Facing the Abyss",
    "section": "\n References",
    "text": "References\n\nhttps://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/\nColour Palettes\n\nOver 2500 colour palettes are available in the paletteer package. Can you find tayloRswift? wesanderson? harrypotter? timburton? You could also find/define palettes that are in line with your Company’s logo / colour schemes.\n Here are the Qualitative Palettes: (searchable) \n\n\n\n\n\n\n And the Quantitative/Continuous palettes: (searchable) \n\n\n\n\n\n\n Use the commands:\n\n## For Qual variable-&gt; colour/fill:\nscale_colour_paletteer_d(\n  name = \"Legend Name\",\n  palette = \"package::palette\",\n  dynamic = TRUE / FALSE\n)\n\n## For Quant variable-&gt; colour/fill:\nscale_colour_paletteer_c(\n  name = \"Legend Name\",\n  palette = \"package::palette\",\n  dynamic = TRUE / FALSE\n)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Workflow",
      "<iconify-icon icon=\"guidance:falling-rocks\" width=\"1.2em\" height=\"1.2em\"></iconify-icon><iconify-icon icon=\"game-icons:falling\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Facing the Abyss"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html",
    "href": "content/courses/Analytics/Inference/listing.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Important\n\n\n\nStatistical inference is the process of drawing conclusions about the entire population based on the information in a sample.\n\n\nIn this Section we will examine samples from populations and find procedures for estimating parameters such as means and sd. We will also devise procedures for comparing means and variances across more than one population. The conditions that make these procedures possible and accurate will also be studied and we will find alternative methods when those assumptions breakdown.\nBased on our ideas of data and types of variables, here is a table of what we may infer, based on the underlying data:\n\nData Types and Inference\n\n\n\n\n\n\n\n\nVariable(s)\nEstimating What?\nPopulation Parameter\nSample Statistic\n\n\n\n\nSingle Qual variable\nProportion\np\n\\(\\hat{p}\\)\n\n\nSingle Quant variable\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nTwo Qual Variables\nDifference in Proportions\n\\(p_1 -p_2\\)\n\\(\\hat{p_1} - \\hat{p_2}\\)\n\n\nOne Qual, one Quant\nDifference in Means\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x_1}-\\bar{x_2}\\)\n\n\nTwo Quant variables\nCorrelation\n\\(\\rho\\)\nr\n\n\n\nWe will examine inference procedures for all these cases.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#what-is-inference",
    "href": "content/courses/Analytics/Inference/listing.html#what-is-inference",
    "title": "Statistical Inference",
    "section": "",
    "text": "Important\n\n\n\nStatistical inference is the process of drawing conclusions about the entire population based on the information in a sample.\n\n\nIn this Section we will examine samples from populations and find procedures for estimating parameters such as means and sd. We will also devise procedures for comparing means and variances across more than one population. The conditions that make these procedures possible and accurate will also be studied and we will find alternative methods when those assumptions breakdown.\nBased on our ideas of data and types of variables, here is a table of what we may infer, based on the underlying data:\n\nData Types and Inference\n\n\n\n\n\n\n\n\nVariable(s)\nEstimating What?\nPopulation Parameter\nSample Statistic\n\n\n\n\nSingle Qual variable\nProportion\np\n\\(\\hat{p}\\)\n\n\nSingle Quant variable\nMean\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\nTwo Qual Variables\nDifference in Proportions\n\\(p_1 -p_2\\)\n\\(\\hat{p_1} - \\hat{p_2}\\)\n\n\nOne Qual, one Quant\nDifference in Means\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x_1}-\\bar{x_2}\\)\n\n\nTwo Quant variables\nCorrelation\n\\(\\rho\\)\nr\n\n\n\nWe will examine inference procedures for all these cases.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#an-idea-to-encourage-you-stats-lessons-from-sholay",
    "href": "content/courses/Analytics/Inference/listing.html#an-idea-to-encourage-you-stats-lessons-from-sholay",
    "title": "Statistical Inference",
    "section": "An Idea to Encourage You: Stats Lessons from Sholay!!",
    "text": "An Idea to Encourage You: Stats Lessons from Sholay!!\n\nGabbar: “Kitne Aadmi thay?\nStats Teacher: How many observations do you have? n &lt; 30 is a joke.\n\nGabbar: Kya Samajh kar aaye thay? Gabbar khus hoga? Sabaasi dega kya?\nStats Teacher: What are the levels in your Factors? Are they binary? Don’t do ANOVA just yet!\n\nGabbar: (Fires off three rounds ) Haan, ab theek hai!\nStats Teacher: Yes, now the dataset is balanced wrt the factor (Treatment and Control).\n\nGabbar: Is pistol mein teen zindagi aur teen maut bandh hai. Dekhte hain kisko kya milega.\nStats Teacher: This is our Research Question, for which we will Design an Experiment.\n\nGabbar: (Twirls the chambers of his revolver) “Hume kuchh nahi pataa!”\nStats Teacher: Let us perform a non-parametric Permutation Test for this Factor!\n\nGabbar: “Kamaal ho gaya!”\nStats Teacher: Fantastic! Our p-value is so small that we can reject the NULL Hypothesis!!\n\nGo and like this post at: https://www.linkedin.com/pulse/stat-lessons-from-sholay-arvind-venkatadri-wgtrf/?trackingId=c0b4UCTLRea6U%2Bj%2Bm4TCtw%3D%3D",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#references",
    "href": "content/courses/Analytics/Inference/listing.html#references",
    "title": "Statistical Inference",
    "section": "References",
    "text": "References\n\nhttps://www.openintro.org/book/os/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/listing.html#modules",
    "href": "content/courses/Analytics/Inference/listing.html#modules",
    "title": "Statistical Inference",
    "section": "Modules",
    "text": "Modules",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(vcd)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#setting-up-r-packages",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggmosaic) # plotting mosaic plots for Categorical Data\n\n### Dataset from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\nlibrary(vcd)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#introduction",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Introduction",
    "text": "Introduction\nMany experiments gather qualitative data across different segments of a population, for example, opinion about a topic among people who belong to different income groups, or who live in different parts of a city. This should remind us of the Likert Plots that we plotted earlier. In this case the two variables, dependent and independent, are both Qualitative, and we can calculate counts and proportions.\nHow does one Qual variable affect the other? How do counts/proportions of the dependent variable vary with the levels of the independent variable? This is our task for this module.\nHere is a quick example of the kind of data we might look at here, taken from the British Medical Journal:\n\n\n\n\n\nFigure 1: Breast Feeding\n\n\nClearly, we can see differences in counts/proportions of women who breast-fed their babies for three months or more, based on whether they were “printers wives” or “farmers’ wives”!\nIs there a doctor in the House?\n\n The CLT for Two Proportions\nWe first need to establish some model assumptions prior to making our analysis. As before, we wish to see if the CLT applies here, and if so, in what form. The difference between two proportions \\(\\hat{p_1}-\\hat{p_2}\\) can be modeled using a normal distribution when:\n\n\nIndependence (extended): The data are independent within and between the two groups. Generally this is satisfied if the data come from two independent random samples or if the data come from a randomized experiment.\n\nSuccess-failure condition: The success-failure condition holds for both groups, where we check successes and failures in each group separately. That is, we should have at least 10 successes and 10 failures in each of the two groups.\n\nWhen these conditions are satisfied, the standard error of \\(\\hat{p_1}-\\hat{p_2}\\) is well-approximated by:\n\\[\nSE(\\hat{p_1}-\\hat{p_2}) = \\sqrt{\\frac{\\hat{p_1}*(1-\\hat{p_1})}{n_1}} + \\sqrt{\\frac{\\hat{p_2}*(1-\\hat{p_2})}{n_2}}\n\\]\nwhere \\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) represent the sample proportions, and \\(n_1\\) and \\(n_2\\) represent the sample sizes.\nWe can represent the Confidence Intervals as:\n\\[\n\\begin{eqnarray}\nCI(p_1 - p_2) &=& (\\hat{p_1} - \\hat{p_2}) \\pm 1.96 * SE(\\hat{p_1}-\\hat{p_2})\\\\\n&=& (\\hat{p_1} - \\hat{p_2}) \\pm 1.96 * \\left(\\sqrt{\\frac{\\hat{p_1}*(1-\\hat{p_1})}{n_1}} + \\sqrt{\\frac{\\hat{p_2}*(1-\\hat{p_2})}{n_2}}\\right)\n\\end{eqnarray}\n\\]",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#case-study-1-gss2002-dataset",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#case-study-1-gss2002-dataset",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Case Study-1: GSS2002 dataset",
    "text": "Case Study-1: GSS2002 dataset\nWe saw how we could perform inference for a single proportion. We can extend this idea to multiple proportions too.\nLet us try a dataset with Qualitative / Categorical data. This is the General Social Survey GSS dataset from the resampledata package, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical ( both variables ), we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\ndata(GSS2002, package = \"resampledata\")\nglimpse(GSS2002)\n\nRows: 2,765\nColumns: 21\n$ ID            &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ Region        &lt;fct&gt; South Central, South Central, South Central, South Centr…\n$ Gender        &lt;fct&gt; Female, Male, Female, Female, Male, Male, Female, Female…\n$ Race          &lt;fct&gt; White, White, White, White, White, White, White, White, …\n$ Education     &lt;fct&gt; HS, Bachelors, HS, Left HS, Left HS, HS, Bachelors, HS, …\n$ Marital       &lt;fct&gt; Divorced, Married, Separated, Divorced, Divorced, Divorc…\n$ Religion      &lt;fct&gt; Inter-nondenominational, Protestant, Protestant, Protest…\n$ Happy         &lt;fct&gt; Pretty happy, Pretty happy, NA, NA, NA, Pretty happy, NA…\n$ Income        &lt;fct&gt; 30000-34999, 75000-89999, 35000-39999, 50000-59999, 4000…\n$ PolParty      &lt;fct&gt; \"Strong Rep\", \"Not Str Rep\", \"Strong Rep\", \"Ind, Near De…\n$ Politics      &lt;fct&gt; Conservative, Conservative, NA, NA, NA, Conservative, NA…\n$ Marijuana     &lt;fct&gt; NA, Not legal, NA, NA, NA, NA, NA, NA, Legal, NA, NA, NA…\n$ DeathPenalty  &lt;fct&gt; Favor, Favor, NA, NA, NA, Favor, NA, NA, Favor, NA, NA, …\n$ OwnGun        &lt;fct&gt; No, Yes, NA, NA, NA, Yes, NA, NA, Yes, NA, NA, NA, NA, N…\n$ GunLaw        &lt;fct&gt; Favor, Oppose, NA, NA, NA, Oppose, NA, NA, Oppose, NA, N…\n$ SpendMilitary &lt;fct&gt; Too little, About right, NA, About right, NA, Too little…\n$ SpendEduc     &lt;fct&gt; Too little, Too little, NA, Too little, NA, Too little, …\n$ SpendEnv      &lt;fct&gt; About right, About right, NA, Too little, NA, Too little…\n$ SpendSci      &lt;fct&gt; About right, About right, NA, Too little, NA, Too little…\n$ Pres00        &lt;fct&gt; Bush, Bush, Bush, NA, NA, Bush, Bush, Bush, Bush, NA, NA…\n$ Postlife      &lt;fct&gt; Yes, Yes, NA, NA, NA, Yes, NA, NA, Yes, NA, NA, NA, NA, …\n\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name  class levels    n missing\n1         Region factor      7 2765       0\n2         Gender factor      2 2765       0\n3           Race factor      3 2765       0\n4      Education factor      5 2760       5\n5        Marital factor      5 2765       0\n6       Religion factor     13 2746      19\n7          Happy factor      3 1369    1396\n8         Income factor     24 1875     890\n9       PolParty factor      8 2729      36\n10      Politics factor      7 1331    1434\n11     Marijuana factor      2  851    1914\n12  DeathPenalty factor      2 1308    1457\n13        OwnGun factor      3  924    1841\n14        GunLaw factor      2  916    1849\n15 SpendMilitary factor      3 1324    1441\n16     SpendEduc factor      3 1343    1422\n17      SpendEnv factor      3 1322    1443\n18      SpendSci factor      3 1266    1499\n19        Pres00 factor      5 1749    1016\n20      Postlife factor      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\nskimr::skim(GSS2002)\n\n\nData summary\n\n\nName\nGSS2002\n\n\nNumber of rows\n2765\n\n\nNumber of columns\n21\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n20\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nRegion\n0\n1.00\nFALSE\n7\nNor: 684, Sou: 486, Sou: 471, Mid: 435\n\n\nGender\n0\n1.00\nFALSE\n2\nFem: 1537, Mal: 1228\n\n\nRace\n0\n1.00\nFALSE\n3\nWhi: 2188, Bla: 410, Oth: 167\n\n\nEducation\n5\n1.00\nFALSE\n5\nHS: 1485, Bac: 443, Lef: 400, Gra: 230\n\n\nMarital\n0\n1.00\nFALSE\n5\nMar: 1269, Nev: 708, Div: 445, Wid: 247\n\n\nReligion\n19\n0.99\nFALSE\n13\nPro: 1460, Cat: 673, Non: 379, Chr: 65\n\n\nHappy\n1396\n0.50\nFALSE\n3\nPre: 784, Ver: 415, Not: 170\n\n\nIncome\n890\n0.68\nFALSE\n24\n400: 170, 300: 166, 250: 140, 500: 136\n\n\nPolParty\n36\n0.99\nFALSE\n8\nInd: 528, Not: 515, Not: 449, Str: 408\n\n\nPolitics\n1434\n0.48\nFALSE\n7\nMod: 522, Con: 210, Sli: 209, Sli: 159\n\n\nMarijuana\n1914\n0.31\nFALSE\n2\nNot: 545, Leg: 306\n\n\nDeathPenalty\n1457\n0.47\nFALSE\n2\nFav: 899, Opp: 409\n\n\nOwnGun\n1841\n0.33\nFALSE\n3\nNo: 605, Yes: 310, Ref: 9\n\n\nGunLaw\n1849\n0.33\nFALSE\n2\nFav: 737, Opp: 179\n\n\nSpendMilitary\n1441\n0.48\nFALSE\n3\nAbo: 615, Too: 414, Too: 295\n\n\nSpendEduc\n1422\n0.49\nFALSE\n3\nToo: 992, Abo: 278, Too: 73\n\n\nSpendEnv\n1443\n0.48\nFALSE\n3\nToo: 793, Abo: 439, Too: 90\n\n\nSpendSci\n1499\n0.46\nFALSE\n3\nAbo: 629, Too: 461, Too: 176\n\n\nPres00\n1016\n0.63\nFALSE\n5\nBus: 885, Gor: 781, Nad: 57, Oth: 16\n\n\nPostlife\n1554\n0.44\nFALSE\n2\nYes: 975, No: 236\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nID\n0\n1\n1383\n798.33\n1\n692\n1383\n2074\n2765\n▇▇▇▇▇\n\n\n\n\n\nNote how all variables are Categorical !! Education has five levels, and of course DeathPenalty has three:\n\nGSS2002 %&gt;% count(Education)\n\n\n  \n\n\nGSS2002 %&gt;% count(DeathPenalty)\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty and set up a Contingency Table.\n\ngss2002 &lt;- GSS2002 %&gt;%\n  dplyr::select(Education, DeathPenalty) %&gt;%\n  tidyr::drop_na(., c(Education, DeathPenalty))\n##\ngss_table &lt;- mosaic::tally(DeathPenalty ~ Education, data = gss2002) %&gt;%\n  addmargins()\ngss_table\n\n            Education\nDeathPenalty Left HS   HS Jr Col Bachelors Graduate  Sum\n      Favor      117  511     71       135       64  898\n      Oppose      72  200     16        71       50  409\n      Sum        189  711     87       206      114 1307\n\n\nContingency Table Plots\nThe Contingency Table can be plotted, as we have seen, using a mosaicplot using several packages. Let us do a quick recap:\n\n\nUsing vcd\nUsing ggmosaic\nUsing ggformula\n\n\n\n\nmosaic::tally(DeathPenalty ~ Education, data = gss2002) %&gt;%\n  vcd::mosaic(gp = shading_hsv)\n\n\n\n\n\n\n\n\n\n\n# library(ggmosaic)\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nggplot(data = gss2002) +\n  geom_mosaic(aes(\n    x = product(DeathPenalty, Education),\n    fill = DeathPenalty\n  )) +\n  scale_fill_brewer(name = \"Death Penalty\", palette = \"Set1\") +\n  labs(title = \"Mosaic Plot of Death Penalty by Education\")\n\n\n\n\n\n\n\n\n\nAs seen before, it needs a little more work, to convert the Contingency Table into a tibble:\n\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\ngss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n\n  # Add two more columns to facilitate mosaic/Marrimekko Plot\n  mutate(\n    edu_count = sum(count),\n    edu_prop = count / sum(count)\n  ) %&gt;%\n  ungroup()\n###\ngf_col(edu_prop ~ Education,\n  data = gss_summary,\n  width = ~edu_count,\n  fill = ~DeathPenalty,\n  stat = \"identity\",\n  position = \"fill\",\n  color = \"black\"\n) %&gt;%\n  gf_text(edu_prop ~ Education,\n    label = ~ scales::percent(edu_prop),\n    position = position_stack(vjust = 0.5)\n  ) %&gt;%\n  gf_facet_grid(~Education,\n    scales = \"free_x\",\n    space = \"free_x\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(\n    name = \"Death Penalty\",\n    palette = \"Set1\"\n  )) %&gt;%\n  gf_labs(\n    title = \"Mosaic Plot of Death Penalty by Education\",\n    x = \"Education Level\",\n    y = \"Proportion of Votes for Death Penalty\"\n  )",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#hypotheses-definition",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#hypotheses-definition",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "Hypotheses Definition",
    "text": "Hypotheses Definition\nWhat would our Hypotheses be relating to the proportions of votes for or against the Death Penalty?\n\\(H_0: \\text{Education does not affect votes for Death Penalty}\\\\\\)\n\\(H_a: \\text{Education affects votes for Death Penalty}\\\\\\)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#inference-for-two-proportions",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#inference-for-two-proportions",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "Inference for Two Proportions",
    "text": "Inference for Two Proportions\nWe are now ready to perform our statistical inference. We will use the standard Pearson chi-square test, and develop and intuition for it. We will then do a permutation test to have an alternative method to complete the same task.\n\n\nCode\nIntuitive Explanation\n\n\n\nLet us now perform the base chisq test: We need a contingency table and then the chisq test: We will calculate the observed-chi-squared value, and compare it with the critical value.\n\n# Chi-square test\nmosaic::xchisq.test(mosaic::tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  x\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n  117      511       71      135       64   \n(129.86) (488.51) ( 59.78) (141.54) ( 78.33)\n [1.27]   [1.04]   [2.11]   [0.30]   [2.62] \n&lt;-1.13&gt;  &lt; 1.02&gt;  &lt; 1.45&gt;  &lt;-0.55&gt;  &lt;-1.62&gt; \n         \n   72      200       16       71       50   \n( 59.14) (222.49) ( 27.22) ( 64.46) ( 35.67)\n [2.79]   [2.27]   [4.63]   [0.66]   [5.75] \n&lt; 1.67&gt;  &lt;-1.51&gt;  &lt;-2.15&gt;  &lt; 0.81&gt;  &lt; 2.40&gt; \n         \nkey:\n    observed\n    (expected)\n    [contribution to X-squared]\n    &lt;Pearson residual&gt;\n\n# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(mosaic::tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\n# Determine the Chi-Square critical value\nX_squared_critical &lt;- qchisq(\n  p = .05,\n  df = (5 - 1) * (2 - 1), # (nrows-1) * (ncols-1)\n  lower.tail = FALSE\n)\nX_squared_critical\n\n[1] 9.487729\n\n\nWe see that our observed \\(X^2 = 23.45\\); the critical value X_squared_critical is \\(9.48\\), which is much smaller! The p-value is \\(0.0001029\\), very low as we would expect, indicating that the NULL Hypothesis should be rejected in favour of the alternate hypothesis, that opinions about the DeathPenalty are related to Education.\n\n\n\n\n\n\n\n\nLet us now dig into that cryptic-looking table above!\n\n\nLet us look at the Contingency Table that we have:\n\n\n\n\n\n    \n\n      \n\nDeathPenalty\n                Left HS\n                HS\n                Jr Col\n                Bachelors\n                Graduate\n                Sum\n              \n\n\nFavor\n                  117\n                  511\n                  71\n                  135\n                  64\n                  898\n                \n\nOppose\n                  72\n                  200\n                  16\n                  71\n                  50\n                  409\n                \n\nSum\n                  189\n                  711\n                  87\n                  206\n                  114\n                  1307\n                \n\n\n\n\n\n\nFigure 2: Contingency Table\n\n\n\n In the chi-square test, we check whether the two (or more) categorical variables are independent. To do this we perform a simple check on the Contingency Table. We first re-compute the totals in each row and column, based on what we could expect if there was independence (NULL Hypothesis). If the two variables were independent, then there should be no difference between real and expected scores.\nHow do we know what scores to expect if there was no relationship between the variables?\nConsider the entry in location (1,1): 117. The number of expected entries there is the probability of an entry landing in that square times the total number of entries:\n\\[\n\\begin{align}\n\\text{Expected Value[1,1]}\n&= p_{row_1} * p_{col_1} * Total~Scores\\\\\\\n&= \\Large{\\frac{\\sum_{r_{1}}}{\\sum_{r_{all}c_{all}}} * \\frac{\\sum_{c_{1}}}{\\sum_{r_{all}c_{all}}} * \\sum_{r_{all}c_{all}}} \\\\\n&= \\frac{898}{1307} * \\frac{189}{1307} * 1307\\\\\\\n&= 130\n\\end{align}\n\\]\nProceeding in this way for all the 15 entries in the Contingency Table, we get the “Expected” Contingency Table. Here are both tables for comparison:\n\n\n\nExpected Contingency Table\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n130\n489\n60\n142\n78\n898\n\n\nOppose\n59\n222\n27\n64\n36\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\n\n\n\nActual Contingency Table\n\n\nLeft HS\nHS\nJr Col\nBachelors\nGraduate\nSum\n\n\n\nFavor\n117\n511\n71\n135\n64\n898\n\n\nOppose\n72\n200\n16\n71\n50\n409\n\n\nSum\n189\n711\n87\n206\n114\n1307\n\n\n\n\n\nAnd here are the mosaic plots for the actual and expected Contingency Tables, along with the association plot showing the differences, as we did when plotting Proportions:\n\n\n\n\n\nActual\n\n\n\n\n\nExpected\n\n\n\n\n\nTile-Wise Differences\n\n\n\n\n\nNow, the Pearson Residual in each cell is equivalent to the z-score of that cell. Recall the z-score idea: we subtract the mean and divide by the std. deviation to get the z-score.\nIn the Contingency Table, we have counts which are usually modeled as an (integer) Poisson distribution, for which mean (i.e Expected value) and variance are identical. Thus we get the Pearson Residual as:\n\\[\nr_{i,j} = \\frac{(Actual - Expected)}{\\sqrt{\\displaystyle Expected}}\n\\]\nand therefore:\n\\[\nr_{i,j} = \\frac{(o_{i,j}- e_{i,j})}{\\sqrt{\\displaystyle e_{i,j}}}\n\\]\nThe sum of all the squared Pearson residuals is the chi-square statistic, χ2, upon which the inferential analysis follows.\n\\[\nχ2 = \\sum_{i=1}^R\\sum_{j=1}^C{r_{i,j}^2}\n\\]\nwhere R and C are number of rows and columns in the Contingency Table, the levels in the two Qual variables.\nFor location [1,1], its contribution to χ2 would be: \\((117-130)^2/130 = 1.3\\). Do try to compute all of these and the \\(X^2\\) statistic by hand !!\nAll right, what of all this? How did this \\(X^2\\) distribution come from? Here is a lovely, brief explanation from this StackOverflow Post:\n\n\nIn a Contingency Table the Null Hypothesis states that the variables in the rows and the variable in the columns are independent.\n\nThe cell counts \\(E_{ij}\\) are assumed to be Poisson distributed with mean = \\(E_{ij}\\) and as they are Poisson, their variance is also \\(E_{ij}\\).\n\nAsymptotically the Poisson distribution approaches the normal distribution, with mean = \\(E_{ij}\\) and standard deviation with \\(\\sqrt{E_{ij}}\\) so, asymptotically \\(\\large{\\frac{(X_{ij} - E_{ij})}{\\sqrt{E_{ij}}}}\\) is approximately standard normal \\(N(0,1)\\).\n\nIf you square standard normal variables and sum these squares then the result is a chi-square random variable so \\(\\sum_{i,j}\\left(\\frac{(X_{ij}-E_{ij})}{\\sqrt{E_{ij}}}\\right)^2\\) has a (asymptotically) a chi-square distribution.\n\nAsymptotics must hold and that is why most textbooks state that the result of the test is valid when all expected cell counts \\(E_{ij}\\) are larger than 5, but that is just a rule of thumb that makes the approximation ‘’good enough’’.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#permutation-test-for-education",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#permutation-test-for-education",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "Permutation Test for Education\n",
    "text": "Permutation Test for Education\n\nWe will now perform the permutation test for the difference between proportions. We will first get an intuitive idea of the permutation, and then perform it using both mosaic and infer.\n\n\n Permutation Visually Demonstrated\nCode\n\n\n\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. We will first look visually at a permutation exercise. We will create dummy data that contains the following case study:\n\nA set of identical resumes was sent to male and female evaluators. The candidates in the resumes were of both genders. We wish to see if there was difference in the way resumes were evaluated, by male and female evaluators. (We use just one male and one female evaluator here, to keep things simple!)\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n         M \n-0.3333333 \n\n\n\n\n\n\nSo, we have a solid disparity in percentage of selection between the two evaluators! Now we pretend that there is no difference between the selections made by either set of evaluators. So we can just:\n\nPool up all the evaluations\n\nArbitrarily re-assign a given candidate(selected or rejected) to either of the two sets of evaluators, by permutation.\n\n\nHow would that pooled shuffled set of evaluations look like?\n\n\n\n  \n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nAs can be seen, the ratio is different!\nWe can now check out our Hypothesis that there is no bias. We can shuffle the data many many times, calculating the ratio each time, and plot the distribution of the differences in selection ratio and see how that artificially created distribution compares with the originally observed figure from Mother Nature.\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nnull_dist &lt;- do(4999) * diff(mean(\n  candidate_selected ~ shuffle(evaluator),\n  data = data\n))\n# null_dist %&gt;% names()\nnull_dist %&gt;%\n  gf_histogram(~M,\n    fill = ~ (M &lt;= obs_difference),\n    bins = 25, show.legend = FALSE,\n    xlab = \"Bias Proportion\",\n    ylab = \"How Often?\",\n    title = \"Permutation Test on Difference between Groups\",\n    subtitle = \"\"\n  ) %&gt;%\n  gf_vline(xintercept = ~obs_difference, color = \"red\") %&gt;%\n  gf_label(500 ~ obs_difference,\n    label = \"Observed\\n Bias\",\n    show.legend = FALSE\n  )\nmean(~ M &lt;= obs_difference, data = null_dist)\n\n\n\n\n\n\n \n\n\n[1] 0.00220044\n\n\n\nWe see that the artificial data can hardly ever (\\(p = 0.0022\\)) mimic what the real world experiment is showing. Hence we had good reason to reject our NULL Hypothesis that there is no bias.\n\n\nWe should now repeat the test with permutations on Education:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nnull_chisq &lt;- do(4999) *\n  chisq.test(mosaic::tally(DeathPenalty ~ shuffle(Education),\n    data = gss2002\n  ))\n\nhead(null_chisq)\n\n\n  \n\n\ngf_histogram(~X.squared, data = null_chisq) %&gt;%\n  gf_vline(\n    xintercept = observedChi2,\n    color = \"red\"\n  ) %&gt;%\n  gf_refine(annotate(\"text\",\n    y = 500, x = observedChi2,\n    label = \"Observed\\n Chi-Square\"\n  )) %&gt;%\n  gf_labs(\n    title = \"Permutation Test on Chi-Square Statistic\",\n    x = \"Chi-Square Statistic\",\n    y = \"How Often?\"\n  )\n\n\n\n\n\n\nprop1(~ X.squared &gt;= observedChi2, data = null_chisq)\n\nprop_TRUE \n    2e-04 \n\n\nThe p-value is well below our threshold of \\(0.05\\), so we would conclude that Education has a significant effect on DeathPenalty opinion!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#inference-for-proportions-case-study-2-tbd-dataset",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#inference-for-proportions-case-study-2-tbd-dataset",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Inference for Proportions Case Study-2: TBD dataset",
    "text": "Inference for Proportions Case Study-2: TBD dataset\nTo be Written Up. Yes, but when, Arvind?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#conclusion",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Conclusion",
    "text": "Conclusion\nIn our basic \\(X^2\\) test, we calculate the test statistic of \\(X^2\\) and look up a theoretical null distribution for that statistic, and see how unlikely our observed value is.\nWhy would a permutation test be a good idea here? With a permutation test, there are no assumptions of the null distribution: this is computed based on real data. We note in passing that, in this case, since the number of cases in each cell of the Contingency Table are fairly high ( &gt;= 5) the resulting NULL distribution is of the \\(X^2\\) variety.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#wait-but-why",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#your-turn",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n Your Turn",
    "text": "Your Turn",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/190-TwoProp/index.html#references",
    "title": "🃏 Inference Test for Two Proportions",
    "section": "\n References",
    "text": "References\n\n\nOpenIntro Modern Statistics: Chapter 17\n\nChapter 8: The Chi-Square Test, from Statistics at Square One. The British Medical Journal. https://www.bmj.com/about-bmj/resources-readers/publications/statistics-square-one/8-chi-squared-tests. Very readable and easy to grasp. Especially if you like watching Grey’s Anatomy and House.\nExploring the underlying theory of the chi-square test through simulation - part 1 https://www.rdatagen.net/post/a-little-intuition-and-simulation-behind-the-chi-square-test-of-independence/\n\nExploring the underlying theory of the chi-square test through simulation - part 2 https://www.rdatagen.net/post/a-little-intuition-and-simulation-behind-the-chi-square-test-of-independence-part-2/\n\nAn Online \\(\\Xi^2\\)-test calculator. https://www.statology.org/chi-square-test-of-independence-calculator/\n\nhttps://saylordotorg.github.io/text_introductory-statistics/s13-04-comparison-of-two-population-p.html\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggmosaic\n0.3.3\nJeppson, Hofmann, and Cook (2021)\n\n\nresampledata\n0.3.2\nChihara and Hesterberg (2018)\n\n\nscales\n1.4.0\nWickham, Pedersen, and Seidel (2025)\n\n\nvcd\n1.4.13\n\nMeyer, Zeileis, and Hornik (2006); Zeileis, Meyer, and Hornik (2007); Meyer et al. (2024)\n\n\n\n\n\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. John Wiley & Sons Hoboken NJ. https://github.com/lchihara/MathStatsResamplingR?tab=readme-ov-file.\n\n\nJeppson, Haley, Heike Hofmann, and Di Cook. 2021. ggmosaic: Mosaic Plots in the “ggplot2” Framework. https://doi.org/10.32614/CRAN.package.ggmosaic.\n\n\nMeyer, David, Achim Zeileis, and Kurt Hornik. 2006. “The Strucplot Framework: Visualizing Multi-Way Contingency Tables with Vcd.” Journal of Statistical Software 17 (3): 1–48. https://doi.org/10.18637/jss.v017.i03.\n\n\nMeyer, David, Achim Zeileis, Kurt Hornik, and Michael Friendly. 2024. vcd: Visualizing Categorical Data. https://doi.org/10.32614/CRAN.package.vcd.\n\n\nWickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2025. scales: Scale Functions for Visualization. https://doi.org/10.32614/CRAN.package.scales.\n\n\nZeileis, Achim, David Meyer, and Kurt Hornik. 2007. “Residual-Based Shadings for Visualizing (Conditional) Independence.” Journal of Computational and Graphical Statistics 16 (3): 507–25. https://doi.org/10.1198/106186007X237856.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference Test for Two Proportions"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html",
    "title": "🃏 Inference for a Single Mean",
    "section": "",
    "text": "…neither let us despair over how small our successes are. For however much our successes fall short of our desire, our efforts aren’t in vain when we are farther along today than yesterday.\n— John Calvin",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#setting-up-r-packages",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Setting up R packages",
    "text": "Setting up R packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(infer)\nlibrary(broom) # Clean test results in tibble form\nlibrary(resampledata) # Datasets from Chihara and Hesterberg's book\nlibrary(openintro) # More datasets\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#introduction",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Introduction",
    "text": "Introduction\nIn this module, we will answer a basic Question: What is the mean \\(\\mu\\) of the population?\nRecall that the mean is the first of our Summary Statistics. We wish to know more about the mean of the population from which we have drawn our data sample.\nWe will do this is in several ways, based on the assumptions we are willing to adopt about our data. First we will use a toy dataset with one “imaginary” sample, normally distributed and made up of 50 observations. Since we “know the answer” we will be able to build up some belief in the tests and procedures, which we will dig into to form our intuitions.\nWe will then use a real-world dataset to make inferences on the means of Quant variables therein, and decide what that could tell us.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#statistical-inference-is-almost-an-attitude",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#statistical-inference-is-almost-an-attitude",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Statistical Inference is almost an Attitude!",
    "text": "Statistical Inference is almost an Attitude!\nAs we will notice, the process of Statistical Inference is an attitude: ain’t nothing happenin’! We look at data that we might have received or collected ourselves, and look at it with this attitude, seemingly, of some disbelief. We state either that:\n\nthere is really nothing happening with our research question, and that anything we see in the data is the outcome of random chance.\nthe value/statistic indicated by the data is off the mark and ought to be something else.\n\nWe then calculate how slim the chances are of the given data sample showing up like that, given our belief. It is a distance measurement of sorts. If those chances are too low, then that might alter our belief. This is the attitude that lies at the heart of Hypothesis Testing.\n\n\n\n\n\n\nImportant\n\n\n\nThe calculation of chances is both a logical, and a possible procedure since we are dealing with samples from a population. If many other samples give us quite different estimates, then we would discredit the one we derive from it.\n\n\nEach test we perform will mechanize this attitude in different ways, based on assumptions and conveniences. (And history)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#case-study-1-toy-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#case-study-1-toy-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Case Study #1: Toy data",
    "text": "Case Study #1: Toy data\nSince the CLT assumes the sample is normally-distributed, let us generate a sample that is just so:\n\n\n\nset.seed(40) # for replication\n#\n# Data as individual vectors\n# ( for t.tests etc)\n# Generate normally distributed data with mean = 2, sd = 2, length = 50\ny &lt;- rnorm(n = 50, mean = 2, sd = 2)\n\n# And as tibble too\nmydata &lt;- tibble(y = y)\nmydata",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inspecting-and-charting-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nmydata %&gt;%\n  gf_density(~y) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_labs(\n    title = \"Densities of Original Data Variables\",\n    subtitle = \"Compared with Normal Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteObservations from Density Plots\n\n\n\n\nThe variable \\(y\\) appear to be centred around\nIt does not seem to be normally distributed…\nSo assumptions are not always valid…\n\n\n\nResearch Question\nResearch Questions are always about the population! Here goes:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nCould the mean of the population \\(\\mu\\), from which y has been drawn, be zero?\n\n\nAssumptions\n\n\n\n\n\n\nNoteTesting for Normality\n\n\n\nThe y-variable does not appear to be normally distributed. This would affect the test we can use to make inferences about the population mean.\nThere are formal tests for normality too. We will do them in the next case study. For now, let us proceed naively.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inference",
    "title": "🃏 Inference for a Single Mean",
    "section": "Inference",
    "text": "Inference\n\n\nThe t-test\nWilcoxon’s Signed-Rank Test\nUsing Permutation and Bootstrap\nIntuitive\n\n\n\nA. Model\nWe have \\(mean(y) = \\bar{y}.\\) We formulate “our disbelief” of \\(\\bar{y}\\) with a NULL Hypothesis, about the population as follows:\n\\[\n\\ H_0: \\mu = 0\n\\] And the alternative hypothesis, again about the population as\n\\[\nH_a:\\mu \\ne 0\n\\]\nB. Code\n\n# t-test\nt1 &lt;- mosaic::t_test(\n  y, # Name of variable\n  mu = 0, # belief of population mean\n  alternative = \"two.sided\"\n) %&gt;% # Check both sides\n\n  broom::tidy() # Make results presentable, and plottable!!\nt1\n\n\n  \n\n\n\n\n\n\n\n\n\nImportantRecall Confidence Intervals\n\n\n\nRecall how we calculated means, standard deviations from data (samples). If we could measure the entire population, then there would be no uncertainty in our estimates for means and sd-s. Since we are forced to sample, we can only estimate population parameters based on the sample estimates and state how much off we might be.\nConfidence intervals for population means are given by:\n\\[\n\\begin{eqnarray*}\nCI & = & \\bar{y} ~ {\\pm ~ constant * Standard Error}\\\\\n& = & \\bar{y} ~ {\\pm ~ 1.96 * {sd/\\sqrt{n}}}\n\\end{eqnarray*}\n\\]\n\nAssuming the y is normally-distributed, the \\(constant = 1.96\\) for confidence level of 95%. What that means is that if we take multiple such samples like \\(y\\) from the population, their means (which are random) will land within \\(CI\\) of the population mean (which is fixed!) 95% of the time. Uff…! May remind you of Buffon’s Needle…\n\n\n\n\n\n\n\n\n\n\n\n\nSo \\(\\bar{y}\\) i.e. the estimate is \\(2.045689\\). The confidence intervals do not straddle zero. The chances that this particular value of mean (\\(2.045689\\)) would randomly occur under the assumption that \\(\\mu\\) is zero, are exceedingly slim, \\(p.value = 1.425495e-08\\). Hence we can reject the NULL hypothesis that the true population, of which y is a sample, could have mean \\(\\mu = 0\\).\n\n\n“Signed Rank” Values: A Small Digression\nWhen the Quant variable we want to test for is not normally distributed, we need to think of other ways to perform our inference. Our assumption about normality has been invalidated.\nMost statistical tests use the actual values of the data variables. However, in these cases where assumptions are invalidated, the data are used in rank-transformed sense/order. In some cases the signed-rank of the data values is used instead of the data itself. The signed ranks are then tested to see if there are more of one polarity than the other, roughly speaking, and how probable this could be.\nSigned Rank is calculated as follows:\n\nTake the absolute value of each observation in a sample\n\nPlace the ranks in order of (absolute magnitude). The smallest number has rank = 1 and so on.\n\nGive each of the ranks the sign of the original observation ( + or -)\n\n\nsigned_rank &lt;- function(x) {\n  sign(x) * rank(abs(x))\n}\n\nSince we are dealing with the mean, the sign of the rank becomes important to use.\nA. Model\n\\[\nmean(signed\\_rank(y)) = \\beta_0\n\\]\n\\[\nH_0: \\mu_0 = 0\n\\] \\[\nH_a: \\mu_0 \\ne 0\n\\]\nB. Code\n\n# Standard Wilcoxon Signed_Rank Test\nt2 &lt;- wilcox.test(y, # variable name\n  mu = 0, # belief\n  alternative = \"two.sided\",\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\nt2\n\n\n  \n\n\n# Can also do this equivalently\n# t-test with signed_rank data\nt3 &lt;- t.test(signed_rank(y),\n  mu = 0,\n  alternative = \"two.sided\",\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\nt3\n\n\n  \n\n\n\nAgain, the confidence intervals do not straddle \\(0\\), and we need to reject the belief that the mean is close to zero.\n\n\n\n\n\n\nNote\n\n\n\nNote how the Wilcoxon Test reports results about \\(y\\), even though it computes with \\(signed-rank(y)\\). The “equivalent t-test” with signed-rank data cannot do this, since it uses “rank” data, and reports the same result.\n\n\n\n\nWe saw from the diagram created by Allen Downey that there is only one test 1! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code.\nWe can use two packages in R, mosaic to develop our intuition for what are called permutation based statistical tests; and a more recent package called infer in R which can do pretty much all of this, including visualization.\nWe will stick with mosaic for now. We will do a permutation test first, and then a bootstrap test. In subsequent modules, we will use infer also.\nFor the Permutation test, we mechanize our belief that \\(\\mu = 0\\) by shuffling the polarities of the y observations randomly 4999 times to generate other samples from the population \\(y\\) could have come from2. If these samples can frequently achieve \\(\\bar{y_i} \\leq 0\\), then we might believe that the population mean may be 0!\nWe see that the means here that chances that the randomly generated means can exceed our real-world mean are about \\(0\\)! So the mean is definitely different from \\(0\\).\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n# Calculate exact mean\nobs_mean &lt;- mean(~y, data = mydata)\nbelief1 &lt;- 0 # What we think the mean is\nobs_diff_mosaic &lt;- obs_mean - belief1\nobs_diff_mosaic\n\n[1] 2.045689\n\n## Steps in Permutation Test\n## Repeatedly Shuffle polarities of data observations\n## Take means\n## Compare all means with the real-world observed one\nnull_dist_mosaic &lt;-\n  mosaic::do(9999) * mean(\n    ~ abs(y) *\n      sample(c(-1, 1), # +/- 1s multiply y\n        length(y), # How many +/- 1s?\n        replace = T\n      ), # select with replacement\n    data = mydata\n  )\n##\nrange(null_dist_mosaic$mean)\n\n[1] -1.754293  1.473298\n\n##\n## Plot this NULL distribution\ngf_histogram(\n  ~mean,\n  data = null_dist_mosaic,\n  fill = ~ (mean &gt;= obs_diff_mosaic),\n  bins = 50, title = \"Distribution of Permutation Means under Null Hypothesis\",\n  subtitle = \"Why is the mean of the means zero??\"\n) %&gt;%\n  gf_labs(\n    x = \"Calculated Random Means\",\n    y = \"How Often do these occur?\"\n  ) %&gt;%\n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n\n\n\n# p-value\n# Null distributions are always centered around zero. Why?\nprop(~ mean &gt;= obs_diff_mosaic,\n  data = null_dist_mosaic\n)\n\nprop_TRUE \n        0 \n\n\nLet us try the bootstrap test now: Here we simulate samples, similar to the one at hand, using repeated sampling the sample itself, with replacement, a process known as bootstrapping, or bootstrap sampling.\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\n## Resample with replacement from the one sample of 50\n## Calculate the mean each time\nnull_toy_bs &lt;- mosaic::do(4999) *\n  mean(\n    ~ sample(y,\n      replace = T\n    ), # select with replacement\n    data = mydata\n  )\n\n## Plot this NULL distribution\ngf_histogram(\n  ~mean,\n  data = null_toy_bs,\n  bins = 50,\n  title = \"Distribution of Bootstrap Means\"\n) %&gt;%\n  gf_labs(\n    x = \"Calculated Random Means\",\n    y = \"How Often do these occur?\"\n  ) %&gt;%\n  gf_vline(xintercept = ~belief1, colour = \"red\")\n\n\n\n\n\n\nprop(~ mean &gt;= belief1,\n  data = null_toy_bs\n) +\n  prop(~ mean &lt;= -belief1,\n    data = null_toy_bs\n  )\n\nprop_TRUE \n        1 \n\n\n\n\n\n\n\n\nNotePermutation vs Bootstrap\n\n\n\nThere is a difference between the two. The bootstrap test uses the sample at hand to generate many similar samples without access to the population, and calculates the statistic needed (i.e. mean). No Hypothesis is stated. The distribution of bootstrap samples looks “similar” to that we might obtain by repeatedly sampling the population itself. (centred around a population parameter, i.e. \\(\\mu\\))\nThe permutation test generates many permutations of the data and generates appropriates measures/statistics under the NULL hypothesis. Which is why the permutation test has a NULL distribution centered at \\(0\\) in this case, our NULL hypothesis.\nAs student Sneha Manu Jacob remarked in class, Permutation flips the signs of the data values in our sample; Bootstrap flips the number of times each data value is (re)used. Good Insight!!\n\n\n\n\nYes, the t-test works, but what is really happening under the hood of the t-test? The inner mechanism of the t-test can be stated in the following steps:\n\nCalculate the mean of the sample \\(\\bar{y}\\).\nCalculate the sd of the sample, and, assuming the sample is normally distributed, calculate the standard error (i.e. \\(\\frac{sd}{\\sqrt{n}}\\))\nTake the difference between the sample mean \\(\\bar{y}\\) and our expected/believed population mean \\(\\mu\\).\nWe expect that the population mean ought to be within the confidence interval of the sample mean \\(\\bar{y}\\).\nFor a normally distributed sample, the confidence interval is given by \\(\\pm1.96 * standarderror\\), to be 95% sure that the sample mean is a good estimate for the population mean.\nTherefore if the difference between actual and believed is far beyond the confidence interval, hmm…we cannot think our belief is correct and we change our opinion.\n\nLet us translate that mouthful into calculations!\n\nmean_belief_pop &lt;- 0.0 # Assert our belief\n# Sample Mean\nmean_y &lt;- mean(y)\nmean_y\n\n[1] 2.045689\n\n## Sample standard error\nstd_error &lt;- sd(y) / sqrt(length(y))\nstd_error\n\n[1] 0.3014752\n\n## Confidence Interval of Observed Mean\nconf_int &lt;- tibble(ci_low = mean_y - 1.96 * std_error, ci_high = mean_y + 1.96 * std_error)\nconf_int\n\n\n  \n\n\n## Difference between actual and believed mean\nmean_diff &lt;- mean_y - mean_belief_pop\nmean_diff\n\n[1] 2.045689\n\n## Test Statistic\nt &lt;- mean_diff / std_error\nt\n\n[1] 6.785596\n\n\nWe see that the difference between means is 6.78 times the std_error! At a distance of 1.96 (either way) the probability of this data happening by chance already drops to 5%!! At this distance of 6.78, we would have negligible probability of this data occurring by chance!\nHow can we visualize this?\n\n\n\n\n\n\n\n\n\n\n\n\nIf X ~ N(2.046, 0.3015), then \n\n\n    P(X &lt;= 1.443e-07) = P(Z &lt;= -6.786) = 5.78e-12\n\n\n    P(X &gt;  1.443e-07) = P(Z &gt;  -6.786) = 1\n\n\n\n\n\n\n\n\n\n\n[1] 5.780412e-12",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#case-study-2-exam-data",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#case-study-2-exam-data",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Case Study #2: Exam data",
    "text": "Case Study #2: Exam data\nLet us now choose a dataset from the openintro package:\n\ndata(\"exam_grades\")\nexam_grades\n\n\n  \n\n\n\nResearch Question\nThere are quite a few Quant variables in the data. Let us choose course_grade as our variable of interest. What might we wish to find out?\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nIn general, the Teacher in this class is overly generous with grades unlike others we know of, and so the average course-grade is equal to 80% !!\n\n\n\n Inspecting and Charting Data\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nexam_grades %&gt;%\n  gf_density(~course_grade) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_labs(\n    title = \"Density of Course Grade\",\n    subtitle = \"Compared with Normal Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nHmm…data looks normally distributed. But this time we will not merely trust our eyes, but do a test for it.\nTesting Assumptions in the Data\n\n\n\n\n\n\nNoteIs the data normally distributed?\n\n\n\n\nstats::shapiro.test(x = exam_grades$course_grade) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe Shapiro-Wilkes Test tests whether a data variable is normally distributed or not. Without digging into the maths of it, let us say it makes the assumption that the variable is so distributed and then computes the probability of how likely this is. So a high p-value (\\(0.47\\)) is a good thing here.\nWhen we have large Quant variables ( i.e. with length &gt;= 5000), the shapiro.test does not work, and we use an Anderson-Darling3 test to confirm normality:\n\nlibrary(nortest)\n# Especially when we have &gt;= 5000 observations\nnortest::ad.test(x = exam_grades$course_grade) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nSo course_grade is a normally-distributed variable. There are no exceptional students! Hmph!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inference-1",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#inference-1",
    "title": "🃏 Inference for a Single Mean",
    "section": "Inference",
    "text": "Inference\n\n\nt.test\nWilcoxon test\nUsing Permutation and Bootstrap\n\n\n\nA. Model\nWe have that \\(mean(course\\_grade) = \\beta_0\\). As before, we formulate “our (dis)belief” in this sample mean with a NULL Hypothesis about the population, as follows:\n\\[\n\\ H_0: \\mu= 80\n\\]\n\\[\nH_a: \\mu \\ne 80\n\\]\nB. Code\n\n# t-test\nt4 &lt;- mosaic::t_test(\n  exam_grades$course_grade, # Name of variable\n  mu = 80, # belief\n  alternative = \"two.sided\"\n) %&gt;% # Check both sides\n  broom::tidy()\nt4\n\n\n  \n\n\n\nSo, we can reject the NULL Hypothesis that the average grade in the population of students who have taken this class is 80, since there is a minuscule chance that we would see an observed sample mean of 72.238, if the population mean \\(\\mu\\) had really been \\(80\\).\n\n\n\n# t-test\nt5 &lt;- wilcox.test(\n  exam_grades$course_grade, # Name of variable\n  mu = 90, # belief\n  alternative = \"two.sided\",\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;% # Check both sides\n\n  broom::tidy() # Make results presentable, and plottable!!\nt5\n\n\n  \n\n\n\nThis test too suggests that the average course grade is different from 80.\n\n\n\n\n\n\nNoteWhy compare on both sides?\n\n\n\nNote that we have computed whether the average course_grade is generally different from 80 for this Teacher. We could have computed whether it is greater, or lesser than 80 ( or any other number too). Read this article for why it is better to do a “two.sided” test in most cases.\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n# Calculate exact mean\nobs_mean_grade &lt;- mean(~course_grade, data = exam_grades)\nbelief &lt;- 80\nobs_grade_diff &lt;- belief - obs_mean_grade\n## Steps in a Permutation Test\n## Repeatedly Shuffle polarities of data observations\n## Take means\n## Compare all means with the real-world observed one\nnull_dist_grade &lt;-\n  mosaic::do(4999) *\n    mean(\n      ~ (course_grade - belief) *\n        sample(c(-1, 1), # +/- 1s multiply y\n          length(course_grade), # How many +/- 1s?\n          replace = T\n        ), # select with replacement\n      data = exam_grades\n    )\n\n## Plot this NULL distribution\ngf_histogram(\n  ~mean,\n  data = null_dist_grade,\n  fill = ~ (mean &gt;= obs_grade_diff),\n  bins = 50,\n  title = \"Distribution of Permuted Difference-Means under Null Hypothesis\",\n  subtitle = \"Why is the mean of the means zero??\"\n) %&gt;%\n  gf_labs(\n    x = \"Calculated Random Means\",\n    y = \"How Often do these occur?\"\n  ) %&gt;%\n  gf_vline(xintercept = obs_grade_diff, colour = \"red\") %&gt;%\n  gf_vline(xintercept = -obs_grade_diff, colour = \"red\")\n\n\n\n\n\n\n# p-value\n# Permutation distributions are always centered around zero. Why?\nprop(~ mean &gt;= obs_grade_diff,\n  data = null_dist_grade\n) +\n  prop(~ mean &lt;= -obs_grade_diff,\n    data = null_dist_grade\n  )\n\nprop_TRUE \n        0 \n\n\nAnd let us now do the bootstrap test:\n\nnull_grade_bs &lt;- mosaic::do(4999) *\n  mean(\n    ~ sample(course_grade,\n      replace = T\n    ), # select with replacement\n    data = exam_grades\n  )\n\n## Plot this NULL distribution\ngf_histogram(\n  ~mean,\n  data = null_grade_bs,\n  fill = ~ (mean &gt;= obs_grade_diff),\n  bins = 50,\n  title = \"Distribution of Bootstrap Means\"\n) %&gt;%\n  gf_labs(\n    x = \"Calculated Random Means\",\n    y = \"How Often do these occur?\"\n  ) %&gt;%\n  gf_vline(xintercept = ~belief, colour = \"red\")\n\n\n\n\n\n\nprop(~ mean &gt;= belief,\n  data = null_grade_bs\n) +\n  prop(~ mean &lt;= -belief,\n    data = null_grade_bs\n  )\n\nprop_TRUE \n        0 \n\n\nThe permutation test shows that we are not able to “generate” the believed mean-difference with any of the permutations. Likewise with the bootstrap, we are not able to hit the believed mean with any of the bootstrap samples.\nHence there is no reason to believe that the belief (80) might be a reasonable one and we reject our NULL Hypothesis that the mean is equal to 80.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#workflow-for-inference-for-a-single-mean",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#workflow-for-inference-for-a-single-mean",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Workflow for Inference for a Single Mean",
    "text": "Workflow for Inference for a Single Mean\nA series of tests deal with one mean value of a sample. The idea is to evaluate whether that mean is representative of the mean of the underlying population. Depending upon the nature of the (single) variable, the test that can be used are as follows:\n\n\n\n\n\nflowchart TD\n    A[Inference for Single Mean] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n or\\n Anderson-Darling Test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes\\n Parametric| D[t.test]\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[t.test\\n with\\n Signed-Ranks of Data]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap]\n    C --&gt;|No\\n Non-Parametric| Q[Permutation]",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#wait-but-why",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nWe can only sample from a population, and calculate sample statistics\nBut we still want to know about population parameters\n\nAll our tests and measures of uncertainty with samples are aimed at obtaining a confident measure of a population parameter.\nMeans are the first on the list!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#conclusion",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nIf samples are normally distributed, we use a t.test.\nElse we try non-parametric tests such as the Wilcoxon test.\nSince we now have compute power at our fingertips, we can leave off considerations of normality and simply proceed with either a permutation or a boostrap test.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#sec-references",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#sec-references",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n References",
    "text": "References\n\nOpenIntro Modern Statistics, Chapter #17\n\nBootstrap based Inference using the infer package: https://infer.netlify.app/articles/t_test\n\nMichael Clark & Seth Berry. Models Demystified: A Practical Guide from t-tests to Deep Learning. https://m-clark.github.io/book-of-models/\n\nUniversity of Warwickshire. SAMPLING: Searching for the Approximation Method use to Perform rational inference by Individuals and Groups. https://sampling.warwick.ac.uk/#Overview",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#additional-readings",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#additional-readings",
    "title": "🃏 Inference for a Single Mean",
    "section": "\n Additional Readings",
    "text": "Additional Readings\n\nhttps://mine-cetinkaya-rundel.github.io/quarto-tip-a-day/posts/21-diagrams/\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nexplore\n1.3.5\nKrasser (2025)\n\n\ninfer\n1.0.9\nCouch et al. (2021)\n\n\nopenintro\n2.5.0\nÇetinkaya-Rundel et al. (2024)\n\n\nresampledata\n0.3.2\nChihara and Hesterberg (2018)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2024. openintro: Datasets and Supplemental Functions from “OpenIntro” Textbooks and Labs. https://doi.org/10.32614/CRAN.package.openintro.\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. John Wiley & Sons Hoboken NJ. https://github.com/lchihara/MathStatsResamplingR?tab=readme-ov-file.\n\n\nCouch, Simon P., Andrew P. Bray, Chester Ismay, Evgeni Chasnovski, Benjamin S. Baumer, and Mine Çetinkaya-Rundel. 2021. “infer: An R Package for Tidyverse-Friendly Statistical Inference.” Journal of Open Source Software 6 (65): 3661. https://doi.org/10.21105/joss.03661.\n\n\nKrasser, Roland. 2025. explore: Simplifies Exploratory Data Analysis. https://doi.org/10.32614/CRAN.package.explore.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/100-OneMean/index.html#footnotes",
    "title": "🃏 Inference for a Single Mean",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html↩︎\nhttps://stats.stackexchange.com/q/171748↩︎\nhttps://www.r-bloggers.com/2021/11/anderson-darling-test-in-r-quick-normality-check/↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Inference for a Single Mean"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html",
    "title": "Permutation Tests",
    "section": "",
    "text": "The mosaic package provides the shuffle() function as a synonym for sample(). When used without additional arguments, this will permute its first argument.\n\nShow the Code# library(mosaic)\nshuffle(1:10)\n\n [1]  9  6  8  3  1 10  2  7  4  5\n\n\nApplying shuffle() to an explanatory variable in a model allows us to test the null hypothesis that the explanatory variable has, in fact, no explanatory power. This idea can be used to test\n\nthe equivalence of two or more means,\nthe equivalence of two or more proportions,\nwhether a regression parameter is 0. (Correlations between two variables) For example:\n\nCoupled with mosaic::do() we can repeat a shuffle many times, computing a desired statistic each time we shuffle. The distribution of this computed statistic is a NULL distribution, which can then be compared with the observed statistic to decide upon the Hypothesis Test and p-value."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html#permutation-tests",
    "href": "content/courses/Analytics/Inference/Modules/70-PermTest/files/perm-tutorial.html#permutation-tests",
    "title": "Permutation Tests",
    "section": "Permutation Tests",
    "text": "Permutation Tests\nCase Study-1: Hot Wings Orders vs Gender\nA student conducted a study of hot wings and beer consumption at a Bar. She asked patrons at the bar to record their consumption of hot wings and beer over the course of several hours. She wanted to know if people who ate more hot wings would then drink more beer. In addition, she investigated whether or not gender had an impact on hot wings or beer consumption.\n\nShow the CodeBeerwings &lt;- read.csv(\"../../../../../../materials/data/resampling/Beerwings.csv\")\ninspect(Beerwings)\n\n\ncategorical variables:  \n    name     class levels  n missing\n1 Gender character      2 30       0\n                                   distribution\n1 F (50%), M (50%)                             \n\nquantitative variables:  \n      name   class min    Q1 median    Q3 max     mean        sd  n missing\n1       ID integer   1  8.25   15.5 22.75  30 15.50000  8.803408 30       0\n2 Hotwings integer   4  8.00   12.5 15.50  21 11.93333  4.784554 30       0\n3     Beer integer   0 24.00   30.0 36.00  48 26.20000 11.842064 30       0\n\n\nLet us calculate the observed difference in Hotwings consumption between Males and Females ( Gender)\n\nShow the Codemean(Hotwings ~ Gender, data = Beerwings)\n\n        F         M \n 9.333333 14.533333 \n\nShow the Codeobs_diff_wings &lt;- mosaic::diffmean(data = Beerwings, Hotwings ~ Gender)\nobs_diff_wings\n\ndiffmean \n     5.2 \n\n\n\nShow the Codegf_boxplot(data = Beerwings, Hotwings ~ Gender, title = \"Hotwings Consumption by Gender\")\n\n\n\n\n\n\n\nThe observed difference in mean consumption of Hotwings between Males and Females is 5.2. Could this have occurred by chance? Here is our formulation of the Hypotheses:\n\\[\nNULL\\ Hypothesis\\ H_0 =&gt; No\\ difference\\ between\\ means\\ across\\ groups\\\\\nAlternative\\ Hypothesis\\\nH_a =&gt;Significant\\ difference\\ between\\ the\\ means\\\n\\]\nSo we perform a Permutation Test to check:\n\nShow the Codenull_dist_wings &lt;- do(1000) * diffmean(Hotwings ~ shuffle(Gender), data = Beerwings)\nnull_dist_wings %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_wings, ~diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\n\n\n\nShow the Codeprop1(~ diffmean &gt;= obs_diff_wings, data = null_dist_wings)\n\n  prop_TRUE \n0.001998002 \n\n\nThe \\(\\color{red}{red\\ line}\\) shows the actual measured mean difference in Hot Wings consumption. The probability that our Permutation distribution is able to equal or exceed that number is \\(0.001998002\\) and we have to reject the Null Hypothesis that the means are identical.\nTo test whether eating more hotwings would lead to increased beer consumption, we need a regression model, which we can again test with a permutation test.\n\nShow the Codelm(Beer ~ Hotwings, data = Beerwings)\n\n\nCall:\nlm(formula = Beer ~ Hotwings, data = Beerwings)\n\nCoefficients:\n(Intercept)     Hotwings  \n      3.040        1.941  \n\n\nCase Study-2: Verizon\nThe following example is used throughout this article. Verizon was an Incumbent Local Exchange Carrier (ILEC), responsible for maintaining land-line phone service in certain areas. Verizon also sold long-distance service, as did a number of competitors, termed Competitive Local Exchange Carriers (CLEC). When something went wrong, Verizon was responsible for repairs, and was supposed to make repairs as quickly for CLEC long-distance customers as for their own. The New York Public Utilities Commission (PUC) monitored fairness by comparing repair times for Verizon and different CLECs, for different classes of repairs and time periods. In each case a hypothesis test was performed at the 1% significance level, to determine whether repairs for CLEC’s customers were significantly slower than for Verizon’s customers. There were hundreds of such tests. If substantially more than 1% of the tests were significant, then Verizon would pay large penalties. These tests were performed using t tests; Verizon proposed using permutation tests instead.\n\nShow the Codeverizon &lt;- read.csv(\"../../../../../../materials/data/resampling/Verizon.csv\")\ninspect(verizon)\n\n\ncategorical variables:  \n   name     class levels    n missing\n1 Group character      2 1687       0\n                                   distribution\n1 ILEC (98.6%), CLEC (1.4%)                    \n\nquantitative variables:  \n  name   class min   Q1 median   Q3   max     mean       sd    n missing\n1 Time numeric   0 0.75   3.63 7.35 191.6 8.522009 14.78848 1687       0\n\n\n\nShow the Codemean(Time ~ Group, data = verizon)\n\n     CLEC      ILEC \n16.509130  8.411611 \n\nShow the Codeobs_diff_verizon &lt;- diffmean(Time ~ Group, data = verizon)\nobs_diff_verizon\n\ndiffmean \n-8.09752 \n\n\n\nShow the Codenull_dist_verizon &lt;- do(1000) * diffmean(Time ~ shuffle(Group), data = verizon)\ngf_histogram(data = null_dist_verizon, ~diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_wings, colour = \"red\")\n\n\n\n\n\n\nShow the Codeprop1(~ diffmean &gt;= obs_diff_wings, data = null_dist_verizon)\n\n prop_TRUE \n0.01298701 \n\n\nCase Story-3: Recidivism\nDo criminals released after a jail term commit crimes again?\n\nShow the Coderecidivism &lt;- read.csv(\"../../../../../../materials/data/resampling/Recidivism.csv\")\ninspect(recidivism)\n\n\ncategorical variables:  \n     name     class levels     n missing\n1  Gender character      2 17019       3\n2     Age character      5 17019       3\n3   Age25 character      2 17019       3\n4 Offense character      2 17022       0\n5   Recid character      2 17022       0\n6    Type character      3 17022       0\n                                   distribution\n1 M (87.7%), F (12.3%)                         \n2 25-34 (36.6%), 35-44 (23.7%) ...             \n3 Over 25 (81.9%), Under 25 (18.1%)            \n4 Felony (80.6%), Misdemeanor (19.4%)          \n5 No (68.4%), Yes (31.6%)                      \n6 No Recidivism (68.4%), New (20.2%) ...       \n\nquantitative variables:  \n  name   class min  Q1 median  Q3  max     mean       sd    n missing\n1 Days integer   0 241    418 687 1095 473.3275 283.1393 5386   11636\n\n\nThere are some missing values in the variable  Age25. The  complete.cases command gives the row numbers where values are not missing. We create a new data frame omitting the rows where there is a missing value in the  ‘Age25’  variable.\n\nShow the Coderecidivism_na &lt;- recidivism %&gt;% tidyr::drop_na(Age25)\n\n\nAlso, the variable Recid is a factor variable coded “Yes” or “No”. We convert it to a numeric variable of 1’s and 0’s.\n\nShow the Coderecidivism_na &lt;- recidivism_na %&gt;% mutate(Recid2 = ifelse(Recid == \"Yes\", 1, 0))\n\nobs_diff_recid &lt;- diffmean(Recid2 ~ Age25, data = recidivism_na)\nobs_diff_recid\n\n  diffmean \n0.05919913 \n\nShow the Codenull_dist_recid &lt;- do(1000) * diffmean(Recid2 ~ shuffle(Age25), data = recidivism_na)\n\ngf_histogram(~diffmean, data = null_dist_recid) %&gt;%\n  gf_vline(xintercept = obs_diff_recid, colour = \"red\")\n\n\n\n\n\n\n\nCase Study-4: Matched Pairs: Results from a diving championship.\n\nShow the CodeDiving2017 &lt;- read.csv(\"../../../../../../materials/data/resampling/Diving2017.csv\")\nhead(Diving2017)\n\n\n  \n\n\nShow the Codeinspect(Diving2017)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1    Name character     12 12       0\n2 Country character      8 12       0\n                                   distribution\n1  SI Yajie (8.3%) ...                         \n2 Canada (16.7%), China (16.7%) ...            \n\nquantitative variables:  \n       name   class    min       Q1  median      Q3   max    mean       sd  n\n1 Semifinal numeric 313.70 322.2000 325.625 356.575 382.8 338.500 22.94946 12\n2     Final numeric 283.35 318.5875 358.925 387.150 397.5 350.475 40.02204 12\n  missing\n1       0\n2       0\n\n\nThe data is made up of paired observations per swimmer. So we need to take the difference between the two swim records for each swimmer and then shuffle the differences to either polarity. Another way to look at this is to shuffle the records between Semifinal and Final on a per Swimmer basis.\n\nShow the CodeDiving2017\n\n\n  \n\n\nShow the CodeDiving2017 %&gt;% diffmean(data = ., Final ~ Semifinal, only.2 = FALSE)\n\n  318.7-313.7  320.55-318.7 322.75-320.55  325.5-322.75  325.75-325.5 \n       12.350       -63.050         5.225        85.125      -114.150 \n   346-325.75    355.15-346 360.85-355.15  367.5-360.85   382.8-367.5 \n      102.200       -54.150        28.600        31.950         4.050 \n\nShow the Codeobs_diff_swim &lt;- mean(~ Final - Semifinal, data = Diving2017)\nobs_diff_swim\n\n[1] 11.975\n\n\n\nShow the Codepolarity &lt;- c(rep(1, 6), rep(-1, 6))\npolarity\n\n [1]  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1\n\nShow the Codenull_dist_swim &lt;- do(100000) * mean(\n  data = Diving2017,\n  ~ (Final - Semifinal) * resample(polarity,\n    replace = TRUE\n  )\n)\nnull_dist_swim %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_swim, ~mean) %&gt;%\n  gf_vline(xintercept = obs_diff_swim, colour = \"red\")\n\n\n\n\n\n\n\nCase Study #5: Flight Delays\nLaGuardia Airport (LGA) is one of three major airports that serves the New York City metropolitan area. In 2008, over 23 million passengers and over 375 000 planes flew in or out of LGA. United Airlines and America Airlines are two major airlines that schedule services at LGA. The data set FlightDelays contains information on all 4029 departures of these two airlines from LGA during May and June 2009.\n\nShow the CodeflightDelays &lt;- read.csv(\"../../../../../../materials/data/resampling/FlightDelays.csv\")\n\ninspect(flightDelays)\n\n\ncategorical variables:  \n         name     class levels    n missing\n1     Carrier character      2 4029       0\n2 Destination character      7 4029       0\n3  DepartTime character      5 4029       0\n4         Day character      7 4029       0\n5       Month character      2 4029       0\n6   Delayed30 character      2 4029       0\n                                   distribution\n1 AA (72.1%), UA (27.9%)                       \n2 ORD (44.3%), DFW (22.8%), MIA (15.1%) ...    \n3 8-Noon (26.1%), Noon-4pm (26%) ...           \n4 Fri (15.8%), Mon (15.6%), Tue (15.6%) ...    \n5 June (50.4%), May (49.6%)                    \n6 No (85.2%), Yes (14.8%)                      \n\nquantitative variables:  \n          name   class min   Q1 median   Q3  max      mean         sd    n\n1           ID integer   1 1008   2015 3022 4029 2015.0000 1163.21645 4029\n2     FlightNo integer  71  371    691  787 2255  827.1035  551.30939 4029\n3 FlightLength integer  68  155    163  228  295  185.3011   41.78783 4029\n4        Delay integer -19   -6     -3    5  693   11.7379   41.63050 4029\n  missing\n1       0\n2       0\n3       0\n4       0\n\n\nThe variables in the flightDelays dataset are:\n\nflightDelay dataset variables\n\n\n\n\n\nVariable\nDescription\n\n\n\nCarrier\nUA=United Airlines, AA=American Airlines\n\n\nFlightNo\nFlight number\n\n\nDestination\nAirport code\n\n\nDepartTime\nScheduled departure time in 4 h intervals\n\n\nDay\nDay of the Week\n\n\nMonth\nMay or June\n\n\nDelay\nMinutes flight delayed (negative indicates early departure)\n\n\nDelayed30\nDeparture delayed more than 30 min? Yes or No\n\n\nFlightLength\nLength of time of flight (minutes)\n\n\n\n\nLet us compute the proportion of times that each carrier’s flights was delayed more than 20 min. We will conduct a two-sided test to see if the difference in these proportions is statistically significant.\n\n\nShow the Codeprop(data = flightDelays, Delay &gt;= 20 ~ Carrier)\n\nprop_TRUE.AA prop_TRUE.UA \n   0.1713696    0.2226180 \n\nShow the Codeobs_diff_delay &lt;- diffprop(data = flightDelays, Delay &gt;= 20 ~ Carrier)\nobs_diff_delay\n\n  diffprop \n0.05124841 \n\n\nWe see carrier AA has a 17.13% chance of delays&gt;= 20, while UA has 22.26% chance. The difference is 5.12%. Is this statistically significant? We take the Delays for both Carriers and perform a permutation test by shuffle on the carrier variable:\n\nShow the Codenull_dist_delay &lt;- do(10000) * diffprop(data = flightDelays, Delay &gt;= 20 ~ shuffle(Carrier))\nnull_dist_delay %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_delay, ~diffprop) %&gt;% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\n\n\n\n\nIt appears that the difference indelay times is significant. We can compute the p-value based on this test:\n\nShow the Code2 * mean(null_dist_delay &gt;= obs_diff_delay)\n\n[1] 6e-04\n\n\nwhich is very small. Hence we reject the null Hypothesis that there is no difference between carriers on delay times.\n\nCompute the variance in the flight delay lengths for each carrier. Conduct a test to see if the variance for United Airlines differs from that of American Airlines.\n\n\nShow the Codevar(data = flightDelays, Delay ~ Carrier)\n\n      AA       UA \n1606.457 2037.525 \n\nShow the Code# There is no readymade function in mosaic called `diffvar`...so...we construct one\nobs_diff_var &lt;- diff(var(data = flightDelays, Delay ~ Carrier))\nobs_diff_var\n\n      UA \n431.0677 \n\n\nThe difference in variances in Delay between the two carriers is \\(-431.0677\\). In our Permutation Test, we shuffle the Carrier variable:\n\nShow the Codeobs_diff_var &lt;- diff(var(data = flightDelays, Delay ~ Carrier))\nnull_dist_var &lt;-\n  do(10000) * diff(var(data = flightDelays, Delay ~ shuffle(Carrier)))\nnull_dist_var %&gt;% head()\n\n\n  \n\n\nShow the Code# The null distribution variable is called `UA`\ngf_histogram(data = null_dist_var, ~UA) %&gt;% gf_vline(xintercept = obs_diff_delay, color = \"red\")\n\n\n\n\n\n\nShow the Code2 * mean(null_dist_var &gt;= obs_diff_var)\n\n[1] 0.3022\n\n\nClearly there is no case for a significant difference in variances!\nCase Study #6: Walmart vs Target\nIs there a difference in the price of groceries sold by the two retailers Target and Walmart? The data set Groceries contains a sample of grocery items and their prices advertised on their respective web sites on one specific day.\n\nInspect the data set, then explain why this is an example of matched pairs data.\nCompute summary statistics of the prices for each store.\nConduct a permutation test to determine whether or not there is a difference in the mean prices.\nCreate a histogram bar-chart of the difference in prices. What is unusual about Quaker Oats Life cereal?\nRedo the hypothesis test without this observation. Do you reach the same conclusion?\n\n\nShow the Codegroceries &lt;- read.csv(\"../../../../../../materials/data/resampling/Groceries.csv\") %&gt;% mutate(Product = stringr::str_squish(Product))\nhead(groceries)\n\n\n  \n\n\nShow the Codeinspect(groceries)\n\n\ncategorical variables:  \n     name     class levels  n missing\n1 Product character     30 30       0\n2    Size character     24 30       0\n                                   distribution\n1 Annie's Macaroni & Cheese (3.3%) ...         \n2 18oz (10%), 12oz (6.7%) ...                  \n\nquantitative variables:  \n     name   class  min     Q1 median    Q3  max     mean       sd  n missing\n1  Target numeric 0.99 1.8275  2.545 3.140 7.99 2.762333 1.582128 30       0\n2 Walmart numeric 1.00 1.7600  2.340 2.955 6.98 2.705667 1.560211 30       0\n\n\nWe see that the comparison is to be made between two prices for the same product, and hence this is one more example of paired data, as in Case Study #4. Let us plot the prices for the products:\n\nShow the Codegf_col(\n  data = groceries,\n  Target ~ Product,\n  fill = \"#0073C299\",\n  width = 0.5\n) %&gt;%\n  gf_col(\n    data = groceries,\n    -Walmart ~ Product,\n    fill = \"#EFC00099\",\n    ylab = \"Prices\",\n    width = 0.5\n  ) %&gt;%\n  gf_col(\n    data = groceries %&gt;% filter(Product == \"Quaker Oats Life Cereal Original\"),\n    -Walmart ~ Product,\n    fill = \"red\",\n    width = 0.5\n  ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  gf_theme(ggplot2::theme(axis.text.x = element_text(\n    size = 8,\n    face = \"bold\",\n    vjust = 0,\n    hjust = 1\n  ))) %&gt;%\n  gf_theme(ggplot2::coord_flip())\n\n\n\n\n\n\n\nWe see that the price difference between Walmart and Target prices is highest for the Product named Quaker Oats Life Cereal Original. Let us check the mean difference in prices:\n\nShow the Codediffmean(data = groceries, Walmart ~ Target, only.2 = FALSE)\n\n   1-0.99    1.22-1 1.42-1.22 1.49-1.42 1.59-1.49 1.62-1.59 1.79-1.62 1.94-1.79 \n-0.580000  0.170000  0.210000 -0.100000  0.190000  0.070000  0.180000  0.160000 \n1.99-1.94 2.12-1.99 2.39-2.12  2.5-2.39  2.59-2.5 2.64-2.59 2.79-2.64 2.82-2.79 \n 0.090000  0.010000  0.200000  0.600000 -0.200000 -0.600000  0.660000  0.040000 \n2.99-2.82 3.19-2.99 3.49-3.19 3.99-3.49 4.79-3.99 7.19-4.79 7.99-7.19 \n 0.220000  1.263333 -1.183333 -0.480000  2.290000  2.190000  0.000000 \n\nShow the Codeobs_diff_price &lt;- mean(~ Walmart - Target, data = groceries)\nobs_diff_price\n\n[1] -0.05666667\n\n\nLet us perform the pair-wise permutation test on prices, by shuffling the two store names:\n\nShow the Codepolarity &lt;- c(rep(1, 15), rep(-1, 15))\npolarity\n\n [1]  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n[26] -1 -1 -1 -1 -1\n\nShow the Codenull_dist_price &lt;- do(100000) * mean(\n  data = groceries,\n  ~ (Walmart - Target) * resample(polarity,\n    replace = TRUE\n  )\n)\nnull_dist_price %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_price, ~mean) %&gt;%\n  gf_vline(xintercept = obs_diff_price, colour = \"red\")\n\n\n\n\n\n\nShow the Code2 * (sum(null_dist_price &gt;= obs_diff_price + 1) / (100000 + 1)) # P-value\n\n[1] 0\n\n\nDoes not seem to be any significant difference in prices…\nSuppose we knock off the Quaker Cereal data item…\n\nShow the Codewhich(groceries$Product == \"Quaker Oats Life Cereal Original\")\n\n[1] 2\n\nShow the Codegroceries_less &lt;- groceries[-2, ]\ngroceries_less\n\n\n  \n\n\nShow the Codeobs_diff_price_less &lt;- mean(~ Walmart - Target, data = groceries_less)\nobs_diff_price_less\n\n[1] -0.1558621\n\nShow the Codepolarity_less &lt;- c(rep(1, 15), rep(-1, 14)) # Due to resampling this small bias makes no difference\nnull_dist_price_less &lt;- do(100000) * mean(\n  data = groceries_less,\n  ~ (Walmart - Target) * resample(polarity_less,\n    replace = TRUE\n  )\n)\nnull_dist_price_less %&gt;% head()\n\n\n  \n\n\nShow the Codegf_histogram(data = null_dist_price_less, ~mean) %&gt;%\n  gf_vline(xintercept = obs_diff_price_less, colour = \"red\")\n\n\n\n\n\n\nShow the Code1 - mean(null_dist_price_less &gt;= obs_diff_price_less) # P-value\n\n[1] 0.01592\n\n\nCase Study 7: Proportions between Categorical Variables\nLet us try a dataset with Qualitative / Categorical data. This is a General Social Survey dataset, and we have people with different levels of Education stating their opinion on the Death Penalty. We want to know if these two Categorical variables have a correlation, i.e. can the opinions in favour of the Death Penalty be explained by the Education level?\nSince data is Categorical, we need to take counts in a table, and then implement a chi-square test. In the test, we will permute the Education variable to see if we can see how significant its effect size is.\n\nShow the CodeGSS2002 &lt;- read.csv(\"../../../../../../materials/data/resampling/GSS2002.csv\")\ninspect(GSS2002)\n\n\ncategorical variables:  \n            name     class levels    n missing\n1         Region character      7 2765       0\n2         Gender character      2 2765       0\n3           Race character      3 2765       0\n4      Education character      5 2760       5\n5        Marital character      5 2765       0\n6       Religion character     13 2746      19\n7          Happy character      3 1369    1396\n8         Income character     24 1875     890\n9       PolParty character      8 2729      36\n10      Politics character      7 1331    1434\n11     Marijuana character      2  851    1914\n12  DeathPenalty character      2 1308    1457\n13        OwnGun character      3  924    1841\n14        GunLaw character      2  916    1849\n15 SpendMilitary character      3 1324    1441\n16     SpendEduc character      3 1343    1422\n17      SpendEnv character      3 1322    1443\n18      SpendSci character      3 1266    1499\n19        Pres00 character      5 1749    1016\n20      Postlife character      2 1211    1554\n                                    distribution\n1  North Central (24.7%) ...                    \n2  Female (55.6%), Male (44.4%)                 \n3  White (79.1%), Black (14.8%) ...             \n4  HS (53.8%), Bachelors (16.1%) ...            \n5  Married (45.9%), Never Married (25.6%) ...   \n6  Protestant (53.2%), Catholic (24.5%) ...     \n7  Pretty happy (57.3%) ...                     \n8  40000-49999 (9.1%) ...                       \n9  Ind (19.3%), Not Str Dem (18.9%) ...         \n10 Moderate (39.2%), Conservative (15.8%) ...   \n11 Not legal (64%), Legal (36%)                 \n12 Favor (68.7%), Oppose (31.3%)                \n13 No (65.5%), Yes (33.5%) ...                  \n14 Favor (80.5%), Oppose (19.5%)                \n15 About right (46.5%) ...                      \n16 Too little (73.9%) ...                       \n17 Too little (60%) ...                         \n18 About right (49.7%) ...                      \n19 Bush (50.6%), Gore (44.7%) ...               \n20 Yes (80.5%), No (19.5%)                      \n\nquantitative variables:  \n  name   class min  Q1 median   Q3  max mean       sd    n missing\n1   ID integer   1 692   1383 2074 2765 1383 798.3311 2765       0\n\n\nNote how all variables are Categorical !! Education has five levels:\n\nShow the CodeGSS2002 %&gt;% count(Education)\n\n\n  \n\n\nShow the CodeGSS2002 %&gt;% count(DeathPenalty)\n\n\n  \n\n\n\nLet us drop NA entries in Education and Death Penalty. And set up a table for the chi-square test.\n\nShow the Codegss2002 &lt;- GSS2002 %&gt;%\n  dplyr::select(Education, DeathPenalty) %&gt;%\n  tidyr::drop_na(., c(Education, DeathPenalty))\ndim(gss2002)\n\n[1] 1307    2\n\nShow the Codegss_summary &lt;- gss2002 %&gt;%\n  mutate(\n    Education = factor(\n      Education,\n      levels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\"),\n      labels = c(\"Bachelors\", \"Graduate\", \"Jr Col\", \"HS\", \"Left HS\")\n    ),\n    DeathPenalty = as.factor(DeathPenalty)\n  ) %&gt;%\n  group_by(Education, DeathPenalty) %&gt;%\n  summarise(count = n()) %&gt;% # This is good for a chisq test\n\n  # Add two more columns to faciltate mosaic/Marrimekko Plot\n  #\n  mutate(\n    edu_count = sum(count),\n    edu_prop = count / sum(count)\n  ) %&gt;%\n  ungroup()\n\ngss_summary\n\n\n  \n\n\nShow the Code# We can plot a heatmap-like `mosaic chart` for this table, using `ggplot`:\n# https://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2\n\nggplot(data = gss_summary, aes(x = Education, y = edu_prop)) +\n  geom_bar(aes(width = edu_count, fill = DeathPenalty), stat = \"identity\", position = \"fill\", colour = \"black\") +\n  geom_text(aes(label = scales::percent(edu_prop)), position = position_stack(vjust = 0.5)) +\n\n\n  # if labels are desired\n  facet_grid(~Education, scales = \"free_x\", space = \"free_x\") +\n  theme(scale_fill_brewer(palette = \"RdYlGn\")) +\n  # theme(panel.spacing.x = unit(0, \"npc\")) + # if no spacing preferred between bars\n  theme_void()\n\n\n\n\n\n\n\nLet us now perform the base chisq test: We need a table and then the chisq test:\n\nShow the Codegss_table &lt;- tally(DeathPenalty ~ Education, data = gss2002)\ngss_table\n\n            Education\nDeathPenalty Bachelors Graduate  HS Jr Col Left HS\n      Favor        135       64 511     71     117\n      Oppose        71       50 200     16      72\n\nShow the Code# Get the observed chi-square statistic\nobservedChi2 &lt;- mosaic::chisq(tally(DeathPenalty ~ Education, data = gss2002))\nobservedChi2\n\nX.squared \n 23.45093 \n\nShow the Code# Actual chi-square test\nstats::chisq.test(tally(DeathPenalty ~ Education, data = gss2002))\n\n\n    Pearson's Chi-squared test\n\ndata:  tally(DeathPenalty ~ Education, data = gss2002)\nX-squared = 23.451, df = 4, p-value = 0.0001029\n\n\nWe should now repeat the test with permutations on Education:\n\nShow the Codenull_chisq &lt;- do(10000) * chisq.test(tally(DeathPenalty ~ shuffle(Education), data = gss2002))\n\nhead(null_chisq)\n\n\n  \n\n\nShow the Codegf_histogram(~X.squared, data = null_chisq) %&gt;%\n  gf_vline(xintercept = observedChi2, color = \"red\")\n\n\n\n\n\n\nShow the Codegf_histogram(~p.value, data = null_chisq, binwidth = 0.1, center = 0.05)\n\n\n\n\n\n\n\nSo we would conclude that Education has a significant effect on DeathPenalty opinion!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/60-SimTest/files/sim-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/60-SimTest/files/sim-tutorial.html",
    "title": "Simulation",
    "section": "",
    "text": "In this module we will use simulation to solve several problems in Business Decision Making."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#setting-up-r-packages",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\n\nlibrary(resampledata)"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#case-study-1-icecream",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#case-study-1-icecream",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "\n Case Study-1: IceCream!!",
    "text": "Case Study-1: IceCream!!\nWhat is there to not like about icecreams!! Here is a dataset that has data on Sugar and Calories between Vanilla and Chocolate icecreams, across several brands of icecreams. Is this a sample of paired data? Let us check:\n\n Inspecting and Charting Data\ndata(\"IceCream\")\nIceCream\ninspect(IceCream)\n\n\n\n\n  \n\n\n\n\n\n\ncategorical variables:  \n   name  class levels  n missing                                  distribution\n1 Brand factor     39 39       0 Baskin Robbins (2.6%) ...                    \n\nquantitative variables:  \n               name   class   min    Q1 median    Q3 max      mean        sd  n\n1   VanillaCalories integer 120.0 140.0    160 240.0 307 191.41026 58.644207 39\n2        VanillaFat numeric   4.5   7.5      9  15.5  21  11.28718  4.431655 39\n3      VanillaSugar numeric  10.0  12.5     17  21.0  27  17.13077  4.841333 39\n4 ChocolateCalories integer 120.0 140.0    170 260.0 320 198.74359 63.063342 39\n5      ChocolateFat numeric   5.0   7.5      9  14.7  21  11.12051  4.597378 39\n6    ChocolateSugar numeric  12.0  15.0     18  22.3  33  18.97436  5.402812 39\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n6       0\n\n\n\nHmm…the data are about calories, fat, and sugar between two flavours of icecream sold by each brand. There are 39 brands.\nLet us plot the data first:\nIceCream %&gt;%\n  gf_col(fct_reorder(Brand, VanillaCalories) ~ VanillaCalories,\n    fill = \"red\"\n  ) %&gt;%\n  gf_col(fct_reorder(Brand, VanillaCalories) ~ -ChocolateCalories,\n    fill = \"green\",\n    xlab = \"Calories\", ylab = \"Brand\",\n    title = \"Calories across Icecream Brands\",\n    subtitle = \"Vanilla = Red, Green = Chocolate\"\n  ) %&gt;%\n  gf_theme(theme_classic())\nIceCream %&gt;%\n  gf_col(fct_reorder(Brand, VanillaFat) ~ VanillaFat,\n    fill = \"red\"\n  ) %&gt;%\n  gf_col(fct_reorder(Brand, VanillaFat) ~ -ChocolateFat,\n    fill = \"green\",\n    xlab = \"Fat\", ylab = \"Brand\",\n    title = \"Calories across Icecream Brands\",\n    subtitle = \"Vanilla = Red, Green = Chocolate\"\n  ) %&gt;%\n  gf_theme(theme_classic())\nIceCream %&gt;%\n  gf_col(fct_reorder(Brand, VanillaSugar) ~ VanillaSugar,\n    fill = \"red\"\n  ) %&gt;%\n  gf_col(fct_reorder(Brand, VanillaSugar) ~ -ChocolateSugar,\n    fill = \"green\",\n    xlab = \"Sugar\", ylab = \"Brand\",\n    title = \"Calories across Icecream Brands\",\n    subtitle = \"Vanilla = Red, Green = Chocolate\"\n  ) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe may hypothesize that say, the fat content in the two flavours might be similar on a per brand basis. That is, if say Baskin Robbins has high sugar in the vanilla flavour, it is likely to have high sugar also in its chocolate flavour.\nLet us see what are the observed differences in the mean values of calories, sugar, and fat across brands:\n\nIceCream %&gt;%\n  mutate(\n    diff_calories = VanillaCalories - ChocolateCalories,\n    diff_fat = VanillaFat - ChocolateFat,\n    diff_sugar = VanillaSugar - ChocolateSugar\n  ) %&gt;%\n  summarise(\n    mean_diff_calories = mean(diff_calories),\n    mean_diff_fat = mean(diff_fat),\n    mean_diff_sugar = mean(diff_sugar)\n  )\n\n\n  \n\n\n\nHmm…while the numbers showing difference in means are quite different, we need to perform tests to infer whether these difference are statistically significant.\n\n Hypothesis\nHow do we specify our Hypotheses? (Of course, there is more than one!)\nWrite the Null and Alternate hypotheses here.\n\n Null Distribution Computations\nHow do we compute the NULL distributions, for each of the three components of the ice creams, using pair-wise analysis?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#conclusions",
    "href": "content/courses/Analytics/Inference/Modules/120-PairedMeans/files/paired-means-tutorial.html#conclusions",
    "title": "Tutorial on Inference for Two Paired Means",
    "section": "\n Conclusions",
    "text": "Conclusions\nSo are there significant differences in sugar, fat, and calorie content across the two flavours?\nIs this conclusion different if you don’t use paired-data, and just treat the data as independent readings?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#introduction",
    "title": "🧭 Basics of Statistical Inference",
    "section": " Introduction",
    "text": "Introduction\nIn this set of modules we will explore Data, understand what types of data variables there are, and the kinds of statistical tests and visualizations we can create with them.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#the-big-ideas-in-stats",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#the-big-ideas-in-stats",
    "title": "🧭 Basics of Statistical Inference",
    "section": "The Big Ideas in Stats",
    "text": "The Big Ideas in Stats\nSteven Stigler(Stigler 2016) is the author of the book “The Seven Pillars of Statistical Wisdom”. The Big Ideas in Statistics from that book are:\n\n1.Aggregation\n\nThe first pillar I will call Aggregation, although it could just as well be given the nineteenth-century name, “The Combination of Observations,” or even reduced to the simplest example, taking a mean. Those simple names are misleading, in that I refer to an idea that is now old but was truly revolutionary in an earlier day—and it still is so today, whenever it reaches into a new area of application. How is it revolutionary? By stipulating that, given a number of observations, you can actually gain information by throwing information away! In taking a simple arithmetic mean, we discard the individuality of the measures, subsuming them to one summary.\n\n\n\n2.Information\n\nIn the early eighteenth century it was discovered that in many situations the amount of information in a set of data was only proportional to the square root of the number n of observations, not the number n itself.\n\n\n\n3.Likelihood\n\nBy the name I give to the third pillar, Likelihood, I mean the calibration of inferences with the use of probability. The simplest form for this is in significance testing and the common P-value.\n\n\n\n4.Intercomparison\n\nIt represents what was also once a radical idea and is now commonplace: that statistical comparisons do not need to be made with respect to an exterior standard but can often be made in terms interior to the data themselves. The most commonly encountered examples of intercomparisons are Student’s t-tests and the tests of the analysis of variance.\n\n\n\n5.Regression\n\nI call the fifth pillar Regression, after Galton’s revelation of 1885, explained in terms of the bivariate normal distribution. Galton arrived at this by attempting to devise a mathematical framework for Charles Darwin’s theory of natural selection, overcoming what appeared to Galton to be an intrinsic contradiction in the theory: selection required increasing diversity, in contradiction to the appearance of the population stability needed for the definition of species.\n\n\n\n6.Design of Experiments and Observations\n\nThe sixth pillar is Design, as in “Design of Experiments,” but conceived of more broadly, as an ideal that can discipline our thinking in even observational settings.Starting in the late nineteenth century, a new understanding of the topic appeared, as Charles S. Peirce and then Fisher discovered the extraordinary role randomization could play in inference.\n\n\n\n7.Residuals\n\nThe most common appearances in Statistics are our model diagnostics (plotting residuals), but more important is the way we explore high-dimensional spaces by fitting and comparing nested models.\n\nIn our work with Statistical Models, we will be working with all except Idea 6 above.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#what-is-a-statistical-model",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#what-is-a-statistical-model",
    "title": "🧭 Basics of Statistical Inference",
    "section": "What is a Statistical Model?",
    "text": "What is a Statistical Model?\nFrom Daniel Kaplan’s book:\n\n“Modeling” is a process of asking questions. “Statistical” refers in part to data – the statistical models you will construct will be rooted in data. But it refers also to a distinctively modern idea: that you can measure what you don’t know and that doing so contributes to your understanding.\n\nThe conclusions you reach from data depend on the specific questions you ask. The word “modeling” highlights that your goals, your beliefs, and your current state of knowledge all influence your analysis of data. You examine your data to see whether they are consistent with the hypotheses that frame your understanding of the system under study.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#types-of-statistical-models-based-on-purpose",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#types-of-statistical-models-based-on-purpose",
    "title": "🧭 Basics of Statistical Inference",
    "section": "Types of Statistical Models Based on Purpose",
    "text": "Types of Statistical Models Based on Purpose\nThere are three main uses for statistical models. They are closely related, but distinct enough to be worth enumerating.\nDescription. Sometimes you want to describe the range or typical values of a quantity. For example, what’s a “normal” white blood cell count? Sometimes you want to describe the relationship between things. Example: What’s the relationship between the price of gasoline and consumption by automobiles?\nClassification or Prediction. You often have information about some observable traits, qualities, or attributes of a system you observe and want to draw conclusions about other things that you can’t directly observe. For instance, you know a patient’s white blood-cell count and other laboratory measurements and want to diagnose the patient’s illness.\nAnticipating the consequences of interventions. Here, you intend to do something: you are not merely an observer but an active participant in the system. For example, people involved in setting or debating public policy have to deal with questions like these: To what extent will increasing the tax on gasoline reduce consumption? To what extent will paying teachers more increase student performance?\nThe appropriate form of a model depends on the purpose. For example, a model that diagnoses a patient as ill based on an observation of a high number of white blood cells can be sensible and useful. But that same model could give absurd predictions about intervention: Do you really think that lowering the white blood cell count by bleeding a patient will make the patient better?\nTo anticipate correctly the effects of an intervention you need to get the direction of cause (polarity) and effect (magnitude) correct in your models.\n\n\n\n\n\n\nNote\n\n\n\nAn effect size tells how the output of a model changes when a simple change is made to the input.Effect sizes always involve two variables: a response variable and a single explanatory variable. Effect size is always about a model. The model might have one explanatory variable or many explanatory variables. Each explanatory variable will have its own effect size, so a model with multiple explanatory variables will have multiple effect sizes.\n\n\n\n\n\n\n\n\nNote\n\n\n\nBut for a model used for classification or prediction, it may be unnecessary to represent causation correctly. Instead, other issues, e.g., the reliability of data, can be the most important. One of the thorniest issues in statistical modeling – with tremendous consequences for science, medicine, government, and commerce – is how you can legitimately draw conclusions about interventions from models based on data collected without performing these interventions.\n\n\n\nTypes of Models Based on Data Variables\nLet us look at the famous dataset pertaining to Francis Galton’s work on the heights of children and the heights of their parents. We can create 4 kinds of models based on the types of variables in that dataset.\n\n\n\n\n\n\n\nVariables and Models",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#linear-models-everywhere",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#linear-models-everywhere",
    "title": "🧭 Basics of Statistical Inference",
    "section": "Linear Models Everywhere",
    "text": "Linear Models Everywhere\nOne method in this set of modules is to take the modern view that all these models can be viewed from a standpoint of the Linear Model, also called Linear Regression \\(y = \\beta_1 *x + \\beta_0\\) . For example, it is relatively straightforward to imagine Plot B (Quant vs Quant ) as an example of a Linear Model, with the dependent variable modelled as \\(y\\) and the independent one as \\(x\\). We will try to work up to the intuition that this model can be used to understand all the models in the Figure.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#a-flowchart-of-statistical-inference-tests",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#a-flowchart-of-statistical-inference-tests",
    "title": "🧭 Basics of Statistical Inference",
    "section": "A Flowchart of Statistical Inference Tests",
    "text": "A Flowchart of Statistical Inference Tests\n\n\n\n\n\nflowchart TD\n    A[Inference for Means] --&gt;|Check Assumptions|B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test\\n Outliers: Box Plots]\n    B --&gt; M[Means]\n\n subgraph Means\n    direction TB\n      subgraph Single-Mean\n        direction LR\n        OM[Single Mean]&lt;--&gt;|p| TT[t.test]\n        TWT[t.test \\n Welch]&lt;--&gt;|p diff var|OM[Single Mean]\n        WT[wilcox.test]&lt;--&gt;|np|OM[Single Mean]\n      end\n      \n      subgraph Paired-Means\n        direction LR\n        TM[Paired Means]&lt;--&gt;|p| TTP[t.test with pairs]\n        WTP[wilcox.test with pairs\\n Mann-Whitney U Test]--&gt;|np|TM\n      end\n      \n      subgraph Multiple-Means\n        direction LR\n        MM[Multiple Means] --&gt;|p| ANO[ANOVA]\n        KW[kruskal.test]&lt;--&gt;|np indep| MM\n        FT[friedman.test]&lt;--&gt;|np dep| MM\n      end\n      \nM --&gt;Single-Mean\nSingle-Mean--&gt;Paired-Means\nPaired-Means--&gt;Multiple-Means\n\nend\n\n%%subgraph LM\n%%  direction BT\n%%  LM[Linear Model]--&gt;Means\n%%end",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/10-Intro/index.html#references",
    "title": "🧭 Basics of Statistical Inference",
    "section": "References",
    "text": "References\n\nChester Ismay and Albert Y. Kim. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. Available Online https://moderndive.com/index.html\nhttp://drafts.jsvine.com/the-magic-criteria/\nTihamér von Ghyczy, The Fruitful Flaws of Strategy Metaphors. Harvard Business Review, 2003. https://hbr.org/2003/09/the-fruitful-flaws-of-strategy-metaphors\nDaniel T. Kaplan, Statistical Models (second edition). Available online. https://dtkaplan.github.io/SM2-bookdown/\nDaniel T. Kaplan, Compact Introduction to Classical Inference, 2020. Available Online. https://dtkaplan.github.io/CompactInference/\nDaniel T. Kaplan and Frank Shaw, Statistical Modeling: Computational Technique. Available online https://www.mosaic-web.org/go/SM2-technique/\nJonas Kristoffer Lindeløv, Common statistical tests are linear models (or: how to teach stats) https://lindeloev.github.io/tests-as-linear/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🧭 Basics of Statistical Inference"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html",
    "title": "Sampling",
    "section": "",
    "text": "Continuing to treat the NHANES dataset as a population, We will try to replicate the process of sampling and CLT for another variable in the NHANES variable, AlcoholYear.\n\n\nTry sample sizes of 25, 50, 100, 500.\n\n\n\nWrite your observations here!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#sampling-alcoholyear",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#sampling-alcoholyear",
    "title": "Sampling",
    "section": "",
    "text": "Try sample sizes of 25, 50, 100, 500."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/20-SampProb/files/sampling-tutorial.html#conclusion",
    "title": "Sampling",
    "section": "",
    "text": "Write your observations here!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "knitr::opts_chunk$set(echo = TRUE, message = TRUE, warning = TRUE, fig.align = \"center\")\nlibrary(tidyverse)\nlibrary(mosaic) # Our go-to package\nlibrary(infer) # An alternative package for inference using tidy data\nlibrary(broom) # Clean test results in tibble form\nlibrary(skimr) # data inspection\n\nlibrary(resampledata) # Datasets from Chihara and Hesterberg's book\nlibrary(openintro) # datasets\nlibrary(gt) # for tables\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]\n\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#introduction",
    "title": "Inference for Two Independent Means",
    "section": "",
    "text": "flowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]\n\n\n\n\nflowchart TD\n    A[Inference for Independent Means] --&gt;|Check Assumptions| B[Normality: Shapiro-Wilk Test shapiro.test\\n Variances: Fisher F-test var.test]\n    B --&gt; C{OK?}\n    C --&gt;|Yes, both\\n Parametric| D[t.test]\n    D &lt;--&gt;F[Linear Model\\n Method] \n    C --&gt;|Yes, but not variance\\n Parametric| W[t.test with\\n Welch Correction]\n    W&lt;--&gt;F\n    C --&gt;|No\\n Non-Parametric| E[wilcox.test]\n    E &lt;--&gt; G[Linear Model\\n with\\n Signed-Ranks]\n    C --&gt;|No\\n Non-Parametric| P[Bootstrap\\n or\\n Permutation]\n    P &lt;--&gt; Q[Linear Model\\n with Signed-Rank\\n with Permutation]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nA.  Check for Normality\nStatistical tests for means usually require a couple of checks1 2:\n\nAre the data normally distributed?\n\nAre the data variances similar?:\n\nLet us also complete a check for normality: the shapiro.wilk test checks whether a Quant variable is from a normal distribution; the NULL hypothesis is that the data are from a normal distribution.\nB.  Check for Variances\n\n\n\n\n\n\nImportantConditions:\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\nType this in your console: help(yrbss)Type help(wilcox.test) in your Console.\n\nUsing the Parametric t.test\nUsing the non-parametric wilcox.test\nUsing the Linear Model Interpretation\nUsing the Permutation Test\nAll Tests Together\nA.  Check for Normality\nB.  Check for Variances\nAll Tests Together\nA.  Check for Normality\nB.  Check for Variances\n Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Case Study #2: Youth Risk Behavior Surveillance System (YRBSS) survey\nEvery two years, the Centers for Disease Control and Prevention in the USA conduct the Youth Risk Behavior Surveillance System (YRBSS) survey, where it takes data from highschoolers (9th through 12th grade), to analyze health patterns. We will work with a selected group of variables from a random sample of observations during one of the years the YRBSS was conducted.\n\n Inspecting and Charting Data\n\ndata(yrbss)\nyrbss\nyrbss_inspect &lt;- inspect(yrbss)\nyrbss_inspect$categorical\nyrbss_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have 13K data entries, and with 13 different variables, some Qual and some Quant. Many entries are missing too, typical of real-world data and something we will have to account for in our computations. The meaning of each variable can be found by bringing up the help file. \nIn this tutorial, our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nDoes weight of highschoolers in this dataset vary with gender?\n\n\n\n Inspecting and Charting Data\nFirst, histograms and densities of the variable we are interested in:\nyrbss_select_gender &lt;- yrbss %&gt;%\n  select(weight, gender, physically_active_7d) %&gt;%\n  drop_na(weight) # Sadly dropping off NA data\n\nyrbss_select_gender %&gt;%\n  gf_density(~weight,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_theme(theme_classic())\nyrbss_select_gender %&gt;%\n  gf_boxplot(weight ~ gender,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapped Distribution plot shows some difference in the means; and the Boxplots show visible difference in the medians.\n\n\n\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\n\n\n\n\n\n\nNoteShapiro-Wilks Test\n\n\n\nThe longest data it can take (in R) is 5000. Since our data is longer, we will cannot use this procedure and have to resort to visual means.\n\n\nmale_student_weights &lt;- yrbss_select_gender %&gt;%\n  filter(gender == \"male\") %&gt;%\n  select(weight)\nfemale_student_weights &lt;- yrbss_select_gender %&gt;%\n  filter(gender == \"female\") %&gt;%\n  select(weight)\n# shapiro.test(male_student_weights$weight)\n# shapiro.test(female_student_weights$weight)\n\nyrbss_select_gender %&gt;%\n  gf_density(~weight,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_facet_grid(~gender) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nDistributions are not too close to normal…perhaps a hint of a rightward skew, suggesting that there are some obese students.\nWe can plot Q-Q plots3 for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\nyrbss_select_gender %&gt;%\n  gf_qq(~ weight | gender) %&gt;%\n  gf_qqline(ylab = \"scores\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nNo real evidence (visually) of the variables being normally distributed.\n\n\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\n\nvar.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n# qf(0.975,6164, 6413)\n\n\n  \n\n\n\nThe p.value being so small, we are able to reject the NULL Hypothesis that the variances of weight are nearly equal across the two exercise regimes.\n\n\n\n\n\n\nImportantConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nThis means that the parametric t.test must be eschewed in favour of the non-parametric wilcox.test. We will use that, and also attempt linear models with rank data, and a final permutation test.\n\n Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across gender. So accordingly:\n\\[\nH_0: \\mu_{male} = \\mu_{female}\\\\\n\\\\\\\nH_a: \\mu_{male} \\ne \\mu_{female}\\\n\\]\n\n Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_gender &lt;- diffmean(weight ~ gender, data = yrbss_select_gender)\n\nobs_diff_gender\n\n\n\ndiffmean \n11.70089 \n\n\n\n\n Inference\n\n\nUsing the wilcox.test\nUsing the Linear Model\nUsing the Permutation Test\n\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and the variances are significantly different, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test:4\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\nWe will use the mosaic variant).  Our model would be:\n\\[\nmean(rank(Weight_{male})) - mean(rank(Weight_{female})) =\n\\beta_0\n\\\\\\\nH_0: \\beta_0 = 0;\\\\\n\\\\\\\nH_a: \\beta_0 \\ne 0\n\\]\n\nwilcox.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is negligible and we are able to reject the NULL hypothesis that the means are equal.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n# Create a sign-rank function\n# signed_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(rank(weight) ~ gender,\n  data = yrbss_select_gender\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nTipDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The gender variable was treated as a binary “dummy” variable5.\n\n\n\n\nWe saw from the diagram created by Allen Downey that there is only one test6! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. For the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\nnull_dist_weight &lt;-\n  do(9999) * diffmean(data = yrbss_select_gender, weight ~ shuffle(gender))\nnull_dist_weight\ngf_histogram(data = null_dist_weight, ~diffmean, bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;%\n  gf_theme(theme_classic())\ngf_ecdf(data = null_dist_weight, ~diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;%\n  gf_theme(theme_classic())\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_weight)\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n        1 \n\n\n\nClearly the observed_diff_weight is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!\n\n\n\n\n\n\nWe can put all the test results together to get a few more insights about the tests:\n\nwilcox.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy() %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"wilcox.test\")\n\nlm(rank(weight) ~ gender,\n  data = yrbss_select_gender\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  ) %&gt;%\n  gt() %&gt;%\n  tab_style(\n    style = list(cell_fill(color = \"cyan\"), cell_text(weight = \"bold\")),\n    locations = cells_body(columns = p.value)\n  ) %&gt;%\n  tab_header(title = \"Linear Model with Ranked Data\")\n\n\n\n\n\n\nwilcox.test\n\n\nestimate\nstatistic\np.value\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n-11.33999\n10808212\n0\n-11.34003\n-10.87994\nWilcoxon rank sum test with continuity correction\ntwo.sided\n\n\n\n\n\n\n\n\nLinear Model with Ranked Data\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n4836.157\n42.52745\n113.71848\n0\n4752.797\n4919.517\n\n\ngendermale\n2851.246\n59.55633\n47.87478\n0\n2734.507\n2967.986\n\n\n\n\n\n\nThe wilcox.test and the linear model with rank data offer the same results. This is of course not surprising!\n\n Case Study #3: Weight vs Exercise in the YRBSS Survey\nNext, consider the possible relationship between a highschooler’s weight and their physical activity.\nFirst, let’s create a new variable physical_3plus, which will be coded as either “yes” if the student is physically active for at least 3 days a week, and “no” if not. Recall that we have several missing data in that column, so we will (sadly) drop these before generating the new variable:\n\nyrbss_select_phy &lt;- yrbss %&gt;%\n  drop_na(physically_active_7d, weight) %&gt;%\n  mutate(\n    physical_3plus = if_else(physically_active_7d &gt;= 3, \"yes\", \"no\"),\n    physical_3plus = factor(physical_3plus,\n      labels = c(\"yes\", \"no\"),\n      levels = c(\"yes\", \"no\")\n    )\n  ) %&gt;%\n  select(weight, physical_3plus)\n\n# Let us check\nyrbss_select_phy %&gt;% count(physical_3plus)\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nDoes weight vary based on whether students exercise on more or less than 3 days a week? (physically_active_7d &gt;= 3 days)\n\n\n\n Inspecting and Charting Data\nWe can make distribution plots for weight by physical_3plus:\n\ngf_boxplot(weight ~ physical_3plus,\n  fill = ~physical_3plus,\n  data = yrbss_select_phy, xlab = \"Days of Exercise &gt;=3 \"\n) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\ngf_density(~weight,\n  fill = ~physical_3plus,\n  data = yrbss_select_phy\n) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss_select_phy %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test. But before that a few more checks on the data:\n\n\n\nAs stated before, statistical tests for means usually require a couple of checks:\n\nAre the data normally distributed?\n\nAre the data variances similar?\n\nLet us also complete a visual check for normality,with plots since we cannot do a shapiro.test:\nyrbss_select_phy %&gt;%\n  gf_density(~weight,\n    fill = ~physical_3plus,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Exercise Frequency\"\n  ) %&gt;%\n  gf_facet_grid(~physical_3plus) %&gt;%\n  gf_fitdistr(dist = \"dnorm\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nAgain, not normally distributed…\nWe can plot Q-Q plots for both variables, and also compare both data with normally-distributed data generated with the same means and standard deviations:\nyrbss_select_phy %&gt;%\n  gf_qq(~ weight | physical_3plus, color = ~physical_3plus) %&gt;%\n  gf_qqline(ylab = \"Weight\") %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nThe QQ-plots confirm that he tow data variables are not normally distributed.\n\n\nLet us check if the two variables have similar variances: the var.test does this for us, with a NULL hypothesis that the variances are not significantly different:\nvar.test(weight ~ physical_3plus,\n  data = yrbss_select_phy,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n# Critical F value\nqf(0.975, 4021, 8341)\n\n\n\n\n  \n\n\n\n[1] 1.054398\n\n\n\nThe p.value states the probability of the data being what it is, assuming the NULL hypothesis that variances were similar. It being so small, we are able to reject this NULL Hypothesis that the variances of weight are nearly equal across the two exercise frequencies. (Compare the statistic in the var.test with the critical F-value)\n\n\n\n\n\n\nImportantConditions\n\n\n\n\nThe two variables are not normally distributed.\nThe two variances are also significantly different.\n\n\n\nHence we will have to use non-parametric tests to infer if the means are similar.\n\n Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across physical_3plus. So accordingly:\n\\[\nH_0: \\mu_{physical-3plus-Yes} = \\mu_{physical-3plus-No}\\\\\n\\\\\\\nH_a: \\mu_{physical-3plus-Yes} \\ne \\mu_{physical-3plus-No}\\\\\n\\]\n\n Observed and Test\nStatistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_phy &lt;- diffmean(weight ~ physical_3plus, data = yrbss_select_phy)\n\nobs_diff_phy\n\n\n\n diffmean \n-1.774584 \n\n\n\n\n\n\n\n\nUsing parametric t.test\nUsing non-parametric paired Wilcoxon test\n\n\n\nWell, the variables are not normally distributed, and the variances are significantly different so a standard t.test is not advised. We can still try:\n\nmosaic::t_test(weight ~ physical_3plus,\n  var.equal = FALSE, # Welch Correction\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is \\(8.9e-08\\) ! And the Confidence Interval is clear of \\(0\\). So the t.test gives us good reason to reject the Null Hypothesis that the means are similar. But can we really believe this, given the non-normality of data?\n\n\nHowever, we have seen that the data variables are not normally distributed. So a Wilcoxon Test, using signed-ranks, is indicated: (recall the model!)\n\n# For stability reasons, it may be advisable to use rounded data or to set digits.rank = 7, say,\n# such that determination of ties does not depend on very small numeric differences (see the example).\n\nwilcox.test(weight ~ physical_3plus,\n  conf.int = TRUE,\n  conf.level = 0.95,\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe nonparametric wilcox.test also suggests that the means for weight across physical_3plus are significantly different.\nUsing the Linear Model Interpretation\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  physical.3plus) = \\beta_0 + \\beta_1 \\times physical.3plus\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\nlm(rank(weight) ~ physical_3plus,\n  data = yrbss_select_phy\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\nHere too, the linear model using rank data arrives at a conclusion similar to that of the Mann-Whitney U test.\nUsing Permutation Tests\nWe will do this in two ways, just for fun: one using mosaic and the other using infer.\nBut first, we need to initialize the test, which we will save as obs_diff.\nobs_diff_infer &lt;- yrbss_select_phy %&gt;%\n  infer::specify(weight ~ physical_3plus) %&gt;%\n  infer::calculate(stat = \"diff in means\", order = c(\"yes\", \"no\"))\nobs_diff_infer\nobs_diff_mosaic &lt;- mosaic::diffmean(~ weight | physical_3plus, data = yrbss_select_phy)\nobs_diff_mosaic\nobs_diff_phy\n\n\n\n\n  \n\n\n\n diffmean \n-1.774584 \n\n\n diffmean \n-1.774584 \n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that obs_diff_infer is a 1 X 1 dataframe; obs_diff_mosaic is a scalar!!\n\n\n\nInference Using mosaic\n\n\nWe already have the observed difference, obs_diff_mosaic. Now we generate the null distribution using permutation, with mosaic:\n\nnull_dist_mosaic &lt;- do(999) * diffmean(~ weight | shuffle(physical_3plus), data = yrbss_select_phy)\n\nWe can also generate the histogram of the null distribution, compare that with the observed diffrence and compute the p-value and confidence intervals:\n\ngf_histogram(~diffmean, data = null_dist_mosaic) %&gt;%\n  gf_vline(xintercept = obs_diff_mosaic, colour = \"red\")\n\n\n\n\n\n\n# p-value\nprop(~ diffmean != obs_diff_mosaic, data = null_dist_mosaic)\n\nprop_TRUE \n        1 \n\n# Confidence Intervals for p = 0.95\nmosaic::cdata(~diffmean, p = 0.95, data = null_dist_mosaic)\n\n\n  \n\n\n\n\n\n\n\nYour Turn\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\n\ndata(yrbss)\nyrbss\nyrbss_inspect &lt;- inspect(yrbss)\nyrbss_inspect$categorical\nyrbss_inspect$quantitative\n\n\n  \n\n\n  \n\n\n  \n\n\n\nWe have 13K data entries, and with 13 different variables, some Qual and some Quant. Many entries are missing too, typical of real-world data and something we will have to account for in our computations. The meaning of each variable can be found by bringing up the help file. \nIn this tutorial, our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nDoes weight of highschoolers in this dataset vary with gender?"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nFirst, histograms and densities of the variable we are interested in:\nyrbss_select_gender &lt;- yrbss %&gt;%\n  select(weight, gender, physically_active_7d) %&gt;%\n  drop_na(weight) # Sadly dropping off NA data\n\nyrbss_select_gender %&gt;%\n  gf_density(~weight,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_theme(theme_classic())\nyrbss_select_gender %&gt;%\n  gf_boxplot(weight ~ gender,\n    fill = ~gender,\n    alpha = 0.5,\n    title = \"Highschoolers' Weights by Gender\"\n  ) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapped Distribution plot shows some difference in the means; and the Boxplots show visible difference in the medians."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across gender. So accordingly:\n\\[\nH_0: \\mu_{male} = \\mu_{female}\\\\\n\\\\\\\nH_a: \\mu_{male} \\ne \\mu_{female}\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test-statistic-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test Statistic",
    "text": "Observed and Test Statistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_gender &lt;- diffmean(weight ~ gender, data = yrbss_select_gender)\n\nobs_diff_gender\n\n\n\ndiffmean \n11.70089"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference-1",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inference-1",
    "title": "Inference for Two Independent Means",
    "section": "\n Inference",
    "text": "Inference\n\n\nUsing the wilcox.test\nUsing the Linear Model\nUsing the Permutation Test\n\n\n\nSince the data variables do not satisfy the assumption of being normally distributed, and the variances are significantly different, we use the classical wilcox.test, which implements what we need here: the Mann-Whitney U test:4\n\nThe Mann-Whitney test as a test of mean ranks. It first ranks all your values from high to low, computes the mean rank in each group, and then computes the probability that random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed. No assumptions about distributions are needed so far. (emphasis mine)\n\nWe will use the mosaic variant).  Our model would be:\n\\[\nmean(rank(Weight_{male})) - mean(rank(Weight_{female})) =\n\\beta_0\n\\\\\\\nH_0: \\beta_0 = 0;\\\\\n\\\\\\\nH_a: \\beta_0 \\ne 0\n\\]\n\nwilcox.test(weight ~ gender,\n  data = yrbss_select_gender,\n  conf.int = TRUE,\n  conf.level = 0.95\n) %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nThe p.value is negligible and we are able to reject the NULL hypothesis that the means are equal.\n\n\nWe can apply the linear-model-as-inference interpretation to the ranked data data to implement the non-parametric test as a Linear Model:\n\\[\nlm(rank(weight) \\sim  gender) = \\beta_0 + \\beta_1 * gender\n\\\\\nH_0: \\beta_1 = 0\\\\\n\\\\\\\nH_a: \\beta_1 \\ne 0\\\\\n\\]\n\n# Create a sign-rank function\n# signed_rank &lt;- function(x) {sign(x) * rank(abs(x))}\n\nlm(rank(weight) ~ gender,\n  data = yrbss_select_gender\n) %&gt;%\n  broom::tidy(\n    conf.int = TRUE,\n    conf.level = 0.95\n  )\n\n\n  \n\n\n\n\n\n\n\n\n\nTipDummy Variables in lm\n\n\n\nNote how the Qual variable was used here in Linear Regression! The gender variable was treated as a binary “dummy” variable5.\n\n\n\n\nWe saw from the diagram created by Allen Downey that there is only one test6! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. For the specific data at hand, we need to shuffle the records between Semifinal and Final on a per Swimmer basis and take the test statistic (difference between the two swim records for each swimmer). Another way to look at this is to take the differences between Semifinal and Final scores and shuffle the differences to either polarity. We will follow this method in the code below:\nnull_dist_weight &lt;-\n  do(9999) * diffmean(data = yrbss_select_gender, weight ~ shuffle(gender))\nnull_dist_weight\ngf_histogram(data = null_dist_weight, ~diffmean, bins = 25) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;%\n  gf_theme(theme_classic())\ngf_ecdf(data = null_dist_weight, ~diffmean) %&gt;%\n  gf_vline(xintercept = obs_diff_gender, colour = \"red\") %&gt;%\n  gf_theme(theme_classic())\nprop1(~ diffmean &lt;= obs_diff_gender, data = null_dist_weight)\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprop_TRUE \n        1 \n\n\n\nClearly the observed_diff_weight is much beyond anything we can generate with permutations with gender! And hence there is a significant difference in weights across gender!"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-3",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#inspecting-and-charting-data-3",
    "title": "Inference for Two Independent Means",
    "section": "\n Inspecting and Charting Data",
    "text": "Inspecting and Charting Data\nWe can make distribution plots for weight by physical_3plus:\n\ngf_boxplot(weight ~ physical_3plus,\n  fill = ~physical_3plus,\n  data = yrbss_select_phy, xlab = \"Days of Exercise &gt;=3 \"\n) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\ngf_density(~weight,\n  fill = ~physical_3plus,\n  data = yrbss_select_phy\n) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nThe box plots show how the medians of the two distributions compare, but we can also compare the means of the distributions using the following to first group the data by the physical_3plus variable, and then calculate the mean weight in these groups using the mean function while ignoring missing values by setting the na.rm argument to TRUE.\n\nyrbss_select_phy %&gt;%\n  group_by(physical_3plus) %&gt;%\n  summarise(mean_weight = mean(weight, na.rm = TRUE))\n\n\n  \n\n\n\nThere is an observed difference, but is this difference large enough to deem it “statistically significant”? In order to answer this question we will conduct a hypothesis test. But before that a few more checks on the data:"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-2",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#hypothesis-2",
    "title": "Inference for Two Independent Means",
    "section": "\n Hypothesis",
    "text": "Hypothesis\nBased on the graphs, how would we formulate our Hypothesis? We wish to infer whether there is difference in mean weight across physical_3plus. So accordingly:\n\\[\nH_0: \\mu_{physical-3plus-Yes} = \\mu_{physical-3plus-No}\\\\\n\\\\\\\nH_a: \\mu_{physical-3plus-Yes} \\ne \\mu_{physical-3plus-No}\\\\\n\\]"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#observed-and-test",
    "title": "Inference for Two Independent Means",
    "section": "\n Observed and Test",
    "text": "Observed and Test\nStatistic\nWhat would be the test statistic we would use? The difference in means. Is the observed difference in the means between the two groups of scores non-zero? We use the diffmean function, from mosaic:\nobs_diff_phy &lt;- diffmean(weight ~ physical_3plus, data = yrbss_select_phy)\n\nobs_diff_phy\n\n\n\n diffmean \n-1.774584"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#your-turn",
    "title": "Inference for Two Independent Means",
    "section": "Your Turn",
    "text": "Your Turn\n\nCalculate a 95% confidence interval for the average height in meters (height) and interpret it in context.\nCalculate a new confidence interval for the same parameter at the 90% confidence level. Comment on the width of this interval versus the one obtained in the previous exercise.\nConduct a hypothesis test evaluating whether the average height is different for those who exercise at least three times a week and those who don’t.\nNow, a non-inference task: Determine the number of different options there are in the dataset for the hours_tv_per_school_day there are.\nCome up with a research question evaluating the relationship between height or weight and sleep. Formulate the question in a way that it can be answered using a hypothesis test and/or a confidence interval. Report the statistical results, and also provide an explanation in plain language. Be sure to check all assumptions, state your \\(\\alpha\\) level, and conclude in context."
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/110-TwoMeans/files/inf_for_numerical_data.html#footnotes",
    "title": "Inference for Two Independent Means",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://stats.stackexchange.com/questions/2492/is-normality-testing-essentially-useless↩︎\nhttps://www.allendowney.com/blog/2023/01/28/never-test-for-normality/↩︎\nhttps://stats.stackexchange.com/questions/92374/testing-large-dataset-for-normality-how-and-is-it-reliable↩︎\nhttps://stats.stackexchange.com/q/113337↩︎\nhttps://en.wikipedia.org/wiki/Dummy_variable_(statistics)↩︎\nhttps://allendowney.blogspot.com/2016/06/there-is-still-only-one-test.html↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html",
    "title": "🃏 Testing a Single Proportion",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(infer)\n\n## Datasets from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)\nlibrary(openintro)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#setting-up-r-packages",
    "title": "🃏 Testing a Single Proportion",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(infer)\n\n## Datasets from Chihara and Hesterberg's book (Second Edition)\nlibrary(resampledata)\n\n## Datasets from Cetinkaya-Rundel and Hardin's book (First Edition)\nlibrary(openintro)\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#introduction",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Introduction",
    "text": "Introduction\nOften we hear reports that a certain percentage of people support a certain political party, or that a certain proportion of people are in favour of a certain policy. Such statements are the result of a desire to infer a proportion in the population, which is what we will investigate here.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#workflow-sampling-theory-for-proportions",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#workflow-sampling-theory-for-proportions",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Workflow: Sampling Theory for Proportions",
    "text": "Workflow: Sampling Theory for Proportions\nWe have seen how sampling from a population works when we wish to estimate means:\n\nThe sample means \\(\\bar{x}\\) are centred around the population mean \\(\\mu\\);\nThe samples means are normally distributed\n\nThe uncertainty in using \\(\\bar{x}\\) as an estimate for \\(\\mu\\) is given by a Confidence interval defined by some constant times the Standard Error of the sample \\(\\frac{s}{\\sqrt(n)}\\);\nThe larger the size of the sample, the tighter the Confidence Interval.\n\nNow then: does a similar logic work for proportions too, as for means?\n\n The CLT for Proportions\n\nSample proportions are also centred around population proportions\n\nSuccess-failure condition: If \\(\\hat{p} *n &gt;= 10\\) and \\((1-\\hat{p})*n &gt;= 10\\) are both satisfied, then the we can assume that the sampling distribution of the proportion is normal. And so:\nThe Standard Error for a sample proportion is given by \\(SE = \\sqrt\\frac{\\hat{p}(1-\\hat{p})}{n}\\), where \\(\\hat{p}\\) is the sample proportion\nWe would calculate the Confidence Intervals in a similar fashion, based on the desired probability of error, as:\n\n\\[\np = \\hat{p} \\pm 1.96*{SE}\n\\]",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#case-study-1-yrbss-survey",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#case-study-1-yrbss-survey",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Case Study #1: YRBSS Survey",
    "text": "Case Study #1: YRBSS Survey\nWe will be analyzing the same dataset called the Youth Risk Behavior Surveillance System (YRBSS) survey from the openintro package, which uses data from high schoolers to help discover health patterns. The dataset is called yrbss.\n\n Workflow: Read the Data\n\ndata(yrbss, package = \"openintro\")\nyrbss\n\n\n  \n\n\n\nWhen summarizing the YRBSS data, the Centers for Disease Control and Prevention seeks insight into the population parameters. Accordingly, in this tutorial, our research questions are:\n\n\n\n\n\n\nNoteResearch Questions\n\n\n\n\nWhat are the counts within each category for the amount of days these students have texted while driving within the past 30 days?\nWhat proportion of people on earth have texted while driving each day for the past 30 days without wearing helmets?\n\n\n\nQuestion 1 pertains to the data set yrbss, our “sample”. To answer this, you can answer the question, “What proportion of people in your sample reported that they have texted while driving each day for the past 30 days?” with a statistic. Question 2 is an inference we need to make about the population of highschoolers. While the question “What proportion of people on earth have texted while driving each day for the past 30 days?” is answered with an estimate of the parameter.\nFor our first Research Question, we will choose the column helmet_12m: Remember that you can use filter to limit the dataset to just non-helmet wearers. Here, we will name the (filtered ) dataset no_helmet.\n\nyrbss %&gt;%\n  group_by(helmet_12m) %&gt;%\n  count()\n\n\n  \n\n\n##\nyrbss %&gt;%\n  group_by(text_while_driving_30d) %&gt;%\n  count()\n\n\n  \n\n\n\nAlso, it may be easier to calculate the proportion if we create a new variable that specifies whether the individual has texted every day while driving over the past 30 days or not. We will call this variable text_ind.\n\nno_helmet_text &lt;- yrbss %&gt;%\n  filter(helmet_12m == \"never\") %&gt;%\n  mutate(text_ind = ifelse(text_while_driving_30d == \"30\", \"yes\", \"no\")) %&gt;%\n  # removing most of the other variables\n  select(age, gender, text_ind)\nno_helmet_text\n\n\n  \n\n\n##\nno_helmet_text %&gt;%\n  drop_na() %&gt;%\n  count(text_ind)\n\n\n  \n\n\n##\nno_helmet_text %&gt;%\n  drop_na() %&gt;%\n  summarize(prop = prop(text_ind, success = \"yes\"), n = n())\n\n\n  \n\n\n\nThis is the observed_statistic: the proportion of people in this sample who do text when they drive without a helmet.\nVisualizing a Single Proportion\nWe can quickly plot this, just for the sake of visual understanding of the proportions:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nno_helmet_text %&gt;%\n  drop_na() %&gt;%\n  gf_bar(~text_ind) %&gt;%\n  gf_labs(\n    x = \"texted?\",\n    title = \"High-Schoolers who texted every day\",\n    subtitle = \"While driving with no helmet on!!\"\n  )",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#inference-for-a-single-proportion",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#inference-for-a-single-proportion",
    "title": "🃏 Testing a Single Proportion",
    "section": "Inference for a Single Proportion",
    "text": "Inference for a Single Proportion\nBased on this sample in the yrbss data, we wish to infer proportions for the population of high-schoolers.\nHypothesis Testing for a Single Proportion\nConsider the inference we did for a single mean. What was our NULL Hypothesis? That the population mean \\(\\mu = 0\\). for two means? That they might be equal. What might a suitable NULL Hypothesis be for a single proportion? What attitude of ain’t nothing happenin’ might we adopt?\n\n\n\n\n\n\nImportant\n\n\n\nWith proportions, we usually look for a “no difference” situation, i.e. a ratio of unity!! So our NULL hypothesis would be a proportion of 1:1 for texters and no-texters, so a proportion of \\(0.5\\)!!\n\n\n\n\nClassical Test\nUncertainty in Estimation\n Bootstrap test\n\n\n\nThe simplest test in R for a single proportion is the binom.test:\n\nmosaic::binom.test(~text_ind, data = no_helmet_text, success = \"yes\")\n\n\n\n\ndata:  no_helmet_text$text_ind  [with success = yes]\nnumber of successes = 463, number of trials = 6503, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.06506429 0.07771932\nsample estimates:\nprobability of success \n            0.07119791 \n\nmosaic::binom.test(~text_ind, data = no_helmet_text, success = \"yes\") %&gt;%\n  broom::tidy()\n\n\n  \n\n\n\nHow do we understand this result? That the sample tells us the \\(\\hat{p} = 0.07119\\) and that based on this the population proportion of those who text while driving without a helmet is also not 0.5, since the p-value is \\(2.2e-16\\). So we reject the NULL hypothesis and accept the alternative hypothesis.\n\n\nThe Confidence Intervals from the binom.test inform us about our population proportion estimate: It lies within the interval [0.06506429, 0.07771932]. We know that this is also given by:\n\\[\n\\begin{eqnarray}\nCI &=& \\hat{p} ~ \\pm 1.96*SE\\\\\n&=& \\hat{p} ~ \\pm 1.96*\\sqrt{\\hat{p}* (1-\\hat{p})/n}\\\\\n&=& 0.0711 \\pm 1.96*\\sqrt{0.0711 * (1- 0.0711)/6847}\\\\\n&=& 0.0711 \\pm 0.006\\\\\n&=& [0.065, 0.771]\n\\end{eqnarray}\n\\]\n\n Permutation Visually Demonstrated\nWe saw from the diagram created by Allen Downey that there is only one test! We will now use this philosophy to develop a technique that allows us to mechanize several Statistical Models in that way, with nearly identical code. We will first look visually at a permutation exercise. We will create dummy data that contains the following case study:\n\nA set of identical resumes was sent to male and female evaluators. The candidates in the resumes were of both genders. We wish to see if there was difference in the way resumes were evaluated, by male and female evaluators. (We use just one male and one female evaluator here, to keep things simple!)\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n\n         M \n-0.3333333 \n\n\n\n\n\n\nSo, we have a solid disparity in percentage of selection between the two evaluators! Now we pretend that there is no difference between the selections made by either set of evaluators. So we can just:\n\nPool up all the evaluations\n\nArbitrarily re-assign a given candidate(selected or rejected) to either of the two sets of evaluators, by permutation.\n\n\nHow would that pooled shuffled set of evaluations look like?\n\n\n\n  \n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nAs can be seen, the ratio is different!\nWe can now check out our Hypothesis that there is no bias. We can shuffle the data many many times, calculating the ratio each time, and plot the distribution of the differences in selection ratio and see how that artificially created distribution compares with the originally observed figure from Mother Nature.\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nnull_dist &lt;- do(4999) * diff(mean(\n  candidate_selected ~ shuffle(evaluator),\n  data = data\n))\n# null_dist %&gt;% names()\nnull_dist %&gt;%\n  gf_histogram(~M,\n    fill = ~ (M &lt;= obs_difference),\n    bins = 25, show.legend = FALSE,\n    xlab = \"Bias Proportion\",\n    ylab = \"How Often?\",\n    title = \"Permutation Test on Difference between Groups\",\n    subtitle = \"\"\n  ) %&gt;%\n  gf_vline(xintercept = ~obs_difference, color = \"red\") %&gt;%\n  gf_label(500 ~ obs_difference,\n    label = \"Observed\\n Bias\",\n    show.legend = FALSE\n  )\nmean(~ M &lt;= obs_difference, data = null_dist)\n\n\n\n\n\n\n \n\n\n[1] 0.00220044\n\n\n\nWe see that the artificial data can hardly ever (\\(p = 0.0022\\)) mimic what the real world experiment is showing. Hence we had good reason to reject our NULL Hypothesis that there is no bias.\n\n\n\nThe inferential tools for estimating a single population proportion are analogous to those used for estimating single population means: the bootstrap confidence interval and the hypothesis test.\n\nno_helmet_text %&gt;%\n  drop_na() %&gt;%\n  specify(response = text_ind, success = \"yes\") %&gt;%\n  generate(reps = 999, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"prop\") %&gt;%\n  get_ci(level = 0.95)\n\n\n  \n\n\n\nNote that since the goal is to construct an interval estimate for a proportion, it’s necessary to both include the success argument within specify, which accounts for the proportion of non-helmet wearers than have consistently texted while driving the past 30 days, in this example, and that stat within calculate is here “prop”, signaling that we are trying to do some sort of inference on a proportion.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#case-study-2-tbd",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#case-study-2-tbd",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Case Study #2: TBD",
    "text": "Case Study #2: TBD\nTo be Written up in the foreseeable future.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#an-interactive-app",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#an-interactive-app",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n An interactive app",
    "text": "An interactive app\nhttps://openintro.shinyapps.io/CLT_prop/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#wait-but-why",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nIn business, or “design research”, one encounters things that are proportions in a target population:\n\nAdoption of a service or an app\nPeople preferring a particular product\nBeliefs which are of Yes/No type: Is this Govt. doing the right thing with respect to taxes?\nKnowing what this population proportion is a necessary step to take a decision about what you will do about it.\n(Other than plot a *&%#$$%^& pie chart)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#conclusion",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#conclusion",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nWe have seen how the CLT works with proportions, in a manner similar to that with means\nThe Standard Error (and therefore the CI) for the inference of a proportion is related to the actual population proportion, which is very different behaviour from that with means, where SE was just a number that depended on the sample size\nBootstrap procedures work with inference for a single proportion. (Permutation when there are two)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#your-turn",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nType data(package = \"resampledata\") and data(package = \"resampledata3\") in your RStudio console. This will list the datasets in both these package. Try loading a few of these and infering for single proportions.\nNational Health and Nutrition Examination Survey (NHANES) dataset. Install the package NHANES and explore the dataset for proportions that might be interesting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#references",
    "href": "content/courses/Analytics/Inference/Modules/180-OneProp/index.html#references",
    "title": "🃏 Testing a Single Proportion",
    "section": "\n References",
    "text": "References\n\nStackExchange. prop.test vs binom.test in R. https://stats.stackexchange.com/q/551329\n\nMine Çetinkaya-Rundel and Johanna Hardin, OpenIntro Modern Statistics: Chapter 17\n\nLaura M. Chihara, Tim C. Hesterberg, Mathematical Statistics with Resampling and R. 3 August 2018.© 2019 John Wiley & Sons, Inc.\n\nOpenIntro Statistics Github Repo: https://github.com/OpenIntroStat/openintro-statistics\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggbrace\n0.1.2\nHuber (2025)\n\n\nopenintro\n2.5.0\nÇetinkaya-Rundel et al. (2024)\n\n\nresampledata\n0.3.2\nChihara and Hesterberg (2018)\n\n\n\n\n\n\nÇetinkaya-Rundel, Mine, David Diez, Andrew Bray, Albert Y. Kim, Ben Baumer, Chester Ismay, Nick Paterno, and Christopher Barr. 2024. openintro: Datasets and Supplemental Functions from “OpenIntro” Textbooks and Labs. https://doi.org/10.32614/CRAN.package.openintro.\n\n\nChihara, Laura M., and Tim C. Hesterberg. 2018. Mathematical Statistics with Resampling and r. John Wiley & Sons Hoboken NJ. https://github.com/lchihara/MathStatsResamplingR?tab=readme-ov-file.\n\n\nHuber, Nicolas. 2025. ggbrace: Curly Braces for “ggplot2”. https://doi.org/10.32614/CRAN.package.ggbrace.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "🃏 Testing a Single Proportion"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "",
    "text": "library(tidyverse) # Tidy data processing\nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Data inspection and Statistical Inference\nlibrary(broom) # Tidy outputs from Statistical Analyses\nlibrary(infer) # Statistical Inference, Permutation/Bootstrap\nlibrary(patchwork) # Arranging Plots\nlibrary(ggprism) # Interesting Categorical Axes\nlibrary(supernova) # Beginner-Friendly ANOVA Tables\nlibrary(paletteer) # Color Palettes\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#setting-up-r-packages",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "",
    "text": "library(tidyverse) # Tidy data processing\nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Data inspection and Statistical Inference\nlibrary(broom) # Tidy outputs from Statistical Analyses\nlibrary(infer) # Statistical Inference, Permutation/Bootstrap\nlibrary(patchwork) # Arranging Plots\nlibrary(ggprism) # Interesting Categorical Axes\nlibrary(supernova) # Beginner-Friendly ANOVA Tables\nlibrary(paletteer) # Color Palettes\n\n\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#introduction",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#introduction",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Introduction",
    "text": "Introduction\nSuppose we have three sales strategies on our website, to sell a certain product, say men’s shirts. We have observations of customer website interactions over several months. How do we know which strategy makes people buy the fastest ?\nIf there is a University course that is offered in parallel in three different classrooms, is there a difference between the average marks obtained by students in each of the classrooms?\nIn each case we have a set of Quant observations in each Qual category: Interaction Time vs Sales Strategy in the first example, and Student Marks vs Classroom in the second. We can take mean scores in each category and decide to compare them. How do we make the comparisons? One way would be to compare them pair-wise, doing as many t-tests as there are pairs. But with this rapidly becomes intractable and also dangerous: with increasing number of groups, the number of mean-comparisons becomes very large \\(N\\choose 2\\) and with each comparison the possibility of some difference showing up, just by chance, increases! And we end up making the wrong inference and perhaps the wrong decision. The trick is of course to make comparisons all at once and ANOVA is the technique that allows us to do just that.\nIn this tutorial, we will compare the Hatching Time of frog spawn1, at three different lab temperatures.\nIn this tutorial, our research question is:\n\n\n\n\n\n\nNoteResearch Question\n\n\n\nBased on the sample dataset at hand, how does frogspawn hatching time vary with different temperature settings?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-read-the-data",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-read-the-data",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Read the Data",
    "text": "Workflow: Read the Data\nDownload the data by clicking the button below.\n Download the frogs data \n\n\n\n\n\n\nImportantData Folder\n\n\n\nSave the CSV in a subfolder titled “data” inside your R work folder.\n\n\n\nfrogs_orig &lt;- read_csv(\"data/frogs.csv\")\nfrogs_orig\n\n\n  \n\n\n\nOur response variable is the hatching Time. Our explanatory variable is a factor, Temperature, with 3 levels: 13°C, 18°C and 25°C. Different samples of spawn were subject to each of these temperatures respectively.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-clean-the-data",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-clean-the-data",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Clean the Data",
    "text": "Workflow: Clean the Data\nThe data is in wide-format, with a separate column for each Temperature, and a common column for Sample ID. This is good for humans, but poor for a computer: there are NA entries since not all samples of spawn can be subject to all temperatures. (E.g. Sample ID #1 was maintained at 13°C, and there are NAs in the other two columns, which we don’t need).\nWe will first stack up the Temperature columns into a single column, separate that into pieces and then retain just the number part (13, 18, 25), getting rid of the word Temperature from the column titles. Then the remaining numerical column with temperatures (13, 18, 25) will be converted into a factor.\nWe will use pivot_longer()and separate_wider_regex() to achieve this. [See this animation for pivot_longer(): https://haswal.github.io/pivot/ ]\nfrogs_orig %&gt;%\n  pivot_longer(\n    .,\n    cols = starts_with(\"Temperature\"),\n    cols_vary = \"fastest\",\n    # new in pivot_longer\n    names_to = \"Temp\",\n    values_to = \"Time\"\n  ) %&gt;%\n  drop_na() %&gt;%\n  ##\n  separate_wider_regex(\n    cols = Temp,\n    # knock off the unnecessary \"Temperature\" word\n    # Just keep the digits thereafter\n    patterns = c(\"Temperature\", TempFac = \"\\\\d+\"),\n    cols_remove = TRUE\n  ) %&gt;%\n  # Convert Temp into TempFac, a 3-level factor\n  mutate(TempFac = factor(\n    x = TempFac,\n    levels = c(13, 18, 25),\n    labels = c(\"13\", \"18\", \"25\")\n  )) %&gt;%\n  rename(\"Id\" = `Frogspawn sample id`) -&gt; frogs_long\nfrogs_long\n##\nfrogs_long %&gt;% count(TempFac)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nSo we have cleaned up our data and have 20 samples for Hatching Time per TempFac setting.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-eda",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-eda",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: EDA",
    "text": "Workflow: EDA\nLet us plot some histograms and boxplots of Hatching Time:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\ngf_histogram(~Time,\n  fill = ~TempFac,\n  data = frogs_long, alpha = 0.5\n) %&gt;%\n  gf_vline(xintercept = ~ mean(Time)) %&gt;%\n  gf_labs(\n    title = \"Histograms of Hatching Time Distributions vs Temperature\",\n    x = \"Hatching Time\", y = \"Count\"\n  ) %&gt;%\n  gf_text(7 ~ (mean(Time) + 2),\n    label = \"Overall Mean\"\n  ) %&gt;%\n  gf_refine(\n    scale_fill_paletteer_d(\"ggthemes::colorblind\"),\n    guides(fill = guide_legend(title = \"Temperature level (°C)\"))\n  )\n\n\n\n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\ngf_boxplot(\n  data = frogs_long,\n  Time ~ TempFac,\n  fill = ~TempFac,\n  alpha = 0.5\n) %&gt;%\n  gf_vline(xintercept = ~ mean(Time)) %&gt;%\n  gf_labs(\n    title = \"Boxplots of Hatching Time Distributions vs Temperature\",\n    x = \"Temperature\", y = \"Hatching Time\",\n    caption = \"Using ggprism\"\n  ) %&gt;%\n  gf_refine(\n    scale_fill_paletteer_d(\"ggthemes::colorblind\"),\n    scale_x_discrete(guide = \"prism_bracket\"),\n    guides(fill = guide_legend(title = \"Temperature level (°C)\"))\n  )\n\n\n\n\n\n\n\nThe histograms look well separated and the box plots also show very little overlap. So we can reasonably hypothesize that Temperature has a significant effect on Hatching Time.\nLet’s go ahead with our ANOVA test.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-anova",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-anova",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: ANOVA",
    "text": "Workflow: ANOVA\nWe will first execute the ANOVA test with code and evaluate the results. Then we will do an intuitive walkthrough of the process and finally, hand-calculate entire analysis for clear understanding. For now, a little faith!\n\n\nCode\nIntuitive\nFrogs Demonstrated\n\n\n\nR offers a very simple command aov to execute an ANOVA test: Note the familiar formula of stating the variables:\n\nfrogs_anova &lt;- aov(Time ~ TempFac, data = frogs_long)\n\nThis creates an ANOVA model object, called frogs_anova. We can examine the ANOVA model object best with a package called supernova2:\n\n# library(supernova)\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nsupernova::pairwise(frogs_anova,\n  correction = \"Bonferroni\", # Try \"Tukey\"\n  alpha = 0.05, # 95% CI calculation\n  var_equal = TRUE, # We'll see\n  plot = TRUE\n)\n\n\n\n\n\n\n\n\n  group_1 group_2    diff pooled_se       t    df   lower  upper p_adj\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18      13       -5.300     0.257 -20.608    57  -5.861 -4.739 .0000\n2 25      13      -10.100     0.257 -39.272    57 -10.661 -9.539 .0000\n3 25      18       -4.800     0.257 -18.664    57  -5.361 -4.239 .0000\n\n\nThis table + error-bar plot gives us a clear comparison between each pair of the three groups of observations defined by TempFac. The differences in spawn hatching Time between each pair of TempFac settings are given by the diff column. Also shown are the confidence intervals for each of these differences (none of which include \\(0\\)); the p-values for each of these differences is also negligible. Thus we can conclude that the effect of temperature on hatching time is significant.\n\n\n\n\n\n\nNote\n\n\n\nTo find which specific value of TempFac has the most effect will require pairwise comparison of the group means, using a standard t-test. The confidence level for such repeated comparisons will need what is called Bonferroni correction3 to prevent us from detecting a significant (pair-wise) difference simply by chance. To do this we take \\(\\alpha = 0.05\\), the confidence level used and divide it by \\(K\\), the number of pair-wise comparisons we intend to make. This new value is used to decide on the significance of the estimated parameter. So the pairwise comparisons in our current data will have to use \\(\\alpha/3 = 0.0166\\) as the confidence level. The supernova::pairwise() function did this for us very neatly!\nThere are also other ways, such as the “Tukey correction” for multiple tests.\n\n\n\n\nAll that is very well, but what is happening under the hood of the aov() command?\nConsider a data set with a single Quant and a single Qual variable. The Qual variable has two levels, the Quant data has 20 observations per Qual level.\n\n\n\n\n  \n\n\n\n\n\n\n\nAll Data: In Fig A, the horizontal black line is the overall mean of quant, denoted as \\(\\mu_{tot}\\). The vertical black lines to the points show the departures of each point from this overall mean. The sum of squares of these vertical black lines in Fig A is called the Total Sum of Squares (SST).\n\\[\nSST = \\Sigma (y - \\mu_{tot})^2\n\\tag{1}\\]\nGrouped Data: In Fig B, the horizontal green and red lines are the means of the individual groups, respectively \\(\\mu_A\\) and \\(\\mu_B\\). The green and red vertical lines are the departures, or errors, of each point from its own group-mean. The sum of the squares of the green and red lines is called the Total Error Sum of Squares (SSE).\n\\[\nSSE = \\Sigma [(y - \\mu_A)^2] + \\Sigma (y - \\mu_B)^2]\n\\tag{2}\\]\nImprovement: We take the difference in the squared error sums:\n\\[\nSSA = SST - SSE\n\\tag{3}\\]\n\\(SSA\\) is called the Treatment Sum of Squares, the “improvement” in going from believing in one mean to believing in two.\nImprovement Ratio: \\(SSA/SSE\\) might now help us decide whether two means are better than one.\nLet us compute these numbers for our toy dataset:\n\ndemo_anova &lt;- aov(quant ~ qual, data = toydata)\nsupernova::supernova(demo_anova)\n\n Analysis of Variance Table (Type III SS)\n Model: quant ~ qual\n\n                               SS df      MS       F   PRE     p\n ----- --------------- | -------- -- ------- ------- ----- -----\n Model (error reduced) |  823.407  1 823.407 139.356 .7857 .0000\n Error (from model)    |  224.529 38   5.909                    \n ----- --------------- | -------- -- ------- ------- ----- -----\n Total (empty model)   | 1047.935 39  26.870                    \n\n\nWhat do we see?\n\n\nAll Data: \\(SST = 1047.935\\).\n\nGrouped Data: \\(SSE = 224.529\\).\n\nImprovement: \\(SSA = SST-SSE\\) = \\(823.407\\).\n\nImprovement Ratio: Before we set up this ratio, we must realize that each of these measures uses a different number of observations! So the comparison is done after scaling each of \\(SSA\\) and \\(SSE\\) by the number of observations influencing them. (a sort of per capita, or average, squared error, an idea we saw when we defined Standard Errors): \\(F_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\\), where \\(df_{SSA} = 1\\) and \\(df_{SSE} = 38\\) are respectively the degrees of freedom in \\(SSA\\) and \\(SSE\\).\n\nLarge Enough Ratio?: The value of the F-statistic from the table above is \\(\\frac{823.407}{5.909} = 139.356\\). Is this ratio big enough? F-statistic is compared with a critical value of the F-critical to help us decide. (Here, it is.)\n\nBelief: So we now believe in the idea of two means.\n\nBack to Mean Differences: Finally, in order to find which of the means is significantly different from others (if there are more than two!), we need to make a pair-wise comparison of the means, applying the Bonferroni correction as stated before. This means we divide the critical p.value we expect by the number of comparisons we make between levels of the Qual variable. supernova did this for us in the error-bar plot above.\n\n\n\n\n\n\n\nImportantWhy “ANOVA”?\n\n\n\nWhen divide each of \\(SSA\\) and \\(SSE\\) by their degrees of freedom, this gives us a ratio of variances, the F-statistic. And so we are in effect deciding if means are significantly different by analyzing (a ratio of) variances! Hence the name, AN-alysis O-f VA-riance, ANOVA.\n\n\nSo this may seem like a great Hero’s Journey, where we start with means and differences, go into sums of squares, differences and comparisons of error ratios, and return to the means where we started, only to know them properly now.\n\n\nNow that we understand what aov() is doing, let us hand-calculate the numbers for our frogs dataset and check. Let us visualize our calculations first.\n\n\n\n\n\nSST\n\n\n\n\n\nSSE\n\n\n\n\nLet us get the ready table from supernova first, and then systematically calculate all numbers with understanding:\n\nsupernova::supernova(frogs_anova)\n\n Analysis of Variance Table (Type III SS)\n Model: Time ~ TempFac\n\n                               SS df      MS       F   PRE     p\n ----- --------------- | -------- -- ------- ------- ----- -----\n Model (error reduced) | 1020.933  2 510.467 385.897 .9312 .0000\n Error (from model)    |   75.400 57   1.323                    \n ----- --------------- | -------- -- ------- ------- ----- -----\n Total (empty model)   | 1096.333 59  18.582                    \n\n\nHere are the SST, SSE, and the SSA:\n\n# Calculate overall sum squares SST\nfrogs_overall &lt;- frogs_long %&gt;%\n  summarise(\n    overall_mean_time = mean(Time),\n    # Overall mean across all readings\n    # The Black Line\n\n    SST = sum((Time - overall_mean_time)^2),\n    n = n()\n  ) # Always do this with `summarise`\n\nfrogs_overall\n\n\n  \n\n\n##\nSST &lt;- frogs_overall$SST\nSST\n\n[1] 1096.333\n\n\n\n# Calculate sums of square errors *within* each group\n# with respect to individual group means\nfrogs_within_groups &lt;- frogs_long %&gt;%\n  group_by(TempFac) %&gt;%\n  summarise(\n    grouped_mean_time = mean(Time), # The Coloured Lines\n    grouped_variance_time = var(Time),\n    group_error_squares = sum((Time - grouped_mean_time)^2),\n    n = n()\n  )\nfrogs_within_groups\n##\nfrogs_SSE &lt;- frogs_within_groups %&gt;%\n  summarise(SSE = sum(group_error_squares))\n##\nSSE &lt;- frogs_SSE$SSE\nSSE\n\n\n  \n\n\n\n[1] 75.4\n\n\n\nSST\nSSE\nSSA &lt;- SST - SSE\nSSA\n\n[1] 1096.333\n[1] 75.4\n[1] 1020.933\n\n\nWe have \\(SST = 1096\\), \\(SSE = 75.4\\) and therefore \\(SSA = 1020.9\\).\nIn order to calculate the F-Statistic, we need to compute the variances, using these sum of squares. We obtain variances by dividing by their Degrees of Freedom:\n\\[\nF_{stat} = \\frac{SSA / df_{SSA}}{SSE / df_{SSE}}\n\\]\nwhere \\(df_{SSA}\\) and \\(df_{SSE}\\) are respectively the degrees of freedom in SSA and SSE.\nLet us calculate these Degrees of Freedom.\nWith \\(k = 3\\) levels in the factor TempFac, and \\(n = 20\\) points per level, \\(SST\\) clearly has degree of freedom \\(kn-1 = 3*20~ -1 = 59\\), since it uses all observations but loses one degree to calculate the global mean. (If each level did not have the same number of points \\(n\\), we simply take all observations less one as the degrees of freedom for \\(SST\\)).\n\\(SSE\\) has \\(k*(n-1) = 3 * (20 -1) = 57\\) as degrees of freedom, since each of the \\(k\\) groups there are \\(n\\) observations and each group loses one degree to calculate its own group mean.\nAnd therefore \\(SSA\\), being their difference, has \\(kn-1 -k*(n-1) = k-1 = 2\\) degrees of freedom.\nThese are, of course, as shown in the df column in the supernova tabel above. We can still calculate these in R, for the sake of method and clarity (and pedantry):\n\n# Error Sum of Squares SSE\ndf_SSE &lt;- frogs_long %&gt;%\n  # Takes into account \"unbalanced\" situations\n  # Where groups are not equal in size\n  group_by(TempFac) %&gt;%\n  summarise(per_group_df_SSE = n() - 1) %&gt;%\n  summarise(df_SSE = sum(per_group_df_SSE)) %&gt;%\n  as.numeric()\n\n\n## Overall Sum of Squares SST\ndf_SST &lt;- frogs_long %&gt;%\n  summarise(df_SST = n() - 1) %&gt;%\n  as.integer()\n\n\n# Treatment Sum of Squares SSA\nk &lt;- length(unique(frogs_long$TempFac))\ndf_SSA &lt;- k - 1\n\nThe degrees of freedom for the quantities are:\ndf_SST\ndf_SSE\ndf_SSA\n\n\n\n[1] 59\n[1] 57\n[1] 2\n\n\n\nNow we are ready to compute the F-statistic: dividing each sum-of-squares byt its degrees of freedom gives us variances which we will compare, using the F-statistic as a ratio:\n\n# Finally F_Stat!\n# Combine the sum-square_error for each level of the factor\n# Weighted by degrees of freedom **per level**\n# Which are of course equal here ;-D\n\nMSE &lt;- frogs_within_groups %&gt;%\n  summarise(mean_square_error = sum(group_error_squares / df_SSE)) %&gt;%\n  as.numeric()\nMSE\n\n[1] 1.322807\n\n##\nMSA &lt;- SSA / df_SSA # This is OK\nMSA\n\n[1] 510.4667\n\n##\nF_stat &lt;- MSA / MSE\nF_stat\n\n[1] 385.8966\n\n\nThe F-stat is compared with a critical value of the F-statistic, F_crit which is computed using the formula for the f-distribution in R. As with our hypothesis tests, we set the significance level to \\(\\alpha = 0.95\\), but here with the Bonferroni correction, and quote the two relevant degrees of freedom as parameters to qf() which computes the critical F value F_critical as a quartile:\n\nF_crit &lt;-\n  qf(\n    p = (1 - 0.05 / 3), # Significance level is 5% + Bonferroni Correction\n    df1 = df_SSA, # Numerator degrees of freedom\n    df2 = df_SSE # Denominator degrees of freedom\n  )\nF_crit\nF_stat\n\n[1] 4.403048\n[1] 385.8966\n\n\nThe F_crit value can also be seen in a plot4,5:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nmosaic::xpf(\n  q = F_crit,\n  df1 = df_SSA, df2 = df_SSE, method = \"gg\",\n  log.p = FALSE, lower.tail = TRUE,\n  return = \"plot\"\n) %&gt;%\n  gf_vline(xintercept = F_crit) %&gt;%\n  gf_label(0.75 ~ 5.5,\n    label = \"F_critical\",\n    inherit = F, show.legend = F\n  ) %&gt;%\n  gf_labs(\n    title = \"F distribution for Frogs Data\",\n    subtitle = \"F_critical = 4.403\"\n  )\n\n\n\n\n\n\n\nAny value of F more than the F_crit occurs with smaller probability than \\(0.05/3 = 0.017\\). Our F_stat is much higher than F_crit, by orders of magnitude! And so we can say with confidence that Temperature has a significant effect on spawn Time.\nAnd that is how ANOVA computes!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#stating-the-model",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#stating-the-model",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Stating the Model",
    "text": "Stating the Model\nAnd supernova gives us a nice linear equation relating Hatching_Time to TempFac:\n\nsupernova::equation(frogs_anova)\n\nFitted equation:\nTime = 26.3 + -5.3*TempFac18 + -10.1*TempFac25 + e\n\n\nTempFac18 and TempFac25 are binary {0,1} coded variables, representing the test situation. e is the remaining error. The equation models the means at each value of TempFac.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-checking-anova-assumptions",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-checking-anova-assumptions",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Checking ANOVA Assumptions",
    "text": "Workflow: Checking ANOVA Assumptions\nANOVA makes 3 fundamental assumptions:\n\nData (and errors) are normally distributed.\nVariances are equal.\nObservations are independent.\n\nWe can check these using checks and graphs.\n\n Checks for Normality\nThe shapiro.wilk test tests if a vector of numeric data is normally distributed and rejects the hypothesis of normality when the p-value is less than or equal to 0.05. \n\nshapiro.test(x = frogs_long$Time)\n\n\n    Shapiro-Wilk normality test\n\ndata:  frogs_long$Time\nW = 0.92752, p-value = 0.001561\n\n\nThe p-value is very low and we cannot reject the (alternative) hypothesis that the overall data is not normal. How about normality at each level of the factor?\n\n\n\nfrogs_long %&gt;%\n  group_by(TempFac) %&gt;%\n  group_modify(~ .x %&gt;%\n    select(Time) %&gt;%\n    as_vector() %&gt;%\n    shapiro.test() %&gt;%\n    broom::tidy())\n\n\n\n\n\n  \n\n\n\n\n\nThe shapiro.wilk test makes a NULL Hypothesis that the data are normally distributed and estimates the probability that the given data could have happened by chance. Except for TempFac = 18 the p.values are less than 0.05 and we can reject the NULL hypothesis that each of these is normally distributed. Perhaps this is a sign that we need more than 20 samples per factor level. Let there be more frogs !!! இன்னும தவளைகள் வேண்டும்!! !!\nWe can also check the residuals post-model:\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nfrogs_anova$residuals %&gt;%\n  as_tibble() %&gt;%\n  gf_dhistogram(~value, data = .) %&gt;%\n  gf_labs(\n    title = \"Residuals Histogram\",\n    x = \"Residuals\", y = \"Count\"\n  ) %&gt;%\n  gf_fitdistr()\n##\nfrogs_anova$residuals %&gt;%\n  as_tibble() %&gt;%\n  gf_qq(~value, data = .) %&gt;%\n  gf_qqstep() %&gt;%\n  gf_labs(\n    title = \"Residuals Q-Q Plot\",\n    x = \"Theoretical Quantiles\", y = \"Sample Quantiles\"\n  ) %&gt;%\n  gf_qqline()\n##\nshapiro.test(frogs_anova$residuals)\n\n\n\n\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  frogs_anova$residuals\nW = 0.94814, p-value = 0.01275\n\n\n\nUnsurprisingly, the residuals are also not normally distributed either.\n\n Check for Similar Variance\nResponse data with different variances at different levels of an explanatory variable are said to exhibit heteroscedasticity. This violates one of the assumptions of ANOVA.\nTo check if the Time readings are similar in variance across levels of TempFac, we can use the Levene Test, or since our per-group observations are not normally distributed, a non-parametric rank-based Fligner-Killeen Test. The NULL hypothesis is that the data are with similar variances. The tests assess how probable this is with the given data assuming this NULL hypothesis:\nfrogs_long %&gt;%\n  group_by(TempFac) %&gt;%\n  summarise(variance = var(Time))\n# Not too different...OK on with the test\nDescTools::LeveneTest(Time ~ TempFac, data = frogs_long)\n##\nfligner.test(Time ~ TempFac, data = frogs_long)\n\n\n\n\n  \n\n\n  \n\n\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  Time by TempFac\nFligner-Killeen:med chi-squared = 0.53898, df = 2, p-value = 0.7638\n\n\n\nIt seems that there is no cause for concern here; the data do not have significantly different variances.\n\n Independent Observations\nThis is an experiment design concern; the way the data is gathered must be specified such that data for each level of the factors ( factor combinations if there are more than one) should be independent.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-effect-size",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-effect-size",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: Effect Size",
    "text": "Workflow: Effect Size\nThe simplest way to find the actual effect sizes detected by an ANOVA test is something we have already done, with the supernova package: Here is the table and plot again:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nfrogs_supernova &lt;-\n  supernova::pairwise(frogs_anova,\n    plot = TRUE,\n    alpha = 0.05,\n    correction = \"Bonferroni\"\n  )\n\n\n\n\n\n\nfrogs_supernova\n\n\n  group_1 group_2    diff pooled_se       t    df   lower  upper p_adj\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18      13       -5.300     0.257 -20.608    57  -5.861 -4.739 .0000\n2 25      13      -10.100     0.257 -39.272    57 -10.661 -9.539 .0000\n3 25      18       -4.800     0.257 -18.664    57  -5.361 -4.239 .0000\n\n\nThis table, the plot, and the equation we set up earlier all give us the sense of how the TempFac affects Time. The differences are given pair-wise between levels of the Qual factor, TempFac, and the standard error has been declared in pooled fashion (all groups together).\n\nWe can also use (paradoxically) the summary.lm() command:\n\ntidy_anova &lt;-\n  frogs_anova %&gt;%\n  summary.lm() %&gt;%\n  broom::tidy()\ntidy_anova\n\n\n  \n\n\n\nIt may take a bit of effort to understand this. First the TempFac is arranged in order of levels, and the mean at the \\(TempFac = 13\\) is titled Intercept. That is \\(26.3\\). The other two means for levels \\(18\\) and \\(25\\) are stated as differences from this intercept, \\(-5.3\\) and \\(-10.1\\) respectively. The p.value for all these effect sizes is well below the desired confidence level of \\(0.05\\).\n\n\n\n\n\n\nNoteStandard Errors\n\n\n\nObserve that the std.error for the intercept is \\(0.257\\) while that for TempFac18 and TempFac25 is \\(0.257 \\times \\sqrt2 = 0.363\\) since the latter are differences in means, while the former is a single mean. The Variance of a difference is the sum of the individual variances, which are equal here.\n\n\nWe can easily plot bar-chart with error bars for the effect size:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\ntidy_anova %&gt;%\n  mutate(\n    hi = estimate + std.error,\n    lo = estimate - std.error\n  ) %&gt;%\n  gf_hline(\n    data = ., yintercept = 0,\n    colour = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  gf_col(estimate ~ term,\n    fill = \"grey\",\n    color = \"black\",\n    width = 0.15\n  ) %&gt;%\n  gf_errorbar(hi + lo ~ term,\n    color = \"blue\",\n    width = 0.2\n  ) %&gt;%\n  gf_point(estimate ~ term,\n    color = \"red\",\n    size = 3.5\n  ) %&gt;%\n  gf_refine(scale_x_discrete(\"Temperature (°C)\",\n    labels = c(\"13°C\", \"18°C\", \"25°C\")\n  )) %&gt;%\n  gf_labs(\n    title = \"Effect Size of Temperature on Spawn Time\",\n    subtitle = \"ANOVA: Frogs Spawn Time vs Temperature Setting\",\n    caption = \"Relative Effect Values\",\n    x = \"Temperature (°C)\", y = \"Effect Size (Spawn Time)\"\n  )\n\n\n\n\n\n\n\nIf we want an “absolute value” plot for effect size, it needs just a little bit of work:\n\n# Merging group averages with `std.error`\n# Set graph theme\ntheme_set(new = theme_custom())\n#\n\nfrogs_long %&gt;%\n  group_by(TempFac) %&gt;%\n  summarise(mean = mean(Time)) %&gt;%\n  cbind(std.error = tidy_anova$std.error) %&gt;%\n  mutate(\n    hi = mean + std.error,\n    lo = mean - std.error\n  ) %&gt;%\n  gf_hline(\n    data = ., yintercept = 0,\n    colour = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  gf_col(mean ~ TempFac,\n    fill = \"grey\",\n    color = \"black\", width = 0.15\n  ) %&gt;%\n  gf_errorbar(hi + lo ~ TempFac,\n    color = \"blue\",\n    width = 0.2\n  ) %&gt;%\n  gf_point(mean ~ TempFac,\n    color = \"red\",\n    size = 3.5\n  ) %&gt;%\n  gf_refine(scale_x_discrete(\"Temperature (°C)\",\n    labels = c(\"13°C\", \"18°C\", \"25°C\")\n  )) %&gt;%\n  gf_labs(\n    title = \"Effect Size of Temperature on Spawn Time\",\n    subtitle = \"ANOVA: Frogs Spawn Time vs Temperature Setting\",\n    caption = \"Absolute Effect Values\",\n    x = \"Temperature (°C)\", y = \"Effect Size (Spawn Time)\"\n  )\n\n\n\n\n\n\n\nIn both graphs, note the difference in the error-bar heights.\nThe ANOVA test does not tell us that the “treatments” (i.e. levels of TempFac) are equally effective. We need to use a multiple comparison procedure to arrive at an answer to that question. We compute the pair-wise differences in effect-size:\n\nfrogs_anova %&gt;% stats::TukeyHSD()\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Time ~ TempFac, data = frogs_long)\n\n$TempFac\n       diff        lwr       upr p adj\n18-13  -5.3  -6.175224 -4.424776     0\n25-13 -10.1 -10.975224 -9.224776     0\n25-18  -4.8  -5.675224 -3.924776     0\n\n\nWe see that each of the pairwise differences in effect-size is significant, with p = 0 !\n\nUsing other packages\n\n\nUsing ggstatsplot\nUsing supernova\n\n\n\nThere is a very neat package called ggstatsplot6 that allows us to plot very comprehensive statistical graphs. Let us quickly do this:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nlibrary(ggstatsplot)\nfrogs_long %&gt;%\n  ggstatsplot::ggbetweenstats(\n    x = TempFac, y = Time,\n    colour = TempFac, alpha = 0.8,\n    type = \"parametric\",\n    pairwise.comparisons = TRUE,\n    p.adjust.method = \"bonferroni\",\n    conf.level = 0.95,\n    # Plot parameters for the points\n    point.args = list(\n      position = ggplot2::position_jitterdodge(dodge.width = 0.6), alpha =\n        0.8, size = 3, stroke = 0, na.rm = TRUE\n    ),\n\n    # Plot parameters for the boxplots\n    boxplot.args = list(width = 0.3, alpha = 0.2, na.rm = TRUE),\n\n    # Plot parameters for the violin plots\n    violin.args = list(width = 0.5, alpha = 0.2, na.rm = TRUE),\n    title = \"ANOVA : Frogs Spawn Time vs Temperature Setting\"\n  ) +\n  scale_colour_paletteer_d(\"ggthemes::colorblind\") +\n  labs(x = \"Temperature (°C)\", y = \"Spawn Time (seconds)\")\n\n\n\n\n\n\n\nProbably a case of too much in one plot, but it does give us a lot of information, including the pairwise comparisons and the effect sizes. The ggstatsplot package is very useful for quick visualizations of statistical tests.\n\n\nWe can also obtain crisp-looking anova tables from the new supernova package 7, which is based on the methods discussed in Judd et al. Section 14\nlibrary(supernova)\nsupernova::supernova(frogs_anova)\nsupernova::pairwise(frogs_anova)\n\n\n\n Analysis of Variance Table (Type III SS)\n Model: Time ~ TempFac\n\n                               SS df      MS       F   PRE     p\n ----- --------------- | -------- -- ------- ------- ----- -----\n Model (error reduced) | 1020.933  2 510.467 385.897 .9312 .0000\n Error (from model)    |   75.400 57   1.323                    \n ----- --------------- | -------- -- ------- ------- ----- -----\n Total (empty model)   | 1096.333 59  18.582                    \n\n\n\n\n\n  group_1 group_2    diff pooled_se       q    df   lower  upper p_adj\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 18      13       -5.300     0.257 -20.608    57  -6.175 -4.425 .0000\n2 25      13      -10.100     0.257 -39.272    57 -10.975 -9.225 .0000\n3 25      18       -4.800     0.257 -18.664    57  -5.675 -3.925 .0000\n\n\n\nThe supernova table clearly shows the reduction the Sum of Squares as we go from a NULL (empty) model to a full ANOVA model.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-anova-using-permutation-tests",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#workflow-anova-using-permutation-tests",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Workflow: ANOVA using Permutation Tests",
    "text": "Workflow: ANOVA using Permutation Tests\nWe wish to establish the significance of the effect size due to each of the levels in TempFac. From the normality tests conducted earlier we see that except at one level of TempFac, the times are are not normally distributed. Hence we opt for a Permutation Test to check for significance of effect.\nAs remarked in Ernst8, the non-parametric permutation test can be both exact and also intuitively easier for students to grasp.\nWe proceed with a Permutation Test for TempFac. We shuffle the levels (13, 18, 25) randomly between the Times and repeat the ANOVA test each time and calculate the F-statistic. The Null distribution is the distribution of the F-statistic over the many permutations and the p-value is given by the proportion of times the F-statistic equals or exceeds that observed.\nWe will use infer to do this: We calculate the observed F-stat with infer, which also has a very direct, if verbose, syntax for doing permutation tests:\n\nobserved_infer &lt;-\n  frogs_long %&gt;%\n  specify(Time ~ TempFac) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  calculate(stat = \"F\")\nobserved_infer\n\n\n  \n\n\n\nWe see that the observed F-Statistic is of course \\(385.8966\\) as before. Now we use infer to generate a NULL distribution using permutation of the factor TempFac:\n\nnull_dist_infer &lt;- frogs_long %&gt;%\n  specify(Time ~ TempFac) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  generate(reps = 4999, type = \"permute\") %&gt;%\n  calculate(stat = \"F\")\n##\nnull_dist_infer\n\n\n  \n\n\n##\nnull_dist_infer %&gt;%\n  visualise(method = \"simulation\") +\n  shade_p_value(obs_stat = observed_infer$stat, direction = \"right\") +\n  scale_x_continuous(trans = \"log10\", expand = c(0, 0)) +\n  coord_cartesian(xlim = c(0.2, 500), clip = \"off\") +\n  annotation_logticks(outside = FALSE) +\n  theme_custom()\n\n\n\n\n\n\n\nAs seen, the infer based permutation test also shows that the permutationally generated F-statistics are nowhere near that which was observed. The effect of TempFac is very strong.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#wait-but-why",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#wait-but-why",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nIn marketing, design, or business research, similar quantities may be measured across different locations, or stores, or categories of people, for instance.\nANOVA is the tool to decide if the Quant variable has differences across the Qual categories.\nThis approach can be extended to more than one Qual variable, and also if there is another Quant variable in the mix.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#conclusions",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#conclusions",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Conclusions",
    "text": "Conclusions\nWe have discussed ANOVA as a means of modelling the effects of a Categorical variable on a Continuous (Quant) variable. ANOVA can be carried out using the standard formula aov when assumptions on distributions, variances, and independence are met. Permutation ANOVA tests can be carried out when these assumptions do not quite hold.\n\n\n\n\n\n\nNoteTwo-Way ANOVA\n\n\n\nWhat if we have two Categorical variables as predictors?\nWe then need to perform a Two-Way ANOVA analysis, where we look at the predictors individually (main effects) and together (interaction effects). Here too, we need to verify if the number of observations are balanced across all combinations of factors of the two Qualitative predictors. There are three different classical approaches (Type1, Type2, and Type3 ANOVA) for testing hypotheses in ANOVA for unbalanced designs, as they are called. (Langsrud 2003).\n\n\n\n\n\n\n\n\nNoteInformative Hypothesis Testing: Models which incorporate a priori Beliefs\n\n\n\nNote that when we specified our research question, we had no specific hypothesis about the means, other than that they might be different. In many situations, we may have reason to believe in the relative “ordering” of the means for different levels of the Categorical variable. The one-sided t-test is the simplest example (e.g., \\(\\mu_1 &gt;= 0\\) and \\(\\mu_1 &gt;= \\mu_2\\)); this readily extends to the multi-parameter setting, where more than one inequality constraint can be imposed on the parameters (e.g., \\(\\mu_1 &lt;= \\mu_2 &lt;= \\mu_3\\).\nIt is possible to incorporate these beliefs into the ANOVA model, using what is called as informative hypothesis testing, which have certain advantages compared to unconstrained models. The R package called restriktor has the capability to develop such models with beliefs.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#your-turn",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#your-turn",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry the simple datasets at https://www.performingmusicresearch.com/datasets/\nCan you try to ANOVA-analyse the datasets we dealt with in plotting Groups with Boxplots?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#sec-references",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#sec-references",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "\n References",
    "text": "References\n\nThe ANOVA tutorial at Our Coding Club\n\nAntoine Soetewey. How to: one-way ANOVA by hand. https://statsandr.com/blog/how-to-one-way-anova-by-hand/\n\nANOVA in R - Stats and R https://statsandr.com/blog/anova-in-r/\n\nMichael Crawley.(2013) The R Book,second edition. Chapter 11.\n\nDavid C Howell, Permutation Tests for Factorial ANOVA Designs\n\nMarti Anderson, Permutation tests for univariate or multivariate analysis of variance and regression\n\nJudd, Charles M., Gary H. McClelland, and Carey S. Ryan.(2017). “Introduction to Data Analysis.” In, 1–9. Routledge. https://doi.org/10.4324/9781315744131-1.\n\nPatil, I. (2021). Visualizations with statistical details: The ‘ggstatsplot’ approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\n\nLangsrud, Øyvind. (2003). ANOVA for unbalanced data: Use type II instead of type III sums of squares. Statistics and Computing. 13. 163-167. https://doi.org/10.1023/A:1023260610025. https://www.researchgate.net/publication/220286726_ANOVA_for_unbalanced_data_Use_type_II_instead_of_type_III_sums_of_squares\n\nKim TK. (2017). Understanding one-way ANOVA using conceptual figures. Korean J Anesthesiol. 2017 Feb;70(1):22-26. https://ekja.org/upload/pdf/kjae-70-22.pdf\n\n\nAnova – Type I/II/III SS explained.https://mcfromnz.wordpress.com/2011/03/02/anova-type-iiiiii-ss-explained/\n\nBidyut Ghosh (Aug 28, 2017). One-way ANOVA in R. https://datascienceplus.com/one-way-anova-in-r/\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nDescTools\n0.99.60\nSignorell (2025)\n\n\nggprism\n1.0.6\nDawson (2025)\n\n\nggstatsplot\n0.13.1\nPatil (2021)\n\n\nggtext\n0.1.2\nWilke and Wiernik (2022)\n\n\nrestriktor\n0.6.10\nVanbrabant and Kuiper (2024)\n\n\nsupernova\n3.0.0\nBlake et al. (2024)\n\n\n\n\n\n\nBlake, Adam, Jeff Chrabaszcz, Ji Son, and Jim Stigler. 2024. supernova: Judd, McClelland, & Ryan Formatting for ANOVA Output. https://doi.org/10.32614/CRAN.package.supernova.\n\n\nDawson, Charlotte. 2025. ggprism: A “ggplot2” Extension Inspired by “GraphPad Prism”. https://doi.org/10.32614/CRAN.package.ggprism.\n\n\nLangsrud, Øyvind. 2003. Statistics and Computing 13 (2): 163–67. https://doi.org/10.1023/a:1023260610025.\n\n\nPatil, Indrajeet. 2021. “Visualizations with statistical details: The ‘ggstatsplot’ approach.” Journal of Open Source Software 6 (61): 3167. https://doi.org/10.21105/joss.03167.\n\n\nSignorell, Andri. 2025. DescTools: Tools for Descriptive Statistics. https://doi.org/10.32614/CRAN.package.DescTools.\n\n\nVanbrabant, Leonard, and Rebecca Kuiper. 2024. restriktor: Restricted Statistical Estimation and Inference for Linear Models. https://doi.org/10.32614/CRAN.package.restriktor.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. ggtext: Improved Text Rendering Support for “ggplot2”. https://doi.org/10.32614/CRAN.package.ggtext.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#footnotes",
    "href": "content/courses/Analytics/Inference/Modules/130-ThreeMeansOrMore/index.html#footnotes",
    "title": "Comparing Multiple Means with ANOVA",
    "section": "Footnotes",
    "text": "Footnotes\n\nThe ANOVA tutorial at Our Coding Club.↩︎\nhttps://github.com/UCLATALL/supernova↩︎\nhttps://www.openintro.org/go/?id=anova-supplement↩︎\nPruim R, Kaplan DT, Horton NJ (2017). “The mosaic Package: Helping Students to ‘Think with Data’ Using R.” The R Journal, 9(1), 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.↩︎\nmosaic::xpf() gives both a graph and the probabilities.↩︎\nggplot2 Based Plots with Statistical Details • ggstatsplot https://indrajeetpatil.github.io/ggstatsplot/↩︎\nhttps://github.com/UCLATALL/supernova↩︎\nErnst, Michael D. 2004. “Permutation Methods: A Basis for Exact Inference.” Statistical Science 19 (4): 676–85. doi:10.1214/088342304000000396.↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Statistical Inference",
      "Comparing Multiple Means with ANOVA"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/listing.html",
    "href": "content/courses/Analytics/Descriptive/listing.html",
    "title": "Descriptive Analytics",
    "section": "",
    "text": "Winston Chang (2024). R Graphics Cookbook.https://r-graphics.org",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/listing.html#references",
    "href": "content/courses/Analytics/Descriptive/listing.html#references",
    "title": "Descriptive Analytics",
    "section": "",
    "text": "Winston Chang (2024). R Graphics Cookbook.https://r-graphics.org",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTipFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(children = n()) %&gt;%\n  # just to check\n  mutate(\n    No_of_families = as.integer(children / nkids),\n    # Why do we divide\n\n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)\n  )\nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;%\n  gf_col(No_of_families ~ nkids) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(No_of_families,\n    colorBy = \"data\",\n    legend = FALSE\n  ) %&gt;% # Or \"series\"\n\n  # https://echarts4r.john-coene.com/articles/grid.html\n  # echarts4r does not \"automatically\" name the axes!\n  # And look at the \"categorical\" x-axis below!\n\n  e_x_axis(\n    name = \"Family Size\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25, ) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;%\n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"Family Size (nkids)\", nameLocation = \"center\",\n    nameGap = 20, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 20,\n    nameTextStyle = list(align = \"center\"),\n    nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(family, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(family) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 25, nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 5) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nNoteSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;%\n  e_charts() %&gt;%\n  e_histogram(serie = height) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$height)),\n    label = list(\n      label = \"Mean Height\",\n      label.position = \"end\"\n    ),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;%\n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(height) %&gt;%\n  e_mark_line(\n    data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n    # This code colours both v-lines red...how?\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(\n    data = list(xAxis = measures %&gt;%\n      filter(sex == \"F\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n\n    # This piece of code has no effect...wonder why not?\n    # BOTH lines are in red ...why??\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;%\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(father) %&gt;%\n  e_density(mother) %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$mother)),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_mark_line(data = list(\n    xAxis = mean(Galton$father),\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  )) %&gt;%\n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  e_charts(height = 400) %&gt;%\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids, sex) %&gt;%\n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  e_charts(nkids, height = 300) %&gt;%\n  e_bar(mean_height, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\", nameGap = 25,\n    type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;%\n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;%\n  group_by(family, sex) %&gt;%\n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;%\n  select(family, sex, height, diff_height) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(diff_height, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  # Fit a trend line\n  e_lm(height ~ diff_height,\n    name = c(\"Female\", \"Male\")\n  ) %&gt;%\n  e_x_axis(\n    max = 18, min = -5,\n    name = \"Father - Mother Height\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50,\n    name = \"Children's Heights\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n())\nNHANES %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n()) %&gt;%\n  e_charts(Education, height = 300) %&gt;%\n  e_bar(count) %&gt;%\n  e_y_axis(max = 1750) %&gt;%\n  e_x_axis(type = \"category\") %&gt;%\n  e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip()\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(rows = 1, cols = 3) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4. How are people Ages distributed across levels of Education?\n\n\n# Recall there are missing data\n# gf_boxplot(Age ~ Education,\n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;%\n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;%\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;%\n  group_by(Race1) %&gt;%\n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;%\n  group_by(Education, Race1) %&gt;%\n  summarize(n = n()) %&gt;%\n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;%\n  mutate(percapita_educated = (n / population) * 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;% # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;%\n  e_y_axis(max = 35) %&gt;%\n  e_facet(rows = 2, cols = 3) %&gt;%\n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(BMI)\nNHANES %&gt;%\n  group_by(Race1) %&gt;%\n  e_charts(height = 350) %&gt;%\n  e_density(BMI) %&gt;%\n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%\n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(\n  path = \"../data/banned.xlsx\",\n  sheet = \"Sorted by Author & Title\"\n)\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;-\n  banned %&gt;%\n  group_by(State) %&gt;%\n  summarise(total = n()) %&gt;%\n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;%\n  group_by(State, `Type of Ban`) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;%\n  #  pivot_wider(.,id_cols = State,\n  #              names_from = `Type of Ban`,\n  #              values_from = count) %&gt;% janitor::clean_names() %&gt;%\n  #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n  #                  banned_from_libraries = 0,\n  #                  banned_pending_investigation = 0,\n  #                  banned_from_classrooms = 0)) %&gt;%\n  # mutate(total = sum(across(where(is.integer)))) %&gt;%\n  gf_col(count ~ reorder(State, total),\n    fill = ~`Type of Ban`\n  ) %&gt;%\n  gf_labs(\n    x = \"Count of Banned Books\",\n    y = \"State\"\n  ) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/22-Histograms/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html",
    "title": "\n Evolution and Flow",
    "section": "",
    "text": "R Tutorial\n\n\n\n\n\n\n\n“My stories run up and bite me in the leg – I respond by writing them down – everything that goes on during the bite. When I finish, the idea lets go and runs off.”\n— Ray Bradbury, science-fiction writer (22 Aug 1920-2012)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#slides-and-tutorials",
    "title": "\n Evolution and Flow",
    "section": "",
    "text": "R Tutorial\n\n\n\n\n\n\n\n“My stories run up and bite me in the leg – I respond by writing them down – everything that goes on during the bite. When I finish, the idea lets go and runs off.”\n— Ray Bradbury, science-fiction writer (22 Aug 1920-2012)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#setting-up-r-packages",
    "title": "\n Evolution and Flow",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(ggstream)\nlibrary(ggformula)\n# remotes::install_github(\"corybrunson/ggalluvial@main\", build_vignettes = TRUE)\nlibrary(ggalluvial)\nlibrary(ggsankeyfier)\n# install.packages(\"devtools\")\n# devtools::install_github(\"davidsjoberg/ggsankey\")\nlibrary(ggsankey)\nlibrary(networkD3)\nlibrary(echarts4r) # Interactive graphs\n\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#what-time-evolution-charts-can-we-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#what-time-evolution-charts-can-we-plot",
    "title": "\n Evolution and Flow",
    "section": "\n What Time Evolution Charts can we plot?",
    "text": "What Time Evolution Charts can we plot?\nIn these cases, the x-axis is typically time…and we chart the variable of another Quant variable with respect to time, using a line geometry.\nLet is take a healthcare budget dataset from Our World in Data: We will plot graphs for 5 countries (India, China, Brazil, Russia, Canada ).\n\n\n\n\n\n\nImportantAnd Introducting echarts4r\n\n\n\nWe will also build interactive versions of these charts using echarts4r!\n\n\nDownload this data by clicking on the button below:\n Download the Health data \n\nhealth &lt;-\n  read_csv(\"data/public-health-expenditure-share-GDP-OWID.csv\")\n\nhealth_filtered &lt;- health %&gt;%\n  filter(Entity %in% c(\n    \"India\",\n    \"China\",\n    \"United States\",\n    \"United Kingdom\",\n    \"Russia\",\n    \"Sweden\"\n  ))\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\ngf_point(\n  data = health_filtered,\n  public_health_expenditure_pc_gdp ~ Year,\n  colour = ~Entity,\n  ylab = \"Healthcare Budget\\n as % of GDP\",\n  title = \"Line Charts to show Evolution (over Time )\"\n) %&gt;%\n  gf_line()\n###\ngf_area(\n  data = health_filtered,\n  public_health_expenditure_pc_gdp ~ Year,\n  fill = ~Entity, alpha = 0.3,\n  ylab = \"Healthcare Budget\\n as % of GDP\",\n  title = \"Area Charts to show Evolution (over Time )\"\n) %&gt;%\n  gf_line(colour = ~Entity)\n\n\n\n\n\n\n\n\n\n\n\n\n\nhealth_filtered %&gt;%\n  group_by(Entity) %&gt;%\n  e_charts(Year) %&gt;%\n  e_scatter(public_health_expenditure_pc_gdp) %&gt;%\n  e_line(public_health_expenditure_pc_gdp) %&gt;%\n  e_x_axis(name = \"Year\", min = 1850, max = 2050) %&gt;%\n  e_y_axis(\n    name = \"Public Health Expenditure\",\n    nameLocation = \"middle\", nameGap = 25\n  ) %&gt;%\n  e_tooltip()\n###\nhealth_filtered %&gt;%\n  group_by(Entity) %&gt;%\n  e_charts(Year) %&gt;%\n  e_scatter(public_health_expenditure_pc_gdp) %&gt;%\n  e_area(public_health_expenditure_pc_gdp) %&gt;%\n  e_x_axis(name = \"Year\", min = 1850, max = 2050) %&gt;%\n  e_y_axis(\n    name = \"Public Health Expenditure\",\n    nameLocation = \"middle\", nameGap = 25\n  ) %&gt;%\n  e_tooltip()",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#what-space-evolution-charts-can-we-plot",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#what-space-evolution-charts-can-we-plot",
    "title": "\n Evolution and Flow",
    "section": "\n What Space Evolution Charts can we plot?",
    "text": "What Space Evolution Charts can we plot?\nHere, the space can be any Qual variable, and we can chart another Quant or Qual variable move across levels of the first chosen Qual variable.\nFor instance we can contemplate enrollment at a University, and show how students move from course to course in a University. Or how customers drift from one category of products or brands to another….or the movement of cricket players from one IPL Team to another !!\nHere is what Thomas Lin Pedersen says:\n\nA parallel sets diagram is a type of visualisation showing the interaction between multiple categorical variables. If the variables have an intrinsic order the representation can be thought of as a Sankey Diagram. If each variable is a point in time it will resemble an Alluvial diagram.\n\n\n\n\n\n\n\n\n\n\n(a) ggsankey aesthetics\n\n\n\n\n\n\n\n\n\n(b) ggsankeyfier aesthetics\n\n\n\n\n\n\nFigure 1: Geometric Aesthetics from two Sankey Plot Packages\n\n\n\nThe Qualitative variables being connected are mapped to stages/axes\n\nEach level within a Qual variable is mapped to nodes / strata / lodes;\nAnd the connections between the strata of the axes are called flows / edges / links / alluvia.\n\n\nSuch diagrams are best used when you want to show a many-to-many mapping between two domains or multiple paths through a set of stages E.g Students pursruing different degrees going through multiple courses with multiple departments during a semester of study. Here students, degrees, courses, departments would be some variables we would plot and we would visualize the number of students moving across courses and deparments based on their degree etc.\nHere is an example of a Sankey Diagram: This diagram show how energy is converted or transmitted before being consumed or lost: supplies are on the left, and demands are on the right. (Data: Department of Energy & Climate Change via Tom Counsell)1:\n\n\n\n\n\n\n\n\nNoteSwitching to ggplot here\n\n\n\nFor the next few charts, there are (as yet) no equivalents in ggformula. Hence we will use ggplot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#case-study-1-titanic-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#case-study-1-titanic-dataset",
    "title": "\n Evolution and Flow",
    "section": "\n Case Study-1: Titanic Dataset",
    "text": "Case Study-1: Titanic Dataset\n\n\n\ndata(Titanic, package = \"datasets\")\nTitanic &lt;- Titanic %&gt;% as_tibble()\nTitanic\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\nNoteTable Form Data\n\n\n\nNote that this data is in tidy wide / table form, with separate columns for each Qualitative variable and a separate count column, which we saw when we examined Categorical Data. This is, in my opinion, intuitively the best form of data to plot a Sankey plot with. Each variable gives us “one part in the flow”. But there are other forms such as the tidy long form which we have been using practically all this while. You will find examples of on the ggalluvial website using tidy long form data. https://corybrunson.github.io/ggalluvial/\n\n\n\n\n\nUsing ggplot\nUsing ggsankeyfier\nUsing echarts4r\n\n\n\n\n\n\n##\nTitanic %&gt;% ggplot(\n  data = .,\n\n  # Select the Categorical Variables for the vertical Axes / Stages\n  aes(\n    axis1 = Class,\n    axis2 = Sex,\n    axis3 = Age,\n    axis4 = Survived,\n    y = n\n  ), fill = \"white\"\n) +\n\n  # Alluvials between Categorical Axes\n  geom_alluvium(aes(fill = Survived),\n    colour = \"black\",\n    linewidth = 0.25\n  ) +\n\n  # Vertical segments for each Categorical Variable2\n  geom_stratum(\n    colour = \"black\",\n    linewidth = 1,\n    fill = \"white\"\n  ) +\n\n  # Labels for each \"level\" of the Categorical Axes\n  geom_text(\n    stat = \"stratum\", size = 3,\n    aes(label = after_stat(stratum))\n  ) +\n\n\n\n  # Scales and Colours\n  scale_x_discrete(\n    limits = c(\"Class\", \"Sex\", \"Age\", \"Survived\"),\n    expand = c(0.1, 0.1)\n  ) +\n  scale_fill_brewer(palette = \"Set1\") +\n  xlab(\"Demographic\") +\n  ggtitle(\n    \"Passengers on the maiden voyage of the Titanic\",\n    \"Stratified by demographics and survival\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is how the package ggalluvial defines the elements of a typical alluvial plot:\n\nAn axis is a dimension (variable) along which the data are vertically arranged at a fixed horizontal position. The plot above uses three categorical axes: Class, Sex, and Age.\nThe groups at each axis are depicted as opaque blocks called strata. For example, the Class axis contains four strata: 1st, 2nd, 3rd, and Crew.\nHorizontal (x-) splines called alluvia span the entire width of the plot. In this plot, each alluvium corresponds to a fixed strata value of each axis variable, indicated by its vertical position at the axis, as well as of the Survived variable, indicated by its fill color.\nThe segments of the alluvia between pairs of adjacent axes are flows.\nThe alluvia intersect the strata at lodes. The lodes are not visualized in the above plot, but they can be inferred as filled rectangles extending the flows through the strata at each end of the plot or connecting the flows on either side of the center stratum.\n\n\n\n\nThe ggsankeyfier also plots alluvial and sankey diagrams. This package takes data in long-form. See this article.. ggsankeyfier has builtin commands to convert data from wide to long:\n\n\n\nTitanic %&gt;%\n  as_tibble() %&gt;%\n  ggsankeyfier::pivot_stages_longer(\n    data = .,\n    stages_from = c(\"Class\", \"Sex\", \"Age\", \"Survived\"),\n    values_from = \"n\",\n    additional_aes_from = \"Survived\"\n  ) -&gt; Titanic_long\nTitanic_long\n\n\n\n\n\n  \n\n\n\n\n\nThis data is in long form, with stages defining the axes in the graph, and the node variable giving us levels within each (Qualitative) axis. The edge_id labels both ends (from and to) of each connector or edge/flow/alluvium.\nLet us plot this now:\n\n\n\nTitanic_long %&gt;%\n  ggplot(aes(\n    x = stage, y = n,\n    group = node, connector = connector,\n    edge_id = edge_id\n  )) +\n  geom_sankeynode(v_space = \"auto\") +\n  geom_sankeyedge(aes(fill = Survived), v_space = \"auto\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(x = \"\", title = \"Titanic Survival\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us make an interactive graph for this dataset using echarts4.\n\nClassSex &lt;-\n  Titanic %&gt;%\n  group_by(Class, Sex) %&gt;%\n  summarise(cs = sum(n)) %&gt;%\n  ungroup() %&gt;%\n  rename(\"source\" = Class, \"target\" = Sex, \"value\" = cs)\n\nSexAge &lt;-\n  Titanic %&gt;%\n  group_by(Sex, Age) %&gt;%\n  summarise(sa = sum(n)) %&gt;%\n  ungroup() %&gt;%\n  rename(\"source\" = Sex, \"target\" = Age, \"value\" = sa)\n\nAgeSurvived &lt;-\n  Titanic %&gt;%\n  group_by(Age, Survived) %&gt;%\n  summarise(as = sum(n)) %&gt;%\n  ungroup() %&gt;%\n  rename(\"source\" = Age, \"target\" = Survived, \"value\" = as)\n\nCombo &lt;- rbind(ClassSex, SexAge, AgeSurvived)\nCombo\n\n\n  \n\n\nCombo %&gt;%\n  e_charts() %&gt;%\n  e_sankey(source, target, value) %&gt;%\n  e_title(\"Titanic: Who lived, and who didn't?\") %&gt;%\n  e_tooltip()\n\n\n\n\n\nThe process with echarts4r is quite different, since the data structure used by this package is different:\n\nThe echarts4r package needs to have source and target columns for axes, along with a value to determine the width of the alluvium. \nThe names in the source and target can repeat, and can appear in both source and target columns in order to create a multi-axis diagram. Hence the data needs to be inherently in long form.\nHowever, for the values, we need to manually calculate the aggregate totals for alluvia between each consecutive pairs of axes (i.e Qual variables). This is not done automatically in echarts4r, but it is with ggalluvial.\nSo we create grouped aggregate summaries for each pair of Qualitative variables that we wish to plot consecutively ( i.e as axis1, axis2…)\nStack these pair-wise alluvia totals into one combo data frame using rbind(), after renaming the variables to “source”, “target” and “value”.\n\nPhew! seems like too much work to do…I wonder if good, old-fashioned pivot-longer will get us here…",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#chord-diagram",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#chord-diagram",
    "title": "\n Evolution and Flow",
    "section": "\n Chord Diagram",
    "text": "Chord Diagram\n\n\n\n\nWe will explore this diagram when we explore network graphs with the tidygraph and ggraph packages.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#dumbbell-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#dumbbell-plots",
    "title": "\n Evolution and Flow",
    "section": "\n Dumbbell Plots",
    "text": "Dumbbell Plots\nA simple plot that can quickly indicate changes in multiple variables/aspects over either a time or a space variable is a dumbbell plot. This is a combination of scatter plot + a segment plot. Let us take our previously loaded health dataset and plot just the change in expenditure for multiple countries, across a time span of 8 years (2010 - 2018)\n\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\n\n\nhealth_2010_2018 &lt;- health %&gt;%\n  # select Years 2010 and 2018\n  filter(Year %in% c(2010, 2018)) %&gt;%\n  # Make separate columns for each year, easier that way\n  # Though not essential\n  pivot_wider(\n    id_cols = c(Entity, Code),\n    names_from = Year,\n    names_prefix = \"Year\",\n    values_from = public_health_expenditure_pc_gdp\n  )\nhealth_2010_2018\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nhealth_2010_2018 %&gt;%\n  # remove NA data across the data set\n  drop_na() %&gt;%\n  # take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;%\n  gf_segment(Entity + Entity ~ Year2010 + Year2018,\n    colour = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  gf_point(Entity ~ Year2018,\n    colour = ~\"2018\"\n  ) %&gt;%\n  gf_point(Entity ~ Year2010,\n    colour = ~\"2010\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Can we do better? Sort the bars, improve axis ticks, title..\n\nhealth_2010_2018 %&gt;%\n  # remove NA data across the data set\n  drop_na() %&gt;%\n  # take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;%\n  # plot segments first\n  gf_segment(\n    reorder(Entity, Year2018) + reorder(Entity, Year2018) ~\n      Year2010 + Year2018,\n    colour = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  # Then plot points\n  gf_point(reorder(Entity, Year2018) ~ Year2018,\n    colour = ~\"2018\",\n    size = 3\n  ) %&gt;%\n  gf_point(\n    reorder(Entity, Year2018) ~ Year2010,\n    colour = ~\"2010\", size = 3,\n    xlab = \"Health Expenditure as Percentage of GDP\",\n    ylab = \"Country\",\n    title = \"Healthcare Budgets Changes between 2010 to 2018\",\n    subtitle = \"Bars are Sorted\",\n    caption = \"And the X-Axis is in percentage\"\n  ) %&gt;%\n  gf_refine(\n    scale_x_continuous(\n      breaks = scales::breaks_width(2),\n      labels = scales::label_percent(suffix = \"%\", scale = 1)\n    ),\n    scale_colour_manual(name = \"Year\", values = c(\"red\", \"green\"))\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhealth_2010_2018 &lt;- health %&gt;%\n  # select Years 2010 and 2018\n  filter(Year %in% c(2010, 2018)) %&gt;%\n  # Make separate columns for each year, easier that way\n  # Though not essential\n  pivot_wider(\n    id_cols = c(Entity, Code),\n    names_from = Year,\n    names_prefix = \"Year\",\n    values_from = public_health_expenditure_pc_gdp\n  )\n\nhealth_2010_2018 %&gt;%\n  # remove NA data across the data set\n  drop_na() %&gt;%\n  # take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;%\n  ggplot() +\n  geom_segment(\n    aes(\n      y = Entity, yend = Entity,\n      x = Year2010, xend = Year2018\n    ),\n    colour = \"grey\",\n    linewidth = 2\n  ) +\n  geom_point(aes(y = Entity, x = Year2018, colour = \"2018\")) +\n  geom_point(aes(y = Entity, x = Year2010, colour = \"2010\"))\n## Can we do better?\n\nhealth_2010_2018 %&gt;%\n  # remove NA data across the data set\n  drop_na() %&gt;%\n  # take the top 20 countries based on 2018 allocation\n  slice_max(n = 20, order_by = Year2018) %&gt;%\n  ggplot() +\n  # plot segments first\n  geom_segment(\n    aes(\n      y = reorder(Entity, Year2018), yend = reorder(Entity, Year2018),\n      x = Year2010, xend = Year2018\n    ),\n    colour = \"grey\",\n    linewidth = 2\n  ) +\n\n  # Then plot points\n  geom_point(aes(\n    y = reorder(Entity, Year2018), x = Year2018,\n    colour = \"2018\"\n  ), size = 3) +\n  geom_point(aes(\n    y = reorder(Entity, Year2018), x = Year2010,\n    colour = \"2010\"\n  ), size = 3) +\n  labs(\n    x = \"Health Expenditure as Percentage of GDP\",\n    y = \"Country\", title = \"Healthcare Budgets\",\n    subtitle = \"Changes between 2010 to 2018\"\n  ) +\n  scale_x_continuous(\n    breaks = scales::breaks_width(2),\n    labels = scales::label_percent(suffix = \"%\", scale = 1)\n  ) +\n  scale_colour_manual(name = \"Year\", values = c(\"red\", \"green\"))",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#wait-but-why",
    "title": "\n Evolution and Flow",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nChanges can be over time, or over “space”\nIn the latter case, we can think of some Quantity changing over (multiple levels of) multiple Qualitative variables. E.g. Sales over Product Type over Showroom Location over Festival Season…\nWhen a single Quant varies over a single multi-level Qual, the Chord Diagram may be simpler than the Sankey/Alluvial. E.g Bird migration across Multiple Locations. This can even show bidirectional changes. ( Sankeys with loops are also possible, however)\nWhen you have a Quant that changes over only one two-level Qual variable, the Dumbbell plot becomes an option.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#conclusion",
    "title": "\n Evolution and Flow",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe see that we can visualize “evolutions” over time and space. The evolutions can represent changes in the quantities of things, or their categorical affiliations or groups.\nWhat business/design data would you depict in this way? Revenue streams? Employment? Expenditures over time and market? Migration? App usage patterns? There are many possibilities!\nNote also that the Bump Charts are a special case of Alluvial/Sankey charts where each node connects/flows to only one other node.\n\n\n\n\nlogsUserNetworkAPI ServerCell TowerData ProcessorOnline PortalsatellitestransmitterStorageUIphone logsMake callpersistdisplayaccess",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#your-turn",
    "title": "\n Evolution and Flow",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nWithin the ggalluvial package are two datasets, majors and vaccinations. Plot alluvial charts for both of these.\nGo to the American Life Panel Website where you will find many public datasets. Try to take one and make charts from it that we have learned in this Module.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#references",
    "title": "\n Evolution and Flow",
    "section": "\n References",
    "text": "References\n\nGlobal Migration, https://download.gsb.bund.de/BIB/global_flow/ A good example of the use of a Chord Diagram.\n\n\nggalluvial cheatsheet,https://cheatography.com/seleven/cheat-sheets/ggalluvial/\n\nJohn Coene, Sankey plots with echarts4r, https://echarts4r.john-coene.com/articles/chart_types.html#sankey\n\nOther packages: Sankey plot | the R Graph Gallery (r-graph-gallery.com)\n\nAnother package: Sankey diagrams in ggplot2 with ggsankey | RCHARTS (r-charts.com)\n\nSankey Charts using networkD3: http://christophergandrud.github.io/networkD3\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\necharts4r\n0.4.5\nCoene (2023)\n\n\nggalluvial\n0.12.5\n\nBrunson (2020); Brunson and Read (2023)\n\n\n\nggsankey\n0.0.99999\nSjoberg (2025)\n\n\nggsankeyfier\n0.1.8\nde Vries (2024)\n\n\nggstream\n0.1.0\nSjoberg (2021)\n\n\nnetworkD3\n0.4.1\nAllaire et al. (2025)\n\n\nscales\n1.4.0\nWickham, Pedersen, and Seidel (2025)\n\n\n\n\n\n\nAllaire, J. J., Christopher Gandrud, Kenton Russell, and CJ Yetman. 2025. networkD3: D3 JavaScript Network Graphs from r. https://doi.org/10.32614/CRAN.package.networkD3.\n\n\nBrunson, Jason Cory. 2020. “ggalluvial: Layered Grammar for Alluvial Plots.” Journal of Open Source Software 5 (49): 2017. https://doi.org/10.21105/joss.02017.\n\n\nBrunson, Jason Cory, and Quentin D. Read. 2023. “ggalluvial: Alluvial Plots in ‘ggplot2’.” http://corybrunson.github.io/ggalluvial/.\n\n\nCoene, John. 2023. Echarts4r: Create Interactive Graphs with “Echarts JavaScript” Version 5. https://doi.org/10.32614/CRAN.package.echarts4r.\n\n\nde Vries, Pepijn. 2024. ggsankeyfier: Create Sankey and Alluvial Diagrams Using “ggplot2”. https://doi.org/10.32614/CRAN.package.ggsankeyfier.\n\n\nSjoberg, David. 2021. ggstream: Create Streamplots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggstream.\n\n\n———. 2025. ggsankey: Sankey, Alluvial and Sankey Bump Plots. https://github.com/davidsjoberg/ggsankey.\n\n\nWickham, Hadley, Thomas Lin Pedersen, and Dana Seidel. 2025. scales: Scale Functions for Visualization. https://doi.org/10.32614/CRAN.package.scales.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/70-EvolutionFlow/index.html#footnotes",
    "title": "\n Evolution and Flow",
    "section": "Footnotes",
    "text": "Footnotes\n\nD3 JavaScript Network Graphs from R: christophergandrud.github.io/networkD3/↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:sankey-diagram\"></iconify-icon> Evolution and Flow"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html",
    "title": "\n Experiments",
    "section": "",
    "text": "Important\n\n\n\n Guys carry more cash than Gals.\n\n\nHave you got money? Open your wallet and show me how much. Guys and Gals. Will this work in these UPI days? And well-dressed Gals don’t have pockets in their clothing, uncle… So then ask “How much did you spend today?” Again, Guys and Gals.\nAnd then a two independent sample test for means. And Permutation and all that stuff.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-1-guys-have-more-pocket-money-than-gals",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-1-guys-have-more-pocket-money-than-gals",
    "title": "\n Experiments",
    "section": "",
    "text": "Important\n\n\n\n Guys carry more cash than Gals.\n\n\nHave you got money? Open your wallet and show me how much. Guys and Gals. Will this work in these UPI days? And well-dressed Gals don’t have pockets in their clothing, uncle… So then ask “How much did you spend today?” Again, Guys and Gals.\nAnd then a two independent sample test for means. And Permutation and all that stuff.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-2-i-will-eat-my-tip-thank-you.",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-2-i-will-eat-my-tip-thank-you.",
    "title": "\n Experiments",
    "section": "\n Free Hunch #2: I will eat my tip, thank you.",
    "text": "Free Hunch #2: I will eat my tip, thank you.\n\n\n\n\n\n\nImportant The average tip people give is higher for people who are non-vegetarians. Regardless of whether you are going Dutch or not.\n\n\n\n\n\n\nAre vegetarians more kanjoos? Or it is the meat-eaters?\nSo Swiggy/Zomato/Dining Out bills. For both sets of people. And then the t-t-t-t-t-test…",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-3-art-design-and-vocation-are-all-diff-different.",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-3-art-design-and-vocation-are-all-diff-different.",
    "title": "\n Experiments",
    "section": "\n Free Hunch #3: Art, Design, and Vocation are all diff-different.",
    "text": "Free Hunch #3: Art, Design, and Vocation are all diff-different.\n\n\n\n\n\n\nImportant Grades are very different between B.Voc, B.Cra, and B.Des folks.\n\n\n\n\n\n\nSo? Grades of course, for a good sample from all three groups of people..and then? ANOVA of course.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-4-chhota-bheem-vs-doraemon-vs-dragon-tales",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-4-chhota-bheem-vs-doraemon-vs-dragon-tales",
    "title": "\n Experiments",
    "section": "\n Free Hunch #4: Chhota Bheem vs Doraemon vs Dragon Tales",
    "text": "Free Hunch #4: Chhota Bheem vs Doraemon vs Dragon Tales\n\n\n\n\n\n\nImportant Doraeomon &gt;&gt; Dragon Tales &gt;&gt; Chhota Bheem!\n\n\n\n\n\n\nThe Anywhere Door makes Doraemon the greatest children’s show on earth. But Dragon Tales also has that Dragon Stone thingy…“I wish, I wish..”\nAnd that flute musical in Chhota Bheem…Uff!\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\nSo? Get Opinion scores from people. Scale of 10. And then? Oh, ANOVA, peasants!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-5-i-am-an-intj",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-5-i-am-an-intj",
    "title": "\n Experiments",
    "section": "\n Free Hunch #5: I am an INTJ",
    "text": "Free Hunch #5: I am an INTJ\n\n\n\n\n\n\nImportant Srishti kids are predominantly introverted\n\n\n\n\n\n\nWhat are we looking at, data-wise? A proportion, which if more than 50% would justify our hunch. So we do an MBTI on some unsuspecting sample of people, and try to generalize that result to the population.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-6-lets-go-to-chefstouch",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#free-hunch-6-lets-go-to-chefstouch",
    "title": "\n Experiments",
    "section": "\n Free Hunch #6: Let’s Go to ChefsTouch(?)",
    "text": "Free Hunch #6: Let’s Go to ChefsTouch(?)\n\n\n\n\n\n\nImportant Most people think the food in the mess/cafeteria is ordinary.\n\n\n\n\n\n\nAgain, a survey of a sample. Opinions, yes or no. A Proportion for the sample, and an extension to the population. A proportion test.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/1000-NoFreeHunch/index.html#references",
    "title": "\n Experiments",
    "section": "References",
    "text": "References\n\nFacing the Abyss: How to Probe Unknown Data. https://shancarter.github.io/ucb-dataviz-fall-2013/classes/facing-the-abyss/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"game-icons:sherlock-holmes\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Experiments"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTipFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(children = n()) %&gt;%\n  # just to check\n  mutate(\n    No_of_families = as.integer(children / nkids),\n    # Why do we divide\n\n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)\n  )\nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;%\n  gf_col(No_of_families ~ nkids) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(No_of_families,\n    colorBy = \"data\",\n    legend = FALSE\n  ) %&gt;% # Or \"series\"\n\n  # https://echarts4r.john-coene.com/articles/grid.html\n  # echarts4r does not \"automatically\" name the axes!\n  # And look at the \"categorical\" x-axis below!\n\n  e_x_axis(\n    name = \"Family Size\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25, ) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;%\n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"Family Size (nkids)\", nameLocation = \"center\",\n    nameGap = 20, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 20,\n    nameTextStyle = list(align = \"center\"),\n    nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(family, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(family) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 25, nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 5) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nNoteSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;%\n  e_charts() %&gt;%\n  e_histogram(serie = height) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$height)),\n    label = list(\n      label = \"Mean Height\",\n      label.position = \"end\"\n    ),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;%\n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(height) %&gt;%\n  e_mark_line(\n    data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n    # This code colours both v-lines red...how?\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(\n    data = list(xAxis = measures %&gt;%\n      filter(sex == \"F\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n\n    # This piece of code has no effect...wonder why not?\n    # BOTH lines are in red ...why??\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;%\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(father) %&gt;%\n  e_density(mother) %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$mother)),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_mark_line(data = list(\n    xAxis = mean(Galton$father),\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  )) %&gt;%\n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  e_charts(height = 400) %&gt;%\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids, sex) %&gt;%\n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  e_charts(nkids, height = 300) %&gt;%\n  e_bar(mean_height, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\", nameGap = 25,\n    type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;%\n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;%\n  group_by(family, sex) %&gt;%\n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;%\n  select(family, sex, height, diff_height) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(diff_height, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  # Fit a trend line\n  e_lm(height ~ diff_height,\n    name = c(\"Female\", \"Male\")\n  ) %&gt;%\n  e_x_axis(\n    max = 18, min = -5,\n    name = \"Father - Mother Height\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50,\n    name = \"Children's Heights\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n())\nNHANES %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n()) %&gt;%\n  e_charts(Education, height = 300) %&gt;%\n  e_bar(count) %&gt;%\n  e_y_axis(max = 1750) %&gt;%\n  e_x_axis(type = \"category\") %&gt;%\n  e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip()\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(rows = 1, cols = 3) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n::: {.callout-note title=“Question”} Q.4. How are people Ages distributed across levels of Education?\n# Recall there are missing data\n# gf_boxplot(Age ~ Education,\n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;%\n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;%\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;%\n  group_by(Race1) %&gt;%\n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;%\n  group_by(Education, Race1) %&gt;%\n  summarize(n = n()) %&gt;%\n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;%\n  mutate(percapita_educated = (n / population) * 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;% # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;%\n  e_y_axis(max = 35) %&gt;%\n  e_facet(rows = 2, cols = 3) %&gt;%\n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(BMI)\nNHANES %&gt;%\n  group_by(Race1) %&gt;%\n  e_charts(height = 350) %&gt;%\n  e_density(BMI) %&gt;%\n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%\n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(\n  path = \"../data/banned.xlsx\",\n  sheet = \"Sorted by Author & Title\"\n)\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;-\n  banned %&gt;%\n  group_by(State) %&gt;%\n  summarise(total = n()) %&gt;%\n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;%\n  group_by(State, `Type of Ban`) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;%\n  #  pivot_wider(.,id_cols = State,\n  #              names_from = `Type of Ban`,\n  #              values_from = count) %&gt;% janitor::clean_names() %&gt;%\n  #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n  #                  banned_from_libraries = 0,\n  #                  banned_pending_investigation = 0,\n  #                  banned_from_classrooms = 0)) %&gt;%\n  # mutate(total = sum(across(where(is.integer)))) %&gt;%\n  gf_col(count ~ reorder(State, total),\n    fill = ~`Type of Ban`\n  ) %&gt;%\n  gf_labs(\n    x = \"Count of Banned Books\",\n    y = \"State\"\n  ) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/24-BoxPlots/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html",
    "title": "\n Counts",
    "section": "",
    "text": "“No matter what happens in life, be good to people. Being good to people is a wonderful legacy to leave behind.”\n— Taylor Swift",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#slides-and-tutorials",
    "title": "\n Counts",
    "section": "",
    "text": "“No matter what happens in life, be good to people. Being good to people is a wonderful legacy to leave behind.”\n— Taylor Swift",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#setting-up-r-packages",
    "title": "\n Counts",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(skimr)\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#what-graphs-will-we-see-today",
    "title": "\n Counts",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQual\nNone\nBar Chart",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Counts",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n3\n                  How, What Kind, What Sort\n                  A Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\n                  Qualitative/Ordinal\n                  Socioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\n                  Median,Percentile",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#inspiration",
    "title": "\n Counts",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: Capital Cities\n\n\nHow much does the (financial) capital of a country contribute to its GDP? Which would be India’s city? What would be the reduction in percentage? And these Germans are crazy.(Toc, toc, toc, toc!)\nNote how the axis variable that defines the bar locations is a …Qual variable!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#graphing-packages-in-r",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#graphing-packages-in-r",
    "title": "\n Counts",
    "section": "\n Graphing Packages in R",
    "text": "Graphing Packages in R\nThere are several Data Visualization packages, even systems, within R.\n\nBase R supports graph making out of the box; (fast, very flexible, but a bit complex)(Update July 2025: There is a new(?) package called tinyplots that creates base R plots with syntax that is much more intuitive. (We will include some here for reference.)\nThe most well known is ggplot https://ggplot2-book.org/ which uses Leland Wilkinson’s concept of a “Grammar of Graphics”; ggformula is a wrapper around ggplot that makes the syntax a little more concise and intuitive.\nThere is the lattice package https://lattice.r-forge.r-project.org/ which uses the “Trellis Graphics” concept framework for data visualization developed by R. A. Becker, W. S. Cleveland, et al.;\nAnd the grid package https://bookdown.org/rdpeng/RProgDA/the-grid-package.html that allows extremely fine control of shapes plotted on the graph.\n\nEach system has its benefits and learning complexities. We will look at plots created using ggformula, and the recently introduced tidyplots package, which allows intuitive creation of publication-ready charts based on the famous and established ggplot framework. We will, where appropriate state ggplot code too for comparison.\nA quick reminder on how mosaic and ggformula and ggplot work in a very similar fashion:\n\n\n\n\n\n\nTipmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = _____)\nWith mosaic, one can create a statistical correlation test between two variables as: cor_test(y ~ x, data = ______ )\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data = _____) In practice, we often use: dataframe %&gt;%  gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\n\n\n\n\n\n\nTipggplot command template\n\n\n\nThe ggplot template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = ---, mapping = aes(x = ---, y = ---)) + geom_----()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;%  ggplot(aes(xvar, yvar)) + geom_----()\n\n\n\n\n\n\n\n\nTiptidyplots command template\n\n\n\ntidyplot(data = ---, x = ---, y = ---, color = ---) |&gt; add_***_***()",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#bar-charts-and-histograms",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#bar-charts-and-histograms",
    "title": "\n Counts",
    "section": "\n Bar Charts and Histograms",
    "text": "Bar Charts and Histograms\nBar Charts show counts of observations with respect to a Qualitative variable. For instance, a shop inventory with shirt-sizes. Each bar has a height proportional to the count per shirt-size, in this example.\nAlthough Histograms may look similar to Bar Charts, the two are different. First, histograms show continuous Quant data. By contrast, bar charts show categorical data, such as shirt-sizes, or apples, bananas, carrots, etc. Visually speaking, histograms do not usually show spaces between bars because these are continuous values, while column charts must show spaces to separate each category.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#how-do-bar-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#how-do-bar-charts-work",
    "title": "\n Counts",
    "section": "\n How do Bar Chart(s) Work?",
    "text": "How do Bar Chart(s) Work?\nBar are used to show “counts” and “tallies” with respect to Qual variables: they answer the question How Many?. For instance, in a survey, how many people vs Gender? In a Target Audience survey on Weekly Consumption, how many low, medium, or high expenditure people?\nEach Qual variable potentially has many levels as we saw in the Nature of Data. For instance, in the above example on Weekly Expenditure, low, medium and high were levels for the Qual variable Expenditure. Bar charts perform internal counts for each level of the Qual variable under consideration. The Bar Plot is then a set of disjoint bars representing these counts; see the icon above, and then that for histograms!! The X-axis is the set of levels in the Qual variable, and the Y-axis represents the counts for each level.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#case-study-1-chicago-taxi-rides-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#case-study-1-chicago-taxi-rides-dataset",
    "title": "\n Counts",
    "section": "\n Case Study-1: Chicago Taxi Rides dataset",
    "text": "Case Study-1: Chicago Taxi Rides dataset\nWe will first look at at a dataset that speaks about taxi rides in Chicago in the year 2022. This is available on Vincent Arel-Bundock’s superb repository of datasets.Let us read into R directly from the website.\n\n R\n\n\n\ntaxi &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/modeldata/taxi.csv\")\n\nThe data has automatically been read into the webr session, so you can continue on to the next code chunk!\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Examine the Data\nAs per our Workflow, we will look at the data using all the three methods we have seen.\n\n\n dplyr\n skimr\n mosaic\n web-r\n\n\n\n\ndplyr::glimpse(taxi)\n\nRows: 10,000\nColumns: 8\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ tip      &lt;chr&gt; \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\", \"yes\"…\n$ distance &lt;dbl&gt; 17.19, 0.88, 18.11, 20.70, 12.23, 0.94, 17.47, 17.67, 1.85, 1…\n$ company  &lt;chr&gt; \"Chicago Independents\", \"City Service\", \"other\", \"Chicago Ind…\n$ local    &lt;chr&gt; \"no\", \"yes\", \"no\", \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\",…\n$ dow      &lt;chr&gt; \"Thu\", \"Thu\", \"Mon\", \"Mon\", \"Sun\", \"Sat\", \"Fri\", \"Sun\", \"Fri\"…\n$ month    &lt;chr&gt; \"Feb\", \"Mar\", \"Feb\", \"Apr\", \"Mar\", \"Apr\", \"Mar\", \"Jan\", \"Apr\"…\n$ hour     &lt;dbl&gt; 16, 8, 18, 8, 21, 23, 12, 6, 12, 14, 18, 11, 12, 19, 17, 13, …\n\n\n\n\n\nskimr::skim(taxi)\n\n\nData summary\n\n\nName\ntaxi\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n5\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\ntip\n0\n1\n2\n3\n0\n2\n0\n\n\ncompany\n0\n1\n5\n28\n0\n7\n0\n\n\nlocal\n0\n1\n2\n3\n0\n2\n0\n\n\ndow\n0\n1\n3\n3\n0\n7\n0\n\n\nmonth\n0\n1\n3\n3\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nrownames\n0\n1\n5000.50\n2886.90\n1\n2500.75\n5000.50\n7500.25\n10000.0\n▇▇▇▇▇\n\n\ndistance\n0\n1\n6.22\n7.38\n0\n0.94\n1.78\n15.56\n42.3\n▇▁▂▁▁\n\n\nhour\n0\n1\n14.18\n4.36\n0\n11.00\n15.00\n18.00\n23.0\n▁▃▅▇▃\n\n\n\n\n\n\n\n\nmosaic::inspect(taxi)\n\n\ncategorical variables:  \n     name     class levels     n missing\n1     tip character      2 10000       0\n2 company character      7 10000       0\n3   local character      2 10000       0\n4     dow character      7 10000       0\n5   month character      4 10000       0\n                                   distribution\n1 yes (92.1%), no (7.9%)                       \n2 other (27.1%) ...                            \n3 no (81.2%), yes (18.8%)                      \n4 Thu (19.6%), Wed (17.5%), Tue (16.3%) ...    \n5 Apr (31.8%), Mar (31.4%), Feb (20.4%) ...    \n\nquantitative variables:  \n      name   class min      Q1  median        Q3     max        mean\n1 rownames numeric   1 2500.75 5000.50 7500.2500 10000.0 5000.500000\n2 distance numeric   0    0.94    1.78   15.5625    42.3    6.224144\n3     hour numeric   0   11.00   15.00   18.0000    23.0   14.177300\n           sd     n missing\n1 2886.895680 10000       0\n2    7.381397 10000       0\n3    4.359904 10000       0\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\ndistance: Continuous Quant variable, the distance of the trip in miles.\n\n\n\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\ntip: Yes/No type Qual variable, whether a tip was given or not.\n\ncompany: 7 levels, the cab company that was used for the ride.\n\nlocal: 2 levels, whether the trip was local or not.\n\nhour : 24 levels, the hour of the day when the trip started.\n\ndow: 7 levels, the day of the week.\n\nmonth: 12 levels, the month of the year.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights on Examining the taxi dataset\n\n\n\n\nThis is a large dataset (10K rows), 8 columns/variables.\nThere are several Qualitative variables: tip(2), company(7) and local(2), dow(7), and month(12). These have levels as shown in the parenthesis.\nNote that hour despite being a discrete/numerical variable, it can be treated as a Categorical variable too.\n\ndistance is Quantitative.\nThere are no missing values for any variable, all are complete with 10K entries.\n\n\n\n\n Data Munging\nWe will convert the tip, company, dow, local, hour, and month variables into factors beforehand.\n\n## Convert `dow`, `local`, `month`, and `hour` into ordered factors\ntaxi_modified &lt;- taxi %&gt;%\n  mutate(\n    ##\n    tip = factor(tip,\n      levels = c(\"yes\", \"no\"),\n      labels = c(\"yes\", \"no\"),\n      ordered = TRUE\n    ),\n    ##\n    company = factor(company), # Any order is OK.\n    ##\n    dow = factor(dow,\n      levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"),\n      labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"),\n      ordered = TRUE\n    ),\n    ##\n    local = factor(local,\n      levels = c(\"yes\", \"no\"),\n      labels = c(\"yes\", \"no\"),\n      ordered = TRUE\n    ),\n    ##\n    month = factor(month,\n      levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\"),\n      labels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\"),\n      ordered = TRUE\n    ),\n    ##\n    hour = factor(hour,\n      levels = c(0:23), labels = c(0:23),\n      ordered = TRUE\n    )\n  )\ntaxi_modified %&gt;% glimpse()\n\nRows: 10,000\nColumns: 8\n$ rownames &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ tip      &lt;ord&gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, yes, y…\n$ distance &lt;dbl&gt; 17.19, 0.88, 18.11, 20.70, 12.23, 0.94, 17.47, 17.67, 1.85, 1…\n$ company  &lt;fct&gt; Chicago Independents, City Service, other, Chicago Independen…\n$ local    &lt;ord&gt; no, yes, no, no, no, yes, no, no, no, no, no, no, no, yes, no…\n$ dow      &lt;ord&gt; Thu, Thu, Mon, Mon, Sun, Sat, Fri, Sun, Fri, Tue, Tue, Sun, W…\n$ month    &lt;ord&gt; Feb, Mar, Feb, Apr, Mar, Apr, Mar, Jan, Apr, Mar, Mar, Apr, A…\n$ hour     &lt;ord&gt; 16, 8, 18, 8, 21, 23, 12, 6, 12, 14, 18, 11, 12, 19, 17, 13, …\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n Hypothesis and Research Questions\nThe target variable for an experiment that resulted in this data might be the tip variable, since that looks like a response, or an outcome. It is a binary i.e. Yes/No type Qual variable.\nWe will use the tip variable to ask questions about the data, and then plot the answers to these questions.\n\n\n\n\n\n\nNoteResearch Questions:\n\n\n\n\nDo more people tip than not?\nDoes a tip depend upon whether the trip is local or not?\nDo some cab company-ies get more tips than others?\nAnd does a tip depend upon the distance, hour of day, and dow and month?\n\nTry and think of more Questions!\n\n\n\n Plotting Barcharts\nLet’s plot some bar graphs: recall that for bar charts, we need to choose Qual variables to count with! In each case, we will state a Hypothesis/Question and try to answer it with a chart.\n\n Question-1: Do more people tip than not?\n\n\n\n\n\n\nNoteQuestion-1: Do more people tip than not?\n\n\n\n\n\nggformula-1\nggplot-1\ntidyplots-1\ntinyplot-1\n web-r-1\n\n\n\n\nShow the Codetheme_set(new = theme_custom())\n\ngf_bar(~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 1A: Counts of Tips\")\n\n\n\n\n\n\nFigure 2\n\n\n\n\n\n\n\nShow the Codetheme_set(new = theme_custom())\n\nggplot(taxi_modified) +\n  geom_bar(aes(x = tip)) +\n  labs(title = \"Plot 1A: Counts of Tips\")\n\n\n\n\n\n\nFigure 3\n\n\n\n\n\n\n\nShow the Codetidyplot(x = tip, data = taxi_modified) %&gt;%\n  add_count_bar() %&gt;%\n  add_title(\"Plot 1A: Counts of Tips\") %&gt;%\n  adjust_size(height = 50, width = 85, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\nShow the Codetinyplot(~tip,\n  data = taxi_modified,\n  type = \"barplot\",\n  main = \"Plot 1A: Counts of Tips\"\n)\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-1\n\nFar more people do tip than not. Which is nice.\n(Future) The counts of tip are very imbalanced and if we are to setup a model for that (logistic regression) we would need to very carefully subset the data for training and testing our model.\n\n\n\n\n Question-2: Does the tip depend upon whether the trip is local or not?\n\n\n\n\n\n\nNoteQuestion-2: Does the tip depend upon whether the trip is local or not?\n\n\n\n\n\nggformula-2\nggplot-2\ntidyplots-2\ntinyplot-2\n web-r-2\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"dodge\"\n  ) %&gt;%\n  gf_labs(title = \"Plot 2A: Dodged Bar Chart\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"stack\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 2B: Stacked Bar Chart\",\n    subtitle = \"Can we spot per group differences in proportions??\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## Showing \"per capita\" percentages\ntaxi_modified %&gt;%\n  gf_bar(~local,\n    fill = ~tip,\n    position = \"fill\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 2C: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## Showing \"per capita\" percentages\n## Better labelling of Y-axis\ntaxi_modified %&gt;%\n  gf_props(~local,\n    fill = ~tip,\n    position = \"fill\"\n  ) %&gt;%\n  gf_labs(\n    title = \"Plot 2D: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = local, fill = tip), position = \"dodge\") +\n  labs(title = \"Plot 2A:Dodged Bar Chart\") +\n  scale_fill_brewer(palette = \"Set1\")\n##\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = local, fill = tip), position = \"stack\") +\n  labs(\n    title = \"Plot 2B: Stacked Bar Chart\",\n    subtitle = \"Can we spot per group differences in proportions??\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\")\n## Showing \"per capita\" percentages\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = local, fill = tip), position = \"fill\") +\n  labs(title = \"Plot 2C: Filled Bar Chart\", subtitle = \"Shows Per group differences in Proportions!\") +\n  scale_fill_brewer(palette = \"Set1\")\n## Showing \"per capita\" percentages\n## Better labelling of Y-axis\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = local, fill = tip), position = \"fill\") +\n  labs(\n    title = \"Plot 2D: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\",\n    y = \"Proportion\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(local,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_count_bar() %&gt;%\n  add_title(\"Plot 2A: Dodged Bar Chart\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(local, colour = tip, data = taxi_modified) %&gt;%\n  add_barstack_absolute() %&gt;%\n  add_title(\"Plot 2B: Stacked Bar Chart\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(local,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_relative() %&gt;%\n  add_title(\"Plot 2C: Dodged Bar Chart\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntinyplot(~ local | tip,\n  data = taxi_modified,\n  type = \"barplot\", palette = \"tableau\",\n  beside = TRUE, # for placing bars beside one another\n  main = \"Plot 2A: Dodged Bar Chart\",\n  legend = \"right!\"\n) # Outside, to the right\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntinyplot(~ local | tip,\n  data = taxi_modified,\n  type = \"barplot\", palette = \"tableau\",\n  beside = FALSE,\n  xlevels = c(\"no\", \"yes\"), # try reversing\n  main = \"Plot 2B: Stacked Bar Chart\",\n  sub = \"Can we spot per group differences in proportions??\",\n  legend = \"right!\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-2\n\nCounting the frequency of tip by local gives us grouped counts, but we cannot tell the percentage per group (local or not) of those who tip and those who do not.\nWe need per-group percentages because the number of local trips are not balanced\nHence with tidyplots, we find that add_barstack_relative gives the clearest visual indication of a difference in proportion.\nLikewise with ggformula, we tried bar charts with position = stack, but finally it is the position = fill that works best.\nWe see that the percentage of tippers is somewhat higher with people who make non-local trips. Not surprising.\n\n\n\n\n Question-3: Do some cab company-ies get more tips than others?\n\n\n\n\n\n\nNoteQuestion-3: Do some cab company-ies get more tips than others?\n\n\n\n\n\nggformula-3\nggplot-3\ntidyplots-3\ntinyplot-3\n web-r-3\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  gf_bar(~company, fill = ~tip, position = \"dodge\") %&gt;%\n  gf_labs(title = \"Plot 3A: Dodged Bar Chart\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(\n    size = 6,\n    angle = 45, hjust = 0.5\n  ))) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  gf_bar(~company, fill = ~tip, position = \"stack\") %&gt;%\n  gf_labs(\n    title = \"Plot 3B: Stacked Bar Chart\",\n    subtitle = \"Can we spot per group differences in proportions??\"\n  ) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## Showing \"per capita\" percentages\ntaxi_modified %&gt;%\n  gf_percents(~company, fill = ~tip, position = \"fill\") %&gt;%\n  gf_labs(\n    title = \"Plot 3C: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## Showing \"per capita\" percentages\n## Better labelling of Y-axis\ntaxi_modified %&gt;%\n  gf_props(~company, fill = ~tip, position = \"fill\") %&gt;%\n  gf_labs(\n    title = \"Plot 3D: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = company, fill = tip), position = \"dodge\") +\n  labs(title = \"Plot 3A: Dodged Bar Chart\") +\n  theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) +\n  scale_fill_brewer(palette = \"Set1\")\n##\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = company, fill = tip), position = \"stack\") +\n  labs(\n    title = \"Plot 3B: Stacked Bar Chart\",\n    subtitle = \"Can we spot per group differences in proportions??\"\n  ) +\n  theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) +\n  scale_fill_brewer(palette = \"Set1\")\n## Showing \"per capita\" percentages\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = company, fill = tip), position = \"fill\") +\n  labs(\n    title = \"Plot 3C: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\"\n  ) +\n  theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) +\n  scale_fill_brewer(palette = \"Set1\")\n## Showing \"per capita\" percentages\n## Better labelling of Y-axis\ntaxi_modified %&gt;%\n  ggplot() +\n  geom_bar(aes(x = company, fill = tip), position = \"fill\") +\n  labs(\n    title = \"Plot 3D: Filled Bar Chart\",\n    subtitle = \"Shows Per group differences in Proportions!\",\n    y = \"Proportions\"\n  ) +\n  theme(theme(axis.text.x = element_text(size = 6, angle = 45, hjust = 1))) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(company,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_count_bar() %&gt;%\n  add_title(\"Plot 3A: Dodged Bar Chart\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_x_axis(rotate_labels = 45)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(company,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_absolute() %&gt;%\n  add_title(\"Plot 3B: Stacked Bar Chart\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_x_axis(rotate_labels = 45)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(company,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_relative() %&gt;%\n  add_title(\"Plot 3C: Stacked Bar Chart\") %&gt;%\n  add_annotation_text(\n    text = \"Proportions of Tippers per Company\",\n    x = 4.25, y = 1.25\n  ) %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_x_axis(title = \"\", rotate_labels = 45) %&gt;%\n  adjust_y_axis(title = \"Proportions\") # Not happening??\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo be Coded!!\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-3\n\nUsing stack-ed, dodge-ed, and fill-ed in ggformula (and bars, absolute-stacked-bars, and relative-stacked-bars in tidyplots) in bar plots gives us different ways of looking at the sets of counts;\n\nfill: gives us a per-group proportion of another Qual variable for a chosen Qual variable. This chart view is useful in Inference for Proportions;\nMost cab company-ies have similar usage, if you neglect the other category of company;\nDoes seem that of all the company-ies, tips are not so good for the Flash Cab company. A driver issue? Or are the cars too old? Or don’t they offer service everywhere?\n\n\n\n\n Question-4: Does a tip depend upon the distance, hour of day, and dow and month?\n\n\n\n\n\n\nNoteQuestion-4: Does a tip depend upon the distance, hour of day, and dow and month?\n\n\n\n\n\nggformula-4\nggplot-4\ntidyplots-4\ntinyplot-4\n web-r-4\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_bar(~hour, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 4A: Counts of Tips by Hour\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_bar(~dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 4B: Counts of Tips by Day of Week\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_bar(~month, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 4C: Counts of Tips by Month\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_bar(~ month | dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 4D: Counts of Tips by Day of Week and Month\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## This may be too busy a graph...\ngf_bar(~ dow | hour, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(\n    title = \"Plot 4E: Counts of Tips by Hour and Day of Week\",\n    subtitle = \"Is this plot arrangement easy to grasp?\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\n## This is better!\ngf_bar(~ hour | dow, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(\n    title = \"Plot 4F: Counts of Tips by Hour and Day of Week\",\n    subtitle = \"Facetted by Day of Week\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_bar(~hour, fill = ~tip, data = taxi_modified) %&gt;%\n  gf_labs(title = \"Plot 4A: Counts of Tips by Hour\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n##\nggplot(taxi_modified) +\n  geom_bar(aes(x = dow, fill = tip)) +\n  labs(title = \"Plot 4B: Counts of Tips by Day of Week\") +\n  scale_fill_brewer(palette = \"Set1\")\n##\nggplot(taxi_modified) +\n  geom_bar(aes(x = month, fill = tip)) +\n  labs(title = \"Plot 4C: Counts of Tips by Month\") +\n  scale_fill_brewer(palette = \"Set1\")\n##\nggplot(taxi_modified) +\n  geom_bar(aes(x = month, fill = tip)) +\n  facet_wrap(~dow) +\n  labs(title = \"Plot 4D: Counts of Tips by Day of Week and Month\") +\n  scale_fill_brewer(palette = \"Set1\")\n##\nggplot(taxi_modified) +\n  geom_bar(aes(x = dow, fill = tip)) +\n  facet_wrap(~hour) +\n  labs(\n    title = \"Plot 4E: Counts of Tips by Hour and Day of Week\",\n    subtitle = \"Is this plot arrangement easy to grasp?\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\")\n##\nggplot(taxi_modified) +\n  geom_bar(aes(x = hour, fill = tip)) +\n  facet_wrap(~dow) +\n  labs(\n    title = \"Plot 4F: Counts of Tips by Hour and Day of Week\",\n    subtitle = \"Swapped the Facets\"\n  ) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(hour,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_count_bar() %&gt;%\n  add_title(\"Plot 4A: Counts of Tips by Hour\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(hour,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_absolute() %&gt;%\n  add_title(\"Plot 4B: Counts of Tips by Hour\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyplots::tidyplot(hour,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_relative() %&gt;%\n  add_title(\"Plot 4C: Counts of Tips by Hour\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##\ntidyplots::tidyplot(month,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_absolute() %&gt;%\n  add_title(\"Plot 4D: Counts of Tips by Day of Week and Month\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly) %&gt;%\n  split_plot(\n    by = dow,\n    ncol = 3, nrow = 3, guides = \"collect\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n##\ntidyplots::tidyplot(dow,\n  colour = tip,\n  data = taxi_modified\n) %&gt;%\n  add_barstack_absolute() %&gt;%\n  add_title(\"Plot 4D: Counts of Tips by Day of Week and Month\") %&gt;%\n  adjust_size(height = 50, width = 80, unit = \"mm\") %&gt;%\n  adjust_colors(colors_discrete_friendly) %&gt;%\n  adjust_x_axis(rotate_labels = 45) %&gt;%\n  split_plot(\n    by = hour,\n    ncol = 3, nrow = 8, guides = \"collect\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo be Coded.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nBusiness Insights-4\n\nNote: We were using fill = ~ tip here! Why is that a good idea?\n\ntips vs hour: There are always more people who tip than those who do not. Of course there are fewer trips during the early morning hours and the late night hours, based on the very small bar-pairs we see at those times\n\ntips vs dow: Except for Sunday, the tip count patterns (Yes/No) look similar across all days.\n\ntips vs month: We have data for 4 months only. Again, the tip count patterns (Yes/No) look similar across all months. Perhaps slightly fewer trips in Jan, when it is cold in Chicago and people may not go out much.\n\ntips vs dow vs month: Very similar counts for tips(Yes/No) across day-of-week and month.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#bar-plot-extras",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#bar-plot-extras",
    "title": "\n Counts",
    "section": "\n Bar Plot Extras",
    "text": "Bar Plot Extras\n\n\n\n\n\n\nNotegf-bar and gf-col\n\n\n\nNote also that gf_bar/geom_bar takes only ONE variable (for the x-axis), whereas gf_col/geom_col needs both X and Y variables since it simply plots columns. Both are useful!\n\n\n\n\n\n\n\n\nNoteAnd we can plot Proportions and Percentages too!\n\n\n\n\n\n\nWe have already seen gf_props in our two case studies above. Also check out gf_percents ! These are both very useful ggformula functions!\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_props(~substance,\n  data = mosaicData::HELPrct, fill = ~sex,\n  position = \"dodge\"\n) %&gt;%\n  gf_labs(\n    title = \"Plotting Proportions using gf_props\",\n    subtitle = \"Option = dodge\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_props(~substance,\n  data = mosaicData::HELPrct, fill = ~sex,\n  position = \"fill\"\n) %&gt;%\n  gf_labs(\n    title = \"Plotting Proportions using gf_props\",\n    subtitle = \"Option = fill\"\n  ) %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntheme_set(new = theme_custom())\n\ngf_percents(~substance,\n  data = mosaicData::HELPrct, fill = ~sex,\n  position = \"dodge\"\n) %&gt;%\n  gf_refine(\n    scale_y_continuous(\n      labels = scales::label_percent(scale = 1)\n    )\n  ) %&gt;%\n  gf_labs(title = \"Plotting Percentages using gf_percents\") %&gt;%\n  gf_refine(scale_fill_brewer(palette = \"Set1\"))",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#are-the-differences-in-proportion-significant",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#are-the-differences-in-proportion-significant",
    "title": "\n Counts",
    "section": "\n Are the Differences in Proportion Significant?",
    "text": "Are the Differences in Proportion Significant?\nWhen we see situations such as this, where data has one or more Qual variables that are binary(Yes/No), we are always interested in whether these proportions of Yes/No are really different, or if we are just seeing the result of random chance. This is usually mechanized by a Stat Test called a Single Proportion Test or, when we have more than one, a Multiple Proportion Test.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#your-turn",
    "title": "\n Counts",
    "section": "\n Your Turn",
    "text": "Your Turn\n  Datasets\n\nClick on the Dataset Icon above, and unzip that archive. Try to make Bar plots with each of them, using one or more Qual variables.\nA dataset from calmcode.io https://calmcode.io/datasets.html\n\n\nAiRbnb Price Data on the French Riviera:\n\n\n\n AiRbnb data\n\n\n\nApartment price vs ground living area:\n\n\n\n Apartment Data\n\n\n\n\nFertility: This rather large and interesting Fertility related dataset from https://vincentarelbundock.github.io/Rdatasets/csv/AER/Fertility.csv\n\n\n Download the Fertility Data \nglimpse / skim / inspect the dataset in each case, state that Data Dictionary, and develop a set of Questions that can be answered by appropriate stat measures, or by using a chart to show the distribution.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#wait-but-why",
    "title": "\n Counts",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nAlways count your chickens count your data before you model or infer!\nCounts first give you an absolute sense of how much data you have.\nCounts by different Qual variables give you a sense of the combinations you have in your data: \\((Male/Female) * (Income-Status) * (Old/Young) * (Urban/Rural)\\) (Say 2 * 3 * 2 * 2 = 24 combinations of data)\nCounts then give an idea whether your data is lop-sided: do you have too many observations of one category(level) and too few of another category(level) in a given Qual variable?\nBalance is important in order to draw decent inferences\nAnd for ML algorithms, to train them properly.\nSince the X-axis in bar charts is Qualitative (the bars don’t touch, remember!) it is possible to sort the bars at will, based on the levels within the Qualitative variables. See the approx Zipf’s Law distribution for the English alphabet below:\n\n\n\n\n\n\nFigure 6: Zipf’s Law\n\n\nIn Figure 6, the letters of the alphabet are “levels” within a Qualitative variable, and these levels have been sorted based on the frequency or count! This is what Sherlock Holmes might have done, or the method how they cracked the code to the treasure in this story.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#conclusion",
    "title": "\n Counts",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nQualitative data variables can be plotted as counts, using Bar Charts\n\ngf_col and gf_bar provide Bar charts; gf_bar performs counts internally, whereas gf_col requires pre-counted data.\nUsing facets allows us to view counts of one Qual variable split over two other Qual variables",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#ai-generated-summary-and-podcast",
    "title": "\n Counts",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis text excerpt focuses on bar charts and histograms as visualization tools for qualitative and quantitative data, respectively. It walks the reader through the creation of bar charts using the R programming language, illustrating the concept through a case study using the Chicago taxi rides dataset. The author explores various scenarios and questions related to taxi tipping, such as the frequency of tips and their dependence on trip locality, company, hour of the day, and day of the week. Finally, the excerpt highlights the importance of understanding data counts before undertaking data modeling or inference, emphasizing the role of bar charts in revealing data distribution and potential imbalances.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/20-BarPlots/index.html#references",
    "title": "\n Counts",
    "section": "\n References",
    "text": "References\n\nDaniel Kaplan and Randall Pruim. ggformula: Formula Interface for ggplot2 (full version). https://www.mosaic-web.org/ggformula/articles/pkgdown/ggformula-long.html\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\ntidyplots\n0.3.1\nEngler (2025)\n\n\ntidyverse\n2.0.0\nWickham et al. (2019)\n\n\ntinyplot\n0.4.2\nMcDermott, Arel-Bundock, and Zeileis (2025)\n\n\n\n\n\n\nEngler, Jan Broder. 2025. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” iMeta, e70018. https://doi.org/10.1002/imt2.70018.\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://doi.org/10.32614/CRAN.package.ggformula.\n\n\nMcDermott, Grant, Vincent Arel-Bundock, and Achim Zeileis. 2025. tinyplot: Lightweight Extension of the Base r Graphics System. https://doi.org/10.32614/CRAN.package.tinyplot.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:counting\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Counts"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html",
    "title": "\n Networks",
    "section": "",
    "text": "“The beginnings and endings of all human undertakings are untidy.”\n— John Galsworthy, author, Nobel laureate (14 Aug 1867-1933)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#setting-up-r-packages",
    "title": "\n Networks",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(igraph) # Network Analysis Library (Handle data and Viz)\nlibrary(tidygraph) # For Network \"Manipulation\"\nlibrary(ggraph) # For Network Visualization\nlibrary(graphlayouts) # For Network Visualization, more layouts and themes\nlibrary(visNetwork) # For Interactive Network Visualization\nlibrary(igraphdata) # For \"Network\" Datasets\nlibrary(sand) # Statistical Analysis of Networks Data\n\nlibrary(tinytable) # Elegant Tables for our data\n\n# For repeatable layouts, some can be random!!\nset.seed(12345)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#introduction",
    "title": "\n Networks",
    "section": "\n Introduction",
    "text": "Introduction\nNetwork graphs show relationships between entities: what sort they are, how strong they are, and even of they change over time.\nWe will examine data structures pertaining both to the entities and the relationships between them and look at the data object that can combine these aspects together. Then we will see how these are plotted, what the structure of the plot looks like. There are also metrics that we can calculate for the network, based on its structure. We will of course examine geometric metaphors that can represent various classes of entities and their relationships.\nNetwork graphs can be rendered both as static and interactive and we will examine R packages that render both kinds of plots.\nThere is a another kind of structure: one that combines spatial and network data in one. We will defer that for a future module !",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#what-kind-network-graphs-will-we-make",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#what-kind-network-graphs-will-we-make",
    "title": "\n Networks",
    "section": "What kind Network graphs will we make?",
    "text": "What kind Network graphs will we make?\nHere is a network map of the characters in Victor Hugo’s Les Miserables:\n\n\nAnd this: the well known Zachary’s Karate Club dataset visualized as a network",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#goals",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#goals",
    "title": "\n Networks",
    "section": "\n Goals",
    "text": "Goals\nAt the end of this Lab session, we should:\n\nknow the types and structures of network data and be able to work with them\nunderstand the basics of modern network packages in R\nbe able to create network visualizations using tidygraph, ggraph( static visualizations ) and visNetwork (interactive visualizations)\nsee directions for how the network metaphor applies in a variety of domains (e.g. biology/ecology, ideas/influence, technology, transportation, to name a few)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#pedagogical-note",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#pedagogical-note",
    "title": "\n Networks",
    "section": "\n Pedagogical Note",
    "text": "Pedagogical Note\nThe method followed will be based on PRIMM:\n\n\nPREDICT Inspect the code and guess at what the code might do, write predictions\n\n\nRUN the code provided and check what happens\n\nINFER what the parameters of the code do and write comments to explain. What bells and whistles can you see?\n\nMODIFY the parameters code provided to understand the options available. Write comments to show what you have aimed for and achieved.\n\nMAKE : take an idea/concept of your own, and graph it.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#graph-metaphors",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#graph-metaphors",
    "title": "\n Networks",
    "section": "\n Graph Metaphors",
    "text": "Graph Metaphors\nNetwork graphs are characterized by two key terms: nodes and edges\n\n\nNodes : Entities\n\nMetaphors: Individual People? Things? Ideas? Places? to be connected in the network.\nSynonyms: vertices. Nodes have IDs.\n\n\n\nEdges: Connections\n\nMetaphors: Interactions? Relationships? Influence? Letters sent and received? Dependence? between the entities.\nSynonyms: links, ties.\n\n\n\nIn R, we create network representations using node and edge information. One way in which these could be organized are:\n\n\nNode list: a data frame with a single column listing the node IDs found in the edge list. You can also add attribute columns to the data frame such as the names of the nodes or grouping variables. ( Type? Class? Family? Country? Subject? Race? )\n\n\nNode Table\n\n\n\n\n\n\n\nID\nNode Name\nAttribute? Qualities?Categories? Family? Country?Planet?\n\n\n1\nNed\nNursery School Teacher\n\n\n2\nJaguar Paw\nMain Character, Apocalypto\n\n\n3\nJohn Snow\nEpidemiologist\n\n\n\n\n\nEdge list: data frame containing two columns: source node and destination node of an edge. Source and Destination have node IDs.\n\nWeighted network graph: An edge list can also contain additional columns describing attributes of the edges such as a magnitude aspect for an edge. If the edges have a magnitude attribute the graph is considered weighted.\n\n\nEdges Table\n\nFrom\nTo\nRelationship\nWeightage\n\n\n\n1\n3\nFinancial Dealings\n6\n\n\n2\n1\nHistory Lessons\n2\n\n\n2\n3\nVaccination\n15\n\n\n\n\n\nLayout: A geometric arrangement of nodes and edges.\n\nMetaphors: Location? Spacing? Distance? Coordinates? Colour? Shape? Size? Provides visual insight due to the arrangement.\n\n\n\nLayout Algorithms : Method to arranges nodes and edges with the aim of optimizing some metric .\n\nMetaphors: Nodes are masses and edges are springs. The Layout algorithm minimizes the stretching and compressing of all springs.(BTW, are the Spring Constants K the same for all springs?…)\n\n\nDirected and undirected network graph: If the distinction between source and target is meaningful, the network is directed. If the distinction is not meaningful, the network is undirected. Directed edges represent an ordering of nodes, like a relationship extending from one node to another, where switching the direction would change the structure of the network. Undirected edges are simply links between nodes where order does not matter.\n\n\n\n\n\n\n\nTipExamples\n\n\n\n\nThe World Wide Web is an example of a directed network because hyperlinks connect one Web page to another, but not necessarily the other way around.\nCo-authorship networks represent examples of un-directed networks, where nodes are authors and they are connected by an edge if they have written a publication together\nWhen people send e-mail to each other, the distinction between the sender (source) and the recipient (target) is clearly meaningful, therefore the network is directed.\n\n\n\n\n\nConnected and Disconnected graphs: If there is some path from any node to any other node, the Networks is said to be Connected. Else, Disconnected.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#predictruninfer-1",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#predictruninfer-1",
    "title": "\n Networks",
    "section": "Predict/Run/Infer-1",
    "text": "Predict/Run/Infer-1\nUsing tidygraph and ggraph\n\ntidygraph and ggraph are modern R packages for network data. Graph Data setup and manipulation is done in tidygraph and graph visualization with ggraph.\n\n\ntidygraph Data -&gt; “Network Object” in R.\n\nggraph Network Object -&gt; Plots using a chosen layout/algo.\n\nBoth leverage the power of igraph, which is the Big Daddy of all network packages. We will be using the Grey’s Anatomy dataset in our first foray into networks.\nStep1. Read the data\nDownload these two datasets into your current project-&gt; data folder.\n Grey’s Anatomy Nodes \n Grey’s Anatomy Edges \ngrey_nodes &lt;- read_csv(\"files/data/grey_nodes.csv\")\ngrey_edges &lt;- read_csv(\"files/data/grey_edges.csv\")\n\ngrey_nodes\ngrey_edges\n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #1\n\n\n\nLook at the output thumbnails. What attributes (i.e. extra information) are seen for Nodes and Edges?\n\n\nStep 2.Create a network object using tidygraph:\nKey function:\n\n\ntbl_graph(): (aka “tibble graph”). Key arguments: nodes, edges and directed. Note this is a very versatile command and can take many input forms, such as data structures that result from other packages. Type ?tbl_graph in the Console and see the Usage section.\n\n\nga &lt;- tbl_graph(\n  nodes = grey_nodes,\n  edges = grey_edges,\n  directed = FALSE\n)\nga\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Node Data: 54 × 7 (active)\n   name               sex   race  birthyear position  season sign    \n   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   \n 1 Addison Montgomery F     White      1967 Attending      1 Libra   \n 2 Adele Webber       F     Black      1949 Non-Staff      2 Leo     \n 3 Teddy Altman       F     White      1969 Attending      6 Pisces  \n 4 Amelia Shepherd    F     White      1981 Attending      7 Libra   \n 5 Arizona Robbins    F     White      1976 Attending      5 Leo     \n 6 Rebecca Pope       F     White      1975 Non-Staff      3 Gemini  \n 7 Jackson Avery      M     Black      1981 Resident       6 Leo     \n 8 Miranda Bailey     F     Black      1969 Attending      1 Virgo   \n 9 Ben Warren         M     Black      1972 Other          6 Aquarius\n10 Henry Burton       M     White      1972 Non-Staff      7 Cancer  \n# ℹ 44 more rows\n#\n# Edge Data: 57 × 4\n   from    to weight type    \n  &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     5    47      2 friends \n2    21    47      4 benefits\n3     5    46      1 friends \n# ℹ 54 more rows\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #2\n\n\n\nWhat information does the graph object contain? What attributes do the nodes have? What about the edges?\n\n\nStep 3. Plot using ggraph\n\n3a. Quick Plot: autograph() This is to check quickly is the data is imported properly and to decide upon going on to a more elaborate plotting.\n\nautograph(ga)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #3\n\n\n\nDescribe this graph, in simple words here. Try to use some of the new domain words we have just acquired: nodes/edges, connected/disconnected, directed/undirected.\n\n\n3b. More elaborate plot\nKey functions:\n\n\nggraph(layout = \"......\"): Create classic node-edge diagrams; i.e. Sets up the graph. Rather like ggplot for networks!\n\nTwo kinds of geom: one set for nodes, and another for edges\n\ngeom_node_point(aes(.....)): Draws node as “points”. Alternatives are circle / arc_bar / tile / voronoi. Remember the geoms that we have seen before in Grammar of Graphics!\ngeom_edge_link0(aes(.....)): Draws edges as “links”. Alternatives are arc / bend / elbow / hive / loop / parallel / diagonal / point / span /tile.\ngeom_node_text(aes(label = ......), repel = TRUE): Adds text labels (non-overlapping). Alternatives are label /...\nlabs(title = \"....\", subtitle = \"....\", caption = \"....\"): Change main titles, axis labels and legend titles. We know this from our work with ggplot.\n\n\n# Write Comments next to each line\n# About what that line does for the overall graph\n\nggraph(graph = ga, layout = \"kk\") +\n  #\n  geom_edge_link0(width = 2, color = \"pink\") +\n  #\n  geom_node_point(\n    shape = 21, size = 8,\n    fill = \"blue\",\n    color = \"green\",\n    stroke = 2\n  ) +\n\n  labs(\n    title = \"Whoo Hoo! My First Silly Grey's Anatomy graph in R!\",\n    subtitle = \"Why did I ever get in this course...\",\n    caption = \"Bro, they are doing cool things in the other classes...\\n And the show is even more cool!\"\n  ) +\n\n  set_graph_style(family = \"Roboto Condensed\", size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #4:\n\n\n\nWhat parameters have been changed here, compared to the earlier graph? Where do you see these changes in the code above?\n\n\nLet us Play with this graph and see if we can make some small changes. Colour? Fill? Width? Size? Stroke? Labs? Of course!\n\n# Change the parameters in each of the commands here to new ones\n# Use fixed values for colours or sizes...etc.\n\nggraph(graph = ga, layout = \"kk\") +\n  geom_edge_link0(width = 2) +\n  geom_node_point(\n    shape = 21, size = 4,\n    fill = \"moccasin\",\n    color = \"firebrick\",\n    stroke = 2\n  ) +\n  labs(\n    title = \"Whoo Hoo! My next silly Grey's Anatomy graph in R!\",\n    subtitle = \"Why did I ever get in this course...\",\n    caption = \"Bro, they are doing cool things in the other classes...\"\n  ) +\n  set_graph_style(family = \"Roboto Condensed\", size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #5\n\n\n\nWhat did the shape parameter achieve? What are the possibilities with shape? How about including alpha?\n\n\n3c. Aesthetic Mapping from Node and Edge attribute columns\nUp to now, we have assigned specific numbers to geometric aesthetics such as shape and size. Now we are ready ( maybe ?) change the meaning and significance of the entire graph and each element within it, and use aesthetics / metaphoric mappings to achieve new meanings or insights. Let us try using aes() inside each geom to map a variable to a geometric aspect.\nDon’t try to use more than 2 aesthetic mappings simultaneously!!\nThe node elements we can tweak are:\n\nTypes of Nodes: geom_node_****()\n\nNode Parameters: inside geom_node_****(aes(...............))\n-aes(alpha  = node-variable) : opacity; a value between 0 and 1\n-aes(shape  = node-variable) : node shape\n-aes(colour = node-variable) : node colour\n-aes(fill   = node-variable) : fill colour for node\n-aes(size   = node-variable) : size of node\n\nThe edge elements we can tweak are:\n\nType of Edges” geom_edge_****()\n\nEdge Parameters: inside geom_edge_****(aes(...............))\n-aes(colour = edge-variable) : colour of the edge\n-aes(width  = edge-variable) : width of the edge\n-aes(label  = some_variable) : labels for the edge\n\nType ?geom_node_point and ?geom-edge_link in your Console for more information.\n\nggraph(graph = ga, layout = \"fr\") +\n  geom_edge_link0(aes(width = weight)) + # change variable here\n\n  geom_node_point(aes(fill = race),\n    colour = \"black\",\n    size = 4, shape = 21\n  ) + # change variable here\n\n  labs(\n    title = \"Whoo Hoo! Yet another Grey's Anatomy graph in R!\",\n    subtitle = \"Colouring Nodes by Attribute\",\n    caption = \"Grey's Anatomy\"\n  ) +\n\n  scale_edge_width(range = c(0.2, 2)) +\n  set_graph_style(family = \"Roboto Condensed\", size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #6\n\n\n\nDescribe some of the changes here. What types of edges worked? Which variables were you able to use for nodes and edges and how? What did not work with either of the two?\n\n\n\n# Arc diagram\n\nggraph(ga, layout = \"linear\") +\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n  geom_node_point(size = 2, colour = \"red\") +\n  labs(\n    edge_width = \"Weight\", title = \"Grey's Anatomy\",\n    subtitle = \"Arc Layout\"\n  ) +\n  set_graph_style(family = \"Roboto Condensed\", size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #7\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?\n\n\n\n# Coord diagram, circular\nggraph(ga, layout = \"linear\", circular = TRUE) + # Note the layout!\n  geom_edge_arc0(aes(width = weight), alpha = 0.8) +\n  scale_edge_width(range = c(0.2, 2)) +\n\n  geom_node_point(size = 3, colour = \"red\") +\n  geom_node_text(aes(label = name),\n    repel = TRUE, size = 2, check_overlap = TRUE,\n    max.overlaps = 25\n  ) +\n  labs(\n    edge_width = \"Weight\",\n    title = \"Grey's Anatomy\",\n    subtitle = \"Arc Layout\"\n  ) +\n  theme(aspect.ratio = 1) +\n  set_graph_style(family = \"Roboto Condensed\", size = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #8\n\n\n\nHow does this graph look “metaphorically” different? Do you see a difference in the relationships between people here? Why?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#hierarchical-layouts",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#hierarchical-layouts",
    "title": "\n Networks",
    "section": "Hierarchical layouts",
    "text": "Hierarchical layouts\nThese provide for some alternative metaphorical views of networks. Note that not all layouts are possible for all datasets!!\n\n# set_graph_style()\n\n# This dataset contains the graph that describes the class\n# hierarchy for the Flare visualization library.\n# Type ?flare in your Console\nhead(flare$vertices)\n\n\n  \n\n\nhead(flare$edges)\n\n\n  \n\n\n# flare class hierarchy\ngraph &lt;- tbl_graph(edges = flare$edges, nodes = flare$vertices)\n\n##\nset_graph_style(family = \"Roboto Condensed\", size = 16)\n##\n\n# dendrogram\nggraph(graph, layout = \"dendrogram\") +\n  geom_edge_diagonal() +\n  labs(title = \"Dendrogram\")\n\n# circular dendrogram\nggraph(graph, layout = \"dendrogram\", circular = TRUE) +\n  geom_edge_diagonal0() +\n  geom_node_point(aes(filter = leaf)) +\n  coord_fixed() +\n  labs(title = \"Circular Dendrogram\")\n\n# rectangular tree map\nggraph(graph, layout = \"treemap\", weight = size) +\n  geom_node_tile(aes(fill = depth), size = 0.25) +\n  scale_fill_distiller(palette = \"Pastel1\") +\n  labs(title = \"Rectangular Tree Map\")\n\n\n# circular tree map\nggraph(graph, layout = \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = depth), size = 0.25, n = 50) +\n  scale_fill_distiller(palette = \"Accent\") +\n  coord_fixed() +\n  labs(title = \"Circular Tree Map\")\n\n\n# icicle\nggraph(graph, layout = \"partition\") +\n  geom_node_tile(aes(y = -y, fill = depth)) +\n  scale_fill_distiller(palette = \"Set3\") +\n  labs(title = \"Icicle Chart\")\n\n# sunburst (circular icicle)\nggraph(graph, layout = \"partition\", circular = TRUE) +\n  geom_node_arc_bar(aes(fill = depth)) +\n  scale_fill_distiller(palette = \"Spectral\") +\n  coord_fixed() +\n  labs(title = \"Circular Icicle\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #9\n\n\n\nHow do graphs look “metaphorically” different? Do they reveal different aspects of the group? How?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#faceting",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#faceting",
    "title": "\n Networks",
    "section": "Faceting",
    "text": "Faceting\nFaceting allows to create sub-plots according to the values of a qualitative attribute on nodes or edges.\nset_graph_style(family = \"Roboto Condensed\", size = 16)\n\n# facet edges by type\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0(aes(color = type)) +\n  geom_node_point() +\n  facet_edges(~type) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet nodes by sex\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0() +\n  geom_node_point() +\n  facet_nodes(~race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1)\n# facet both nodes and edges\nggraph(ga, layout = \"linear\", circular = TRUE) +\n  geom_edge_link0(aes(color = type)) +\n  geom_node_point() +\n  facet_graph(type ~ race) +\n  th_foreground(border = TRUE) +\n  theme(aspect.ratio = 1, legend.position = \"right\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #10\n\n\n\nDoes splitting up the main graph into sub-networks give you more insight? Describe some of these.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#network-analysis-with-tidygraph",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#network-analysis-with-tidygraph",
    "title": "\n Networks",
    "section": "Network analysis with tidygraph",
    "text": "Network analysis with tidygraph\nThe data frame graph representation can be easily augmented with metrics or statistics computed on the graph. Remember how we computed counts with the penguin dataset in Grammar of Graphics.\nBefore computing a metric on nodes or edges use the activate() function to activate either node or edge data frames. Use dplyr verbs (filter, arrange, mutate) to achieve your computation in the proper way.\nNetwork Centrality: Go-To and Go-Through People!\nCentrality is a an “ill-defined” metric of node and edge importance in a network. It is therefore calculated in many ways. Type ?centrality in your Console.\n\n\n\n\n\nStandards\n\nLet’s add a few columns to the nodes and edges based on network centrality measures:\n\nga %&gt;%\n  activate(nodes) %&gt;%\n  # Node with  the most connections?\n  mutate(degree = centrality_degree(mode = c(\"in\"))) %&gt;%\n  filter(degree &gt; 0) %&gt;%\n  activate(edges) %&gt;%\n  # \"Busiest\" edge?\n  mutate(betweenness = centrality_edge_betweenness())\n\n# A tbl_graph: 54 nodes and 57 edges\n#\n# An undirected simple graph with 4 components\n#\n# Edge Data: 57 × 5 (active)\n    from    to weight type         betweenness\n   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;\n 1     5    47      2 friends             20.3\n 2    21    47      4 benefits            44.7\n 3     5    46      1 friends             39  \n 4     5    41      1 friends             66.3\n 5    18    41      6 friends             39  \n 6    21    41     12 benefits            91.5\n 7    37    41      5 professional       164. \n 8    31    41      2 professional        98.8\n 9    20    31      3 professional        47.2\n10    17    31      4 friends            102. \n# ℹ 47 more rows\n#\n# Node Data: 54 × 8\n  name               sex   race  birthyear position  season sign   degree\n  &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 Addison Montgomery F     White      1967 Attending      1 Libra       3\n2 Adele Webber       F     Black      1949 Non-Staff      2 Leo         1\n3 Teddy Altman       F     White      1969 Attending      6 Pisces      4\n# ℹ 51 more rows\n\n\nPackages tidygraph and ggraph can be pipe-lined to perform analysis and visualization tasks in one go.\n\n##\nset_graph_style(family = \"Roboto Condensed\", size = 16)\n##\nggraph(ga, layout = \"nicely\") +\n  geom_edge_link0(aes(width = centrality_edge_betweenness())) +\n\n  geom_node_point(aes(\n    colour = centrality_degree(),\n    size = centrality_degree()\n  )) +\n\n  geom_node_text(aes(label = name), repel = TRUE, size = 2.5) +\n\n  scale_size(name = \"Degree\", range = c(2, 5)) +\n\n  scale_color_fermenter(\n    name = \"Degree\", # USE SAME NAME to Merge legends!!\n    palette = \"Set1\",\n    aesthetics = c(\"colour\", \"fill\"),\n    guide = guide_legend(reverse = FALSE)\n  ) +\n\n  scale_edge_width(name = \"Betweenness\", range = c(0.25, 1)) +\n  labs(\n    title = \"Grey's Anatomy\",\n    subtitle = \"Nodes Scaled by Degree, Edge-width scaled by Betweenness\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #11\n\n\n\nHow do the Centrality Measures show up in the graph? Would you “agree” with the way we have done it? Try to modify the aesthetics by copy-pasting this chunk below and see how you can make an alternative representation.\n\n\nAnalysis and Visualizing Network Communities\nWho is close to whom? Which are the groups you can see?\n\n##\nset_graph_style(family = \"Roboto Condensed\", size = 16)\n##\n# visualize communities of nodes\nga %&gt;%\n  activate(nodes) %&gt;%\n  mutate(community = as.factor(group_louvain())) %&gt;%\n  ggraph(layout = \"graphopt\") +\n  geom_edge_link0() +\n  geom_node_point(aes(fill = community), size = 3, shape = 21) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"Grey's Anatomy\", subtitle = \"Nodes Coloured by Community Detection Algorithm (Louvain)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestions and Inferences #12\n\n\n\nIs the Community depiction clear? How would you do it, with which aesthetic? Copy Paste this chunk below and try.\n\n\nInteractive Graphs with visNetwork\n\nExploring the VisNetwork package. Make graphs wiggle and shake using tidy commands! The package implements interactivity using the physical metaphor of weights and springs we discussed earlier.\nThe visNetwork() function uses a nodes list and edges list to create an interactive graph. The nodes list must include an “id” column, and the edge list must have “from” and “to” columns. The function also plots the labels for the nodes, using the names of the cities from the “label” column in the node list.\nlibrary(visNetwork)\n\n# Prepare the data for plotting by visNetwork\ngrey_nodes\ngrey_edges\n# Relabel greys anatomy nodes and edges for VisNetwork\ngrey_nodes_vis &lt;- grey_nodes %&gt;%\n  rowid_to_column(var = \"id\") %&gt;%\n  rename(\"label\" = name) %&gt;%\n  mutate(sex = case_when(\n    sex == \"F\" ~ \"Female\",\n    sex == \"M\" ~ \"Male\"\n  )) %&gt;%\n  replace_na(., list(sex = \"Transgender?\")) %&gt;%\n  rename(\"group\" = sex)\ngrey_nodes_vis\ngrey_edges_vis &lt;- grey_edges %&gt;%\n  select(from, to) %&gt;%\n  left_join(., grey_nodes_vis,\n    by = c(\"from\" = \"label\")\n  ) %&gt;%\n  left_join(., grey_nodes_vis,\n    by = c(\"to\" = \"label\")\n  ) %&gt;%\n  select(\"from\" = id.x, \"to\" = id.y)\ngrey_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nUsing fontawesome icons\n\ngrey_nodes_vis %&gt;%\n  visNetwork(nodes = ., edges = grey_edges_vis) %&gt;%\n  visNodes(font = list(size = 40)) %&gt;%\n  # Colour and icons for each of the gender-groups\n  visGroups(\n    groupname = \"Female\", shape = \"icon\",\n    icon = list(code = \"f182\", size = 75, color = \"tomato\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  visGroups(\n    groupname = \"Male\", shape = \"icon\",\n    icon = list(code = \"f183\", size = 75, color = \"slateblue\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  visGroups(\n    groupname = \"Transgender?\", shape = \"icon\",\n    icon = list(code = \"f22c\", size = 75, color = \"fuchsia\"),\n    shadow = list(enabled = TRUE)\n  ) %&gt;%\n  # visLegend() %&gt;%\n  # Add the fontawesome icons!!\n  addFontAwesome(version = \"4.7.0\") %&gt;%\n  # Add Interaction Controls\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nThere is another family of icons available in visNetwork, called ionicons. Let’s see how they look:\n\ngrey_nodes_vis %&gt;%\n  visNetwork(nodes = ., edges = grey_edges_vis, ) %&gt;%\n  visLayout(randomSeed = 12345) %&gt;%\n  visNodes(font = list(size = 50)) %&gt;%\n  visEdges(color = \"green\") %&gt;%\n  visGroups(\n    groupname = \"Female\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f25d\",\n      color = \"fuchsia\",\n      size = 125\n    )\n  ) %&gt;%\n  visGroups(\n    groupname = \"Male\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f202\",\n      color = \"green\",\n      size = 125\n    )\n  ) %&gt;%\n  visGroups(\n    groupname = \"Transgender?\",\n    shape = \"icon\",\n    icon = list(\n      face = \"Ionicons\",\n      code = \"f233\",\n      color = \"dodgerblue\",\n      size = 125\n    )\n  ) %&gt;%\n  visLegend() %&gt;%\n  addIonicons() %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  )\n\n\n\n\n\nSome idea of interactivity and controls with visNetwork:\n Star Wars Nodes \n Star Wars Edges \n\n# let's look again at the data\nstarwars_nodes &lt;- read_csv(\"files/data/star-wars-network-nodes.csv\")\nstarwars_edges &lt;- read_csv(\"files/data/star-wars-network-edges.csv\")\n\n# We need to rename starwars nodes dataframe and edge dataframe columns for visNetwork\nstarwars_nodes_vis &lt;-\n  starwars_nodes %&gt;%\n  rename(\"label\" = name)\n\n# Convert from and to columns to **node ids**\nstarwars_edges_vis &lt;-\n  starwars_edges %&gt;%\n  # Matching Source &lt;- Source Node id (\"id.x\")\n  left_join(., starwars_nodes_vis, by = c(\"source\" = \"label\")) %&gt;%\n  # Matching Target &lt;- Target Node id (\"id.y\")\n  left_join(., starwars_nodes_vis, by = c(\"target\" = \"label\")) %&gt;%\n  # Select \"id.x\" and \"id.y\" ONLY\n  # Rename them as \"from\" and \"to\"\n  # keep \"weight\" column for aesthetics of edges\n  select(\"from\" = id.x, \"to\" = id.y, \"value\" = weight)\n\n# Check everything once\nstarwars_nodes_vis\nstarwars_edges_vis\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nOk, let’s make things move and shake!!\n\nvisNetwork(\n  nodes = starwars_nodes_vis,\n  edges = starwars_edges_vis\n) %&gt;%\n  visNodes(font = list(size = 30)) %&gt;%\n  visEdges(color = \"red\")\n\n\n\n\n\n\nvisNetwork(\n  nodes = starwars_nodes_vis,\n  edges = starwars_edges_vis\n) %&gt;%\n  visNodes(\n    font = list(size = 30), shape = \"icon\",\n    icon = list(code = \"f1e3\", size = 75)\n  ) %&gt;%\n  visEdges(color = list(color = \"red\", hover = \"green\", highlight = \"black\")) %&gt;%\n  visInteraction(\n    navigationButtons = TRUE,\n    hover = TRUE,\n    selectConnectedEdges = TRUE,\n    hoverConnectedEdges = TRUE,\n    zoomView = TRUE\n  ) %&gt;%\n  addFontAwesome(version = \"4.7.0\")",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#your-turn",
    "title": "\n Networks",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nNoteAirline Data:\n\n\n\n Airlines Nodes \n Airlines Edges \nStart with this bit of code in your second chunk, after set up\n\n```{r}\n#| label: start up code for Airlines\n#| eval: false ## remove this!!\nairline_nodes &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-NODES.csv\") %&gt;%\n  mutate(Id = Id + 1)\n\nairline_edges &lt;-\n  read_csv(\"./mydatafolder/AIRLINES-EDGES.csv\") %&gt;%\n  mutate(Source = Source + 1, Target = Target + 1)\n```\n\n\n\n\n\n\n\n\n\nNoteThe Famous Zachary Karate Club dataset\n\n\n\n\nStart with pulling this data into your Quarto:\n\n\n```{r}\n#| eval: false ## remove this!\ndata(\"karate\", package = \"igraphdata\")\nkarate\n```\n\n\nTry ?karate in the console\n\nNote that this is not a set of nodes, nor edges, but already a graph-object!\n\nSo no need to create a graph object using tbl_graph.\n\nYou will need to just go ahead and plot using ggraph.\n\n\n\n\n\n\n\n\n\nNoteGame of Thrones:\n\n\n\n GoT Networks \n\nStart with pulling this data into your Rmarkdown:\n\n\n```{r}\n#| label: start-up code for GoT\n#| eval: false ## remove this!!\n\nGoT &lt;- read_rds(\"data/GoT.RDS\")\n```\n\n\nNote that this is a list of 7 graphs from Game of Thrones.\nSelect one using GoT[[index]] where index = 1…7 and then plot directly.\nTry to access the nodes and edges and modify them using any attribute data\n\n\n\n\n\n\n\n\n\nNoteOther Datasets\n\n\n\n\nChoose any other graph dataset from igraphdata\n\n(type ?igraphdata in console)\n\nAsk me for help if you need any\n\n\n\n\nMake-2: Literary Network with TV Show / Book / Story / Play\nYou need to create a Network Graph for your favourite Book, play, TV serial or Show. (E.g. Friends, BBT, or LB or HIMYM, B99, TGP, JTV…or Hamlet, Little Women , Pride and Prejudice, or LoTR)\n\nStep 1. Go to: Literary Networks for instructions.\n\nStep 2. Make your data using the instructions.\n\nIn the nodes excel, use id and names as your columns. Any other details in other columns to the right.\n\nIn your edges excel, use from and to as your first columns.\n\nEntries in these columns can be names or ids but be consistent and don’t mix.\n\n\n\nStep 3. Decide on 3 answers that you to seek and plan to make graphs for.\nStep 4. Create graph objects. Say 3 visualizations.\nStep 5. Write comments/answers in the code and narrative text. Add pictures from the web using Markdown syntax.\nStep 6. Write Reflection ( ok, a short one!) inside your Quarto document. Make sure it renders !!\nStep 7. Group Submission: Submit the render-able .qmd file AND the data. Quarto Markdown with joint authorship. Each person submits on their Assignments. All get the same grade on this one.\n\nAsk me for clarifications on what to do after you have read the Instructions in your group.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/100-Networks/index.html#references",
    "title": "\n Networks",
    "section": "\n References",
    "text": "References\n\n\nHadley Wickham, Danielle Navarro, and Thomas Lin Pedersen, ggplot2: Elegant Graphics for Data Analysis. https://ggplot2-book.org/networks\n\nOmar Lizardo and Isaac Jilbert, Social Networks: An Introduction. https://bookdown.org/omarlizardo/_main/\n\nMark Hoffman, Methods for Network Analysis. https://bookdown.org/markhoff/social_network_analysis/\n\n\nStatistical Analysis of Network Data with R, 2nd Edition.https://github.com/kolaczyk/sand\n\n\nThomas Lin Pedersen - 1 giraffe, 2 giraffe,GO!\n\nTyner, Sam, François Briatte, and Heike Hofmann. 2017. “Network Visualization with ggplot2.” The R Journal 9 (1): 27–59. https://journal.r-project.org/archive/2017/RJ-2017-023/index.html\n\nNetwork Datasets https://icon.colorado.edu/#!/networks\n\nYunran Chen, Introduction to Network Analysis Using R\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggraph\n2.2.1\nPedersen (2024a)\n\n\nggtext\n0.1.2\nWilke and Wiernik (2022)\n\n\ngraphlayouts\n1.2.2\nDavid Schoch (2023)\n\n\nigraph\n2.1.4\n\nCsardi and Nepusz (2006); Csárdi et al. (2025)\n\n\n\nigraphdata\n1.0.1\nCsardi (2015)\n\n\nsand\n2.0.0\nKolaczyk and Csárdi (2020)\n\n\nshowtext\n0.9.7\nQiu and See file AUTHORS for details. (2024)\n\n\ntidygraph\n1.3.1\nPedersen (2024b)\n\n\nvisNetwork\n2.1.2\nAlmende B.V. and Contributors and Thieurmel (2022)\n\n\n\n\n\n\nAlmende B.V. and Contributors, and Benoit Thieurmel. 2022. visNetwork: Network Visualization Using “vis.js” Library. https://doi.org/10.32614/CRAN.package.visNetwork.\n\n\nCsardi, Gabor. 2015. igraphdata: A Collection of Network Data Sets for the “igraph” Package. https://doi.org/10.32614/CRAN.package.igraphdata.\n\n\nCsardi, Gabor, and Tamas Nepusz. 2006. “The Igraph Software Package for Complex Network Research.” InterJournal Complex Systems: 1695. https://igraph.org.\n\n\nCsárdi, Gábor, Tamás Nepusz, Vincent Traag, Szabolcs Horvát, Fabio Zanini, Daniel Noom, and Kirill Müller. 2025. igraph: Network Analysis and Visualization in r. https://doi.org/10.5281/zenodo.7682609.\n\n\nDavid Schoch. 2023. “graphlayouts: Layout Algorithms for Network Visualizations in r.” Journal of Open Source Software 8 (84): 5238. https://doi.org/10.21105/joss.05238.\n\n\nKolaczyk, Eric, and Gábor Csárdi. 2020. sand: Statistical Analysis of Network Data with r, 2nd Edition. https://doi.org/10.32614/CRAN.package.sand.\n\n\nPedersen, Thomas Lin. 2024a. ggraph: An Implementation of Grammar of Graphics for Graphs and Networks. https://doi.org/10.32614/CRAN.package.ggraph.\n\n\n———. 2024b. tidygraph: A Tidy API for Graph Manipulation. https://doi.org/10.32614/CRAN.package.tidygraph.\n\n\nQiu, Yixuan, and authors/contributors of the included software. See file AUTHORS for details. 2024. showtext: Using Fonts More Easily in r Graphs. https://doi.org/10.32614/CRAN.package.showtext.\n\n\nWilke, Claus O., and Brenton M. Wiernik. 2022. ggtext: Improved Text Rendering Support for “ggplot2”. https://doi.org/10.32614/CRAN.package.ggtext.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"bx:network-chart\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Networks"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html",
    "title": " Miscellaneous Graphing Tools, and References",
    "section": "",
    "text": "https://rawgraphs.io\n\n\n\nhttps://datawrapper.de\n\n\n\nhttps://hdlab.stanford.edu/palladio/\n\n\n\nhttps://infogram.com/\n\n\n\nhttps://www.visme.co/chart-maker/\n\n\n\nhttps://flourish.studio/ https://www.figma.com/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:miscellaneous\"></iconify-icon> Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#what-other-free-tools-are-there-on-the-web",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#what-other-free-tools-are-there-on-the-web",
    "title": " Miscellaneous Graphing Tools, and References",
    "section": "",
    "text": "https://rawgraphs.io\n\n\n\nhttps://datawrapper.de\n\n\n\nhttps://hdlab.stanford.edu/palladio/\n\n\n\nhttps://infogram.com/\n\n\n\nhttps://www.visme.co/chart-maker/\n\n\n\nhttps://flourish.studio/ https://www.figma.com/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:miscellaneous\"></iconify-icon> Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#references",
    "title": " Miscellaneous Graphing Tools, and References",
    "section": "References",
    "text": "References\n\nGetting started with Flourish & Figma to create beautiful custom charts, https://inside.mediahack.co.za/getting-started-with-flourish-figma-to-create-beautiful-custom-charts-34e4efb8fd3d\nFlowing Data Chart Types https://flowingdata.com/chart-types/\nGeeks for Geeks: Chart Types https://www.geeksforgeeks.org/r-charts-and-graphs/\nFinancial Times Visual Vocabulary (Interactive) https://ft-interactive.github.io/visual-vocabulary/\nFinancial Times Visual Vocabulary (PDF) https://github.com/Financial-Times/chart-doctor/blob/main/visual-vocabulary/FT4schools_RGS.pdf\nFinancial Times Data Journalism Visuals https://www.ft.com/visual-and-data-journalism\nSeverino Ribecca and John Schwabish , The Graphic Continuum https://www.severinoribecca.one/portfolio-item/the-graphic-continuum/\nWeb based tools for Dataviz https://policyviz.com/resources/data-viz-tools/\nNightingale Data Visualization Society Blog: How to visualize categorical data: https://nightingaledvs.com/endless-river-an-overview-of-dataviz-for-categorical-data/\nJohn Schwabish’s policyviz Data Viz catalogue: https://datastudio.google.com/s/quUUlgosF4U",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:miscellaneous\"></iconify-icon> Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#papers",
    "href": "content/courses/Analytics/Descriptive/Modules/180-RefMat/index.html#papers",
    "title": " Miscellaneous Graphing Tools, and References",
    "section": "Papers",
    "text": "Papers\n1.Christopher G. Healey Department of Computer Science, North Carolina State University. Perception in Visualization",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"eos-icons:miscellaneous\"></iconify-icon> Miscellaneous Graphing Tools, and References"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html",
    "title": "\n Summaries",
    "section": "",
    "text": "“The most certain sign of wisdom is cheerfulness.”\n— Michel de Montaigne, Writer and philosopher",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#using-web-r",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#using-web-r",
    "title": "\n Summaries",
    "section": "\n Using web-R",
    "text": "Using web-R\nThis tutorial uses web-r that allows you to run all code within your browser, on all devices. Most code chunks herein are formatted in a tabbed structure (like in an old-fashioned library) with duplicated code. The tabs in front have regular R code that will work when copy-pasted in your RStudio session. The tab “behind” has the web-R code that can work directly in your browser, and can be modified as well. The R code is also there to make sure you have original code to go back to, when you have made several modifications to the code on the web-r tabs and need to compare your code with the original!\nKeyboard Shortcuts\n\nRun selected code using either:\n\nmacOS: ⌘ + ↩︎/Return\n\nWindows/Linux: Ctrl + ↩︎/Enter\n\n\n\nRun the entire code by clicking the “Run code” button or pressing Shift+↩︎.\n\n\n\n“The best attitude to adopt is one of compassionate patience, which has to include a tolerance for failure.”\n— Gabor Maté",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#setting-up-r-packages",
    "title": "\n Summaries",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(skimr)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#how-do-we-grasp-data",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#how-do-we-grasp-data",
    "title": "\n Summaries",
    "section": "\n How do we Grasp Data?",
    "text": "How do we Grasp Data?\nWe spoke of Experiments and Data Gathering in the first module Nature of Data. This helped us to obtain data.\nAs we discussed in that same Module, for us to grasp the significance of the data, we need to describe it; the actual data is usually too vast for us to comprehend in its entirety. Anything more than a handful of observations in a dataset is enough for us to require other ways of grasping it.\nThe first thing we need to do, therefore, is to reduce it to a few salient numbers that allow us to summarize the data.\n\n\n\n\n\n\nImportantReduction is Addition\n\n\n\nSuch a reduction may seem paradoxical but is one of the important tenets of statistics: reduction, while taking away information, ends up adding to insight.\nSteven Stigler (2016) is the author of the book “The Seven Pillars of Statistical Wisdom”. One of the Big Ideas in Statistics from that book is: Aggregation\n\nThe first pillar I will call Aggregation, although it could just as well be given the nineteenth-century name, “The Combination of Observations,” or even reduced to the simplest example, taking a mean. Those simple names are misleading, in that I refer to an idea that is now old but was truly revolutionary in an earlier day—and it still is so today, whenever it reaches into a new area of application. How is it revolutionary? By stipulating that, given a number of observations, you can actually gain information by throwing information away! In taking a simple arithmetic mean, we discard the individuality of the measures, subsuming them to one summary.\n\n\n\nLet us get some inspiration from Brad Pitt, from the movie Moneyball, which is about applying Data Analytics to the game of baseball.\n\n And then, an example from a more sombre story:\n\n\n\n\n\nUS Population: Reading and Numeracy Levels\n\nYear\nBelow Level #1\nLevel #1\nLevel #2\nLevel #3\nLevels #4 and #5\n\n\n\nNumber in millions (2012/2014)\n8.35\n26.49\n65.10\n71.41\n26.57\n\n\nNumber in millions (2017)\n7.59\n29.23\n66.07\n68.81\n26.75\n\n\n\n\nNote: \n\n\n\n\n\n\n\n\n SOURCE: U.S. Department of Education, National Center for Education Statistics, Program for the International Assessment of Adult Competencies (PIAAC), U.S. PIAAC 2017, U.S. PIAAC 2012/2014.\n\n\n\n\n\n\n\n\n\n\n\nTable 1: US Population: Reading and Numeracy Levels\n\n\n\nThis ghastly-looking Table 1 examines U.S. adults with low English literacy and numeracy skills—or low-skilled adults—at two points in the 2010s, in the years 2012/20141 and 2017, using data from the Program for the International Assessment of Adult Competencies (PIAAC). As can be seen the summary table is quite surprising in absolute terms, for a developed country like the US, and the numbers have increased from 2012/2014 to 2017!\nSo why do we need to summarise data? Summarization is an act of throwing away data to make more sense, as stated by (Stigler 2016) and also in the movie by Brad Pitt aka Billy Beane. To summarize is to understand. Add to that the fact that our Working Memories can hold maybe 7 items, so it means information retention too.\nAnd if we don’t summarise? Jorge Luis Borges, in a fantasy short story published in 1942, titled “Funes the Memorious,” he described a man, Ireneo Funes, who found after an accident that he could remember absolutely everything. He could reconstruct every day in the smallest detail, and he could even later reconstruct the reconstruction, but he was incapable of understanding. Borges wrote, “To think is to forget details, generalize, make abstractions. In the teeming world of Funes, there were only details.” (emphasis mine)\nAggregation can yield great gains above the individual components in data. Funes was big data without Statistics.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#what-graphs-numbers-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#what-graphs-numbers-will-we-see-today",
    "title": "\n Summaries",
    "section": "\n What graphs / numbers will we see today?",
    "text": "What graphs / numbers will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\n“Chart Shape”\n\n\nAll\nAll\nTables and Stat Measures\n\n\n\n\n\nBefore we plot a single chart, it is wise to take a look at several numbers that summarize the dataset under consideration. What might these be? Some obviously useful numbers are:\n\nDataset length: How many rows/observations?\nDataset breadth: How many columns/variables?\nHow many Quant variables?\nHow many Qual variables?\nQuant variables: min, max, mean, median, sd\nQual variables: levels, counts per level\nBoth: means, medians for each level of a Qual variable…",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Summaries",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\nNo\nPronoun\nAnswer\nVariable/Scale\nExample\nWhat Operations?\n\n\n\n1\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\nQuantitative/Ratio\nLength,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\nCorrelation\n\n\n2\nHow Many / Much / Heavy? Few? Seldom? Often? When?\nQuantities with Scale. Differences are meaningful, but not products or ratios\nQuantitative/Interval\npH,SAT score(200-800),Credit score(300-850),SAT score(200-800),Year of Starting College\nMean,Standard Deviation\n\n\n3\nHow, What Kind, What Sort\nA Manner / Method, Type or Attribute from a list, with list items in some \" order\" ( e.g. good, better, improved, best..)\nQualitative/Ordinal\nSocioeconomic status (Low income, Middle income, High income),Education level (HighSchool, BS, MS, PhD),Satisfaction rating(Very much Dislike, Dislike, Neutral, Like, Very Much Like)\nMedian,Percentile\n\n\n4\nWhat, Who, Where, Whom, Which\nName, Place, Animal, Thing\nQualitative/Nominal\nName\nCount no. of cases,Mode\n\n\n\n\n\n\nWe will obviously choose all variables in the dataset, unless they are unrelated ones such as row number or ID which (we think) may not contribute any information and we can disregard.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#how-do-these-summaries-work",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#how-do-these-summaries-work",
    "title": "\n Summaries",
    "section": "\n How do these Summaries Work?",
    "text": "How do these Summaries Work?\nQuant variables: Inspecting the min, max, mean, median, variance and sd of each of the Quant variables tells us straightaway what the ranges of the variables are, and if there are some outliers, which could be normal, or maybe due to data entry error! Comparing two Quant variables for their ranges also tells us that we may have to \\(scale/normalize\\) them for computational ease, if one variable has large numbers and the other has very small ones.\nQual variables: With Qual variables, we understand the levels within each, and understand the total number of combinations of the levels across these. Counts across levels, and across combinations of levels tells us whether the data has sufficient readings for graphing, inference, and decision-making, of if certain levels/classes of data are under or over represented.\nTogether?: We can use Quant and Qual together, to develop the above summaries (min, max,mean, median and sd) for Quant variables, again across levels, and across combinations of levels of single or multiple Quals, along with counts if we are interested in that.\nFor both types of variables, we need to keep an eye open for data entries that are missing! This may point to data gathering errors, which may be fixable. Or we will have to take a decision to let go of that entire observation (i.e. a row). Or even do what is called imputation to fill in values that are based on the other values in the same column, which sounds like we are making up data, but isn’t so really.\nAnd this may also tell us if we are witnessing a Simpson’s Paradox situation. You may have to decide on what to do with this data sparseness, or just check your biases!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#some-quick-definitions",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#some-quick-definitions",
    "title": "\n Summaries",
    "section": "\n Some Quick Definitions",
    "text": "Some Quick Definitions\n\n\n\n\n\n\nImportantMean\n\n\n\nThe sample mean, or average, of a Quantitative data variable can be calculated as the sum of the observed values divided by the number of observations:\n\\[\nmean = \\bar{x} = \\frac{x_1 + x_2+ x_3....+x_n}{n}\n\\]\n\n\n\n\n\n\n\n\nImportantVariance and Standard Deviation\n\n\n\nObservations can be on either side of the mean, naturally. To measure the extent of these differences, we square and sum the differences between individual values and their mean, and take their average to obtain the (sample) variance:\n\\[\nvariance = s^2 = \\frac{(x_1 - \\bar{x})^2 + (x_2 - \\bar{x})^2 + (x_2 - \\bar{x})^2 +...(x_n - \\bar{x})^2 +}{n-1}\n\\] The standard deviation \\(s\\) is just the square root of the variance.\n\n\n(The \\(n-1\\) is a mathematical nuance to allow for the fact that we have used the data to calculate the mean before we get to \\(s^2\\), and hence have “used up” one degree of randomness in the data. It gets us more robust results.)\n\n\n\n\n\n\nImportantMedian\n\n\n\nWhen the observations in a Quant variable are placed in order of their maginitude, the observation in tke middle is the median. Half the observations are below, and half are above, the median",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#case-study-1",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#case-study-1",
    "title": "\n Summaries",
    "section": "\n Case Study-1",
    "text": "Case Study-1\nWe will first use a dataset mpg that is available in R as part of one of the R packages that we have loaded with the library() command.\n\n Examine the Data\nIt is usually a good idea to make crisp business-like tables, for the data itself, and the schema as revealed by one of the outputs of the three methods to be presented below. There are many methods to do this; one of the simplest and effective ones is to use the kable set of commands from the knitr and kableExtra packagepackage:\n\nmpg %&gt;%\n  head(10) %&gt;%\n  kbl(\n    # add Human Readable column names\n    col.names = c(\n      \"Manufacturer\", \"Model\", \"Engine\\nDisplacement\",\n      \"Model\\n Year\", \"Cylinders\", \"Transmission\",\n      \"Drivetrain\", \"City\\n Mileage\", \"Highway\\n Mileage\",\n      \"Fuel\", \"Class\\nOf\\nVehicle\"\n    ),\n    caption = \"MPG Dataset\"\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\n      \"striped\", \"hover\",\n      \"condensed\", \"responsive\"\n    ),\n    full_width = F, position = \"center\"\n  )\n\n\n\nMPG Dataset\n\nManufacturer\nModel\nEngine Displacement\nModel Year\nCylinders\nTransmission\nDrivetrain\nCity Mileage\nHighway Mileage\nFuel\nClass Of Vehicle\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\naudi\na4\n3.1\n2008\n6\nauto(av)\nf\n18\n27\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nauto(l5)\n4\n16\n25\np\ncompact\n\n\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact\n\n\n\n\n\n\nNext we will look at a few favourite statistics or “favstats” that we can derive from data. R is full of packages that can provide very evocative and effective summaries of data. We will first start with the dplyr package from the tidyverse, the skimr package, then the mosaic package. We will look at the summary outputs from these and learn how to interpret them.\n\n\nUsing dplyr::glimpse()\nUsing skimr::skim()\nUsing mosaic::inspect()\n web-r\n\n\n\nThe dplyr package offers a convenient command called glimpse:\n\nglimpse(mpg)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;chr&gt; \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(av)\", \"auto…\n$ drv          &lt;chr&gt; \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4\", \"4\", \"4…\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;chr&gt; \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        &lt;chr&gt; \"compact\", \"compact\", \"compact\", \"compact\", \"compact\", \"c…\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nVery crisp output, giving us the size of the dataset (234 X 11) and the nature of the variable columns, along with their first few entries.\nThe chr variables are usually Categorical/Qualitative.\nThe int or dbl (double precision) are usually Numerical/Quantitative.\nBut be careful! Verify that this is as per your intent, interpret the variables and modify their encoding as needed.\n\n\n\n\n\n\nLet us look at mpgusing skimr::skim().\nFrom the output of ?skimr:\n\nThe format of the results are a single wide data frame combining the results, with some additional attributes and two metadata columns:\n\n\n\nskim_variable: name of the original variable\n\nskim_type: class of the variable\n\nWe can use skim(dataset) directly as shown below:\n\nskimr::skim(mpg) # explicitly stating package name\n\n\nData summary\n\n\nName\nmpg\n\n\nNumber of rows\n234\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nmanufacturer\n0\n1\n4\n10\n0\n15\n0\n\n\nmodel\n0\n1\n2\n22\n0\n38\n0\n\n\ntrans\n0\n1\n8\n10\n0\n10\n0\n\n\ndrv\n0\n1\n1\n1\n0\n3\n0\n\n\nfl\n0\n1\n1\n1\n0\n5\n0\n\n\nclass\n0\n1\n3\n10\n0\n7\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\ndispl\n0\n1\n3.47\n1.29\n1.6\n2.4\n3.3\n4.6\n7\n▇▆▆▃▁\n\n\nyear\n0\n1\n2003.50\n4.51\n1999.0\n1999.0\n2003.5\n2008.0\n2008\n▇▁▁▁▇\n\n\ncyl\n0\n1\n5.89\n1.61\n4.0\n4.0\n6.0\n8.0\n8\n▇▁▇▁▇\n\n\ncty\n0\n1\n16.86\n4.26\n9.0\n14.0\n17.0\n19.0\n35\n▆▇▃▁▁\n\n\nhwy\n0\n1\n23.44\n5.95\n12.0\n18.0\n24.0\n27.0\n44\n▅▅▇▁▁\n\n\n\n\n\nTaken together, we have the following:\n\n\n\n\n\n\nNote\n\n\n\n\nA Data Summary: it lists the dimensions of the mpg dataset: 234 rows and 11 columns. 6 columns are character formatted, the remaining 5 are numeric. The dataset is not “grouped” (more on this later).\nThe second part of the output shows a table with the character variables which are therefore factor variables with levels.\nThe third part shows a table listing the names and summary stats for the numerical variables. We have mean, sd, all the quantiles (p0, p25, p50(median), p75 and p100 percentiles) and a neat little histogram for each. From the histogram we can see that year is two-valued, cyl is three-valued, and cty and hwy are continuous… Again check that this is as you intend them to be. We may need to modify the encoding if needed.\n\n\n\n\n\nWe get very similar output from mosaic::inspect():\n\ninspect(mpg)\n\n\ncategorical variables:  \n          name     class levels   n missing\n1 manufacturer character     15 234       0\n2        model character     38 234       0\n3        trans character     10 234       0\n4          drv character      3 234       0\n5           fl character      5 234       0\n6        class character      7 234       0\n                                   distribution\n1 dodge (15.8%), toyota (14.5%) ...            \n2 caravan 2wd (4.7%) ...                       \n3 auto(l4) (35.5%), manual(m5) (24.8%) ...     \n4 f (45.3%), 4 (44%), r (10.7%)                \n5 r (71.8%), p (22.2%), e (3.4%) ...           \n6 suv (26.5%), compact (20.1%) ...             \n\nquantitative variables:  \n   name   class    min     Q1 median     Q3  max        mean       sd   n\n1 displ numeric    1.6    2.4    3.3    4.6    7    3.471795 1.291959 234\n2  year integer 1999.0 1999.0 2003.5 2008.0 2008 2003.500000 4.509646 234\n3   cyl integer    4.0    4.0    6.0    8.0    8    5.888889 1.611534 234\n4   cty integer    9.0   14.0   17.0   19.0   35   16.858974 4.255946 234\n5   hwy integer   12.0   18.0   24.0   27.0   44   23.440171 5.954643 234\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe see that the output of mosaic::inspect() is organized as follows:\n\nThere are two dataframes/tables in the output, one describing the Qualitative Variables and the other describing the Quantitative Variables.\nIn the table describing the Qual variables, we have:\n\n\nname: Name of the variable in the (parent) dataset. i.e Column Names\n\nclass: format of that column\n\nlevels: All these variables are factors, with levels shown here. Some for example, manufacturer has 15 levels, and there are 234 rows\n\n\n\ninspect also conveniently shows how much data is missing and in which variables. This is a very important consideration in the use of the data for analytics purposes.\n\n\nWe can save and see the outputs separately:\n\nmpg_describe &lt;- inspect(mpg)\nmpg_describe$categorical\nmpg_describe$quantitative\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary and Munging\nUsing skim/inspect/glimpse, we can put together a (brief) data dictionary as follows:\n\n\n\n\n\n\nNoteQualitative Data\n\n\n\n\n\nmodel(chr): Car model name\n\nmanufacturer(chr): Car maker name\n\nfl(chr): fuel type\n\ndrv(chr): type of drive(front, rear, 4W)\n\nclass(chr): type of vehicle ( sedan, pickup…)\n\ntrans(chr): type of transmission ( auto, manual..)\n\n\n\n\n\n\n\n\n\nNoteQuantitative Data\n\n\n\n\n\nhwy(int): Highway Mileage\n\ncty(int): City Mileage\n\ncyl(int): Number of Cylinders. How do we understand this variable? Should this be Qual?\n\ndispl(dbl): Engine piston displacement\n\nyear(int): Year of model\n\n\n\nWe see that there are certain variables that must be converted to factors for analytics purposes, since they are unmistakably Qualitative in nature. Let us do that now, for use later:\n\nmpg_modified &lt;- mpg %&gt;%\n  dplyr::mutate(\n    cyl = as_factor(cyl),\n    fl = as_factor(fl),\n    drv = as_factor(drv),\n    class = as_factor(class),\n    trans = as_factor(trans)\n  )\nglimpse(mpg_modified)\n\nRows: 234\nColumns: 11\n$ manufacturer &lt;chr&gt; \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"…\n$ model        &lt;chr&gt; \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 quattro\", \"…\n$ displ        &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.…\n$ year         &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200…\n$ cyl          &lt;fct&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, …\n$ trans        &lt;fct&gt; auto(l5), manual(m5), manual(m6), auto(av), auto(l5), man…\n$ drv          &lt;fct&gt; f, f, f, f, f, f, f, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, r, …\n$ cty          &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1…\n$ hwy          &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2…\n$ fl           &lt;fct&gt; p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, p, r, …\n$ class        &lt;fct&gt; compact, compact, compact, compact, compact, compact, com…\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#case-study-2",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#case-study-2",
    "title": "\n Summaries",
    "section": "\n Case Study-2",
    "text": "Case Study-2\nInstead of taking a “built-in” dataset , i.e. one that is part of an R package that we can load with library(), let us try the above process with a data set that we obtain from the internet. We will use this superb repository of datasets created by Vincent Arel-Bundock: https://vincentarelbundock.github.io/Rdatasets/articles/data.html\nLet us choose a modest-sized dataset, say this dataset on Doctor Visits, which is available online https://vincentarelbundock.github.io/Rdatasets/csv/AER/DoctorVisits.csv and read it into R.\n\n\n\n\n\n\nImportantReading external data into R\n\n\n\nThe read_csv() command from R package readr allows us to read both locally saved data on our hard disk, or data available in a shared folder online. Avoid using the read.csv() from base R , though it will show up in your code auto-complete set of options!\n\n\n\n# From Vincent Arel-Bundock's dataset website\n# https://vincentarelbundock.github.io/Rdatasets\n#\n# read_csv can read data directly from the net\n# Don't use read.csv()\ndocVisits &lt;- read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/AER/DoctorVisits.csv\")\n\nRows: 5190 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): gender, private, freepoor, freerepat, nchronic, lchronic\ndbl (7): rownames, visits, age, income, illness, reduced, health\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSo, a data frame containing 5,190 observations on 12 variables.\n\n\n\n\n\n\nNoteHow about a locally stored CSV file?\n\n\n\nWe can also use a locally downloaded and stored CSV file. Assuming the file is stored in a subfolder called data inside your R project folder, we can proceed as follows:\n\n```{r}\n#| eval: false\ndocVisits &lt;- read_csv(\"data/DoctorVisits.csv\")\n```\n\n\n\nLet us quickly report the data itself, as in a real report. Note that we can use the features of the kableExtra package to dress up this table too!!\n\ndocVisits %&gt;%\n  head(10) %&gt;%\n  kbl(\n    caption = \"Doctor Visits Dataset\",\n    # Add Human Readable Names if desired\n    # col.names(..names that you may want..)\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\n      \"striped\", \"hover\",\n      \"condensed\", \"responsive\"\n    ),\n    full_width = F, position = \"center\"\n  )\n\n\n\nDoctor Visits Dataset\n\nrownames\nvisits\ngender\nage\nincome\nillness\nreduced\nhealth\nprivate\nfreepoor\nfreerepat\nnchronic\nlchronic\n\n\n\n1\n1\nfemale\n0.19\n0.55\n1\n4\n1\nyes\nno\nno\nno\nno\n\n\n2\n1\nfemale\n0.19\n0.45\n1\n2\n1\nyes\nno\nno\nno\nno\n\n\n3\n1\nmale\n0.19\n0.90\n3\n0\n0\nno\nno\nno\nno\nno\n\n\n4\n1\nmale\n0.19\n0.15\n1\n0\n0\nno\nno\nno\nno\nno\n\n\n5\n1\nmale\n0.19\n0.45\n2\n5\n1\nno\nno\nno\nyes\nno\n\n\n6\n1\nfemale\n0.19\n0.35\n5\n1\n9\nno\nno\nno\nyes\nno\n\n\n7\n1\nfemale\n0.19\n0.55\n4\n0\n2\nno\nno\nno\nno\nno\n\n\n8\n1\nfemale\n0.19\n0.15\n3\n0\n6\nno\nno\nno\nno\nno\n\n\n9\n1\nfemale\n0.19\n0.65\n2\n0\n5\nyes\nno\nno\nno\nno\n\n\n10\n1\nmale\n0.19\n0.15\n1\n0\n0\nyes\nno\nno\nno\nno\n\n\n\n\n\n\n\n Examine the Data\n\n\nUsing dplyr::glimpse()\nUsing skimr::skim()\nUsing mosaic::inspect()\n web-r\n\n\n\n\nglimpse(docVisits)\n\nRows: 5,190\nColumns: 13\n$ rownames  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 1…\n$ visits    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, …\n$ gender    &lt;chr&gt; \"female\", \"female\", \"male\", \"male\", \"male\", \"female\", \"femal…\n$ age       &lt;dbl&gt; 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, …\n$ income    &lt;dbl&gt; 0.55, 0.45, 0.90, 0.15, 0.45, 0.35, 0.55, 0.15, 0.65, 0.15, …\n$ illness   &lt;dbl&gt; 1, 1, 3, 1, 2, 5, 4, 3, 2, 1, 1, 2, 3, 4, 3, 2, 1, 1, 1, 1, …\n$ reduced   &lt;dbl&gt; 4, 2, 0, 0, 5, 1, 0, 0, 0, 0, 0, 0, 13, 7, 1, 0, 0, 1, 0, 0,…\n$ health    &lt;dbl&gt; 1, 1, 0, 0, 1, 9, 2, 6, 5, 0, 0, 2, 1, 6, 0, 7, 5, 0, 0, 0, …\n$ private   &lt;chr&gt; \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"yes\", \"ye…\n$ freepoor  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", …\n$ freerepat &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", …\n$ nchronic  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no\"…\n$ lchronic  &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", …\n\n\n\n\n\n\n\n\nNoteDescriptive Stat Summary from dplyr::glimpse()\n\n\n\nVery crisp output, giving us the size of the dataset (5190 X 13) and the nature of the variable columns, along with their first few entries. There are several Quantitative variables: visits, age, income, illness, reduced and health; the rest seem to be Qualitative variables.\nAlways document your data with variable descriptions when you share it, a data dictionary!\n\n\n\n\n\nskim(docVisits) %&gt;% kbl()\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\ncharacter\ngender\n0\n1\n4\n6\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nprivate\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nfreepoor\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nfreerepat\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nnchronic\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nlchronic\n0\n1\n2\n3\n0\n2\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nrownames\n0\n1\nNA\nNA\nNA\nNA\nNA\n2595.5000000\n1498.3682792\n1.00\n1298.25\n2595.50\n3892.75\n5190.00\n▇▇▇▇▇\n\n\nnumeric\nvisits\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.3017341\n0.7981338\n0.00\n0.00\n0.00\n0.00\n9.00\n▇▁▁▁▁\n\n\nnumeric\nage\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.4063854\n0.2047818\n0.19\n0.22\n0.32\n0.62\n0.72\n▇▂▁▂▅\n\n\nnumeric\nincome\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.5831599\n0.3689067\n0.00\n0.25\n0.55\n0.90\n1.50\n▇▆▅▅▂\n\n\nnumeric\nillness\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.4319846\n1.3841524\n0.00\n0.00\n1.00\n2.00\n5.00\n▇▂▂▁▁\n\n\nnumeric\nreduced\n0\n1\nNA\nNA\nNA\nNA\nNA\n0.8618497\n2.8876284\n0.00\n0.00\n0.00\n0.00\n14.00\n▇▁▁▁▁\n\n\nnumeric\nhealth\n0\n1\nNA\nNA\nNA\nNA\nNA\n1.2175337\n2.1242665\n0.00\n0.00\n0.00\n2.00\n12.00\n▇▁▁▁▁\n\n\n\n\n\n\n\n\n\n\n\nNoteDescriptive Stat Summary from skimr::skim()\n\n\n\n\nA Data Summary: it lists the dimensions of the docVisits dataset: 5190 rows and 13 columns. 6 columns are character formatted, the remaining 7 are numeric. The dataset is not “grouped” (more on this later).\nThe second part of the output shows a table with the character variables which are therefore factor variables with levels.\nThe third part shows a table listing the names and summary stats for the numerical variables. We have mean, sd, all the quantiles (p0, p25, p50(median), p75 and p100 percentiles) and a neat little histogram for each.\nCan we consider the health Goldberg score a Qualitative variable, to be understood as “ranks” between a minimum and maximum? It is just possible…\n\n\n\n\n\n\ninspect(docVisits)\n\n\ncategorical variables:  \n       name     class levels    n missing\n1    gender character      2 5190       0\n2   private character      2 5190       0\n3  freepoor character      2 5190       0\n4 freerepat character      2 5190       0\n5  nchronic character      2 5190       0\n6  lchronic character      2 5190       0\n                                   distribution\n1 female (52.1%), male (47.9%)                 \n2 no (55.7%), yes (44.3%)                      \n3 no (95.7%), yes (4.3%)                       \n4 no (79%), yes (21%)                          \n5 no (59.7%), yes (40.3%)                      \n6 no (88.3%), yes (11.7%)                      \n\nquantitative variables:  \n      name   class  min      Q1  median      Q3     max         mean\n1 rownames numeric 1.00 1298.25 2595.50 3892.75 5190.00 2595.5000000\n2   visits numeric 0.00    0.00    0.00    0.00    9.00    0.3017341\n3      age numeric 0.19    0.22    0.32    0.62    0.72    0.4063854\n4   income numeric 0.00    0.25    0.55    0.90    1.50    0.5831599\n5  illness numeric 0.00    0.00    1.00    2.00    5.00    1.4319846\n6  reduced numeric 0.00    0.00    0.00    0.00   14.00    0.8618497\n7   health numeric 0.00    0.00    0.00    2.00   12.00    1.2175337\n            sd    n missing\n1 1498.3682792 5190       0\n2    0.7981338 5190       0\n3    0.2047818 5190       0\n4    0.3689067 5190       0\n5    1.3841524 5190       0\n6    2.8876284 5190       0\n7    2.1242665 5190       0\n\n\n\n\n\n\n\n\nNoteDescriptive Stat Summary from mosaic::inspect()\n\n\n\nWe see that the output of mosaic::inspect() is organized very similarly to the output from skim. Is there any missing data? Both skim and mosaic report on the data completion for each variable in the dataset.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n Data Dictionary\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nvisits\nNumber of doctor visits in past 2 weeks.\n\n\ngender\nFactor indicating gender.\n\n\nage\nAge in years divided by 100.\n\n\nincome\nAnnual income in tens of thousands of dollars.\n\n\nillness\nNumber of illnesses in past 2 weeks.\n\n\nreduced\nNumber of days of reduced activity in past 2 weeks due to illness or injury.\n\n\nhealth\nGeneral health questionnaire score using Goldberg’s method.\n\n\nprivate\nFactor. Does the individual have private health docVisits?\n\n\nfreepoor\nFactor. Does the individual have free government health docVisits due to low income?\n\n\nfreerepat\nFactor. Does the individual have free government health docVisits due to old age, disability or veteran status?\n\n\nnchronic\nFactor. Is there a chronic condition not limiting activity?\n\n\nlchronic\nFactor. Is there a chronic condition limiting activity?\n\n\n\nHere too, we should convert the variables that are obviously Qualitative into factors, ordered or otherwise:\n\ndocVisits_modified &lt;- docVisits %&gt;%\n  mutate(\n    gender = as_factor(gender),\n    private = as_factor(private),\n    freepoor = as_factor(freepoor),\n    freerepat = as_factor(freerepat),\n    nchronic = as_factor(nchronic),\n    lchronic = as_factor(lchronic)\n  )\ndocVisits_modified\n\n\n  \n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#groups-and-counts-of-qualitative-variables",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#groups-and-counts-of-qualitative-variables",
    "title": "\n Summaries",
    "section": "\n Groups and Counts of Qualitative Variables",
    "text": "Groups and Counts of Qualitative Variables\nWhat is the most important dialogue uttered in the movie “Sholay”?\nRecall our discussion in Types of Data Variables. We have looked at means, limits, and percentiles of Quantitative variables. Another good idea to examine datasets is to look at counts, proportions, and frequencies with respect to Qualitative variables.\nWe typically do this with the dplyr package from the tidyverse.\n\n\nmpg dataset\ndocVisits dataset\n web-r\n\n\n\n\nmpg_modified %&gt;% dplyr::count(cyl)\n\n\n  \n\n\nmpg_modified %&gt;% mosaic::count(drv) # does the same thing! Counts!\n\n\n  \n\n\nmpg_modified %&gt;% count(fl)\n\n\n  \n\n\n### All combinations of cut, color, clarity\n### Overwhelming??\nmpg_modified %&gt;%\n  count(across(where(is.factor)))\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Groups and Counts (mpg)\n\n\n\n\nWe see that the groups for each level of cyl, drv, and fl are not the same size: for instance the group with the “r”-level fl type is largest at 168 observations, and the “r”-level in drv has only 25 observations.\nGroup counts based on cyl are also not balanced.\nCounting all combinations of these three factors shows counts of 1 for several combination and does not lead to any decent amount of aggregation.\n\nThese aspects may need to be factored into downstream modelling or machine learning tasks. (Usually by stratification wrt levels of the Qualitative variables)\nThe levels are not too many, so tables work, and so would bar charts, which we will examine next. If there are too many levels in any factor, tables are a better option. Bar charts can still be plotted, but it may be preferable to lump smaller categories/levels together. (Using the forcats package)\n\n\n\n\n\n## Counting by the obvious factor variables\ndocVisits %&gt;% count(gender)\n\n\n  \n\n\ndocVisits %&gt;% count(private)\n\n\n  \n\n\ndocVisits %&gt;% count(freepoor)\n\n\n  \n\n\ndocVisits %&gt;% count(freerepat)\n\n\n  \n\n\ndocVisits %&gt;% count(lchronic)\n\n\n  \n\n\ndocVisits %&gt;% count(nchronic)\n\n\n  \n\n\n\n\n# Now for all Combinations...\n# Maybe too much to digest...\ndocVisits %&gt;% count(across(where(is.character)))\n\n\n  \n\n\n# Shall we try counting by some variables that might be factors?\n# Even if they are labeled as &lt;dbl&gt;?\n#\ndocVisits %&gt;% count(illness)\n\n\n  \n\n\ndocVisits %&gt;% count(health)\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Groups and Counts (docVisits)\n\n\n\n\nMost of the counts are roughly balanced across the levels of the factors; however, freepoor and lchronic show unbalanced counts…\nThe factors are too numerous for a combination count table to very useful..\nCounting by illness and health does show that these two columns have a limited set of integer entries across over 5000 rows!! So these can be thought of as factors if needed in the analysis. So not every integer variable is necessaily a number!!\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#groups-and-summaries-of-quantitative-variables",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#groups-and-summaries-of-quantitative-variables",
    "title": "\n Summaries",
    "section": "\n Groups and Summaries of Quantitative Variables",
    "text": "Groups and Summaries of Quantitative Variables\nWe saw that we could obtain numerical summary stats such as means, medians, quartiles, maximum/minimum of entire Quantitative variables, i.e the complete column. However, we often need identical numerical summary stats of parts of a Quantitative variable. Why?\nNote that we have Qualitative variables as well in a typical dataset. These Qual variables help us to group the entire dataset based on their combinations of levels. We can now think of summarizing Quant variables within each such group.\nLet us work through these ideas for both our familiar datasets.\n\n\nmpg dataset\ndocvisits dataset\n web-r\n\n\n\n\nmpg_modified %&gt;%\n  group_by(cyl) %&gt;%\n  summarize(average_hwy = mean(hwy), count = n())\n\n\n  \n\n\nmpg_modified %&gt;%\n  group_by(cyl, fl) %&gt;%\n  summarize(average_hwy = mean(hwy), count = n())\n\n\n  \n\n\n# Perhaps the best method for us!\nmpg_modified %&gt;%\n  mosaic::favstats(hwy ~ cyl, data = .) # Don't use fav_stats with formula!!!\n\n\n  \n\n\n# Be aware of the first column format here!\nmpg_modified %&gt;%\n  mosaic::favstats(hwy ~ cyl + fl, data = .) # Don't use fav_stats with formula!!!\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Grouped Quant Summaries (mpg)\n\n\n\n\nWe have quite some variation of mean_hwy mileage over cyl , though the groups/level are quite imbalanced. This is of course a small dataset.\nThe number of groups are large enough (&gt;&gt; 7!) to warrant a chart, which we will make in our next module on Distributions.\nMean price varies quite some based on cyland fl. Some groups are non-existent, hence we see “NA” and “NaN” in the output of mosaic::favstats.; and also on combinations of cyl:fl. This should point to the existence of some interaction effect when modelling for price.\n\n\n\n\n\n\ndocVisits_modified %&gt;%\n  group_by(gender) %&gt;%\n  summarize(average_visits = mean(visits), count = n())\n\n\n  \n\n\n##\ndocVisits_modified %&gt;%\n  group_by(gender) %&gt;%\n  summarize(average_visits = mean(visits), count = n())\n\n\n  \n\n\n##\ndocVisits_modified %&gt;%\n  group_by(freepoor, nchronic) %&gt;%\n  summarise(\n    mean_income = mean(income),\n    average_visits = mean(visits),\n    count = n()\n  )\n\n\n  \n\n\n##\ndocVisits_modified %&gt;%\n  mosaic::favstats(income ~ gender, data = .) # Don't use fav_stats with formula!!!\n\n\n  \n\n\n##\ndocVisits_modified %&gt;%\n  mosaic::favstats(income ~ freepoor + nchronic, data = .) # Don't use fav_stats with formula!!!\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Grouped Quant Summaries (docVisits)\n\n\n\nClearly the people who are freepoor ( On Govt Insurance) AND with a chronic condition are those who have lower average income and a higher average number of visits to the doctor…but there are relatively few of them (n = 55) in this dataset.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#more-on-dplyr",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#more-on-dplyr",
    "title": "\n Summaries",
    "section": "\n More on dplyr",
    "text": "More on dplyr\nThe dplyr package is capable of doing much more than just count, group_by and summarize. We will encounter this package many times more as we build our intuition about data visualization. A full tutorial on dplyr is linked to the icon below:\n\n\n\ndplyr Tutorial",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#reporting-tables-for-data-and-the-data-schema",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#reporting-tables-for-data-and-the-data-schema",
    "title": "\n Summaries",
    "section": "\n Reporting Tables for Data and the Data Schema",
    "text": "Reporting Tables for Data and the Data Schema\n\n\n\n\n\n\nImportantData and the Data Schema are Different!!\n\n\n\nNote that all the three methods (dplyr::glimpse(), skimr::skim(), and mosaic::inspect()) report the schema of the original dataframe. The schema are also formatted as data frames! However they do not “contain” the original data! Do not confuse between the data and it’s reported schema!\n\n\nAs stated earlier, it is usually a good idea to make crisp business-like tables, for the data itself, and of the schema as revealed by one of the outputs of the three methods (glimpse/skim/inspect) presented above. There are many table-making methods in R to do this; one of the simplest and effective ones is to use the kable set of commands from the knitr and kableExtra packages that we have installed already:\n\nmpg %&gt;%\n  head(10) %&gt;%\n  kbl(\n    col.names = c(\n      \"Manufacturer\", \"Model\", \"Engine\\nDisplacement\",\n      \"Model\\n Year\", \"Cylinders\", \"Transmission\",\n      \"Drivetrain\", \"City\\n Mileage\", \"Highway\\n Mileage\",\n      \"Fuel\", \"Class\\nOf\\nVehicle\"\n    ),\n    longtable = FALSE, centering = TRUE,\n    caption = \"MPG Dataset\"\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\n      \"striped\", \"hover\",\n      \"condensed\", \"responsive\"\n    ),\n    full_width = F, position = \"center\"\n  )\n\n\n\nMPG Dataset\n\nManufacturer\nModel\nEngine Displacement\nModel Year\nCylinders\nTransmission\nDrivetrain\nCity Mileage\nHighway Mileage\nFuel\nClass Of Vehicle\n\n\n\naudi\na4\n1.8\n1999\n4\nauto(l5)\nf\n18\n29\np\ncompact\n\n\naudi\na4\n1.8\n1999\n4\nmanual(m5)\nf\n21\n29\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nmanual(m6)\nf\n20\n31\np\ncompact\n\n\naudi\na4\n2.0\n2008\n4\nauto(av)\nf\n21\n30\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nauto(l5)\nf\n16\n26\np\ncompact\n\n\naudi\na4\n2.8\n1999\n6\nmanual(m5)\nf\n18\n26\np\ncompact\n\n\naudi\na4\n3.1\n2008\n6\nauto(av)\nf\n18\n27\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nmanual(m5)\n4\n18\n26\np\ncompact\n\n\naudi\na4 quattro\n1.8\n1999\n4\nauto(l5)\n4\n16\n25\np\ncompact\n\n\naudi\na4 quattro\n2.0\n2008\n4\nmanual(m6)\n4\n20\n28\np\ncompact\n\n\n\n\n\n\nAnd for the schema from skim(), with some extra bells and whistles on the table:\n\nskim(mpg) %&gt;%\n  kbl(align = \"c\", caption = \"Skim Output for mpg Dataset\") %&gt;%\n  kable_paper(full_width = F)\n\n\nSkim Output for mpg Dataset\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\ncharacter\nmanufacturer\n0\n1\n4\n10\n0\n15\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nmodel\n0\n1\n2\n22\n0\n38\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\ntrans\n0\n1\n8\n10\n0\n10\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\ndrv\n0\n1\n1\n1\n0\n3\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nfl\n0\n1\n1\n1\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\ncharacter\nclass\n0\n1\n3\n10\n0\n7\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\ndispl\n0\n1\nNA\nNA\nNA\nNA\nNA\n3.471795\n1.291959\n1.6\n2.4\n3.3\n4.6\n7\n▇▆▆▃▁\n\n\nnumeric\nyear\n0\n1\nNA\nNA\nNA\nNA\nNA\n2003.500000\n4.509646\n1999.0\n1999.0\n2003.5\n2008.0\n2008\n▇▁▁▁▇\n\n\nnumeric\ncyl\n0\n1\nNA\nNA\nNA\nNA\nNA\n5.888889\n1.611535\n4.0\n4.0\n6.0\n8.0\n8\n▇▁▇▁▇\n\n\nnumeric\ncty\n0\n1\nNA\nNA\nNA\nNA\nNA\n16.858974\n4.255946\n9.0\n14.0\n17.0\n19.0\n35\n▆▇▃▁▁\n\n\nnumeric\nhwy\n0\n1\nNA\nNA\nNA\nNA\nNA\n23.440171\n5.954643\n12.0\n18.0\n24.0\n27.0\n44\n▅▅▇▁▁\n\n\n\n\n\nSee https://haozhu233.github.io/kableExtra/ for more options on formatting the table with kableExtra.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#a-quick-quiz",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#a-quick-quiz",
    "title": "\n Summaries",
    "section": "\n A Quick Quiz",
    "text": "A Quick Quiz\n\n\n\n\n\n\nWarning\n\n\n\nIt is always a good idea to look for variables in data that may be incorrectly formatted. For instance, a variable marked as numerical may have the values 1-2-3-4 which represent options, sizes, or say months. in which case it would have to be interpreted as a factor.\n\n\nLet us take a small test with the mpg dataset:\n\nWhat is the number of qualitative/categorical variables in the mpg data?    \n\nHow many manufacturers are named in this dataset?    \n\nHow many levels does the variable drv have?    \n\nHow many quantitative/numerical variables shown in the mpg data?    \n\nBut the variable hwy\ncty\ncyl\ndispl   is actually a qualitative variable.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#your-turn",
    "title": "\n Summaries",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry your hand at these datasets. Look at the data, state the data dictionary, contemplate a few Research Questions and answer them with Summaries and Tables in Quarto!\nTry adding more summary functions to the summary table? Which might you choose? Why?\n\n\n\n\n\n\n\nNoteStar Trek Books\n\n\n\n\n\n Start Trek Book data\n\n\nWhich would be the Group By variables here? And what would you summarize? With which function?\n\n\n\n\n\n\n\n\nNoteMath Anxiety! Hah!\n\n\n\n\n\n Math Anxiety data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#wait-but-why",
    "title": "\n Summaries",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nData Summaries give you the essentials, without getting bogged down in the details(just yet).\n\nSummaries help you “live with your data”; this is an important step in understanding it, and deciding what to do with it.\n\nSummaries help evoke Questions and Hypotheses, which may lead to inquiries, analysis, and insights\n\n\nGrouped Summaries should tell you if:\n\ncounts of groups in your target audience are lopsided/imbalanced; Go and Get your data again.\n\nthere are visible differences in Quant data across groups, so your target audience could be nicely fractured;\netc.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#conclusion",
    "title": "\n Summaries",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nThe three methods (glimpse/skim/inspect) given here give us a very comprehensive look into the structure of the dataset.\nThe favstats method allows us to compute a whole lot of metrics for Quant variables for each level of one or more *Qual variables.\nUse the kable set of commands to make a smart-looking of the data and the outputs of any of the three methods.\n\nMake these part of your Workflow.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#ai-generated-summary-and-podcast",
    "title": "\n Summaries",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis is a tutorial on using the R programming language to perform descriptive statistical analysis on data sets. The tutorial focuses on summarizing data using various R packages like dplyr, skimr, and mosaic. It emphasizes the importance of understanding the data’s structure, identifying different types of variables (qualitative and quantitative), and calculating summary statistics such as means, medians, and frequencies. The tutorial provides examples using real datasets and highlights the significance of data summaries in gaining initial insights, formulating research questions, and identifying potential issues with the data.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/10-FavStats/index.html#references",
    "title": "\n Summaries",
    "section": "\n References",
    "text": "References\n\nLock, Lock, Lock, Lock, and Lock. (2021). Statistics: Unlocking the Power of Data, 3rd Edition). https://media.wiley.com/product_data/excerpt/69/11196821/1119682169-32.pdf\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\npalmerpenguins\n0.1.1\nHorst, Hill, and Gorman (2020)\n\n\nskimr\n2.1.5\nWaring et al. (2022)\n\n\n\n\n\n\nHorst, Allison Marie, Alison Presmanes Hill, and Kristen B Gorman. 2020. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://doi.org/10.5281/zenodo.3960218.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nStigler, Stephen M. 2016. “The Seven Pillars of Statistical Wisdom,” March. https://doi.org/10.4159/9780674970199.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, and Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://doi.org/10.32614/CRAN.package.skimr.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:summary-kpi\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Summaries"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html",
    "title": "Analysis of Time Series in R",
    "section": "",
    "text": "library(tidyverse) # For tidy data processing and plotting\nlibrary(lubridate) # Deal with dates\n\nlibrary(mosaic) # Out go to package for everything\n\nlibrary(fpp3) # Robert Hyndman's time series analysis package\nlibrary(timetk) # Convert data frames to time series-specific objects\nlibrary(forecast) # Make forecasts and decompose time series\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#setting-up-r-packages",
    "title": "Analysis of Time Series in R",
    "section": "",
    "text": "library(tidyverse) # For tidy data processing and plotting\nlibrary(lubridate) # Deal with dates\n\nlibrary(mosaic) # Out go to package for everything\n\nlibrary(fpp3) # Robert Hyndman's time series analysis package\nlibrary(timetk) # Convert data frames to time series-specific objects\nlibrary(forecast) # Make forecasts and decompose time series\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#introduction",
    "title": "Analysis of Time Series in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe have seen how to plot different formats of time series, and how to create summary plots using packages like tsibble and timetk.\nWe will now see how a time series can be broken down to its components so as to systematically understand and analyze it. Thereafter, we examine how to model the timeseries, and make forecasts, a task more like synthesis.\nWe have to begin by answering fundamental questions such as:\n\nWhat are the types of time series?\nHow does one process and analyze time series data?\nHow does one plot time series?\nHow to decompose it? How to extract a level, a trend, and seasonal components from a time series?\nWhat is auto correlation etc.\nWhat is a stationary time series?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study--1-walmart-sales-dataset-from-timetk",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study--1-walmart-sales-dataset-from-timetk",
    "title": "Analysis of Time Series in R",
    "section": "\n Case Study -1: Walmart Sales Dataset from timetk\n",
    "text": "Case Study -1: Walmart Sales Dataset from timetk\n\nLet us inspect what datasets are available in the package timetk. Type data(package = \"timetk\") in your Console to see what datasets are available.\nLet us choose the Walmart Sales dataset. See here for more details: Walmart Recruiting - Store Sales Forecasting |Kaggle\nwalmart_sales_weekly\nglimpse(walmart_sales_weekly)\ninspect(walmart_sales_weekly)\n# Try this in your Console\n# help(\"walmart_sales_weekly\")\n\n\n\n\n  \n\n\n\nRows: 1,001\nColumns: 17\n$ id           &lt;fct&gt; 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_1, 1_…\n$ Store        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Dept         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Date         &lt;date&gt; 2010-02-05, 2010-02-12, 2010-02-19, 2010-02-26, 2010-03-…\n$ Weekly_Sales &lt;dbl&gt; 24924.50, 46039.49, 41595.55, 19403.54, 21827.90, 21043.3…\n$ IsHoliday    &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA…\n$ Type         &lt;chr&gt; \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A…\n$ Size         &lt;dbl&gt; 151315, 151315, 151315, 151315, 151315, 151315, 151315, 1…\n$ Temperature  &lt;dbl&gt; 42.31, 38.51, 39.93, 46.63, 46.50, 57.79, 54.58, 51.45, 6…\n$ Fuel_Price   &lt;dbl&gt; 2.572, 2.548, 2.514, 2.561, 2.625, 2.667, 2.720, 2.732, 2…\n$ MarkDown1    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown2    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown3    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown4    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ MarkDown5    &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CPI          &lt;dbl&gt; 211.0964, 211.2422, 211.2891, 211.3196, 211.3501, 211.380…\n$ Unemployment &lt;dbl&gt; 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 8.106, 7…\n\n\n\n\n\ncategorical variables:  \n       name     class levels    n missing\n1        id    factor   3331 1001       0\n2 IsHoliday   logical      2 1001       0\n3      Type character      1 1001       0\n                                   distribution\n1 1_1 (14.3%), 1_3 (14.3%), 1_8 (14.3%) ...    \n2 FALSE (93%), TRUE (7%)                       \n3 A (100%)                                     \n\nDate variables:  \n  name class      first       last min_diff max_diff    n missing\n1 Date  Date 2010-02-05 2012-10-26   0 days   7 days 1001       0\n\nquantitative variables:  \n           name   class         min          Q1      median          Q3\n1         Store numeric      1.0000      1.0000      1.0000      1.0000\n2          Dept numeric      1.0000      3.0000     13.0000     93.0000\n3  Weekly_Sales numeric   6165.7300  28257.3000  39886.0600  77943.5700\n4          Size numeric 151315.0000 151315.0000 151315.0000 151315.0000\n5   Temperature numeric     35.4000     57.7900     69.6400     80.4900\n6    Fuel_Price numeric      2.5140      2.7590      3.2900      3.5940\n7     MarkDown1 numeric    410.3100   4039.3900   6154.1400  10121.9700\n8     MarkDown2 numeric      0.5000     40.4800    144.8700   1569.0000\n9     MarkDown3 numeric      0.2500      6.0000     25.9650    101.6400\n10    MarkDown4 numeric      8.0000    577.1400   1822.5500   3750.5900\n11    MarkDown5 numeric    554.9200   3127.8800   4325.1900   6222.2500\n12          CPI numeric    210.3374    211.5312    215.4599    220.6369\n13 Unemployment numeric      6.5730      7.3480      7.7870      7.8380\n           max         mean           sd    n missing\n1       1.0000 1.000000e+00 0.000000e+00 1001       0\n2      95.0000 3.585714e+01 3.849159e+01 1001       0\n3  148798.0500 5.464634e+04 3.627627e+04 1001       0\n4  151315.0000 1.513150e+05 0.000000e+00 1001       0\n5      91.6500 6.830678e+01 1.420767e+01 1001       0\n6       3.9070 3.219699e+00 4.260286e-01 1001       0\n7   34577.0600 8.090766e+03 6.550983e+03  357     644\n8   46011.3800 2.941315e+03 7.873661e+03  294     707\n9   55805.5100 1.225400e+03 7.811934e+03  350     651\n10  32403.8700 3.746085e+03 5.948867e+03  357     644\n11  20475.3200 5.018655e+03 3.254071e+03  357     644\n12    223.4443 2.159969e+02 4.337818e+00 1001       0\n13      8.1060 7.610420e+00 3.825958e-01 1001       0\n\n\n\nThe data is described as:\n\nA tibble: 9,743 x 3\n\n\nid Factor. Unique series identifier (4 total)\n\nStore Numeric. Store ID.\n\nDept Numeric. Department ID.\n\nDate Date. Weekly timestamp.\n\nWeekly_Sales Numeric. Sales for the given department in the given store.\n\nIsHoliday Logical. Whether the week is a “special” holiday for the store.\n\nType Character. Type identifier of the store.\n\nSize Numeric. Store square-footage\n\nTemperature Numeric. Average temperature in the region.\n\nFuel_Price Numeric. Cost of fuel in the region.\n\nMarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5 Numeric. Anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n\nCPI Numeric. The consumer price index.\n\nUnemployment Numeric. The unemployment rate in the region.\n\n\nVery cool to know that mosaic::inspect() identifies date variables separately!\n\n\n\n\n\n\nNoteNot yet a time series!\n\n\n\nThis is still a tibble, with a time-oriented variable of course, but not yet a time-series object. The data frame has the YMD columns repeated for each Dept, giving us what is called “long” form data. To deal with this repetition, we will always need to split the Weekly_Sales by the Dept column before we plot or analyze.\n\n\n\n#|label: walmart sales tsibble\nwalmart_tsibble &lt;-\n  walmart_sales_weekly %&gt;%\n  as_tsibble(\n    index = Date, # Time Variable\n    key = c(id, Store, Dept, Type)\n  )\n\n#  Identifies unique \"subject\" who are measures\n#  All other variables such as Weekly_sales become \"measured variable\"\n#  Each observation should be uniquely identified by index and key\nwalmart_tsibble\n\n\n  \n\n\n\n\n Basic Time Series Plots\nThe easiest way is to use autoplot from the feasts package. You may need to specify the actual measured variable, if there is more than one numerical column:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nfeasts::autoplot(walmart_tsibble, .vars = Weekly_Sales)\n\n\n\n\n\n\n\ntimetk gives us interactive plots that may be more evocative than the static plot above. The basic plot function with timetk is plot_time_series. There are arguments for the date variable, the value you want to plot, colours, groupings etc.\nLet us explore this dataset using timetk, using our trusted method of asking Questions:\n\n\n\n\n\n\nNote\n\n\n\nQ.1 How are the weekly sales different for each Department?\nThere are 7 number of Departments. So we should be fine plotting them and also facetting with them, as we will see in a bit:\n\nwalmart_tsibble %&gt;%\n  timetk::plot_time_series(\n    .date_var = Date,\n    .value = Weekly_Sales,\n    .color_var = Dept,\n    .legend_show = TRUE,\n    .title = \"Walmart Sales Data by Department\",\n    .smooth = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.2. What do the sales per Dept look like during the month of December (Christmas time) in 2012? Show the individual Depts as facets.\nWe can of course zoom into the interactive plot above, but if we were to plot it anyway:\n\n# Only include rows from  1 to December 31, 2011\n# Data goes only up to Oct 2012\n\nwalmart_tsibble %&gt;%\n  # Each side of the time_formula is specified as the character 'YYYY-MM-DD HH:MM:SS',\n  timetk::filter_by_time(\n    .date_var = Date,\n    .start_date = \"2011-12-01\",\n    .end_date = \"2011-12-31\"\n  ) %&gt;%\n  plot_time_series(\n    .date_var = Date,\n    .value = Weekly_Sales,\n    .color_var = Dept,\n    .facet_vars = Dept,\n    .facet_ncol = 2,\n    .smooth = FALSE\n  ) # Only 4 points per graph\n\n\n\n\n\nClearly the “unfortunate” Dept#13 has seen something of a Christmas drop in sales, as has Dept#38 ! The rest, all is well, it seems…\n\n\n\n Too much noise? How about some averaging?\n\n\n\n\n\n\nNote\n\n\n\nQ.3 How do we smooth out some of the variations in the time series to be able to understand it better?\nSometimes there is too much noise in the time series observations and we want to take what is called a rolling average. For this we will use the function timetk::slidify to create an averaging function of our choice, and then apply it to the time series using regular dplyr::mutate\n\n# Let's take the average of Sales for each month in each Department.\n# Our **function** will be named \"rolling_avg_month\":\n\nrolling_avg_month &lt;- timetk::slidify(\n  .period = 4, # every 4 weeks\n  .f = mean, # The function to use\n  .align = \"center\", # Aligned with middle of month\n  .partial = TRUE\n) # To catch any leftover half weeks\nrolling_avg_month\n\nfunction (...) \n{\n    slider_2(..., .slider_fun = slider::pslide, .f = .f, .period = .period, \n        .align = .align, .partial = .partial, .unlist = .unlist)\n}\n&lt;bytecode: 0x14098a6e0&gt;\n&lt;environment: 0x14098fcc0&gt;\n\n\nOK, slidify creates a function! Let’s apply it to the Walmart Sales time series…\n\nwalmart_tsibble %&gt;%\n  # group_by(Dept) %&gt;% # Is this needed?\n  mutate(avg_monthly_sales = rolling_avg_month(Weekly_Sales)) %&gt;%\n  # ungroup() %&gt;% # Is this needed?\n  timetk::plot_time_series(Date, avg_monthly_sales,\n    .color_var = Dept, # Does the grouping!\n    .smooth = FALSE\n  )\n\n\n\n\n\nCurves are smoother now. Need to check whether the averaging was done on a per-Dept basis…should we have had a group_by(Dept) before the averaging, and ungroup() before plotting? Try it !!\n\n\n\n Decomposing Time Series: Trends, Seasonal Patterns, and Cycles\nEach data point (\\(Y_t\\)) at time \\(t\\) in a Time Series can be expressed as either a sum or a product of 4 components, namely, Seasonality(\\(S_t\\)), Trend(\\(T_t\\)), Cyclic, and Error(\\(e_t\\)) (a.k.a White Noise).\n\n\nTrend: pattern exists when there is a long-term increase or decrease in the data.\n\nSeasonal: pattern exists when a series is influenced by seasonal factors (e.g., the quarter of the year, the month, or day of the week).\n\nCyclic: pattern exists when data exhibit rises and falls that are not of fixed period (duration usually of at least 2 years). Often combined with Trend into “Trend-Cycle”.\n\nError or Noise: Random component\n\nWhen data is non-seasonal this means breaking it up into only trend and irregular components. To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is common to use a smoothing method, such as calculating the simple moving average of the time series.\ntimetk has the ability to achieve this: Let us plot the trend, seasonal, cyclic and irregular aspects of Weekly_Sales for Dept 38:\n\nwalmart_tsibble %&gt;%\n  filter(Dept == \"38\") %&gt;%\n  timetk::plot_stl_diagnostics(\n    .data = .,\n    .date_var = Date,\n    .value = Weekly_Sales\n  )\n\n\n\n\n\nWe can do this for all Depts using fable and fabletools:\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nwalmart_decomposed &lt;-\n  walmart_tsibble %&gt;%\n  # If we want to filter, we do it here\n  # filter(Dept == \"38\") %&gt;%\n  #\n\n  fabletools::model(stl = STL(Weekly_Sales))\n\nfabletools::components(walmart_decomposed)\n\n\n  \n\n\nfeasts::autoplot(components((walmart_decomposed)))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study-2-dataset-from-nycflights13",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#case-study-2-dataset-from-nycflights13",
    "title": "Analysis of Time Series in R",
    "section": "\n Case Study #2: Dataset from nycflights13\n",
    "text": "Case Study #2: Dataset from nycflights13\n\nLet us try the flights dataset from the package nycflights13. Try data(package = \"nycflights13\") in your Console.\nWe have the following datasets in the nycflights13 package:\n\n\nairlines Airline names.\n\nairports Airport metadata\n\nflights Flights data\n\nplanes Plane metadata.\n\nweather Hourly weather data\n\nLet us analyze the flights data:\n\ndata(\"flights\", package = \"nycflights13\")\nglimpse(flights)\n\nRows: 336,776\nColumns: 19\n$ year           &lt;int&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2…\n$ month          &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ day            &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ dep_time       &lt;int&gt; 517, 533, 542, 544, 554, 554, 555, 557, 557, 558, 558, …\n$ sched_dep_time &lt;int&gt; 515, 529, 540, 545, 600, 558, 600, 600, 600, 600, 600, …\n$ dep_delay      &lt;dbl&gt; 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, -2, -2, -1…\n$ arr_time       &lt;int&gt; 830, 850, 923, 1004, 812, 740, 913, 709, 838, 753, 849,…\n$ sched_arr_time &lt;int&gt; 819, 830, 850, 1022, 837, 728, 854, 723, 846, 745, 851,…\n$ arr_delay      &lt;dbl&gt; 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2, -3, 7, -1…\n$ carrier        &lt;chr&gt; \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6\", \"…\n$ flight         &lt;int&gt; 1545, 1714, 1141, 725, 461, 1696, 507, 5708, 79, 301, 4…\n$ tailnum        &lt;chr&gt; \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668DN\", \"N394…\n$ origin         &lt;chr&gt; \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LGA\",…\n$ dest           &lt;chr&gt; \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FLL\", \"IAD\",…\n$ air_time       &lt;dbl&gt; 227, 227, 160, 183, 116, 150, 158, 53, 140, 138, 149, 1…\n$ distance       &lt;dbl&gt; 1400, 1416, 1089, 1576, 762, 719, 1065, 229, 944, 733, …\n$ hour           &lt;dbl&gt; 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6…\n$ minute         &lt;dbl&gt; 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 0…\n$ time_hour      &lt;dttm&gt; 2013-01-01 05:00:00, 2013-01-01 05:00:00, 2013-01-01 0…\n\nmosaic::inspect(flights)\n\n\ncategorical variables:  \n     name     class levels      n missing\n1 carrier character     16 336776       0\n2 tailnum character   4043 334264    2512\n3  origin character      3 336776       0\n4    dest character    105 336776       0\n                                   distribution\n1 UA (17.4%), B6 (16.2%), EV (16.1%) ...       \n2 N725MQ (0.2%), N722MQ (0.2%) ...             \n3 EWR (35.9%), JFK (33%), LGA (31.1%)          \n4 ORD (5.1%), ATL (5.1%), LAX (4.8%) ...       \n\nquantitative variables:  \n             name   class  min   Q1 median   Q3  max        mean          sd\n1            year integer 2013 2013   2013 2013 2013 2013.000000    0.000000\n2           month integer    1    4      7   10   12    6.548510    3.414457\n3             day integer    1    8     16   23   31   15.710787    8.768607\n4        dep_time integer    1  907   1401 1744 2400 1349.109947  488.281791\n5  sched_dep_time integer  106  906   1359 1729 2359 1344.254840  467.335756\n6       dep_delay numeric  -43   -5     -2   11 1301   12.639070   40.210061\n7        arr_time integer    1 1104   1535 1940 2400 1502.054999  533.264132\n8  sched_arr_time integer    1 1124   1556 1945 2359 1536.380220  497.457142\n9       arr_delay numeric  -86  -17     -5   14 1272    6.895377   44.633292\n10         flight integer    1  553   1496 3465 8500 1971.923620 1632.471938\n11       air_time numeric   20   82    129  192  695  150.686460   93.688305\n12       distance numeric   17  502    872 1389 4983 1039.912604  733.233033\n13           hour numeric    1    9     13   17   23   13.180247    4.661316\n14         minute numeric    0    8     29   44   59   26.230100   19.300846\n        n missing\n1  336776       0\n2  336776       0\n3  336776       0\n4  328521    8255\n5  336776       0\n6  328521    8255\n7  328063    8713\n8  336776       0\n9  327346    9430\n10 336776       0\n11 327346    9430\n12 336776       0\n13 336776       0\n14 336776       0\n\ntime variables:  \n       name   class               first                last min_diff   max_diff\n1 time_hour POSIXct 2013-01-01 05:00:00 2013-12-31 23:00:00   0 secs 25200 secs\n       n missing\n1 336776       0\n\n\nWe have time-related columns; Apart from year, month, day we have time_hour; and time-event numerical data such as arr_delay (arrival delay) and dep_delay (departure delay). We also have categorical data such as carrier, origin, dest, flight and tailnum of the aircraft. It is also a large dataset containing 330K entries. Enough to play with!!\nLet us replace the NAs in arr_delay and dep_delay with zeroes for now, and convert it into a time-series object with tsibble:\n\nflights_delay_ts &lt;- flights %&gt;%\n  mutate(\n    arr_delay = replace_na(arr_delay, 0),\n    dep_delay = replace_na(dep_delay, 0)\n  ) %&gt;%\n  select(\n    time_hour, arr_delay, dep_delay,\n    carrier, origin, dest,\n    flight, tailnum\n  ) %&gt;%\n  tsibble::as_tsibble(\n    index = time_hour,\n    # All the remaining identify unique entries\n    # Along with index\n    # Many of these variables are common\n    # Need *all* to make unique entries!\n    key = c(carrier, origin, dest, flight, tailnum),\n    validate = TRUE\n  ) # Making sure each entry is unique\n\n\nflights_delay_ts\n\n\n  \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.1. Plot the monthly average arrival delay by carrier\n\n\nUsing tsibble\nUsing timetk\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n#\nmean_arr_delays_by_carrier &lt;-\n  flights_delay_ts %&gt;%\n  group_by(carrier) %&gt;%\n  index_by(month = ~ yearmonth(.)) %&gt;%\n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n\n  summarise(\n    mean_arr_delay =\n      mean(arr_delay, na.rm = TRUE)\n  )\n\nmean_arr_delays_by_carrier\n# colours from the \"I want Hue\" website\ncolour_values &lt;- c(\n  \"#634bd1\", \"#35d25a\", \"#757bff\", \"#fa9011\", \"#72369a\",\n  \"#617d00\", \"#d094ff\", \"#81d8a4\", \"#e63d2d\", \"#0080c0\",\n  \"#9e4500\", \"#98a9ff\", \"#efbd7f\", \"#474e8c\", \"#ffa1e4\",\n  \"#8a3261\", \"#a6c1f8\", \"#a16e96\"\n)\n\n# Plotting with ggformula\nmean_arr_delays_by_carrier %&gt;%\n  gf_hline(yintercept = 0, color = \"grey\") %&gt;%\n  gf_line(\n    mean_arr_delay ~ month,\n    group = ~carrier,\n    color = ~carrier,\n    linewidth = 1.5,\n    title = \"Average Monthly Arrival Delays by Carrier\",\n    caption = \"Using tsibble + ggformula\"\n  ) %&gt;%\n  gf_facet_wrap(vars(carrier), nrow = 4) %&gt;%\n  gf_refine(scale_color_manual(\n    name = \"Airlines\",\n    values = colour_values\n  )) %&gt;%\n  gf_theme(theme(axis.text.x = element_text(\n    angle = 45,\n    hjust = 1,\n    size = 6\n  )))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nmean_arr_delays_by_carrier2 &lt;-\n  flights_delay_ts %&gt;%\n  as_tibble() %&gt;%\n  group_by(carrier) %&gt;%\n  # summarize_by_time REQUIRES a tibble\n  # Cannot do this with a tsibble\n  timetk::summarise_by_time(\n    .date_var = time_hour,\n    .by = \"month\",\n    mean_arr_delay = mean(arr_delay)\n  )\n\n\nmean_arr_delays_by_carrier2\ncolour_values &lt;- c(\n  \"#634bd1\", \"#35d25a\", \"#757bff\", \"#fa9011\", \"#72369a\",\n  \"#617d00\", \"#d094ff\", \"#81d8a4\", \"#e63d2d\", \"#0080c0\",\n  \"#9e4500\", \"#98a9ff\", \"#efbd7f\", \"#474e8c\", \"#ffa1e4\",\n  \"#8a3261\", \"#a6c1f8\", \"#a16e96\"\n)\np &lt;- mean_arr_delays_by_carrier2 %&gt;%\n  timetk::plot_time_series(\n    # .data = .,\n    .date_var = time_hour, # no change to time variable name!\n    .value = mean_arr_delay,\n    .color_var = carrier,\n    .facet_vars = carrier,\n    .smooth = FALSE,\n    # .smooth_degree = 1,\n    # keep .smooth off since it throws warnings if there are too few points\n    # Like if we do quarterly or even yearly summaries\n    # Use only for smaller values of .smooth_degree (0,1)\n    .interactive = FALSE, .line_size = 2,\n    .facet_ncol = 4, .legend_show = FALSE, .facet_scales = \"fixed\",\n    .title = \"Average Monthly Arrival Delays by Carrier\",\n    .y_lab = \"Arrival Delays over Time\", .x_lab = \"Time\"\n  ) +\n  geom_hline(yintercept = 0, color = \"grey\") +\n  scale_colour_manual(\n    name = \"Airline\",\n    values = colour_values\n  ) +\n  theme_classic() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(caption = \"Using timetk\")\n\n# Reverse the layers in the plot\n# Zero line BELOW, time series above\n# https://stackoverflow.com/questions/20249653/insert-layer-underneath-existing-layers-in-ggplot2-object\n\np$layers &lt;- rev(p$layers)\np\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nInsights: - Clearly airline OO has had some serious arrival delay problems in the first half of 2013… - most other delays are around the zero-line, with some variations in both directions\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.2. Plot a candlestick chart for total flight delays for a particular month for each origin across airlines!\n\n\nUsing ggformula\nUsing timetk\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nflights_delay_ts %&gt;%\n  mutate(\n    total_delay = arr_delay + dep_delay,\n    month = lubridate::month(time_hour,\n      # Makes ordinal factor with month labels\n      label = TRUE\n    )\n  ) %&gt;%\n  filter(month == \"Dec\") %&gt;%\n  gf_boxplot(total_delay ~ .,\n    color = ~origin,\n    fill = ~origin, alpha = 0.3\n  ) %&gt;%\n  gf_facet_wrap(vars(carrier), nrow = 3, scales = \"free_y\") %&gt;%\n  gf_theme(theme(axis.text.x = element_blank()))\n\n\n\n\n\n\n\n\n\n\nflights_delay_ts %&gt;%\n  mutate(total_delay = arr_delay + dep_delay) %&gt;%\n  timetk::filter_by_time(\n    .start_date = \"2013-12\",\n    .end_date = \"2013-12\"\n  ) %&gt;%\n  timetk::plot_time_series_boxplot(\n    .date_var = time_hour,\n    .value = total_delay,\n    .color_var = origin,\n    .facet_vars = carrier,\n    .period = \"month\",\n    .interactive = FALSE,\n    # .smooth_degree = 1,\n    # keep .smooth off since it throws warnings if there are too few points\n    # Like if we do quarterly or even yearly summaries\n    # Use only for smaller values of .smooth_degree (0,1)\n    .smooth = FALSE\n  )\n\n\n\n\n\n\n\nInsights: - JFK has more outliers in total_delay than EWR and LGA - Just a hint of more delays in April… and July?\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQ.3. Plot a heatmap chart for total flight delays by origin, aggregated by month\n\n\nUsing tsibble + ggformula\nUsing timetk\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\navg_delays_month &lt;- flights_delay_ts %&gt;%\n  group_by(origin) %&gt;%\n  mutate(total_delay = arr_delay + dep_delay) %&gt;%\n  index_by(month = ~ yearmonth(.)) %&gt;%\n  # index_by uses (year, yearquarter, yearmonth, yearweek, as.Date)\n  # to create a new column to show the time-grouping\n  # year / quarter / month/ week, or day...\n  # which IS different from traditional dplyr\n  summarise(mean_monthly_delay = mean(total_delay, na.rm = TRUE))\n\navg_delays_month\n\n\n  \n\n\n# three origins 12 months therefore 36 rows\n# Tsibble index_by + summarise also gives us a  month` column\n\n\n\nggformula::gf_tile(origin ~ month,\n  fill = ~mean_monthly_delay,\n  color = \"black\", data = avg_delays_month,\n  title = \"Mean Flight Delays from NY Airports in 2013\"\n) %&gt;%\n  gf_theme(scale_fill_viridis_c(option = \"A\"))\n\n\n\n\n\n\n# \"magma\" (or \"A\") inferno\" (or \"B\") \"plasma\" (or \"C\")\n# \"viridis\" (or \"D\") \"cividis\" (or \"E\")\n# \"rocket\" (or \"F\") \"mako\" (or \"G\") \"turbo\" (or \"H\")\n\n\n\n\n\n\n\nInsights: - TBD - TBD"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#seasons-trends-cycles-and-random-changes",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#seasons-trends-cycles-and-random-changes",
    "title": "Analysis of Time Series in R",
    "section": "\n Seasons, Trends, Cycles, and Random Changes",
    "text": "Seasons, Trends, Cycles, and Random Changes\nHere are how the different types of patterns in time series are as follows:\n\nTrend: A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as “changing direction”, when it might go from an increasing trend to a decreasing trend.\n\n\nSeasonal: A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known period. The monthly sales of drugs (with the PBS data) shows seasonality which is induced partly by the change in the cost of the drugs at the end of the calendar year.\n\n\nCyclic: A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the “business cycle”. The duration of these fluctuations is usually at least 2 years.\n\nThe function feasts::STL allows us to create these decompositions.\nLet us try to find and plot these patterns in Time Series.\n\nbirths_2000_2014 &lt;- read_csv(\"https://raw.githubusercontent.com/fivethirtyeight/data/master/births/US_births_2000-2014_SSA.csv\")\n##\nbirths_tsibble &lt;-\n  births_2000_2014 %&gt;%\n  mutate(index = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  tsibble::as_tsibble(index = index) %&gt;%\n  select(index, births)\n##\nbirths_STL_yearly &lt;-\n  births_tsibble %&gt;%\n  fabletools::model(STL(births ~ season(period = \"year\")))\n\nfabletools::components(births_STL_yearly)\n\n\n  \n\n\n\n\n\n# Set graph theme\ntheme_set(new = theme_custom())\n##\nfeasts::autoplot(components(births_STL_yearly))"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#conclusion",
    "title": "Analysis of Time Series in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe can plot most of the common Time Series Plots with the help of the tidyverts packages: ( tsibble, feast, fable and fabletools) , along with timetk and ggformula.\nThere are other plot packages to investigate, such as dygraphs\nRecall that we have used the tsibble format for the data. There are other formats such as ts, xts and others which are meant for time series analysis. But for our present purposes, we are able to do things with the capabilities of timetk."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/timeseries-analysis.html#references",
    "title": "Analysis of Time Series in R",
    "section": "\n References",
    "text": "References\n\nRob J Hyndman and George Athanasopoulos, Forecasting: Principles and Practice (3rd ed), Available Online https://otexts.com/fpp3/\nWhat is seasonal adjustment and why is it used?\nThe start-at-zero rule\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html",
    "title": "🕔 Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\n\n# Wrangling\nlibrary(lubridate) # Deal with dates. Part of the tidyverse anyway!\n\nlibrary(fpp3) # Robert Hyndman's textbook package, Loads all the core time series packages, see messages\n\n# Plots\nlibrary(timetk) # Tidy Time series analysis and plots\nlibrary(tsbox) # Plotting and Time Series File Transformations\n# library(TSstudio) # Plots, Decomposition, and Modelling with Time Series.\n# Seems hard to get to work in Quarto ;-()\nlibrary(timetk) # Visualizing, Wrangling and Modelling Time Series by Matt Dancho\n\n# Modelling\nlibrary(sweep) # New (07/2023) package to bring broom-like features to time series models\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman\n\n\n\n\n\n\n\nTipmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = _____)\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data = _____)\nIn practice, we often use: dataframe %&gt;%  gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\n\n\n\n\n\n\nTipggplot command template\n\n\n\nThe ggplot2 template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = ---, mapping = aes(x = ---, y = ---)) + geom_----()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;%  ggplot(aes(xvar, yvar)) + geom_----()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#setting-up-r-packages",
    "title": "🕔 Time Series",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula) # Our Formula based graphing package\n\n# Wrangling\nlibrary(lubridate) # Deal with dates. Part of the tidyverse anyway!\n\nlibrary(fpp3) # Robert Hyndman's textbook package, Loads all the core time series packages, see messages\n\n# Plots\nlibrary(timetk) # Tidy Time series analysis and plots\nlibrary(tsbox) # Plotting and Time Series File Transformations\n# library(TSstudio) # Plots, Decomposition, and Modelling with Time Series.\n# Seems hard to get to work in Quarto ;-()\nlibrary(timetk) # Visualizing, Wrangling and Modelling Time Series by Matt Dancho\n\n# Modelling\nlibrary(sweep) # New (07/2023) package to bring broom-like features to time series models\n\n# devtools::install_github(\"FinYang/tsdl\")\nlibrary(tsdl) # Time Series Data Library from Rob Hyndman\n\n\n\n\n\n\n\nTipmosaic and ggformula command template\n\n\n\nNote the standard method for all commands from the mosaic and ggformula packages: goal( y ~ x | z, data = _____)\nWith ggformula, one can create any graph/chart using: gf_***(y ~ x | z, data = _____)\nIn practice, we often use: dataframe %&gt;%  gf_***(y ~ x | z) which has cool benefits such as “autocompletion” of variable names, as we shall see. The “***” indicates what kind of graph you desire: histogram, bar, scatter, density; the “___” is the name of your dataset that you want to plot with.\n\n\n\n\n\n\n\n\nTipggplot command template\n\n\n\nThe ggplot2 template is used to identify the dataframe, identify the x and y axis, and define visualized layers:\nggplot(data = ---, mapping = aes(x = ---, y = ---)) + geom_----()\nNote: —- is meant to imply text you supply. e.g. function names, data frame names, variable names.\nIt is helpful to see the argument mapping, above. In practice, rather than typing the formal arguments, code is typically shorthanded to this:\ndataframe %&gt;%  ggplot(aes(xvar, yvar)) + geom_----()"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#introduction",
    "title": "🕔 Time Series",
    "section": "\n Introduction",
    "text": "Introduction\nAny metric that is measured over regular time intervals forms a time series. Analysis of Time Series is commercially important because of industrial need and relevance, especially with respect to Forecasting (Weather data, sports scores, population growth figures, stock prices, demand, sales, supply…). For example, in the graph shown below are the temperatures over time in two US cities:\n\n\nWhat can we do with Time Series? As with other datasets, we have to begin by answering fundamental questions, such as:\n\nWhat are the types of time series?\nHow do we visualize time series?\nHow might we summarize time series to get aggregate numbers, say by week, month, quarter or year?\nHow do we decompose the time series into level, trend, and seasonal components?\nHoe might we make a model of the underlying process that creates these time series?\nHow do we make useful forecasts with the data we have?\n\nWe will first look at the multiple data formats for time series in R. Alongside we will look at the R packages that work with these formats and create graphs and measures using those objects. Then we examine data wrangling of time series, where we look at packages that offer dplyr-like ability to group and summarize time series using the time variable. We will finally look at obtaining the components of the time series and try our hand at modelling and forecasting."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-formats-conversion-and-plotting",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-formats-conversion-and-plotting",
    "title": "🕔 Time Series",
    "section": "\n Time Series Formats, Conversion, and Plotting",
    "text": "Time Series Formats, Conversion, and Plotting\nThere are multiple formats for time series data. The ones that we are likely to encounter most are:\n\nThe ts format: We may simply have a single series of measurements that are made over time, stored as a numerical vector. The stats::ts() function will convert a numeric vector into an R time series ts object, which is the most basic time series object in R. The base-R ts object is used by established packages forecast and is also supported by newer packages such as tsbox.\nThe tibble format: the simplest and most familiar data format is of course the standard tibble/data frame, with or without an explicit time column/variable to indicate that the other variables vary with time. The standard tibble object is used by many packages, e.g. timetk & modeltime.\nThe modern tsibble format: this is a new modern format for time series analysis. The special tsibble object (“time series tibble”) is used by fable, feasts and others from the tidyverts set of packages.\n\nThere are many other time-oriented data formats too…probably too many, such a tibbletime and TimeSeries objects. For now the best way to deal with these, should you encounter them, is to convert them (Using tsbox) to a tibble or a tsibble and work with these.\n\n\n\n\n\nStandards\n\nTo start, we will use simple ts data first, and then do another with tibble format that we can plot as is. We will then do more after conversion to tsibble format, and then a third example with a ground-up tsibble dataset.\n\n Base-R ts format data\nThere are a few datasets in base R that are in ts format already.\n\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\n\nstr(AirPassengers)\n\n Time-Series [1:144] from 1949 to 1961: 112 118 132 129 121 135 148 148 136 119 ...\n\n\nThis can be easily plotted using base R and other more recent packages:\n# Base R\nplot(AirPassengers)\n# tsbox static plot\ntsbox::ts_plot(AirPassengers, ylab = \"Passengers\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne can see that there is an upward trend and also seasonal variations that also increase over time. This is an example of a multiplicative time series, which we will discuss later.\nLet us take data that is “time oriented” but not in ts format. We use the command ts to convert a numeric vector to ts format: the syntax of ts() is:\nSyntax: objectName &lt;- ts(data, start, end, frequency), where,\n\n\ndata : represents the data vector\n\nstart : represents the first observation in time series\n\nend : represents the last observation in time series\n\nfrequency : represents number of observations per unit time. For example 1=annual, 4=quarterly, 12=monthly, 7=weekly, etc.\n\nWe will pick simple numerical vector data ( i.e. not a time series ) ChickWeight:\n\nChickWeight %&gt;% head()\n\n\n  \n\n\n# Filter for Chick #1 and for Diet #1\nChickWeight_ts &lt;- ChickWeight %&gt;%\n  filter(Chick == 1, Diet == 1) %&gt;%\n  select(weight, Time)\n\nChickWeight_ts &lt;- stats::ts(ChickWeight_ts$weight, frequency = 2)\nstr(ChickWeight_ts)\n\n Time-Series [1:12] from 1 to 6.5: 42 51 59 64 76 93 106 125 149 171 ...\n\n\nNow we can plot this in many ways:\nplot(ChickWeight_ts) # Using base-R\n# ts_boxable(ChickWeight_ts)\n# Using tsbox\ntsbox::ts_plot(ChickWeight_ts,\n  ylab = \"Weight of Chick #1\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Using TSstudio\nTSstudio::ts_plot(ChickWeight_ts,\n  Xtitle = \"Time\",\n  Ytitle = \"Weight of Chick #1\"\n)\n\n\n\n\n\nWe see that the weights of a young chick specimen increases over time.\n\ntibble data\nThe ts data format can handle only one time series. If we want multiple time series, based on say Qualitative variables, we need other data formats. Using the familiar tibble structure opens up new possibilities.\n\nWe can have multiple time series within a tibble (think of numerical time-series data like GDP, Population, Imports, Exports for multiple countries as with the gapminder1data we saw earlier).\n\nIt also allows for data processing with dplyr such as filtering and summarizing.\n\n\n\ngapminder data\n\n\n\n  \n\n\n\nLet us read and inspect in the US births data from 2000 to 2014. Download this data by clicking on the icon below, and saving the downloaded file in a sub-folder called data inside your project.\n Download the US Births data \nRead this data in:\n\nbirths_2000_2014 &lt;- read_csv(\"../data/US_births_2000-2014_SSA.csv\")\nglimpse(births_2000_2014)\n\nRows: 5,479\nColumns: 5\n$ year          &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 20…\n$ month         &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ date_of_month &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ day_of_week   &lt;dbl&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3,…\n$ births        &lt;dbl&gt; 9083, 8006, 11363, 13032, 12558, 12466, 12516, 8934, 794…\n\ninspect(births_2000_2014)\n\n\nquantitative variables:  \n           name   class  min   Q1 median    Q3   max         mean          sd\n1          year numeric 2000 2003   2007  2011  2014  2006.999270    4.321085\n2         month numeric    1    4      7    10    12     6.522723    3.449075\n3 date_of_month numeric    1    8     16    23    31    15.730243    8.801151\n4   day_of_week numeric    1    2      4     6     7     3.999817    2.000502\n5        births numeric 5728 8740  12343 13082 16081 11350.068261 2325.821049\n     n missing\n1 5479       0\n2 5479       0\n3 5479       0\n4 5479       0\n5 5479       0\n\nbirths_2000_2014\n\n\n  \n\n\n\nThis is just a tibble containing a single data variable births that varies over time. All other variables, although depicting time, are numerical columns. There are no Qualitative variables (yet!).\nPlotting tibble time series\n\n\nUsing ggformula\nUsing tsbox and TSstudio\nUsing ggplot\n\n\n\nWe will now plot this using ggformula. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n# grouping by day_of_week\nbirths_2000_2014 %&gt;%\n  gf_line(births ~ year,\n    group = ~day_of_week,\n    color = ~day_of_week\n  ) %&gt;%\n  gf_point(title = \"By Day of Week\") %&gt;%\n  gf_theme(scale_colour_distiller(palette = \"Paired\"))\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;%\n  gf_line(births ~ year,\n    group = ~date_of_month,\n    color = ~date_of_month\n  ) %&gt;%\n  gf_point(title = \"By Date of Month\") %&gt;%\n  gf_theme(scale_colour_distiller(palette = \"Paired\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot particularly illuminating. This is because the data is daily and we have considerable variation over time, and here we have too much data to visualize. Summaries will help, so we could calculate the the mean births on a month basis in each year and plot that:\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;%\n  # Convert month to factor/Qual variable!\n  # So that we can have discrete colours for each month\n  # Using base::factor()\n  # Could use forcats::as_factor() also\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n  # `month.abb` is a built-in dataset containing names of months.\n  group_by(year, month) %&gt;%\n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\nbirths_2000_2014_monthly %&gt;%\n  gf_line(mean_monthly_births ~ year,\n    group = ~month,\n    colour = ~month, linewidth = 1\n  ) %&gt;%\n  gf_point(size = 1.5, title = \"Summaries of Monthly Births over the years\") %&gt;%\n  # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThese are graphs for the same month each year: we have a January graph and a February graph and so on. So…average births per month were higher in all months during 2005 to 2007 and have dropped since.\n\n\nWe can do similar graphs using day_of_week as our basis for grouping, instead of month:\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;%\n  mutate(day_of_week = base::factor(day_of_week,\n    levels = c(1, 2, 3, 4, 5, 6, 7),\n    labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n  )) %&gt;%\n  group_by(year, day_of_week) %&gt;%\n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_weekly\nbirths_2000_2014_weekly %&gt;%\n  gf_line(mean_daily_births ~ year,\n    group = ~day_of_week,\n    colour = ~day_of_week,\n    linewidth = 1,\n    data = .\n  ) %&gt;%\n  gf_point(size = 2) %&gt;%\n  # palette for 12 colours\n  gf_theme(scale_colour_brewer(palette = \"Paired\"))\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy are fewer babies born on weekends?\n\n\n\nLooks like an interesting story here…there are significantly fewer births on average on Sat and Sun, over the years! Why? Should we watch Grey’s Anatomy ?\n\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that this is still using just tibble data, without converting it or using it as a time series. So far we are simply treating the year/month/day variables are simple variables and using dplyr to group and summarize. We have not created an explicit time or date variable.\n\n\n\n\nLet us create a time variable in our dataset now:\n\n\ntsbox::ts_plot needs just the date and the births columns to plot with and not be confused by the other numerical columns, so let us create a single date column from these three, but retain them for now.\n\nTSstudio::ts_plot also needs a date column.\n\nSo there are several numerical variables for year, month, and day_of_month, day_of_week, and of course the births on a daily basis.\nWe use the lubridate package from the tidyverse:\n\nbirths_timeseries &lt;-\n  births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  select(date, births, year, month, date_of_month, day_of_week)\n\nbirths_timeseries\n\n\n  \n\n\n\n\n\n\n\n\n\nTipExtract from help(tsbox)\n\n\n\nIn data frames, i.e., in a data.frame, a data.table, or a tibble, tsbox stores one or multiple time series in the ‘long’ format. tsbox detects a value, a time column, and zero, one or several id columns. Column detection is done in the following order:\n\nStarting on the right, the first first numeric or integer column is used as value column.\n\nUsing the remaining columns and starting on the right again, the first Date, POSIXct, numeric or character column is used as time column. character strings are parsed by anytime::anytime(). The timestamp, time, indicates the beginning of a period.\n\n\nAll remaining columns are id columns. Each unique combination of id columns points to a (unique) time series.\n\nAlternatively, the time column and the value column to be explicitly named as time and value. If explicit names are used, the column order will be ignored. If columns are detected automatically, a message is returned.\n\n\nPlotting this directly, after selecting the relevant variables, so that they will be auto-detected:\n\nbirths_timeseries %&gt;%\n  select(date, births) %&gt;%\n  tsbox::ts_plot()\n\n[time]: 'date' [value]: 'births' \n\n\n\n\n\n\n\n\n\nbirths_timeseries %&gt;%\n  select(date, births) %&gt;%\n  TSstudio::ts_plot(\n    Xtitle = \"Year\",\n    Ytitle = \"Births\",\n    title = \"Births Time Series\",\n    Xgrid = TRUE, Ygrid = TRUE,\n    slider = TRUE,\n    width = 1\n  ) # linewidth\n\n\n\n\n\nQuite messy, as before. We need use the summarised data, as before. We will do this in the next section.\n\n\nWe will now plot this using ggplot for completeness. Using the separate year/month/week and day_of_week / day_of_month columns, we can plot births over time, colouring by day_of_week, for example:\n# grouping by day_of_week\nbirths_2000_2014 %&gt;%\n  ggplot(aes(year, births,\n    group = day_of_week,\n    color = day_of_week\n  )) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"By Day of Week\") +\n  scale_colour_distiller(palette = \"Paired\")\n# Grouping by date_of_month\nbirths_2000_2014 %&gt;% ggplot(aes(year, births,\n  group = date_of_month,\n  color = date_of_month\n)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"By Date of Month\") +\n  scale_colour_distiller(palette = \"Paired\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirths_2000_2014_monthly &lt;- births_2000_2014 %&gt;%\n  # Convert month to factor/Qual variable!\n  # So that we can have discrete colours for each month\n  # Using base::factor()\n  # Could use forcats::as_factor() also\n\n  mutate(month = base::factor(month, labels = month.abb)) %&gt;%\n  # `month.abb` is a built-in dataset containing names of months.\n\n  group_by(year, month) %&gt;%\n  summarise(mean_monthly_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_monthly\n###\nbirths_2000_2014_monthly %&gt;%\n  ggplot(aes(year, mean_monthly_births,\n    group = month, colour = month\n  )) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1.5) +\n  labs(title = \"Summaries of Monthly Births over the years\") +\n\n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nbirths_2000_2014_weekly &lt;- births_2000_2014 %&gt;%\n  mutate(day_of_week = base::factor(day_of_week,\n    levels = c(1, 2, 3, 4, 5, 6, 7),\n    labels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n  )) %&gt;%\n  group_by(year, day_of_week) %&gt;%\n  summarise(mean_daily_births = mean(births, na.rm = TRUE))\nbirths_2000_2014_weekly\nbirths_2000_2014_weekly %&gt;%\n  ggplot(aes(year, mean_daily_births,\n    group = day_of_week,\n    colour = day_of_week\n  )) +\n  geom_line() +\n  geom_point() +\n\n  # palette for 12 colours\n  scale_colour_brewer(palette = \"Paired\")\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsibble data\nFinally, we have tsibble (“time series tibble”) format data, which contains three main components:\n\nan index variable that defines time;\na set of key variables, usually categorical, that define sets of observations, over time. This allows for each combination of the categorical variables to define a separate time series.\na set of quantitative variables, that represent the quantities that vary over time (i.e index)\n\nHere is Robert Hyndman’s video introducing tsibbles:\n\nThe package tsibbledata contains several ready made tsibble format data.  Let us try PBS, which is a dataset containing Monthly Medicare prescription data in Australia.Run data(package = \"tsibbledata\") in your Console to find out about these.\n\ndata(\"PBS\")\n# inspect(PBS) # does not work since mosaic cannot handle tsibbles\nPBS\n\n\n  \n\n\n\nData Description: This is a large-ish dataset:Run PBS in your console\n\n67K observations\n336 combinations of key variables (Concession, Type, ATC1, ATC2) which are categorical, as foreseen.\nData appears to be monthly, as indicated by the 1M.\nthe time index variable is called Month, formatted as yearmonth, a new type of variable introduced in the tsibble package\n\nNote that there are multiple Quantitative variables (Scripts,Cost), each sliced into 336 time-series, a feature which is not supported in the ts format, but is supported in a tsibble. The Qualitative Variables are described below. Type help(\"PBS\") in your Console.\nThe data is dis-aggregated/grouped using four keys:\n- Concession: Concessional scripts are given to pensioners, unemployed, dependents, and other card holders\n- Type: Co-payments are made until an individual’s script expenditure hits a threshold ($290.00 for concession, $1141.80 otherwise). Safety net subsidies are provided to individuals exceeding this amount.\n- ATC1: Anatomical Therapeutic Chemical index (level 1). 15 types\n- ATC2: Anatomical Therapeutic Chemical index (level 2). 84 types, nested inside ATC1.\nLet us simply plot Cost over time:\n\n\nUsing ggformula\nUsing ggplot\n\n\n\n\nPBS %&gt;%\n  gf_point(Cost ~ Month, data = .) %&gt;%\n  gf_line(title = \"PBS Costs vs time\")\n\n\n\n\n\n\n\n\n\n\nPBS %&gt;% ggplot(aes(Month, Cost)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"PBS Costs vs time\")\n\n\n\n\n\n\n\n\n\n\nThis basic plot is quite messy, and it is now time (sic!) for us to look at summaries of the data using dplyr-like verbs."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-wrangling",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#time-series-wrangling",
    "title": "🕔 Time Series",
    "section": "\n Time-Series Wrangling",
    "text": "Time-Series Wrangling\nWe have now arrived at the need to filter, group, and summarize time-series data. We can do this in two ways, with two packages:\n\n\n\n\n\n\nTiptsibble has dplyr-like functions\n\n\n\nUsing tsibble data, the tsibble package has specialized filter and group_by functions to do with the index (i.e time) variable and the key variables, such as index_by() and group_by_key().\nFiltering based on Qual variables can be done with dplyr. We can use dplyr functions such as group_by, mutate(), filter(), select() and summarise() to work with tsibble objects.\n\n\n\n\n\n\n\n\nTiptimetk also has dplyr-like functions!\n\n\n\nUsing tibbles, timetk provides functions such as summarize_by_time, filter_by_time and slidify that are quite powerful. Again, as with tsibble, dplyr can always be used for other variables (i.e non-time).\n\n\nLet us first see how many observations there are for each combo of keys:\nPBS %&gt;%\n  count()\n# Grouped Counts\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  dplyr::count()\n# dplyr grouping\nPBS %&gt;%\n  dplyr::group_by(ATC1, ATC2) %&gt;%\n  dplyr::count()\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nWe have 336 combinations of Qualitative variables, each combo containing 204 observations (except some! Take a look!): so let us filter for a few such combinations and plot:\n# Costs\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  gf_line(Cost ~ Month,\n    colour = ~Type,\n    data = .\n  ) %&gt;%\n  gf_point(title = \"Costs, per Month\")\n# Scripts\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  gf_line(Scripts ~ Month,\n    colour = ~Type,\n    data = .\n  ) %&gt;%\n  gf_point(title = \"Scripts, per Month\")\n# Costs variable for a specific combo of Qual variables(keys)\nPBS %&gt;%\n  dplyr::filter(\n    Concession == \"General\",\n    ATC1 == \"A\",\n    ATC2 == \"A10\"\n  ) %&gt;%\n  gf_line(Cost ~ Month,\n    colour = ~Type,\n    data = .\n  ) %&gt;%\n  gf_point(title = \"Costs, per Month for General/A/A10 category patients\")\n# Scripts variable for a specific combo of Qual variables(keys)\nPBS %&gt;%\n  dplyr::filter(\n    Concession == \"General\",\n    ATC1 == \"A\",\n    ATC2 == \"A10\"\n  ) %&gt;%\n  gf_line(Scripts ~ Month,\n    colour = ~Type,\n    data = .\n  ) %&gt;%\n  gf_point(title = \"Scripts, per Month for General/A/A10 category patients\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs can be seen, very different time patterns based on the two Types of payment methods, and also with Costs and Scripts. Strongly seasonal for both, with seasonal variation increasing over the years, a clear sign of a multiplicative time series. There is a strong upward trend with both types of subsidies, Safety net and Co-payments. But these trends are somewhat different in magnitude for specific combinations of ATC1 and ATC2 categories.\nWe can use tsibble’s dplyr-like commands to develop summaries by year, quarter, month(original data): Look carefully at the new time variable created each time:\n\n# Original Data\nPBS\n\n\n  \n\n\n# Cost Summary by Month, which is the original data\n# Only grouping happens here\n# New Variable Name to make grouping visible\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  tsibble::index_by(Month_Group = Month) %&gt;%\n  dplyr::summarise(across(\n    .cols = c(Cost, Scripts),\n    .fn = mean,\n    .names = \"mean_{.col}\"\n  ))\n\n\n  \n\n\n# Cost Summary by Quarter\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  tsibble::index_by(Year_Quarter = yearquarter(Month)) %&gt;% # And the change here!\n  dplyr::summarise(across(\n    .cols = c(Cost, Scripts),\n    .fn = mean,\n    .names = \"mean_{.col}\"\n  ))\n\n\n  \n\n\n# Cost Summary by Year\nPBS %&gt;%\n  tsibble::group_by_key(ATC1, ATC2, Concession, Type) %&gt;%\n  index_by(Year_Group = year(Month)) %&gt;% # Note this change!!!\n  dplyr::summarise(across(\n    .cols = c(Cost, Scripts),\n    .fn = mean,\n    .names = \"mean_{.col}\"\n  ))\n\n\n  \n\n\n\nFinally, it may be a good idea to convert some tibble into a tsibble to leverage some of functions that tsibble offers:\n\nbirths_tsibble &lt;- births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  # Convert to tsibble\n  tsibble::as_tsibble(index = date) # Time Variable\n\nbirths_tsibble\n\n\n  \n\n\n\nThis is DAILY data of course. Let us say we want to group by month and plot mean monthly births as before, but now using tsibble and the index variable:\n\n\ntsibble vs timetk: Basic Plot\ntsibble vs timetk: Grouped Plot 1\ntsibble vs timetk: Grouped Plot 2\n\n\n\nbirths_tsibble %&gt;%\n  gf_line(births ~ date,\n    data = .,\n    title = \"Basic tsibble plotted with ggformula\"\n  )\n# timetk **can** plot tsibbles.\nbirths_tsibble %&gt;%\n  timetk::plot_time_series(\n    .date_var = date,\n    .value = births,\n    .title = \"Tsibble Plotted with timetk\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbirths_tsibble %&gt;%\n  tsibble::index_by(month_index = ~ tsibble::yearmonth(.)) %&gt;%\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  gf_point(mean_births ~ month_index,\n    data = .,\n    title = \"Monthly Aggregate with tsibble\"\n  ) %&gt;%\n  gf_line() %&gt;%\n  gf_smooth(se = FALSE, method = \"loess\")\nbirths_timeseries %&gt;%\n  # timetk cannot wrangle tsibbles\n  # timetk needs tibble or data frame\n  timetk::summarise_by_time(\n    .date_var = date,\n    .by = \"month\",\n    mean = mean(births)\n  ) %&gt;%\n  timetk::plot_time_series(date, mean,\n    .title = \"Monthly aggregate births with timetk\",\n    .x_lab = \"year\",\n    .y_lab = \"Mean Monthly Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApart from the bump during in 2006-2007, there are also seasonal trends that repeat each year, which we glimpsed earlier.\n\n\nbirths_tsibble %&gt;%\n  tsibble::index_by(year_index = ~ lubridate::year(.)) %&gt;%\n  dplyr::summarise(mean_births = mean(births, na.rm = TRUE)) %&gt;%\n  gf_point(mean_births ~ year_index, data = .) %&gt;%\n  gf_line() %&gt;%\n  gf_smooth(se = FALSE, method = \"loess\")\nbirths_timeseries %&gt;%\n  timetk::summarise_by_time(\n    .date_var = date,\n    .by = \"year\",\n    mean = mean(births)\n  ) %&gt;%\n  timetk::plot_time_series(date, mean,\n    .title = \"Yearly aggregate births with timetk\",\n    .x_lab = \"year\",\n    .y_lab = \"Mean Yearly Births\"\n  )"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#candle-stick-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#candle-stick-plots",
    "title": "🕔 Time Series",
    "section": "\n Candle-Stick Plots",
    "text": "Candle-Stick Plots\nHmm…can we try to plot boxplots over time (Candle-Stick Plots)? Over month / quarter or year?\n\n Monthly Box Plots\nbirths_tsibble %&gt;%\n  index_by(month_index = ~ yearmonth(.)) %&gt;%\n  # 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ date,\n    group = ~month_index,\n    fill = ~month_index, data = .\n  )\n# plot the groups\n# 180 plots!!\n\nbirths_timeseries %&gt;%\n  # timetk::summarise_by_time(.date_var = date,\n  #                           .by = \"month\",\n  #                           mean = mean(births)) %&gt;%\n  timetk::plot_time_series_boxplot(date, births,\n    .title = \"Monthly births with timetk\",\n    .x_lab = \"year\", .period = \"month\",\n    .y_lab = \"Mean Monthly Births\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Quarterly boxplots\n\nbirths_tsibble %&gt;%\n  index_by(qrtr_index = ~ yearquarter(.)) %&gt;% # 60 quarters over 15 years\n  # No need to summarise, since we want boxplots per year / month\n  gf_boxplot(births ~ date,\n    group = ~qrtr_index,\n    fill = ~qrtr_index,\n    data = .\n  ) # 60 plots!!\n\n\n\n\n\n\nbirths_timeseries %&gt;%\n  timetk::plot_time_series_boxplot(date, births,\n    .title = \"Quarterly births with timetk\",\n    .x_lab = \"year\", .period = \"quarter\",\n    .y_lab = \"Mean Monthly Births\"\n  )\n\n\n\n\n\n\n Yearwise boxplots\n\nbirths_tsibble %&gt;%\n  index_by(year_index = ~ lubridate::year(.)) %&gt;% # 15 years, 15 groups\n  # No need to summarise, since we want boxplots per year / month\n\n  gf_boxplot(births ~ date,\n    group = ~year_index,\n    fill = ~year_index,\n    data = .\n  ) %&gt;% # plot the groups 15 plots\n  gf_labs(title = \"Yearly aggregate births with ggformula\") %&gt;%\n  gf_theme(scale_fill_distiller(palette = \"Spectral\"))\n\n\n\n\n\n\nbirths_timeseries %&gt;%\n  timetk::plot_time_series_boxplot(date, births,\n    .title = \"Yearly aggregate births with timetk\",\n    .x_lab = \"year\", .period = \"year\",\n    .y_lab = \"Births\"\n  )\n\n\n\n\n\nAlthough the graphs are very busy, they do reveal seasonality trends at different periods.\n\nHow about a heatmap? We can cook up a categorical variable based on the number of births (low, fine, high) and use that to create a heatmap:\n\nbirths_2000_2014 %&gt;%\n  mutate(birthrate = case_when(\n    births &gt;= 10000 ~ \"high\",\n    births &lt;= 8000 ~ \"low\",\n    TRUE ~ \"fine\"\n  )) %&gt;%\n  gf_tile(\n    data = .,\n    year ~ month,\n    fill = ~birthrate,\n    color = \"black\"\n  ) %&gt;%\n  gf_theme(scale_x_time(\n    breaks = 1:12,\n    labels = c(\n      \"Jan\", \"Feb\", \"Mar\", \"Apr\",\n      \"May\", \"Jun\", \"Jul\", \"Aug\",\n      \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n    )\n  )) %&gt;%\n  gf_theme(theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#conclusion",
    "title": "🕔 Time Series",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have seen a good few data formats for time series, and how to work with them and plot them. We have also seen how to decompose time series into periodic and aperiodic components, which can be used to make business decisions."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#your-turn",
    "title": "🕔 Time Series",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nChoose some of the datasets in the tsdl and in the tsibbledata packages. Plot basic, filtered and model-based graphs for these and interpret."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references",
    "title": "🕔 Time Series",
    "section": "\n References",
    "text": "References\n\nRobert Hyndman, Forecasting: Principles and Practice (Third Edition). available online\nTime Series Analysis at Our Coding Club"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#readings",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#readings",
    "title": "🕔 Time Series",
    "section": "\n Readings",
    "text": "Readings\n\nThe Nuclear Threat—The Shadow Peace, part 1\n11 Ways to Visualize Changes Over Time – A Guide\nWhat is seasonal adjustment and why is it used?\nThe start-at-zero rule"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#extra-stuff",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#extra-stuff",
    "title": "🕔 Time Series",
    "section": "Extra Stuff",
    "text": "Extra Stuff\nUsing tsbox and TSstudio\nLet us create a time variable in our dataset now:\n\n\ntsbox::ts_plot needs just the date and the births columns to plot with and not be confused by the other numerical columns, so let us create a single date column from these three, but retain them for now.\n\nTSstudio::ts_plot also needs a date column.\n\nSo there are several numerical variables for year, month, and day_of_month, day_of_week, and of course the births on a daily basis.\nWe use the lubridate package from the tidyverse:\n\nbirths_timeseries &lt;-\n  births_2000_2014 %&gt;%\n  mutate(date = lubridate::make_date(\n    year = year,\n    month = month,\n    day = date_of_month\n  )) %&gt;%\n  select(date, births, year, month, date_of_month, day_of_week)\n\nbirths_timeseries\n\n\n  \n\n\n\n\n\n\n\n\n\nTipExtract from help(tsbox)\n\n\n\nIn data frames, i.e., in a data.frame, a data.table, or a tibble, tsbox stores one or multiple time series in the ‘long’ format. tsbox detects a value, a time column, and zero, one or several id columns. Column detection is done in the following order:\n\nStarting on the right, the first first numeric or integer column is used as value column.\n\nUsing the remaining columns and starting on the right again, the first Date, POSIXct, numeric or character column is used as time column. character strings are parsed by anytime::anytime(). The timestamp, time, indicates the beginning of a period.\n\n\nAll remaining columns are id columns. Each unique combination of id columns points to a (unique) time series.\n\nAlternatively, the time column and the value column to be explicitly named as time and value. If explicit names are used, the column order will be ignored. If columns are detected automatically, a message is returned.\n\n\nPlotting this directly, after selecting the relevant variables, so that they will be auto-detected:\n\nbirths_timeseries %&gt;%\n  select(date, births) %&gt;%\n  tsbox::ts_plot()\n\n[time]: 'date' [value]: 'births' \n\n\n\n\n\n\n\n\n\nbirths_timeseries %&gt;%\n  select(date, births) %&gt;%\n  TSstudio::ts_plot(\n    Xtitle = \"Year\",\n    Ytitle = \"Births\",\n    title = \"Births Time Series\",\n    Xgrid = TRUE, Ygrid = TRUE,\n    slider = TRUE,\n    width = 1\n  ) # linewidth\n\n\n\n\n\nQuite messy, as before. We need use the summarised data, as before. We will do this in the next section."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references-1",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#references-1",
    "title": "🕔 Time Series",
    "section": "\n References",
    "text": "References\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/50-Time/files/time-interactive.html#footnotes",
    "title": "🕔 Time Series",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.gapminder.org/data/↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html",
    "title": "\n Change",
    "section": "",
    "text": "Tutorial   \n  R (Interactive Graphs\n\n\n\n\n“The world says: ‘You have needs – satisfy them. You have as much right as the rich and the mighty. Don’t hesitate to satisfy your needs; indeed, expand your needs and demand more.’ This is the worldly doctrine of today. And they believe that this is freedom. The result for the rich is isolation and suicide, for the poor, envy and murder.”\n— Fyodor Dostoevsky",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#sec-slides-and-tutorials",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#sec-slides-and-tutorials",
    "title": "\n Change",
    "section": "",
    "text": "Tutorial   \n  R (Interactive Graphs\n\n\n\n\n“The world says: ‘You have needs – satisfy them. You have as much right as the rich and the mighty. Don’t hesitate to satisfy your needs; indeed, expand your needs and demand more.’ This is the worldly doctrine of today. And they believe that this is freedom. The result for the rich is isolation and suicide, for the poor, envy and murder.”\n— Fyodor Dostoevsky",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#setting-up-r-packages",
    "title": "\n Change",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # Tidy data processing and plotting\nlibrary(ggformula) # Formula based plots\nlibrary(mosaic) # Our go-to package\nlibrary(skimr) # Another Data inspection package\n\nlibrary(GGally) # Corr plots\nlibrary(corrplot) # More corrplots\nlibrary(ggExtra) # Making Combination Plots\n\n# library(devetools)\n# devtools::install_github(\"rpruim/Lock5withR\")\nlibrary(Lock5withR) # Datasets\nlibrary(palmerpenguins) # A famous dataset\n\nlibrary(easystats) # Easy Statistical Analysis and Charts\nlibrary(correlation) # Different Types of Correlations\n# From the easystats collection of packages\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-graphs-will-we-see-today",
    "title": "\n Change",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\nQuant\nQuant\nScatter Plot\n\n\n\n\n\nSome of the very basic and commonly used plots for data are:\n\nScatter Plot for two variables\nContour Plot\nScatter Plot with Confidence Ellipses\nPairwise Correlation Plots for multiple variables\nCorrelogram for multiple variables\nHeatmap for multiple variables\nErrorbar chart for multiple variables\nCombination chart with marginal densities",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Change",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n1\n                  How Many / Much / Heavy? Few? Seldom? Often? When?\n                  Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\n                  Quantitative/Ratio\n                  Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\n                  Correlation",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#inspiration",
    "title": "\n Change",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\n\n\n\nFigure 1: ScatterPlot Inspiration http://www.calamitiesofnature.com/archive/?c=559\n\n\nDoes belief in Evolution depend upon the GSP of of the country? Where is the US in all of this? Does the Bible Belt tip the scales here?\nAnd India?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-is-correlation",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#what-is-correlation",
    "title": "\n Change",
    "section": "\n What is Correlation?",
    "text": "What is Correlation?\nOne of the basic Questions we would have of our data is: Does some variable depend upon another in some way? Does \\(y\\) vary with \\(x\\)? A Correlation Test is designed to answer exactly this question.\nThe word correlation is used in everyday life to denote some form of association. We might say that we have noticed a correlation between rainy days and reduced sales at supermarkets. However, in statistical terms we use correlation to denote association between two quantitative variables. We also assume that the association is linear, that one variable increases or decreases a fixed amount for a unit increase or decrease in the other. The other technique that is often used in these circumstances is regression, which involves estimating the best straight line to summarise the association.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#pearson-correlation-coefficient",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#pearson-correlation-coefficient",
    "title": "\n Change",
    "section": "\n Pearson Correlation coefficient",
    "text": "Pearson Correlation coefficient\nThe degree of association is measured by a correlation coefficient, denoted by r. It is sometimes called Pearson’s correlation coefficient after its originator and is a measure of linear association. (If a curved line is needed to express the relationship, other and more complicated measures of the correlation must be used.)\nThe correlation coefficient is measured on a scale that varies from + 1 through 0 to – 1. Complete correlation between two variables is expressed by either + 1 or -1. When one variable increases as the other increases the correlation is positive; when one decreases as the other increases it is negative.\nIn formal terms, the correlation between two variables \\(x\\) and \\(y\\) is defined as\n\\[\n\\rho = E\\left[\\frac{(x - \\mu_{x}) * (y - \\mu_{y})}{(\\sigma_x)*(\\sigma_y)}\\right]\n\\]\nwhere \\(E\\) is the expectation operator ( i.e taking mean ). Think of this as the average of the products of two scaled variables.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipPearson Correlation uses z-scores\n\n\n\nWe can see \\((x-\\mu_x)/\\sigma_x\\) is a centering and scaling of the variable \\(x\\). Recall from our discussion on Distributions that this is called the z-score of x.\n\n\nPearson correlation assumes that the relationship between the two variables is linear. There are of course many other types of correlation measures: some which work when this is not so. Type vignette(\"types\", package = \"correlation\") in your Console to see the vignette from the correlation package that discusses various types of correlation measures.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#case-study-1-hollywoodmovies2011-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#case-study-1-hollywoodmovies2011-dataset",
    "title": "\n Change",
    "section": "\n Case Study-1: HollywoodMovies2011 dataset",
    "text": "Case Study-1: HollywoodMovies2011 dataset\nLet us look at the HollywoodMovies2011 dataset from the Lock5withR package. The dataset is also available by clicking the icon below ( in case you are not able to install Lock5withR):\n\n\n Hollywood Movies Dataset",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#inspecting-the-data",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#inspecting-the-data",
    "title": "\n Change",
    "section": "\n Inspecting the Data",
    "text": "Inspecting the Data\n\n\n glimpse\n skimr\n mosaic\n web-r\n\n\n\n\nHollywoodMovies2011 -&gt; movies\nglimpse(movies)\n\nRows: 136\nColumns: 14\n$ Movie             &lt;fct&gt; \"Insidious\", \"Paranormal Activity 3\", \"Bad Teacher\",…\n$ LeadStudio        &lt;fct&gt; Sony, Independent, Independent, Warner Bros, Relativ…\n$ RottenTomatoes    &lt;int&gt; 67, 68, 44, 96, 90, 93, 75, 35, 63, 69, 69, 49, 26, …\n$ AudienceScore     &lt;int&gt; 65, 58, 38, 92, 77, 84, 91, 58, 74, 73, 72, 57, 68, …\n$ Story             &lt;fct&gt; Monster Force, Monster Force, Comedy, Rivalry, Rival…\n$ Genre             &lt;fct&gt; Horror, Horror, Comedy, Fantasy, Comedy, Romance, Dr…\n$ TheatersOpenWeek  &lt;int&gt; 2408, 3321, 3049, 4375, 2918, 944, 2534, 3615, NA, 2…\n$ BOAverageOpenWeek &lt;int&gt; 5511, 15829, 10365, 38672, 8995, 6177, 10278, 23775,…\n$ DomesticGross     &lt;dbl&gt; 54.01, 103.66, 100.29, 381.01, 169.11, 56.18, 169.22…\n$ ForeignGross      &lt;dbl&gt; 43.00, 98.24, 115.90, 947.10, 119.28, 83.00, 30.10, …\n$ WorldGross        &lt;dbl&gt; 97.009, 201.897, 216.196, 1328.111, 288.382, 139.177…\n$ Budget            &lt;dbl&gt; 1.5, 5.0, 20.0, 125.0, 32.5, 17.0, 25.0, 80.0, 0.2, …\n$ Profitability     &lt;dbl&gt; 64.672667, 40.379400, 10.809800, 10.624888, 8.873292…\n$ OpeningWeekend    &lt;dbl&gt; 13.27, 52.57, 31.60, 169.19, 26.25, 5.83, 26.04, 85.…\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Data Inspection\n\n\n\nmovies has 136 observations on the following 14 variables.\n\n\nMovie a factor with many levels\n\nLeadStudio a factor with many levels\n\nRottenTomatoes a numeric vector\n\nAudienceScore a numeric vector\n\nStory a factor with many levels\n\nGenre a factor with levels Action, Adventure, Animation, Comedy, Drama, Fantasy, Horror, Romance, Thriller.\n\n\nTheatersOpenWeek a numeric vector. No. of theatres.\n\nBOAverageOpenWeek a numeric vector.\n\nDomesticGross a numeric vector. In million USD.\n\nForeignGross a numeric vector. In million USD.\n\nWorldGross a numeric vector. In million USD.\n\nBudget a numeric vector. In million USD.\n\nProfitability a numeric vector. A ratio\n\nOpeningWeekend a numeric vector. In million USD.\n\nThere are no missing values in the Qual variables; but some entries in the Quant variables are missing. skim throws a warning that we may need to examine later.\n\n\nLet us look at the Quant variables: are these related in anyway? Could the relationship between any two Quant variables also depend upon the level of a Qual variable?",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#scatter-plots",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#scatter-plots",
    "title": "\n Change",
    "section": "\n Scatter Plots",
    "text": "Scatter Plots\nWhich are the numeric variables in movies?\n\n\n R\n web-r\n\n\n\n\nmovies_quant &lt;- movies %&gt;%\n  drop_na() %&gt;%\n  select(where(is.numeric))\nmovies_quant\n\n\n  \n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nNow let us plot their relationships.\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(DomesticGross ~ WorldGross) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Gross Earnings: Domestics vs World\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(Profitability ~ OpeningWeekend) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movies: Does Opening Week Earnings indicate Profitability?\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(RottenTomatoes ~ AudienceScore) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Tomatoes vs Audience\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nWe can split some of the scatter plots using one or other of the Qual variables. For instance, is the relationship between the two ratings the same, regardless of movie genre?\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  gf_point(RottenTomatoes ~ AudienceScore,\n    color = ~Genre\n  ) %&gt;%\n  gf_lm() %&gt;%\n  gf_labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Trends by Genre\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = DomesticGross, y = WorldGross)) +\n  geom_point() +\n  geom_lm() +\n  labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Gross Earnings: Domestics vs World\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(OpeningWeekend, Profitability)) +\n  geom_point() +\n  geom_lm() +\n  labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movies: Does Opening Week Earnings indicate Profitability?\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(AudienceScore, RottenTomatoes)) +\n  geom_point() +\n  geom_lm() +\n  labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Tomatoes vs Audience\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmovies %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(RottenTomatoes, AudienceScore, color = Genre)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    title = \"Scatter Plot\",\n    subtitle = \"Movie Ratings: Trends by Genre\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insight from movies scatter plots\n\n\n\nWe have fitted a trend line to each of the scatter plots.\n\n\nDomesticGross and World Gross are related, though there are fewer movies at the high end of DomesticGross…\n\nAudienceScore and RottenTomatoes seem clearly related…both increase together.\n\nOpeningWeek and Profitability are also related in a linear way. There are just two movies which have been extremely profitable..but they do not influence the slope of the trend line too much, because of their location midway in the range of OpeningWeek. Influence is something that is a key concept in Linear Regression.\nBy and large, there are only small variations in slope across Genres.\n\n\n\n\n\n\n\n\n\nImportantIndependent and Dependent Variables\n\n\n\nNote that we have rather arbitrarily taken AudienceScore as the independent variable, to be plotted on the x-axis, and RottenTomatoes on the y-axis. It could easily have been the other way around, based on our Research Question. Datasets are gathered with specific Research Hypotheses in mind, so check the help file and also with the person who gathered the data about what variable they are interested in!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#quantizing-correlation",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#quantizing-correlation",
    "title": "\n Change",
    "section": "\n Quantizing Correlation",
    "text": "Quantizing Correlation\nSo we see that there are visible relationships between Quant variables. How do we quantize this relationship, into a correlation score?\nThere are two ways: using the GGally and corplot packages, and doing a formal correlation test with the mosaic package.\n\n\n\nUsing GGally\nUsing corrplot\n\n\n\nBy default, GGally::ggpairs() provides:\n\ntwo different comparisons of each pair of columns\ndisplays either the density or count of the respective variable along the diagonal. \nWith different parameter settings, the diagonal can be replaced with the axis values and variable labels.\n\n\n\n\n\nGGally::ggpairs(\n  movies %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"RottenTomatoes\", \"AudienceScore\", \"DomesticGross\", \"ForeignGross\"\n  ),\n  switch = \"both\",\n  # axis labels in more traditional locations(left and bottom)\n\n  progress = FALSE,\n  # no compute progress messages needed\n\n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"),\n  # choosing histogram,not density\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, se = FALSE)),\n  title = \"Movies Data Correlations Plot #1\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insight from Pairs Plot#1\n\n\n\n\nAs we saw earlier from the Scatter Plot, AudienceScore and RottenTomatoes are well correlated, with a correlation score of \\(0.833\\)\n\n\nDomesticGross and ForeignGross are also extremely well correlated, with a score of \\(0.873\\).\nBoth these correlation scores are highly significant, with three stars. (We will speak of significance in a while.)\nNone of the other pairs of variables have good correlation scores.\nNote in passing that both the “Gross” related variables have highly skewed distributions. That is the nature of the movie business!\n\n\n\nLet us also try a few other variables, related to budget and profits. For instance, it would be interesting to see the relationship between Budget and Profitability and even either of the “gross” earnings and Profitability.\n\n\n\nGGally::ggpairs(\n  movies %&gt;% drop_na(),\n  # Select Quant variables only for now\n  columns = c(\n    \"Budget\", \"Profitability\", \"DomesticGross\", \"ForeignGross\"\n  ),\n  switch = \"both\",\n  # axis labels in more traditional locations(left and bottom)\n\n  progress = FALSE,\n  # no compute progress messages needed\n\n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"),\n  # choosing histogram,not density\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.3, se = FALSE)),\n  title = \"Movies Data Correlations Plot #2\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insight from Pairs Plot #2\n\n\n\n\nThe Budget variable has good correlation scores with DomesticGross and ForeignGross\n\n\nProfitability and Budget seem to have a very slight negative correlation, but this does not appear to be significant.\n\n\n\n\n\nIn this chart, the correlation between pairs of variables is shown symbolically as coloured shapes or colours. Circles, Squares, and Ellipse for example.\n\nThe size, colour, and “orientation” of the shapes in question symbolically represent the strength and polarity of the correlation scores.\nThe direction of the semi-major axis + the colour of the ellipse indicate whether the correlation score is positive or negative;\nAnd the more eccentric the ellipse, the higher is the correlation score in value.\n\n\n\n\n\n\n\nNote\n\n\n\nWhereas GGally computes the correlation scores, corplot “merely” displays them in an evocative way. We need to compute the correlations a priori.\n\n\nNote also:\n\n\n\n\n\n\nTip\n\n\n\nR package corrplot provides a visual exploratory tool on correlation matrix that supports automatic variable reordering to help detect hidden patterns among variables. corrplot is very easy to use and provides a rich array of plotting options in visualization method, graphic layout, color, legend, text labels, etc. It also provides p-values and confidence intervals to help users determine the statistical significance of the correlations.\n\n\n\n# library(corrplot)\nmydata_cor &lt;- cor(movies_quant)\nmydata_cor %&gt;%\n  knitr::kable(caption = \"Correlation Scores Matrix\")\n\n\nCorrelation Scores Matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRottenTomatoes\nAudienceScore\nTheatersOpenWeek\nBOAverageOpenWeek\nDomesticGross\nForeignGross\nWorldGross\nBudget\nProfitability\nOpeningWeekend\n\n\n\nRottenTomatoes\n1.0000000\n0.8329740\n-0.0873543\n0.1823480\n0.2085935\n0.0979132\n0.1356232\n-0.0147887\n0.1502764\n0.0986304\n\n\nAudienceScore\n0.8329740\n1.0000000\n0.0259118\n0.1851768\n0.3849406\n0.2557891\n0.3037927\n0.1268649\n0.1047582\n0.2695132\n\n\nTheatersOpenWeek\n-0.0873543\n0.0259118\n1.0000000\n0.0117674\n0.5981162\n0.4850569\n0.5344582\n0.5924941\n0.0547807\n0.5977724\n\n\nBOAverageOpenWeek\n0.1823480\n0.1851768\n0.0117674\n1.0000000\n0.4713164\n0.4522253\n0.4710352\n0.2880262\n0.0964176\n0.5043684\n\n\nDomesticGross\n0.2085935\n0.3849406\n0.5981162\n0.4713164\n1.0000000\n0.8725927\n0.9374780\n0.6497274\n0.1812387\n0.9232259\n\n\nForeignGross\n0.0979132\n0.2557891\n0.4850569\n0.4522253\n0.8725927\n1.0000000\n0.9880383\n0.6707613\n0.1230330\n0.8487202\n\n\nWorldGross\n0.1356232\n0.3037927\n0.5344582\n0.4710352\n0.9374780\n0.9880383\n1.0000000\n0.6830783\n0.1448857\n0.8962294\n\n\nBudget\n-0.0147887\n0.1268649\n0.5924941\n0.2880262\n0.6497274\n0.6707613\n0.6830783\n1.0000000\n-0.1437862\n0.6228180\n\n\nProfitability\n0.1502764\n0.1047582\n0.0547807\n0.0964176\n0.1812387\n0.1230330\n0.1448857\n-0.1437862\n1.0000000\n0.1713962\n\n\nOpeningWeekend\n0.0986304\n0.2695132\n0.5977724\n0.5043684\n0.9232259\n0.8487202\n0.8962294\n0.6228180\n0.1713962\n1.0000000\n\n\n\n\n\n\n\n\n## View the matrix\ncorrplot::corrplot(mydata_cor,\n  method = \"number\",\n  number.cex = 0.6,\n  cl.cex = 0.6, tl.cex = 0.6\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Default plot with circles\ncorrplot(mydata_cor,\n  method = \"circle\",\n  main = \"Correlogram with Circles\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Ellipse plot\ncorrplot(mydata_cor,\n  method = \"ellipse\",\n  main = \"Correlogram with Ellipes\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Heatmap\ncorrplot(mydata_cor,\n  method = \"color\", ## US Spelling only\n  main = \"Correlogram\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Heatmap with numbers\ncorrplot.mixed(mydata_cor,\n  lower = \"color\", number.cex = 0.6,\n  cl.cex = 0.6, tl.cex = 0.6,\n  upper = \"number\",\n  tl.pos = \"l\",\n  main = \"Heatmap?\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from corplots\n\n\n\n\nMost of the variables here have positive correlations, many of them are significant\n\n\n\n\n\n\n\nDoing a Correlation Test\nCorrelations scores can be obtained by conducting a formal test in R. We will use the mosaic function cor_test to get these results:\n\nmosaic::cor_test(Profitability ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Profitability vs Budget\"\n  )\n\n\nMovie Profitability vs Budget\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n-0.08\n-0.96\n0.34\n132\n-0.25\n0.09\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nmosaic::cor_test(DomesticGross ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Domestic Gross vs Budget\"\n  )\n\n\nMovie Domestic Gross vs Budget\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.7\n11.06\n0\n131\n0.6\n0.77\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nmosaic::cor_test(ForeignGross ~ Budget, data = movies) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Movie Foreign Gross vs Budget\"\n  )\n\n\nMovie Foreign Gross vs Budget\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.69\n10.22\n0\n118\n0.58\n0.77\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from Correlation Tests\n\n\n\nThe budget and profitability are not well correlated, sadly. We see this from the p.value which is \\(0.34\\) and the confidence values for the correlation estimate which also cover \\(0\\).\nHowever, both DomesticGross and ForeignGross are well correlated with Budget. Look at the p.value (=0) and the confidence intervals which are unipolar.\n\n\n\n The ErrorBar Plot for Correlations\nAs stated earlier, in our dataset we have a specific dependent or target variable, which represents the outcome of our experiment or our business situation. The remaining variables are usually independent or predictor variables. A very useful thing to know, and to view, would be the correlations of all independent variables. Using the correlation package from the easystats family of R packages, this can be very easily achieved. Let us quickly do this for the familiar mtcars dataset: we will quickly glimpse it, identify the target variable, and plot the correlations:\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n## Target variable: mpg\n## Calculate all correlations\ncor &lt;- correlation::correlation(mtcars)\ncor\n\n\n  \n\n\n\nWe see correlation between all pairs of variables. We need to choose just those with target variable mpg:\n\n\n\n\ncor %&gt;%\n  # Filter for target variable `mpg` and plot\n  filter(Parameter1 == \"mpg\") %&gt;%\n  gf_point(r ~ reorder(Parameter2, r), size = 4) %&gt;%\n  gf_errorbar(CI_low + CI_high ~ reorder(Parameter2, r),\n    width = 0.5\n  ) %&gt;%\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;%\n  gf_labs(\n    title = \"Correlation Errorbar Chart\",\n    subtitle = \"Target variable: mpg\",\n    x = \"Predictor Variable\",\n    y = \"Correlation Score with mpg\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from ErrorBar Plot\n\n\n\n\nSeveral variables are negatively correlated and some are positively correlated with ’mpg`. (The grey line shows “zero correlation”)\nSince none of the error bars straddle zero, the correlations are mostly significant.\n\n\n\n\n\n A New Combination Plot…\nSometimes, a simple scatter, or density alone, or viewed next to one another is not adequate to develop, or convey, our insight. We might just need a combination density + scatter plot. Such a plot can be be constructed from the ground up using ggformula or ggplot; however, there is a nice package called ggExtra that allows the creation of a powerful combination plot:\n\ntheme_set(theme_custom())\n\npenguins %&gt;%\n  drop_na() %&gt;%\n  gf_point(body_mass_g ~ flipper_length_mm, colour = ~species) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_refine(scale_colour_brewer(palette = \"Accent\")) %&gt;%\n  gf_labs(title = \"Scatter Plot with Marginal Densities\") %&gt;%\n  ggExtra::ggMarginal(\n    type = \"density\", groupColour = TRUE,\n    groupFill = TRUE, margins = \"both\"\n  )\n\n\n\n\n\n\n\nAn Interactive Correlation Game\nHead off to this interactive game website where you can play with correlations!\nhttps://openintro.shinyapps.io/correlation_game/\nSimpson’s Paradox\n\n\nSee how the overall correlation/regression line slopes upward, whereas that for the individual groups slopes downward!! This is an example of Simpson’s Paradox!\n\n Your Turn\n\nTry to play this online Correlation Game.\n\n\n\n\n\n\n\nNote2. School Expenditure and Grades.\n\n\n\n Download the School Data \n\n\n\n\n\n\n\n\nNote3. Gas Prices and Consumption\n\n\n\nAs described here. Note the log-transformed Quant data…why do you reckon this was done in the data set itself?\n Download the Gas Consumption Data \n\n\n\n\n\n\n\n\nNote4. Horror Movies (Bah.You awful people..)\n\n\n\n Download the Horror Movie Data \n\n\n\n\n\n\n\n\nNote6. Food Delivery Times\n\n\n\n Download the Food Delivery Data \n\n\n\n Wait, But Why?\n\nScatter Plots, when they show “linear” clouds, tell us that there is some relationship between two Quant variables we have just plotted\nIf so, then if one is the target variable you are trying to design for, then the other independent, or controllable, variable is something you might want to design with.\n\n\n\n\n\n\n\nImportant\n\n\n\nTarget variables are usually plotted on the Y-axis, while Predictor variables are on the X-Axis, in a Scatter Plot. Why? Because \\(y = mx + c\\) !\n\n\n\nCorrelation scores are good indicators of things that are, well, related. While one variable may not necessarily cause another, a good correlation score may indicate how to chose a good predictor.\nThat is something we will see when we examine Linear Regression\n\nAlways, always, plot and test your data! Both numerical summaries as tables, and graphical summaries as charts, are necessary! See below!!\n\n\n\n\n\n\n\nWarningAnd How about these datasets?\n\n\n\n\n\n\n\n    \n\n      \n\ndataset\n                mean_x\n                mean_y\n                std_dev_x\n                std_dev_y\n                corr_x_y\n              \n\n\naway\n                  54.26610\n                  47.83472\n                  16.76982\n                  26.93974\n                  -0.06412835\n                \n\nbullseye\n                  54.26873\n                  47.83082\n                  16.76924\n                  26.93573\n                  -0.06858639\n                \n\ncircle\n                  54.26732\n                  47.83772\n                  16.76001\n                  26.93004\n                  -0.06834336\n                \n\ndino\n                  54.26327\n                  47.83225\n                  16.76514\n                  26.93540\n                  -0.06447185\n                \n\ndots\n                  54.26030\n                  47.83983\n                  16.76774\n                  26.93019\n                  -0.06034144\n                \n\nh_lines\n                  54.26144\n                  47.83025\n                  16.76590\n                  26.93988\n                  -0.06171484\n                \n\nhigh_lines\n                  54.26881\n                  47.83545\n                  16.76670\n                  26.94000\n                  -0.06850422\n                \n\nslant_down\n                  54.26785\n                  47.83590\n                  16.76676\n                  26.93610\n                  -0.06897974\n                \n\nslant_up\n                  54.26588\n                  47.83150\n                  16.76885\n                  26.93861\n                  -0.06860921\n                \n\nstar\n                  54.26734\n                  47.83955\n                  16.76896\n                  26.93027\n                  -0.06296110\n                \n\nv_lines\n                  54.26993\n                  47.83699\n                  16.76996\n                  26.93768\n                  -0.06944557\n                \n\nwide_lines\n                  54.26692\n                  47.83160\n                  16.77000\n                  26.93790\n                  -0.06657523\n                \n\nx_shape\n                  54.26015\n                  47.83972\n                  16.76996\n                  26.93000\n                  -0.06558334\n                \n\n\n\n\n\n\n\n\n\n\n\n\nYes, you did want to plot that cute T-Rex, didn’t you? Here is the data then!!\n\n\n DataSaurus Dirty Dozen\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nCan selling more ice-cream make people drown?\nUse your head about pairs of variables. Do not fall into this trap)\n\n\n\n\n Conclusions\nScatter Plots give a us sense of change; whether it is linear or non-linear. We can get an idea of correlation between variables with a scatter plot. Our workflow for evaluating correlations between target variable and several other predictor variables uses several packages such as GGally, corrplot, correlation, and of course mosaic for correlation tests.\n\n AI Generated Summary and Podcast\nThis document focusses on correlation between quantitative variables. It examines different ways to visualize correlations, including scatter plots and correlograms. The document provides examples of how to use R packages like GGally and corrplot to create these visualizations and correlation tests to assess the strength and significance of relationships between variables. The tutorial uses the HollywoodMovies2011 and mtcars datasets as examples to demonstrate these concepts. \n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp \n\n\n\n\n References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nAntoine Soetewey. Pearson, Spearman and Kendall correlation coefficients by hand https://www.r-bloggers.com/2023/09/pearson-spearman-and-kendall-correlation-coefficients-by-hand/\n\nTaiyun Wei, Viliam Simko. An Introduction to corrplot Package. https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\ndatasauRus\n0.1.9\nGillespie et al. (2025)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggExtra\n0.10.1\nAttali and Baker (2023)\n\n\nlatex2exp\n0.9.6\nMeschiari (2022)\n\n\n\n\n\n\nAttali, Dean, and Christopher Baker. 2023. ggExtra: Add Marginal Histograms to “ggplot2,” and More “ggplot2” Enhancements. https://doi.org/10.32614/CRAN.package.ggExtra.\n\n\nGillespie, Colin, Steph Locke, Rhian Davies, and Lucy D’Agostino McGowan. 2025. datasauRus: Datasets from the Datasaurus Dozen. https://doi.org/10.32614/CRAN.package.datasauRus.\n\n\nMeschiari, Stefano. 2022. Latex2exp: Use LaTeX Expressions in Plots. https://doi.org/10.32614/CRAN.package.latex2exp.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#an-interactive-correlation-game",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#an-interactive-correlation-game",
    "title": "\n Change",
    "section": "An Interactive Correlation Game",
    "text": "An Interactive Correlation Game\nHead off to this interactive game website where you can play with correlations!\nhttps://openintro.shinyapps.io/correlation_game/",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#simpsons-paradox",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#simpsons-paradox",
    "title": "\n Change",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\n\n\nSee how the overall correlation/regression line slopes upward, whereas that for the individual groups slopes downward!! This is an example of Simpson’s Paradox!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#your-turn",
    "title": "\n Change",
    "section": "\n Your Turn",
    "text": "Your Turn\n\nTry to play this online Correlation Game.\n\n\n\n\n\n\n\nNote2. School Expenditure and Grades.\n\n\n\n Download the School Data \n\n\n\n\n\n\n\n\nNote3. Gas Prices and Consumption\n\n\n\nAs described here. Note the log-transformed Quant data…why do you reckon this was done in the data set itself?\n Download the Gas Consumption Data \n\n\n\n\n\n\n\n\nNote4. Horror Movies (Bah.You awful people..)\n\n\n\n Download the Horror Movie Data \n\n\n\n\n\n\n\n\nNote6. Food Delivery Times\n\n\n\n Download the Food Delivery Data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#wait-but-why",
    "title": "\n Change",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nScatter Plots, when they show “linear” clouds, tell us that there is some relationship between two Quant variables we have just plotted\nIf so, then if one is the target variable you are trying to design for, then the other independent, or controllable, variable is something you might want to design with.\n\n\n\n\n\n\n\nImportant\n\n\n\nTarget variables are usually plotted on the Y-axis, while Predictor variables are on the X-Axis, in a Scatter Plot. Why? Because \\(y = mx + c\\) !\n\n\n\nCorrelation scores are good indicators of things that are, well, related. While one variable may not necessarily cause another, a good correlation score may indicate how to chose a good predictor.\nThat is something we will see when we examine Linear Regression\n\nAlways, always, plot and test your data! Both numerical summaries as tables, and graphical summaries as charts, are necessary! See below!!\n\n\n\n\n\n\n\nWarningAnd How about these datasets?\n\n\n\n\n\n\n\n    \n\n      \n\ndataset\n                mean_x\n                mean_y\n                std_dev_x\n                std_dev_y\n                corr_x_y\n              \n\n\naway\n                  54.26610\n                  47.83472\n                  16.76982\n                  26.93974\n                  -0.06412835\n                \n\nbullseye\n                  54.26873\n                  47.83082\n                  16.76924\n                  26.93573\n                  -0.06858639\n                \n\ncircle\n                  54.26732\n                  47.83772\n                  16.76001\n                  26.93004\n                  -0.06834336\n                \n\ndino\n                  54.26327\n                  47.83225\n                  16.76514\n                  26.93540\n                  -0.06447185\n                \n\ndots\n                  54.26030\n                  47.83983\n                  16.76774\n                  26.93019\n                  -0.06034144\n                \n\nh_lines\n                  54.26144\n                  47.83025\n                  16.76590\n                  26.93988\n                  -0.06171484\n                \n\nhigh_lines\n                  54.26881\n                  47.83545\n                  16.76670\n                  26.94000\n                  -0.06850422\n                \n\nslant_down\n                  54.26785\n                  47.83590\n                  16.76676\n                  26.93610\n                  -0.06897974\n                \n\nslant_up\n                  54.26588\n                  47.83150\n                  16.76885\n                  26.93861\n                  -0.06860921\n                \n\nstar\n                  54.26734\n                  47.83955\n                  16.76896\n                  26.93027\n                  -0.06296110\n                \n\nv_lines\n                  54.26993\n                  47.83699\n                  16.76996\n                  26.93768\n                  -0.06944557\n                \n\nwide_lines\n                  54.26692\n                  47.83160\n                  16.77000\n                  26.93790\n                  -0.06657523\n                \n\nx_shape\n                  54.26015\n                  47.83972\n                  16.76996\n                  26.93000\n                  -0.06558334\n                \n\n\n\n\n\n\n\n\n\n\n\n\nYes, you did want to plot that cute T-Rex, didn’t you? Here is the data then!!\n\n\n DataSaurus Dirty Dozen\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nCan selling more ice-cream make people drown?\nUse your head about pairs of variables. Do not fall into this trap)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#conclusions",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#conclusions",
    "title": "\n Change",
    "section": "\n Conclusions",
    "text": "Conclusions\nScatter Plots give a us sense of change; whether it is linear or non-linear. We can get an idea of correlation between variables with a scatter plot. Our workflow for evaluating correlations between target variable and several other predictor variables uses several packages such as GGally, corrplot, correlation, and of course mosaic for correlation tests.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#ai-generated-summary-and-podcast",
    "title": "\n Change",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis document focusses on correlation between quantitative variables. It examines different ways to visualize correlations, including scatter plots and correlograms. The document provides examples of how to use R packages like GGally and corrplot to create these visualizations and correlation tests to assess the strength and significance of relationships between variables. The tutorial uses the HollywoodMovies2011 and mtcars datasets as examples to demonstrate these concepts. \n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/index.html#references",
    "title": "\n Change",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nMinimal R using mosaic. https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nAntoine Soetewey. Pearson, Spearman and Kendall correlation coefficients by hand https://www.r-bloggers.com/2023/09/pearson-spearman-and-kendall-correlation-coefficients-by-hand/\n\nTaiyun Wei, Viliam Simko. An Introduction to corrplot Package. https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\ndatasauRus\n0.1.9\nGillespie et al. (2025)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggExtra\n0.10.1\nAttali and Baker (2023)\n\n\nlatex2exp\n0.9.6\nMeschiari (2022)\n\n\n\n\n\n\nAttali, Dean, and Christopher Baker. 2023. ggExtra: Add Marginal Histograms to “ggplot2,” and More “ggplot2” Enhancements. https://doi.org/10.32614/CRAN.package.ggExtra.\n\n\nGillespie, Colin, Steph Locke, Rhian Davies, and Lucy D’Agostino McGowan. 2025. datasauRus: Datasets from the Datasaurus Dozen. https://doi.org/10.32614/CRAN.package.datasauRus.\n\n\nMeschiari, Stefano. 2022. Latex2exp: Use LaTeX Expressions in Plots. https://doi.org/10.32614/CRAN.package.latex2exp.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"icon-park-outline:change\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Change"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#introduction",
    "title": "Tutorial on Correlations in R",
    "section": "",
    "text": "We will create Tables for Correlations, and graphs for Correlations in R. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula)."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#setting-up-r-packages",
    "title": "Tutorial on Correlations in R",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr)\nlibrary(GGally)\nlibrary(corrplot) # For Correlogram plots\nlibrary(broom) # to properly format stat test results\n\nlibrary(mosaicData) # package containing datasets\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nAll R functions seen in the code are clickable links that take you to online documentation about the function. Try!\n\n\n\n\n\n\nTipThe Formula interface\n\n\n\nNote the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, …)\nWith ggformula, one can create any graph/chart using:\ngf_geometry(y ~ x | z, data = mydata)\nOR\nmydata %&gt;% gf_geometry( y ~ x | z )\nThe second method may be preferable, especially if you have done some data manipulation first! More about this later!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "Tutorial on Correlations in R",
    "section": "\n Case Study 1: Galton Dataset from mosaicData\n",
    "text": "Case Study 1: Galton Dataset from mosaicData\n\nLet us inspect what datasets are available in the package mosaicData. Run this command in your Console:\n\n# Run in Console\ndata(package = \"mosaicData\")\n\nThe popup tab shows a lot of datasets we could use. Let us continue to use the famous Galton dataset and inspect it:\n\ndata(\"Galton\")\n\n\n Inspecting the Data\nThe inspect command already gives us a series of statistical measures of different variables of interest. As discussed previously, we can retain the output of inspect and use it in our reports: (there are ways of dressing up these tables too)\n\ngalton_describe &lt;- inspect(Galton)\n\ngalton_describe$categorical\n\n\n  \n\n\ngalton_describe$quantitative\n\n\n  \n\n\n\nTry help(\"Galton\") in your Console. The dataset is described as:\n\nA data frame with 898 observations on the following variables.\n- family a factor with levels for each family\n- father the father’s height (in inches)\n- mother the mother’s height (in inches)\n- sex the child’s sex: F or M\n- height the child’s height as an adult (in inches)\n- nkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\nThere is a lot of Description generated by the mosaic::inspect() command ! Let us also look at the output of skim:\n\nskimr::skim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? If they are Qual, what are the levels? Are they ordered levels? Which variables could have relationships with others? Why? Write down these Questions!\n\n Correlations and Plots\nWhat Questions might we have, that we could answer with a Statistical Measure, or Correlation chart?\n\n\n\n\n\n\nNoteQuestions\n\n\n\nHow does children’s height correlate with that of father and mother? Is this relationship also affected by sex of the child?\nWith this question, height becomes our target variable, which we should always plot on the dependent y-axis.\n\n\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n\n  # Choose the variables we want to plot for\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE, # no compute progress messages needed\n\n  # Choose the diagonal graphs (always single variable! Think!)\n  diag = list(continuous = \"barDiag\"), # choosing histogram,not density\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.1)),\n  title = \"Galton Data Correlations Plot\"\n) +\n\n  theme_bw()\n\n\n\n\n\n\n\nWe note that children’s height is correlated with that of father and mother. The correlations are both positive, and that with father seems to be the larger of the two. ( Look at the slopes of the lines and the values of the correlation scores. )\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat if we group the Quant variables based on a Qual variable, like sex of the child?\n\n\n\n# Pulling out the list of Quant variables from NHANES\ngalton_quant &lt;- galton_describe$quantitative\ngalton_quant$name\n\n[1] \"father\" \"mother\" \"height\" \"nkids\" \n\nGGally::ggpairs(\n  Galton,\n  mapping = aes(colour = sex), # Colour by `sex`\n\n  # Choose the variables we want to plot for\n  columns = c(\"father\", \"mother\", \"height\", \"nkids\"),\n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE, # no compute progress messages needed\n\n  diag = list(continuous = \"barDiag\"),\n\n  # Choose lower triangle graphs, two-variable graphs\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.1)),\n  title = \"Galton Data Correlations Plot\"\n) +\n\n  theme_bw()\n\n\n\n\n\n\n\nThe split scatter plots are useful, as is the split histogram for height: Clearly the correlation of children’s height with father and mother is positive for both sex-es. The other plots, and even some of the correlations scores are not all useful! Just shows everything we can compute is not necessarily useful immediately.\nIn later modules we will see how to plot correlations when the number of variables is larger still.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nCan we plot a Correlogram for this dataset?\n\n\n\n# library(corrplot)\n\ngalton_num_var &lt;- Galton %&gt;% select(father, mother, height, nkids)\ngalton_cor &lt;- cor(galton_num_var)\ngalton_cor %&gt;%\n  corrplot(\n    method = \"ellipse\",\n    type = \"lower\",\n    main = \"Correlogram for Galton dataset\"\n  )\n\n\n\n\n\n\n\nClearly height is positively correlated to father and mother; interestingly, height is negatively correlated ( slightly) with nkids.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nLet us confirm with a correlation test:\n\n\nWe will use the mosaic function cor_test to get these results:\n\nmosaic::cor_test(height ~ father, data = Galton) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Children vs Fathers\"\n  )\n\n\nChildren vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.28\n8.57\n0\n896\n0.21\n0.33\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\nmosaic::cor_test(height ~ mother, data = Galton) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Children vs Mothers\"\n  )\n\n\nChildren vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.2\n6.16\n0\n896\n0.14\n0.26\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\n\n\n\n\n\n\nImportantCorrelation Scores and Uncertainty\n\n\n\nNote how the mosaic::cor_test() reports a correlation score estimate and the p-value for the same. There is also a confidence interval reported for the correlation score, an interval within which we are 95% sure that the true correlation value is to be found.\nNote that GGally::ggpairs() too reports the significance of the correlation scores estimates using *** or **. This indicates the p-value in the scores obtained by GGally; Presumably, there is an internal cor_test that is run for each pair of variables and the p-value and confidence levels are also computed internally.\n\n\nIn both cases, we used the formula \\(height \\sim other-variable\\), in keeping with our idea of height being the dependent, target variable..\nWe also see the p.value for the estimateed correlation is negligible, and the conf.low/conf.high interval does not straddle \\(0\\). These attest to the significance of the correlation score.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat does this correlation look when split by sex of Child?\n\n\n\n# For the sons\n\nmosaic::cor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"M\")\n) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Sons vs Fathers\"\n  )\ncor_test(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"M\")\n) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Sons vs Mothers\"\n  )\n\n# For the daughters\ncor_test(height ~ father,\n  data = Galton %&gt;% filter(sex == \"F\")\n) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Daughters vs Fathers\"\n  )\ncor_test(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"F\")\n) %&gt;%\n  broom::tidy() %&gt;%\n  knitr::kable(\n    digits = 2,\n    caption = \"Daughters vs Mothers\"\n  )\n\n\nSons vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.39\n9.15\n0\n463\n0.31\n0.47\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nSons vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.33\n7.63\n0\n463\n0.25\n0.41\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nDaughters vs Fathers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.46\n10.72\n0\n431\n0.38\n0.53\nPearson’s product-moment correlation\ntwo.sided\n\n\n\nDaughters vs Mothers\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n0.31\n6.86\n0\n431\n0.23\n0.4\nPearson’s product-moment correlation\ntwo.sided\n\n\n\n\nThe same observation as made above ( p.value and confidence intervals) applies here too and tells us that the estimated correlations are significant.\nVisualizing Uncertainty in Correlation Estimates\nWe can also visualize this uncertainty and the confidence levels in a plot too, using gf_errorbar and a handy set of functions within purrr which is part of the tidyverse. Assuming heights is the target variable we want to correlate every other (quantitative) variable against, we can proceed very quickly as follows: we will first plot correlation uncertainty for one pair of variables to develop the intuition, and then for all variables against the one target variable:\n\nmosaic::cor_test(height ~ mother, data = Galton) %&gt;%\n  broom::tidy() %&gt;%\n  # We need a graph not a table\n  # So comment out this line from the earlier code\n  # knitr::kable(digits = 2,caption = \"Children vs Mothers\")\n\n  rowid_to_column(var = \"index\") %&gt;% # Need an index to plot with\n\n  # Uncertainty as error-bars\n  gf_errorbar(conf.high + conf.low ~ index, linewidth = 2) %&gt;%\n  # Estimate as a point\n  gf_point(estimate ~ index, color = \"red\", size = 6) %&gt;%\n  # Labels\n  gf_text(estimate ~ index - 0.2,\n    label = \"Correlation Score = estimate\"\n  ) %&gt;%\n  gf_text(conf.high * 0.98 ~ index - 0.25,\n    label = \"Upper Limit = estimate + conf.high\"\n  ) %&gt;%\n  gf_text(conf.low * 1.04 ~ index - 0.25,\n    label = \"Lower Limit = estimate - conf.low\"\n  ) %&gt;%\n  gf_theme(theme_bw())\n\n\n\n\n\n\n\nWe can now do this for all variables against the target variable height, which we identified in our research question. We will use the iteration capabilities offered by the tidyverse package, purrr:\n\nall_corrs &lt;- Galton %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off height to get all the remaining ones\n  select(-height) %&gt;%\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, Galton$height)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nall_corrs\n\n\n  \n\n\nall_corrs %&gt;%\n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~estimate,\n    width = 0.2,\n    linewidth = ~ -log10(p.value)\n  ) %&gt;%\n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate),\n    color = \"black\"\n  ) %&gt;%\n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;%\n  # Themes,Titles, and Scales\n  gf_labs(\n    x = NULL, y = \"Correlation with height in Galton\",\n    caption = \"Significance = - log10(p.value)\"\n  ) %&gt;%\n  gf_refine(\n\n    # Scale for colour\n    scale_colour_distiller(\"Correlation\", type = \"div\", palette = \"RdBu\"),\n\n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"significance\",\n      range = c(0.5, 4)\n    )\n  ) %&gt;%\n  gf_refine(guides(linewidth = guide_legend(reverse = TRUE))) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\nWe can clearly see the size of the correlations and the confidence intervals marked in this plot. father has somewhat greater correlation with children’s height, as compared to mother. nkids seems to matter very little. This kind of plot will be very useful when we pursue linear regression models.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nHow can we show this correlation in a set of Scatter Plots + Regression Lines? Can we recreate Galton’s famous diagram?\n\n\n# For the sons\ngf_point(height ~ father,\n  data = Galton %&gt;% filter(sex == \"M\"),\n  title = \"Soms and Fathers\"\n) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"M\"),\n  title = \"Sons and Mothers\"\n) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\n# For the daughters\ngf_point(height ~ father,\n  data = Galton %&gt;% filter(sex == \"F\"),\n  title = \"Daughters and Fathers\"\n) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ mother,\n  data = Galton %&gt;% filter(sex == \"F\"),\n  title = \"Daughters and Mothers\"\n) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn approximation to Galton’s famous plot1 (see Wikipedia):\n\n\n\n\n\n\n\n\ngf_point(height ~ (father + mother) / 2, data = Galton) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_density_2d(n = 8) %&gt;%\n  gf_abline(slope = 1) %&gt;%\n  gf_theme(theme_minimal())\ngf_point(height ~ (father + mother) / 2, data = Galton) %&gt;%\n  gf_smooth(method = \"lm\") %&gt;%\n  gf_ellipse(level = 0.95, color = \"red\") %&gt;%\n  gf_ellipse(level = 0.75, color = \"blue\") %&gt;%\n  gf_ellipse(level = 0.5, color = \"green\") %&gt;%\n  gf_abline(slope = 1) %&gt;%\n  gf_theme(theme_minimal())\n\n\n\n\n\n\n\n\n\n\nHow would you interpret this plot2?"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#case-study2-dataset-from-nhanes",
    "title": "Tutorial on Correlations in R",
    "section": "\n Case Study#2: Dataset from NHANES\n",
    "text": "Case Study#2: Dataset from NHANES\n\nLet us look at the NHANES dataset from the package NHANES:\n\ndata(\"NHANES\")\n\n\n Inspecting the Data\nNHANES_describe &lt;- inspect(NHANES)\n\nNHANES_describe$categorical\nNHANES_describe$quantitative\nNHANES\nskimr::skim(NHANES)\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\n\nTry help(\"NHANES\") in your Console.\n\nThis is survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960’s. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination centre (MEC).\n\nThe dataset is described as: A data frame with 100000 observations on 76 variables. Some of these are:\n- Race1 and Race2: factors with 5 and 6 levels respectively\n- Education a factor with 5 levels\n- HHIncomeMid Total annual gross income for the household in US dollars.\n- Age\n- BMI: Body mass index (weight/height2 in kg/m2)\n- Height: Standing height in cm.\n- Weight: Weight in kg &gt; &gt; - Testosterone: Testosterone total (ng/dL) - PhysActiveDays: Number of days in a typical week that participant does moderate or vigorous-intensity activity.\n- CompHrsDay: Number of hours per day on average participant used a computer or gaming device over the past 30 days.\n\n\n\n\n\n\nImportantMissing Data\n\n\n\nWhy do so many of the variables have missing entries? What could be your guess about the Experiment/Survey`?\n\n\nLet us make some counts of the data, since we have so many factors:\n\nNHANES %&gt;% count(Gender)\n\n\n  \n\n\nNHANES %&gt;% count(Race1)\n\n\n  \n\n\nNHANES %&gt;% count(Race3)\n\n\n  \n\n\nNHANES %&gt;% count(Education)\n\n\n  \n\n\nNHANES %&gt;% count(MaritalStatus)\n\n\n  \n\n\n\nThere is a good mix of factors and counts.\nNow we articulate our Research Questions:\n\n\n\n\n\n\nNoteResearch Questions\n\n\n\n\nDoes Testosterone have a relationship with parameters such as BMI, Weight, Height, PhysActiveDays CompHrsDay and Age?\nDoes HHIncomeMid have a relationship with these same parameters? And with Gender?\nAre there any other pairwise correlations that we should note? (This is especially useful in choosing independent variables for multiple regression)\n\n\n\n( Yes we are concerned with men more than with the women, sadly.)\n\n Correlations and Plots\n\nGGally::ggpairs(NHANES,\n  # Choose the variables we want to plot for\n  columns = c(\n    \"HHIncomeMid\", \"Weight\", \"Height\",\n    \"BMI\", \"Gender\"\n  ),\n\n  # LISTs of graphs needed at different locations\n  # For different combinations of variables\n  diag = list(continuous = \"barDiag\"),\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.01)),\n  upper = list(continuous = \"cor\"),\n  switch = \"both\", # axis labels in more traditional locations\n  progress = FALSE\n) + # No compute progress bars needed\n  theme_bw()\n\n\n\n\n\n\n\nWe see that HHIncomeMid is Quantitative, discrete valued variable, since it is based on a set of median incomes for different ranges of income. BMI, Weight, Height are continuous Quant variables.\nHHIncomeMid also seems to be relatively unaffected by Weight; And is only mildly correlated with Height and BMI, as seen both by the correlation score magnitudes and the slopes of the trend lines.\nThere is a difference in the median income by Gender, but we will defer that kind of test for later, when we do Statistical Inference.\nUnsurprisingly, BMI and Weight have a strong relationship, as do Height and Weight; the latter is of course non-linear, since the Height levels off at a point.\n\nGGally::ggpairs(NHANES,\n  columns = c(\"Testosterone\", \"Weight\", \"Height\", \"BMI\"),\n  diag = list(continuous = \"barDiag\"),\n  lower = list(continuous = wrap(\"smooth\", alpha = 0.01)),\n  upper = list(continuous = \"cor\"),\n  switch = \"both\",\n  progress = FALSE\n) +\n  theme_bw()\n\n\n\n\n\n\n\nIt is clear that Testosterone has strong relationships with Height and Weight but not so much with BMI.\n\n Visualizing Uncertainty in Correlation Estimates\nSince the pairs plot is fairly clear for both target variables, let us head to visualizing the significance and uncertainty in the correlation estimates.\n\nHHIncome_corrs &lt;- NHANES %&gt;%\n  select(where(is.numeric)) %&gt;%\n  # leave off height to get all the remaining ones\n  select(-HHIncomeMid) %&gt;%\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, NHANES$HHIncomeMid)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nHHIncome_corrs\n\n\n  \n\n\nHHIncome_corrs %&gt;%\n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;%\n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~estimate,\n    width = 0.2,\n    linewidth = ~ -log10(p.value + 0.001)\n  ) %&gt;%\n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate),\n    color = \"black\"\n  ) %&gt;%\n  # Themes,Titles, and Scales\n  gf_labs(\n    x = NULL, y = \"Correlations with HouseHold Median Income\",\n    caption = \"Significance = - log10(p.value)\"\n  ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  # Scale for colour\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    scale_colour_distiller(\"Correlation\",\n      type = \"div\",\n      palette = \"RdBu\"\n    ),\n\n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"Significance\", range = c(0.05, 2)),\n    theme(axis.text.y = element_text(size = 6, hjust = 1)),\n    coord_flip()\n  )\n\n\n\n\n\n\n\nIf we select just the variables from our Research Question:\n\nHHIncome_corrs_select &lt;- NHANES %&gt;%\n  select(Height, Weight, BMI) %&gt;% # Only change is here!\n\n  # leave off height to get all the remaining ones\n  # select(- HHIncomeMid) %&gt;%\n\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, NHANES$HHIncomeMid)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nHHIncome_corrs_select\n\n\n  \n\n\nHHIncome_corrs_select %&gt;%\n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~estimate,\n    width = 0.2,\n    linewidth = ~ -log10(p.value + 0.000001)\n  ) %&gt;%\n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate),\n    color = \"black\"\n  ) %&gt;%\n  # Reference line at zero correlation score\n  gf_hline(yintercept = 0, color = \"grey\", linewidth = 2) %&gt;%\n  # Themes,Titles, and Scales\n  gf_labs(\n    x = NULL, y = \"Correlations with HouseHold Median Income\",\n    caption = \"Significance = - log10(p.value + 0.000001)\"\n  ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  # Scale for colour\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    scale_colour_distiller(\"Correlation\",\n      type = \"div\",\n      palette = \"RdBu\"\n    ),\n\n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"Significance\", range = c(0.05, 2)),\n    theme(axis.text.y = element_text(size = 8, hjust = 1)),\n    coord_flip()\n  )\n\n\n\n\n\n\n\nSo we might say taller people make more money? And fatter people make slightly less money? Well, the magnitude of the correlations (aka effect size) are low so we would not imagine this to be a hypothesis that we can defend.\nLet us look at the Testosterone variable: trying all variables shows some paucity of observations ( due to missing data), so we will stick with our chosen variables:\n\nTestosterone_corrs &lt;- NHANES %&gt;%\n  select(Height, Weight, BMI) %&gt;%\n  # leave off height to get all the remaining ones\n  # select(- Testosterone) %&gt;%\n\n  # perform a cor.test for all variables against height\n  purrr::map(\n    .x = .,\n    .f = \\(x) cor.test(x, NHANES$Testosterone)\n  ) %&gt;%\n  # tidy up the cor.test outputs into a tidy data frame\n  map_dfr(broom::tidy, .id = \"predictor\")\n\nTestosterone_corrs\n\n\n  \n\n\nTestosterone_corrs %&gt;%\n  # Reference line at zero correlation score\n  gf_hline(\n    yintercept = 0,\n    color = \"grey\",\n    linewidth = 2\n  ) %&gt;%\n  # arrange the predictors in order of their correlation scores\n  # with the target variable (`height`)\n  # Add errorbars to show uncertainty ranges / confidence intervals\n  # Use errorbar width and linewidth fo emphasis\n  gf_errorbar(\n    conf.high + conf.low ~ reorder(predictor, estimate),\n    color = ~estimate,\n    width = 0.2,\n    linewidth = ~ -log10(p.value + 0.000001)\n  ) %&gt;%\n  # All correlation estimates as points\n  gf_point(estimate ~ reorder(predictor, estimate),\n    color = \"black\"\n  ) %&gt;%\n  # Themes,Titles, and Scales\n  gf_labs(\n    x = NULL, y = \"Correlations with Testosterone Levels\",\n    caption = \"Significance = - log10(p.value + 0.000001)\"\n  ) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  # Scale for colour\n  gf_refine(\n    guides(linewidth = guide_legend(reverse = TRUE)),\n    scale_colour_distiller(\"Correlation\",\n      type = \"div\",\n      palette = \"RdBu\"\n    ),\n\n    # Scale for dumbbells!!\n    scale_linewidth_continuous(\"Significance\", range = c(0.05, 2)),\n    theme(axis.text.y = element_text(size = 8, hjust = 1)),\n    coord_flip()\n  )"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#conclusion",
    "title": "Tutorial on Correlations in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nWe have a decent Correlations related workflow in R:\n\nLoad the dataset\n\ninspect/skim/glimpse the dataset, identify Quant and Qual variables\nIdentify a target variable based on your knowledge of the data, how it was gathered, who gathered it and what was their intent\nDevelop Pair-Wise plots + Correlations using GGally::ggpairs()\n\nDevelop Correlogram corrplot::corrplot\n\nCheck everything with a cor_test: effect size,significance, confidence intervals\nUse purrr + cor.test to plot correlations and confidence intervals for multiple Quant predictor variables against the target variable\nPlot scatter plots using gf_point.\nAdd extra lines using gf_abline() to compare hypotheses that you may have."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#references",
    "title": "Tutorial on Correlations in R",
    "section": "\n References",
    "text": "References\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\ncorrplot\n0.95\nWei and Simko (2024)\n\n\nGGally\n2.2.1\nSchloerke et al. (2024)\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\nmosaicData\n0.20.4\nPruim, Kaplan, and Horton (2023)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://doi.org/10.32614/CRAN.package.ggformula.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nPruim, Randall, Daniel Kaplan, and Nicholas Horton. 2023. mosaicData: Project MOSAIC Data Sets. https://doi.org/10.32614/CRAN.package.mosaicData.\n\n\nSchloerke, Barret, Di Cook, Joseph Larmarange, Francois Briatte, Moritz Marbach, Edwin Thoen, Amos Elberg, and Jason Crowley. 2024. GGally: Extension to “ggplot2”. https://doi.org/10.32614/CRAN.package.GGally.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWei, Taiyun, and Viliam Simko. 2024. R Package “corrplot”: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/30-Correlations/files/correlations.html#footnotes",
    "title": "Tutorial on Correlations in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttp://euclid.psych.yorku.ca/SCS/Gallery/images/galton-corr.jpg&gt;↩︎\nhttps://www.researchgate.net/figure/Galtons-smoothed-correlation-diagram-for-the-data-on-heights-of-parents-and-children_fig15_226400313↩︎"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html",
    "title": "Tutorial: Part of a Whole in R",
    "section": "",
    "text": "We will create Data Visualizations in R to show Parts of a Whole. As always, we will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula). Some specialized plots ( e.g. Fan Plots) may require us to load other R Packages. These will be introduced appropriately.\n\nRecall the standard method for all commands from the mosaic package:\ngoal( y ~ x | z, data = mydata, …)\n\n\n\n\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/60-PartWhole/files/parts.html#references",
    "title": "Tutorial: Part of a Whole in R",
    "section": "",
    "text": "Package\nVersion\nCitation\n\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#setting-up-r-packages",
    "title": "\n Groups and Densities",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(ggformula)\nlibrary(skimr)\n##\nlibrary(tidyplots) # Easily Produced Publication-Ready Plots\nlibrary(tinyplot) # Plots with Base R\nlibrary(tinytable) # Elegant Tables for our data\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#what-graphs-will-we-see-today",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#what-graphs-will-we-see-today",
    "title": "\n Groups and Densities",
    "section": "\n What graphs will we see today?",
    "text": "What graphs will we see today?\n\n\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\n(Qual)\nViolin Plot",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#what-kind-of-data-variables-will-we-choose",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#what-kind-of-data-variables-will-we-choose",
    "title": "\n Groups and Densities",
    "section": "\n What kind of Data Variables will we choose?",
    "text": "What kind of Data Variables will we choose?\n\n\n\n\n\n    \n\n      \n\nNo\n                Pronoun\n                Answer\n                Variable/Scale\n                Example\n                What Operations?\n              \n\n1\n                  How Many / Much / Heavy? Few? Seldom? Often? When?\n                  Quantities, with Scale and a Zero Value.Differences and Ratios /Products are meaningful.\n                  Quantitative/Ratio\n                  Length,Height,Temperature in Kelvin,Activity,Dose Amount,Reaction Rate,Flow Rate,Concentration,Pulse,Survival Rate\n                  Correlation",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#inspiration",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#inspiration",
    "title": "\n Groups and Densities",
    "section": "\n Inspiration",
    "text": "Inspiration\n\n\nError in theme_ipsum(): could not find function \"theme_ipsum\"\n\n\nError in theme_ipsum(): could not find function \"theme_ipsum\"\n\n\nError: object 'p1' not found\n\n\nWhich is the plots above is more evocative of the underlying data? The one which looks like a combo box-plot + density is probably giving us a greater sense of the spread of the data than the good old box plot.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#how-do-these-charts-work",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#how-do-these-charts-work",
    "title": "\n Groups and Densities",
    "section": "\n How do these Chart(s) Work?",
    "text": "How do these Chart(s) Work?\nOften one needs to view multiple densities at the same time. Ridge plots of course give us one option, where we get densities of a Quant variable split by a Qual variable. Another option is to generate a density plot facetted into small multiples using a Qual variable.\nYet another plot that allows comparison of multiple densities side by side is a violin plot. The violin plot combines the aspects of a boxplot(ranking of values, median, quantiles…) with a superimposed density plot. This allows us to look at medians, means, densities, and quantiles of a Quant variable with respect to another Qual variable. Let us see what this looks like!\n\nlibrary(ggnormalviolin)\n# Make data\nd &lt;- data_frame(\n  dist = c(\"A\", \"B\", \"C\", \"D\"),\n  dist_mean = c(80, 90, 110, 130),\n  dist_sd = c(15, 10, 25, 5)\n) %&gt;%\n  mutate(variable = pmap(list(dist_mean, dist_sd), rnorm(500)))\n\n# Make base plot\np &lt;- ggplot(\n  data = d,\n  aes(\n    x = dist,\n    mu = dist_mean,\n    sigma = dist_sd,\n    fill = dist\n  )\n) +\n  theme(legend.position = \"none\")\n\n\n# Add normal violins\np + geom_normalviolin(p_lower_tail = 0.05, p_upper_tail = 0.05) +\n  labs(\n    title = \"Violin Plots of 4 Normal Distributed Variables\",\n    subtitle = \"Means and Standard Deviations are Different\",\n    caption = glue::glue(\"Tail Regions are highlighted\", \" p &lt;= 0.05\"),\n    x = \"Qual Variable Level\",\n    y = \"Quant Variable Values\"\n  )\n\n\n\n\n\n\nFigure 1: Violin Plots for Normal Variables\n\n\n\n\nIn Figure 1, the plots show (very artificial!) distributions of a single Quant variable across levels of another Qual variable. At each level of the Qual variable along the X-axis, we have a violin plot showing the density.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#case-study-1-diamonds-dataset",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#case-study-1-diamonds-dataset",
    "title": "\n Groups and Densities",
    "section": "\n Case Study-1: diamonds dataset",
    "text": "Case Study-1: diamonds dataset\n\n\n\nUsing ggformula\nUsing ggplot\n web-r\n\n\n\n\n\n\ngf_violin(price ~ \"All Diamonds\",\n  data = diamonds,\n  draw_quantiles = c(0, .25, .50, .75)\n) %&gt;%\n  gf_labs(title = \"Plot A: Violin plot for Diamond Prices\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_violin(price ~ cut,\n    draw_quantiles = c(0, .25, .50, .75)\n  ) %&gt;%\n  gf_labs(title = \"Plot B: Price by Cut\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_violin(price ~ cut,\n    fill = ~cut,\n    color = ~cut,\n    alpha = 0.5,\n    draw_quantiles = c(0, .25, .50, .75)\n  ) %&gt;%\n  gf_labs(title = \"Plot C: Price by Cut\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;%\n  gf_violin(price ~ cut,\n    fill = ~cut,\n    colour = ~cut,\n    alpha = 0.5,\n    draw_quantiles = c(0, .25, .50, .75)\n  ) %&gt;%\n  gf_facet_wrap(vars(clarity)) %&gt;%\n  gf_labs(title = \"Plot D: Price by Cut facetted by Clarity\") %&gt;%\n  gf_theme(theme(axis.text.x = element_text(angle = 45, hjust = 1)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndiamonds %&gt;% ggplot() +\n  geom_violin(aes(y = price, x = \"\"),\n    draw_quantiles = c(0, .25, .50, .75)\n  ) + # note: y, not x\n  labs(title = \"Plot A: violin for Diamond Prices\")\n###\ndiamonds %&gt;% ggplot() +\n  geom_violin(aes(cut, price),\n    draw_quantiles = c(0, .25, .50, .75)\n  ) +\n  labs(title = \"Plot B: Price by Cut\")\n###\ndiamonds %&gt;% ggplot() +\n  geom_violin(\n    aes(cut, price,\n      color = cut, fill = cut\n    ),\n    draw_quantiles = c(0, .25, .50, .75),\n    alpha = 0.4\n  ) +\n  labs(title = \"Plot C: Price by Cut\")\n###\ndiamonds %&gt;% ggplot() +\n  geom_violin(\n    aes(cut,\n      price,\n      color = cut, fill = cut\n    ),\n    draw_quantiles = c(0, .25, .50, .75),\n    alpha = 0.4\n  ) +\n  facet_wrap(vars(clarity)) +\n  labs(title = \"Plot D: Price by Cut facetted by Clarity\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nNoteBusiness Insights from diamond Violin Plots\n\n\n\nThe distribution for price is clearly long-tailed (skewed). The distributions also vary considerably based on both cut and clarity. These Qual variables clearly have a large effect on the prices of individual diamonds.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#wait-but-why",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#wait-but-why",
    "title": "\n Groups and Densities",
    "section": "\n Wait, But Why?",
    "text": "Wait, But Why?\n\nBox plots give us an idea of medians, IQR ranges, and outliers. The shape of the density is not apparent from the box.\nDensities give us shapes of distributions, but do not provide visual indication of other metrics like means or medians ( at least not without some effort)\nViolins help us do both!\nViolins can also be cut in half (since they are symmetric, like Buddhist Prayer Wheels), then placed horizontally, and combined with both a boxplot and a dot-plot to give us raincloud plots that look like this. (Yes, there is code over there, which you can reuse.)",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#conclusion",
    "title": "\n Groups and Densities",
    "section": "\n Conclusion",
    "text": "Conclusion\n\nHistograms, Frequency Distributions, and Box Plots are used for Quantitative data variables\nHistograms “dwell upon” counts, ranges, means and standard deviations\n\nFrequency Density plots “dwell upon” probabilities and densities\n\nBox Plots “dwell upon” medians and Quartiles\n\nQualitative data variables can be plotted as counts, using Bar Charts, or using Heat Maps\nViolin Plots help us to visualize multiple distributions at the same time, as when we split a Quant variable wrt to the levels of a Qual variable.\nRidge Plots are density plots used for describing one Quant and one Qual variable (by inherent splitting)\nWe can split all these plots on the basis of another Qualitative variable.(Ridge Plots are already split)\nLong tailed distributions need care in visualization and in inference making!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#your-turn",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#your-turn",
    "title": "\n Groups and Densities",
    "section": "\n Your Turn",
    "text": "Your Turn\n\n\n\n\n\n\nNoteDatasets\n\n\n\n  Datasets\n\nClick on the Dataset Icon above, and unzip that archive. Try to make distribution plots with each of the three tools.\n\n\n\n\n\n\n\n\n\nNoteCalmCode\n\n\n\n\nA dataset from calmcode.io https://calmcode.io/datasets.html\n\n\n\n\n\n\n\n\n\n\nNoteFrom Groups\n\n\n\n\nDatasets from the earlier module on Groups.\n\n\n\ninspect the dataset in each case and develop a set of Questions, that can be answered by appropriate stat measures, or by using a chart to show the distribution.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/28-Violins/index.html#references",
    "title": "\n Groups and Densities",
    "section": "\n References",
    "text": "References\n\nWinston Chang (2024). R Graphics Cookbook. https://r-graphics.org\n\nSee the scrolly animation for a histogram at this website: Exploring Histograms, an essay by Aran Lunzer and Amelia McNamara https://tinlizzie.org/histograms/?s=09\n\nMinimal R using mosaic.https://cran.r-project.org/web/packages/mosaic/vignettes/MinimalRgg.pdf\n\nSebastian Sauer, Plotting multiple plots using purrr::map and ggplot \n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggnormalviolin\n0.2.1\nSchneider (2025)\n\n\nggridges\n0.5.6\nWilke (2024)\n\n\nNHANES\n2.1.0\nPruim (2015)\n\n\nTeachHist\n0.2.1\nLange (2023)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\ntidyplots\n0.3.1\nEngler (2025)\n\n\ntinyplot\n0.4.2\nMcDermott, Arel-Bundock, and Zeileis (2025)\n\n\ntinytable\n0.10.0\nArel-Bundock (2025)\n\n\nvisualize\n4.5.0\nBalamuta (2023)\n\n\n\n\n\n\nArel-Bundock, Vincent. 2025. tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,” “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats. https://doi.org/10.32614/CRAN.package.tinytable.\n\n\nBalamuta, James. 2023. visualize: Graph Probability Distributions with User Supplied Parameters and Statistics. https://doi.org/10.32614/CRAN.package.visualize.\n\n\nEngler, Jan Broder. 2025. “Tidyplots Empowers Life Scientists with Easy Code-Based Data Visualization.” iMeta, e70018. https://doi.org/10.1002/imt2.70018.\n\n\nLange, Carsten. 2023. TeachHist: A Collection of Amended Histograms Designed for Teaching Statistics. https://doi.org/10.32614/CRAN.package.TeachHist.\n\n\nMcDermott, Grant, Vincent Arel-Bundock, and Achim Zeileis. 2025. tinyplot: Lightweight Extension of the Base r Graphics System. https://doi.org/10.32614/CRAN.package.tinyplot.\n\n\nPruim, Randall. 2015. NHANES: Data from the US National Health and Nutrition Examination Study. https://doi.org/10.32614/CRAN.package.NHANES.\n\n\nSchneider, W. Joel. 2025. ggnormalviolin: A “ggplot2” Extension to Make Normal Violin Plots. https://doi.org/10.32614/CRAN.package.ggnormalviolin.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.\n\n\nWilke, Claus O. 2024. ggridges: Ridgeline Plots in “ggplot2”. https://doi.org/10.32614/CRAN.package.ggridges.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"material-symbols:light-group-rounded\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Groups and Densities"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html",
    "title": "\n Graphs",
    "section": "",
    "text": "“Difficulties strengthen the mind, as labor does the body.”\n— Seneca",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#setting-up-r-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#setting-up-r-packages",
    "title": "\n Graphs",
    "section": "\n Setting up R Packages",
    "text": "Setting up R Packages\n\nlibrary(tidyverse) # Data processing with tidy principles\nlibrary(mosaic) # Our go-to package for almost everything\nlibrary(ggformula) # Our plotting package\nlibrary(tidyplots) # New package for publication quality graphs\n\n# devtools::install_github(\"rpruim/Lock5withR\")\nlibrary(Lock5withR)\nlibrary(Lock5Data) # Some neat little datasets from a lovely textbook\nlibrary(kableExtra)\n\nPlot Fonts and Theme\n\nShow the Codelibrary(systemfonts)\nlibrary(showtext)\n## Clean the slate\nsystemfonts::clear_local_fonts()\nsystemfonts::clear_registry()\n##\nshowtext_opts(dpi = 96) # set DPI for showtext\nsysfonts::font_add(\n  family = \"Alegreya\",\n  regular = \"../../../../../../fonts/Alegreya-Regular.ttf\",\n  bold = \"../../../../../../fonts/Alegreya-Bold.ttf\",\n  italic = \"../../../../../../fonts/Alegreya-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/Alegreya-BoldItalic.ttf\"\n)\n\nsysfonts::font_add(\n  family = \"Roboto Condensed\",\n  regular = \"../../../../../../fonts/RobotoCondensed-Regular.ttf\",\n  bold = \"../../../../../../fonts/RobotoCondensed-Bold.ttf\",\n  italic = \"../../../../../../fonts/RobotoCondensed-Italic.ttf\",\n  bolditalic = \"../../../../../../fonts/RobotoCondensed-BoldItalic.ttf\"\n)\nshowtext_auto(enable = TRUE) # enable showtext\n##\ntheme_custom &lt;- function() {\n  font &lt;- \"Alegreya\" # assign font family up front\n\n  theme_classic(base_size = 14, base_family = font) %+replace% # replace elements we want to change\n\n    theme(\n      text = element_text(family = font), # set base font family\n\n      # text elements\n      plot.title = element_text( # title\n        family = font, # set font family\n        size = 24, # set font size\n        face = \"bold\", # bold typeface\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 5, l = 0)\n      ), # margin\n      plot.title.position = \"plot\",\n      plot.subtitle = element_text( # subtitle\n        family = font, # font family\n        size = 14, # font size\n        hjust = 0, # left align\n        margin = margin(t = 5, r = 0, b = 10, l = 0)\n      ), # margin\n\n      plot.caption = element_text( # caption\n        family = font, # font family\n        size = 9, # font size\n        hjust = 1\n      ), # right align\n\n      plot.caption.position = \"plot\", # right align\n\n      axis.title = element_text( # axis titles\n        family = \"Roboto Condensed\", # font family\n        size = 12\n      ), # font size\n\n      axis.text = element_text( # axis text\n        family = \"Roboto Condensed\", # font family\n        size = 9\n      ), # font size\n\n      axis.text.x = element_text( # margin for axis text\n        margin = margin(5, b = 10)\n      )\n\n      # since the legend often requires manual tweaking\n      # based on plot content, don't define it here\n    )\n}\n\n## Use available fonts in ggplot text geoms too!\nupdate_geom_defaults(geom = \"text\", new = list(\n  family = \"Roboto Condensed\",\n  face = \"plain\",\n  size = 3.5,\n  color = \"#2b2b2b\"\n))\n\n## Set the theme\ntheme_set(new = theme_custom())",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#why-visualize",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#why-visualize",
    "title": "\n Graphs",
    "section": "\n Why Visualize?",
    "text": "Why Visualize?\n\nWe can digest information more easily when it is pictorial\nOur Working Memories are both short-term and limited in capacity. So a picture abstracts the details and presents us with an overall summary, an insight, or a story that is both easy to recall and easy on retention.\nData Viz includes shapes that carry strong cultural memories; and impressions for us. These cultural memories help us to use data viz in a universal way to appeal to a wide variety of audiences. (Do humans have a gene for geometry?1);\nIt helps sift facts from mere statements: for example:\n\n\n\n\n\n\nFigure 1: Rape Capital\n\n\n\n\n\n\n\nFigure 2: Data Reveals Crime\n\n\n\nVisuals are a good starting point to make hypotheses of what may be happening in the situation represented by the data",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#why-analyze",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#why-analyze",
    "title": "\n Graphs",
    "section": "\n Why Analyze?",
    "text": "Why Analyze?\n\nMerely looking at visualizations may not necessarily tell us the true magnitude or significance of things.\nWe need analytic methods or statistics to assure ourselves, or otherwise, of what we might suspect is happening\nThese methods also help to remove human bias and ensure that we are speaking with the assurance that our problem deserves.\nAnalysis uses numbers, or metrics, that allow us to crystallize our ambiguous words/guesses into quantities that can be calculated with.\nThese metrics are calculable from our data, of course, but are not directly visible, despite often being intuitive.\n\nSo both visuals and analytics. And as we will see, we will not be content with that: we will visualize our analytics, and analyze our visualizations!\nLet us recall first what we meant by tidy data:\n\n\n\n\n\n\n\nFigure 3: Tidy Data\n\n\n\n\n\n\n\n\n\n\nImportantTidy Data\n\n\n\n\nEach variable is a column;\nEach column contains one kind of data.\nEach observation or case is a row.\nEach observations contains one value for each variable.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#what-is-a-data-visualization",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#what-is-a-data-visualization",
    "title": "\n Graphs",
    "section": "\n What is a Data Visualization?",
    "text": "What is a Data Visualization?\n\n Data Viz = Data + Geometry\nHow many geometric things do we know? Shapes? Lines? Axes? Curves? Angles? Patterns? Textures? Colours? Sizes? Positions? Lengths? Heights? Breadths? Radii? Textures? All these are geometric aspects or aesthetics, each with a unique property. Some “geometric things” which we might consider are shown in the figure below.\n\n\n\n\n\nFigure 4: Common Geometric Aesthetics in Charts\n\n\n\n Mapping\nHow can we manipulate these geometric aesthetics, perhaps like Kandinsky? The aesthetic has a property, an atribute, which we can manipulate in accordance with a data variable! This act of “mapping” a geometric thing to a variable and modifying its essential property is called Data Visualization\nFor instance:\n\n\nlength or height of a bar can be made proportional to theage or income of a person\n\nColour of points can be mapped to gender, with a unique colour for each gender.\n\nPosition along an X-axis can vary in accordance with a height variable, and\n\nPosition along the Y-axis can vary with a bodyWeight variable.\n\nA chart may use more than one aesthetic: position, shape, colour, height and angle, pattern or texture to name several. Usually, each aesthetic is mapped to just one variable to ensure there is no cognitive error. There is of course a choice and you should be able to map any kind of variable to any geometric aspect/aesthetic that may be available.\n\n\n\n\n\n\nNoteA Natural Mapping\n\n\n\nNote that here is also a “natural” mapping between aesthetic and kind of variableQuantitative or Qualitative as seen in Figure 3. For instance, shape is rarely mapped to a Quantitative variable; we understand this because the nature of variation between the Quantitative variable and the shape aesthetic is not similar (i.e. not continuous). Bad choices may lead to bad, or worse, misleading charts!\n\n\n\n\n\n\n\n\n\nFigure 5: Data Vis Components and Features\n\n\n\n\nIn the above chart, it is pretty clear what kind of variable is plotted on the x-axis and the y-axis. What about colour? Could this be considered as another axis in the chart? There are also other aspects that you can choose (not explicitly shown here) such as the plot theme(colours, fonts, backgrounds etc), which may not be mapped to data, but are nonetheless choices to be made. We will get acquainted with this aspect as we build charts.\nAs we will see, Data Variables may be transformed before being mapped to some geometric aesthetic, e.g. we may perform counts with a Qual variable that contains only the entries {S, M, L, XL}. We may also transform the axes (make them logarithmic, or even polar ) to create precisely the shape-meaning we wish. This allows us considerable flexibility in making charts!!",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#sec-data-viz",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#sec-data-viz",
    "title": "\n Graphs",
    "section": "\n Basic Types of Charts",
    "text": "Basic Types of Charts\nWe can therefore think of simple visualizations as combinations of aesthetics, mapped to combinations of variables. Some examples:\n\nGeometries , Combinations, and Graphs\n\n\n\n\n\n\n\nVariable #1\nVariable #2\nChart Names\nChart Shape\n\n\n\nQuant\nNone\nHistogram and Density\n\n\n\n\n\nQual\nNone\nBar Chart\n\n\n\nQuant\nQuant\nScatter Plot, Line Chart, Bubble Plot, Area Chart\n\n\n\n\n\nQuant\nQual\nPie Chart, Donut Chart, Column Chart, Box-Whisker Plot, Radar Chart, Bump Chart, Tree Diagram\n\n\n\n\n\nQual\nQual\nStacked Bar Chart, Mosaic Chart, Sankey, Chord Diagram, Network Diagram",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#conclusion",
    "title": "\n Graphs",
    "section": "\n Conclusion",
    "text": "Conclusion\nLet us take a look at Wickham and Grolemund’s Data Science workflow picture:\n\n\n\n\n\nFigure 6: Data Science Workflow\n\n\nSo there we have it:\n\nWe import and clean the data\nQuestions lead us to identify Types of Variables (Quant and Qual)\n\nSometimes we may need to transform the data (long to wide, summarize, create new variables…)\nFurther Questions lead to relationships between variables, which we describe using Data Visualizations\n\nVisualizations may lead to Hypotheses, which we Analyze or Model\nData Visualizations are Data mapped onto Geometry \nMultiple Variable-to-Geometry Mappings = A Complete Data Visualization\n\nWhich is finally Communicated\n\nYou might think of all these Questions, Answers, Mapping as being equivalent to metaphors as a language in itself. And indeed, in R we use a philosophy called the Grammar of Graphics! We will use this grammar in the R graphics packages that we will encounter. Other parts of the Workflow (Transformation, Analysis and Modelling) are also following similar grammars, as we shall see.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#ai-generated-summary-and-podcast",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#ai-generated-summary-and-podcast",
    "title": "\n Graphs",
    "section": "\n AI Generated Summary and Podcast",
    "text": "AI Generated Summary and Podcast\nThis is a tutorial on data visualization using the R programming language. It introduces concepts such as data types, variables, and visualization techniques. The tutorial utilizes metaphors to explain these concepts, emphasizing the use of geometric aesthetics to represent data. It also highlights the importance of both visual and analytic approaches in understanding data. The tutorial then demonstrates basic chart types, including histograms, scatterplots, and bar charts, and discusses the “Grammar of Graphics” philosophy that guides data visualization in R. The text concludes with a workflow diagram for data science, emphasizing the iterative process of data import, cleaning, transformation, visualization, hypothesis generation, analysis, and communication.\n\n\n\n Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#references",
    "title": "\n Graphs",
    "section": "\n References",
    "text": "References\n\nRandomized Trials:\n\n\n\n \nMartyn Shuttleworth, Lyndsay T Wilson (Jun 26, 2009). What is the Scientific Method? Retrieved Mar 12, 2024 from Explorable.com: https://explorable.com/what-is-the-scientific-method\n\nAdam E.M. Eltorai, Jeffrey A. Bakal, Paige C. Newell, Adena J. Osband (editors). (March 22, 2023) Translational Surgery: Handbook for Designing and Conducting Clinical and Translational Research. A very lucid and easily explained set of chapters. ( I have a copy. Yes.)\n\nPart III. Clinical: fundamentals\nPart IV: Statistical principles\n\n\nhttps://safetyculture.com/topics/design-of-experiments/\nEmi Tanaka. https://emitanaka.org/teaching/monash-wcd/2020/week09-DoE.html\n\nOpen Intro Stats: Types of Variables\nLock, Lock, Lock, Lock, and Lock. Statistics: Unlocking the Power of Data, Third Edition, Wiley, 2021. https://www.wiley.com/en-br/Statistics:+Unlocking+the+Power+of+Data,+3rd+Edition-p-9781119674160)\n\nClaus Wilke. Fundamentals of Data Visualization. https://clauswilke.com/dataviz/\n\nAlbert Rapp. Adding images to ggplot. https://albert-rapp.de/posts/ggplot2-tips/27_images/27_images\n\n\n\n\n R Package Citations\n\n\n\n\nPackage\nVersion\nCitation\n\n\n\nggformula\n0.12.0\nKaplan and Pruim (2023)\n\n\nLock5Data\n3.0.0\nLock (2021)\n\n\nmosaic\n1.9.1\nPruim, Kaplan, and Horton (2017)\n\n\nTeachingDemos\n2.13\nSnow (2024)\n\n\n\n\n\n\nKaplan, Daniel, and Randall Pruim. 2023. ggformula: Formula Interface to the Grammar of Graphics. https://doi.org/10.32614/CRAN.package.ggformula.\n\n\nLock, Robin. 2021. Lock5Data: Datasets for “Statistics: UnLocking the Power of Data”. https://doi.org/10.32614/CRAN.package.Lock5Data.\n\n\nPruim, Randall, Daniel T Kaplan, and Nicholas J Horton. 2017. “The Mosaic Package: Helping Students to ‘Think with Data’ Using r.” The R Journal 9 (1): 77–102. https://journal.r-project.org/archive/2017/RJ-2017-024/index.html.\n\n\nSnow, Greg. 2024. TeachingDemos: Demonstrations for Teaching and Learning. https://doi.org/10.32614/CRAN.package.TeachingDemos.",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/07-Graphs/index.html#footnotes",
    "title": "\n Graphs",
    "section": "Footnotes",
    "text": "Footnotes\n\nhttps://www.xcode.in/genes-and-personality/how-genes-influence-your-math-ability/↩︎",
    "crumbs": [
      "Teaching",
      "Data Viz and Stats",
      "Descriptive Analytics",
      "<iconify-icon icon=\"carbon:chart-3d\" width=\"1.2em\" height=\"1.2em\"></iconify-icon> Graphs"
    ]
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#setup-the-packages",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#setup-the-packages",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "",
    "text": "# options(tibble.print_min = 4L, tibble.print_max = 4L,digits = 3)\nlibrary(tidyverse)\nlibrary(mosaic) # package for stats, simulations, and basic plots\nlibrary(mosaicData) # package containing datasets\nlibrary(ggformula) # package for professional looking plots, that use the formula interface from mosaic\nlibrary(skimr) # Summary statistics about variables in data frames\nlibrary(NHANES) # survey data collected by the US National Center for Health Statistics (NCHS)\n\nlibrary(echarts4r) # Interactive graphs using Javascript in R\nlibrary(plotly) # An older more established package for interactive graphs using Javascript in R\n\n\nggplot2::theme_set(new = theme_classic())"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#introduction",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#introduction",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Introduction",
    "text": "Introduction\nWe will query our dataset, developing insights and new questions as each Table or Bar/Histogram chart yields new information. This process of exploration is iterative, structured, and intuitive. Intermediate results may on occasion be messy or not very insightful!\nWe will consistently use the Project Mosaic ecosystem of packages in R (mosaic, mosaicData and ggformula).\n\n\n\n\n\n\nTipFormula Interface\n\n\n\nNote the standard method for all commands from the mosaic package:goal( y ~ x | z, data = mydata, …) With ggformula, one can create any graph/chart using:gf_geometry(y ~ x | z, data = mydata)\nORmydata %&gt;% gf_geometry( y ~ x | z)\nThe second method may be preferable, especially if you have done some data manipulation first! More later! ggformula supports many types of plots (using geometry), such as scatter, bar, histogram, density, boxplots, maps and many other statistical plots.\n\n\n\n\n\n\n\n\nTipInteractive Graphs with echarts4r\n\n\n\nWe will also start using echarts4r side by side for interactive graphs.\n\nEvery function in the package starts with e_.\nYou start coding a visualization by creating an echarts object with the e_charts() function. That takes your data frame and x-axis column as arguments.\nNext, you add a function for the type of chart (e_line(), e_bar(), etc.) with the y-axis series column name as an argument.\nThe rest is mostly customization! echarts4r takes some effort in getting used to, but it totally worth it!\n\n\n\nThe website for echarts4r is https://echarts4r.john-coene.com/articles/get_started.html. You should also quickly view this short introductory video on echarts4r:"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-1-galton-dataset-from-mosaicdata",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-1: Galton Dataset from mosaicData\n",
    "text": "Case Study-1: Galton Dataset from mosaicData\n\nLet us choose the famous Galton dataset:\n\ndata(\"Galton\")\nGalton &lt;- as_tibble(Galton)\n\n\n Look at the Data:\n\nskim(Galton)\n\n\nData summary\n\n\nName\nGalton\n\n\nNumber of rows\n898\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nfamily\n0\n1\nFALSE\n197\n185: 15, 166: 11, 66: 11, 130: 10\n\n\nsex\n0\n1\nFALSE\n2\nM: 465, F: 433\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nfather\n0\n1\n69.23\n2.47\n62\n68\n69.0\n71.0\n78.5\n▁▅▇▂▁\n\n\nmother\n0\n1\n64.08\n2.31\n58\n63\n64.0\n65.5\n70.5\n▂▅▇▃▁\n\n\nheight\n0\n1\n66.76\n3.58\n56\n64\n66.5\n69.7\n79.0\n▁▇▇▅▁\n\n\nnkids\n0\n1\n6.14\n2.69\n1\n4\n6.0\n8.0\n15.0\n▃▇▆▂▁\n\n\n\n\n\nWhat can we say about the dataset and its variables? How big is the dataset? How many variables? What types are they, Quant or Qual? What are the means, medians and inter-quartile ranges for the Quant variables? If they are Qual, what are the levels? Are they ordered levels?\nThere is a lot of Description generated by the skimr::skim command (and equivalently by the mosaic::inspect() command)! Try both and see which output suits you. The first table above describes the Qual variables: family and sex. The second table describes the Quant variables, and gives us their statistical summaries as well and a neat little histogram to boot. The data are described as: Type help(Galton) in your Console\n\nA data frame with 898 observations on the following variables.\n\n\nfamily an ID for each family, a factor with levels for each family\n\nfather the father’s height (in inches)\n\nmother the mother’s height (in inches)\n\nsex the child’s sex: F or M\n\nheight the child’s height as an adult (in inches)\n\nnkids the number of adult children in the family, or, at least, the number whose heights Galton recorded.\n\n\n\n Counts, and Charts with Counts\nNow that we know the variables, let us look at counts of data observations(rows). We know from our examination of variable types that counting of observations must be done on the basis of Qualitative variables. So let us count and plot the counts in bar charts.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 How many families in the data for each value of nkids(i.e. Count of families by size)?\n\n\n\n\nComputations\nUsing ggformula\nUsing echarts4r\nUsing plotly\n\n\n\n\nGalton_counts &lt;- Galton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(children = n()) %&gt;%\n  # just to check\n  mutate(\n    No_of_families = as.integer(children / nkids),\n    # Why do we divide\n\n    running_count_of_children = cumsum(children),\n    running_count_of_families = cumsum(No_of_families)\n  )\nGalton_counts\n\n\n  \n\n\n\n\n\n\nGalton_counts %&gt;%\n  gf_col(No_of_families ~ nkids) %&gt;%\n  gf_theme(theme_classic())\n\n\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(No_of_families,\n    colorBy = \"data\",\n    legend = FALSE\n  ) %&gt;% # Or \"series\"\n\n  # https://echarts4r.john-coene.com/articles/grid.html\n  # echarts4r does not \"automatically\" name the axes!\n  # And look at the \"categorical\" x-axis below!\n\n  e_x_axis(\n    name = \"Family Size\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"Count\", nameLocation = \"center\", nameGap = 25, ) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"No of Families of each size\")\n\n\n\n\n\n\n\n\nGalton_counts %&gt;%\n  plot_ly(x = ~nkids, y = ~No_of_families) %&gt;%\n  add_bars()\n\n\n\n\n\n\n\n\nInsight: There are 32 1-kid families; and \\(128/8 = 16\\) 8-kid families! There is one great great 15-kid family. (Did you get the idea behind why we divide here?)\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2. What is the count of Children by sex of the child and by family size nkids?\n\n\n\n\nUsing ggformula\nUsing echarts4r\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex %&gt;%\n  gf_col(count_by_sex ~ nkids | sex, fill = ~sex, data = .)\n\n\n\n\n\n\n\n\n\n\nGalton_counts_by_sex &lt;- Galton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(nkids, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex)\nGalton_counts_by_sex\n\n\n  \n\n\nGalton_counts_by_sex %&gt;%\n  e_charts(nkids) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"Family Size (nkids)\", nameLocation = \"center\",\n    nameGap = 20, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 20,\n    nameTextStyle = list(align = \"center\"),\n    nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 25, orient = \"vertical\") %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family Size\")\n\n\n\n\n\n\n\n\nInsight: Hmm…decent gender balance overall, across family sizes nkids.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nFollow up Question: How would we look for “gender balance” in individual families? Should we look at the family column ?\n\n\n\nGalton %&gt;%\n  mutate(family = as.integer(family)) %&gt;%\n  group_by(family, sex) %&gt;%\n  summarise(count_by_sex = n()) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(family) %&gt;%\n  e_bar(count_by_sex) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;%\n  e_y_axis(\n    name = \"How Many Children?\",\n    nameGap = 25, nameLocation = \"center\"\n  ) %&gt;%\n  e_legend(right = 5) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_title(\"Child Counts by Sex over Family ID\")\n\n\n\n\n\nInsight: The No of Children were distributed similarly across family sizenkids… However, this plot is too crowded and does not lead to any great insight. Using family ID was silly to plot against, wasn’t it? Not all exploratory plots will be “necessary” in the end. But they are part of the journey of getting better acquainted with the data!\n\n {{}} Stat Summaries and Distributions\nOK, on to the Quantitative variables now! What Questions might we have, that could relate not to counts by Qual variables, but to the numbers in Quant variables. Stat measures, like their ranges, max and min? Means, medians, distributions? And how these vary on the basis of Qual variables? All this using histograms and densities.\n\n\n\n\n\n\nNoteSummary Stats\n\n\n\nAs Stigler(Stigler 2016) said, summaries are the first thing to look at in data. skimr::skim has already given us a lot summary data for Quant variables. We can now use mosaic::favstats to develop these further, by slicing / facetting these wrt other Qual variables. Let us tabulate some quick stat summaries of the important variables in Galton.\n\n\n\n# summaries facetted by sex of child\nmeasures &lt;- favstats(~ height | sex, data = Galton)\nmeasures\n\n\n  \n\n\n\nInsight: We saw earlier that the mean height of the Children was 66 inches. However, are Sons taller than Daughters? Difference in mean height is 5 inches! AND…that was the same difference between fathers and mothers mean heights! Is it so simple then?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.4 How are the heights of the children distributed? Here is where we need a e_histogram…\n\n\n\nGalton %&gt;%\n  e_charts() %&gt;%\n  e_histogram(serie = height) %&gt;%\n  e_tooltip(trigger = \"item\") %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$height)),\n    label = list(\n      label = \"Mean Height\",\n      label.position = \"end\"\n    ),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # See https://echarts.apache.org/en/option.html#series-line.markLine\n\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_y_axis(name = \"Counts\", nameLocation = \"center\", nameGap = 30) %&gt;%\n  e_title(\"Distribution of Heights in Galton\")\n\n\n\n\n\nInsight: Fairly symmetric distribution…but there are a few very short and some very tall children! Try to change the no. of bins to check of we are missing some pattern. This is not completely easy with echarts4r which uses the “Sturges” algorithm to set the number of bins. Need to figure this out from the echarts Apache API docs.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5 Is there a difference in height distributions between Male and Female children?(Quant variable sliced by Qual variable)\n\n\nWe will use the raw Galton data and previously-computed measures:\n\nGalton %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(height) %&gt;%\n  e_mark_line(\n    data = list(xAxis = measures %&gt;% filter(sex == \"M\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n    # This code colours both v-lines red...how?\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  # Upto here gives one line in red colour, correctly\n\n  e_mark_line(\n    data = list(xAxis = measures %&gt;%\n      filter(sex == \"F\") %&gt;%\n      select(mean) %&gt;% as.numeric()),\n\n    # This piece of code has no effect...wonder why not?\n    # BOTH lines are in red ...why??\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_title(\"Distributions of Height by Sex in Galton\") %&gt;%\n  e_x_axis(name = \"Height\", nameLocation = \"center\") %&gt;%\n  e_legend(right = 5)\n\n\n\n\n\nInsight: There is a visible difference in average heights between girls and boys. Is that significant, however? We will need a statistical inference test to figure that out!! Claus Wilke1 says comparisons of Quant variables across groups are best made between densities and not histograms…\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6 Are Mothers generally shorter than fathers?\n\n\n\nGalton %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(father) %&gt;%\n  e_density(mother) %&gt;%\n  e_mark_line(\n    data = list(xAxis = mean(Galton$mother)),\n    lineStyle = list(\n      color = \"red\", width = 1.5,\n      type = \"solid\"\n    )\n  ) %&gt;%\n  e_mark_line(data = list(\n    xAxis = mean(Galton$father),\n    lineStyle = list(\n      color = \"black\", width = 1.5,\n      type = \"solid\"\n    )\n  )) %&gt;%\n  e_legend(right = 10)\n\n\n\n\n\nInsight: Yes, moms are on average shorter than dads in this dataset. Again, is this difference statistically significant? We will find out in when we do Inference.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7a. Are heights of children different based on the number of kids in the family? And For Male and Female children?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  e_charts(height = 400) %&gt;%\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights over Family Size\")\n\n\n\n\n\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7b. Are heights of children different for Male and Female children?\n\n\n\n# Can do better at colouring/filling and facetting...\nGalton %&gt;%\n  group_by(nkids, sex) %&gt;%\n  e_charts(height = 400) %&gt;% # no x-variable needed for boxplots\n  e_boxplot(height,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50, name = \"height\", nameLocation = \"center\",\n    nameGap = 25, margin = 5\n  ) %&gt;% # adds +/- 5 to y-axis limits\n\n  e_x_axis(\n    name = \"Family Size\",\n    nameLocation = \"center\",\n    nameGap = 25, type = \"category\"\n  ) %&gt;% # makes a category axis showing factors\n\n  e_tooltip() %&gt;%\n  e_title(\"Heights by Sex over Family Size\")\n\n\n\n\n\nInsight: So, at all family “strengths”, the male children are taller than the female children. Box plots are used to show distributions of numeric data values and compare them between multiple groups (i.e Categorical Data, here sex and nkids).\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.8 Does the mean height of children in a family vary with the number of children in the family? (family size)?\n\n\n\nGalton %&gt;%\n  group_by(nkids) %&gt;%\n  summarise(mean_height = mean(height)) %&gt;%\n  e_charts(nkids, height = 300) %&gt;%\n  e_bar(mean_height, colorBy = \"data\", legend = FALSE) %&gt;%\n  e_x_axis(\n    name = \"nkids\", nameLocation = \"center\", nameGap = 25,\n    type = \"category\"\n  ) %&gt;%\n  e_y_axis(name = \"mean height\", nameLocation = \"center\", nameGap = 25) %&gt;%\n  e_tooltip(trigger = \"item\")\n\n\n\n\n\nInsight: Hmm…The graph shows that mean heights do not vary much with family size nkids. We saw this with the box plots earlier. This would be useful information in a Modelling and Prediction exercise.\n\n\n\n\n\n\nNoteFollow-up Question\n\n\n\nQ. 8a. Is height difference between sons and daughters related to height difference between father and mother?\nDifferences between father and mother heights influencing height…this would be like height ~ (father-mother). This would be a relationship between two Quant variables. A histogram would not serve here and we plot this as a Scatter Plot:\n\n\n\nGalton %&gt;%\n  group_by(family, sex) %&gt;%\n  # Parental Height Difference\n  mutate(diff_height = father - mother) %&gt;%\n  select(family, sex, height, diff_height) %&gt;%\n  ungroup() %&gt;%\n  group_by(sex) %&gt;%\n  e_charts(diff_height, height = 300) %&gt;%\n  e_scatter(height, symbol_size = 8) %&gt;%\n  # Fit a trend line\n  e_lm(height ~ diff_height,\n    name = c(\"Female\", \"Male\")\n  ) %&gt;%\n  e_x_axis(\n    max = 18, min = -5,\n    name = \"Father - Mother Height\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_y_axis(\n    max = 80, min = 50,\n    name = \"Children's Heights\",\n    nameLocation = \"center\", nameGap = 25\n  ) %&gt;%\n  e_tooltip(axisPointer = list(type = \"cross\"))\n\n\n\n\n\nInsight: There seems no relationship, or a very small one, between children’s heights on the y-axis and the difference in parental height differences on the x-axis…\nAnd so on…..we can proceed from simple visualizations based on Questions to larger questions that demand inference and modelling. We hinted briefly on these in the above Case Study."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-2-dataset-from-nhanes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study-2: Dataset from NHANES\n",
    "text": "Case Study-2: Dataset from NHANES\n\nLet us try the NHANES dataset. Try help(NHANES) in your Console.\n\ndata(\"NHANES\")\n\n\n Look at the Data\n\nskim(NHANES)\n\n\nData summary\n\n\nName\nNHANES\n\n\nNumber of rows\n10000\n\n\nNumber of columns\n76\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n31\n\n\nnumeric\n45\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\nSurveyYr\n0\n1.00\nFALSE\n2\n200: 5000, 201: 5000\n\n\nGender\n0\n1.00\nFALSE\n2\nfem: 5020, mal: 4980\n\n\nAgeDecade\n333\n0.97\nFALSE\n8\n40: 1398, 0-: 1391, 10: 1374, 20: 1356\n\n\nRace1\n0\n1.00\nFALSE\n5\nWhi: 6372, Bla: 1197, Mex: 1015, Oth: 806\n\n\nRace3\n5000\n0.50\nFALSE\n6\nWhi: 3135, Bla: 589, Mex: 480, His: 350\n\n\nEducation\n2779\n0.72\nFALSE\n5\nSom: 2267, Col: 2098, Hig: 1517, 9 -: 888\n\n\nMaritalStatus\n2769\n0.72\nFALSE\n6\nMar: 3945, Nev: 1380, Div: 707, Liv: 560\n\n\nHHIncome\n811\n0.92\nFALSE\n12\nmor: 2220, 750: 1084, 250: 958, 350: 863\n\n\nHomeOwn\n63\n0.99\nFALSE\n3\nOwn: 6425, Ren: 3287, Oth: 225\n\n\nWork\n2229\n0.78\nFALSE\n3\nWor: 4613, Not: 2847, Loo: 311\n\n\nBMICatUnder20yrs\n8726\n0.13\nFALSE\n4\nNor: 805, Obe: 221, Ove: 193, Und: 55\n\n\nBMI_WHO\n397\n0.96\nFALSE\n4\n18.: 2911, 30.: 2751, 25.: 2664, 12.: 1277\n\n\nDiabetes\n142\n0.99\nFALSE\n2\nNo: 9098, Yes: 760\n\n\nHealthGen\n2461\n0.75\nFALSE\n5\nGoo: 2956, Vgo: 2508, Fai: 1010, Exc: 878\n\n\nLittleInterest\n3333\n0.67\nFALSE\n3\nNon: 5103, Sev: 1130, Mos: 434\n\n\nDepressed\n3327\n0.67\nFALSE\n3\nNon: 5246, Sev: 1009, Mos: 418\n\n\nSleepTrouble\n2228\n0.78\nFALSE\n2\nNo: 5799, Yes: 1973\n\n\nPhysActive\n1674\n0.83\nFALSE\n2\nYes: 4649, No: 3677\n\n\nTVHrsDay\n5141\n0.49\nFALSE\n7\n2_h: 1275, 1_h: 884, 3_h: 836, 0_t: 638\n\n\nCompHrsDay\n5137\n0.49\nFALSE\n7\n0_t: 1409, 0_h: 1073, 1_h: 1030, 2_h: 589\n\n\nAlcohol12PlusYr\n3420\n0.66\nFALSE\n2\nYes: 5212, No: 1368\n\n\nSmokeNow\n6789\n0.32\nFALSE\n2\nNo: 1745, Yes: 1466\n\n\nSmoke100\n2765\n0.72\nFALSE\n2\nNo: 4024, Yes: 3211\n\n\nSmoke100n\n2765\n0.72\nFALSE\n2\nNon: 4024, Smo: 3211\n\n\nMarijuana\n5059\n0.49\nFALSE\n2\nYes: 2892, No: 2049\n\n\nRegularMarij\n5059\n0.49\nFALSE\n2\nNo: 3575, Yes: 1366\n\n\nHardDrugs\n4235\n0.58\nFALSE\n2\nNo: 4700, Yes: 1065\n\n\nSexEver\n4233\n0.58\nFALSE\n2\nYes: 5544, No: 223\n\n\nSameSex\n4232\n0.58\nFALSE\n2\nNo: 5353, Yes: 415\n\n\nSexOrientation\n5158\n0.48\nFALSE\n3\nHet: 4638, Bis: 119, Hom: 85\n\n\nPregnantNow\n8304\n0.17\nFALSE\n3\nNo: 1573, Yes: 72, Unk: 51\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\nID\n0\n1.00\n61944.64\n5871.17\n51624.00\n56904.50\n62159.50\n67039.00\n71915.00\n▇▇▇▇▇\n\n\nAge\n0\n1.00\n36.74\n22.40\n0.00\n17.00\n36.00\n54.00\n80.00\n▇▇▇▆▅\n\n\nAgeMonths\n5038\n0.50\n420.12\n259.04\n0.00\n199.00\n418.00\n624.00\n959.00\n▇▇▇▆▃\n\n\nHHIncomeMid\n811\n0.92\n57206.17\n33020.28\n2500.00\n30000.00\n50000.00\n87500.00\n100000.00\n▃▆▃▁▇\n\n\nPoverty\n726\n0.93\n2.80\n1.68\n0.00\n1.24\n2.70\n4.71\n5.00\n▅▅▃▃▇\n\n\nHomeRooms\n69\n0.99\n6.25\n2.28\n1.00\n5.00\n6.00\n8.00\n13.00\n▂▆▇▂▁\n\n\nWeight\n78\n0.99\n70.98\n29.13\n2.80\n56.10\n72.70\n88.90\n230.70\n▂▇▂▁▁\n\n\nLength\n9457\n0.05\n85.02\n13.71\n47.10\n75.70\n87.00\n96.10\n112.20\n▁▃▆▇▃\n\n\nHeadCirc\n9912\n0.01\n41.18\n2.31\n34.20\n39.58\n41.45\n42.92\n45.40\n▁▂▇▇▅\n\n\nHeight\n353\n0.96\n161.88\n20.19\n83.60\n156.80\n166.00\n174.50\n200.40\n▁▁▁▇▂\n\n\nBMI\n366\n0.96\n26.66\n7.38\n12.88\n21.58\n25.98\n30.89\n81.25\n▇▆▁▁▁\n\n\nPulse\n1437\n0.86\n73.56\n12.16\n40.00\n64.00\n72.00\n82.00\n136.00\n▂▇▃▁▁\n\n\nBPSysAve\n1449\n0.86\n118.15\n17.25\n76.00\n106.00\n116.00\n127.00\n226.00\n▃▇▂▁▁\n\n\nBPDiaAve\n1449\n0.86\n67.48\n14.35\n0.00\n61.00\n69.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nBPSys1\n1763\n0.82\n119.09\n17.50\n72.00\n106.00\n116.00\n128.00\n232.00\n▂▇▂▁▁\n\n\nBPDia1\n1763\n0.82\n68.28\n13.78\n0.00\n62.00\n70.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys2\n1647\n0.84\n118.48\n17.49\n76.00\n106.00\n116.00\n128.00\n226.00\n▃▇▂▁▁\n\n\nBPDia2\n1647\n0.84\n67.66\n14.42\n0.00\n60.00\n68.00\n76.00\n118.00\n▁▁▇▆▁\n\n\nBPSys3\n1635\n0.84\n117.93\n17.18\n76.00\n106.00\n116.00\n126.00\n226.00\n▃▇▂▁▁\n\n\nBPDia3\n1635\n0.84\n67.30\n14.96\n0.00\n60.00\n68.00\n76.00\n116.00\n▁▁▇▇▁\n\n\nTestosterone\n5874\n0.41\n197.90\n226.50\n0.25\n17.70\n43.82\n362.41\n1795.60\n▇▂▁▁▁\n\n\nDirectChol\n1526\n0.85\n1.36\n0.40\n0.39\n1.09\n1.29\n1.58\n4.03\n▅▇▂▁▁\n\n\nTotChol\n1526\n0.85\n4.88\n1.08\n1.53\n4.11\n4.78\n5.53\n13.65\n▂▇▁▁▁\n\n\nUrineVol1\n987\n0.90\n118.52\n90.34\n0.00\n50.00\n94.00\n164.00\n510.00\n▇▅▂▁▁\n\n\nUrineFlow1\n1603\n0.84\n0.98\n0.95\n0.00\n0.40\n0.70\n1.22\n17.17\n▇▁▁▁▁\n\n\nUrineVol2\n8522\n0.15\n119.68\n90.16\n0.00\n52.00\n95.00\n171.75\n409.00\n▇▆▃▂▁\n\n\nUrineFlow2\n8524\n0.15\n1.15\n1.07\n0.00\n0.48\n0.76\n1.51\n13.69\n▇▁▁▁▁\n\n\nDiabetesAge\n9371\n0.06\n48.42\n15.68\n1.00\n40.00\n50.00\n58.00\n80.00\n▁▂▆▇▂\n\n\nDaysPhysHlthBad\n2468\n0.75\n3.33\n7.40\n0.00\n0.00\n0.00\n3.00\n30.00\n▇▁▁▁▁\n\n\nDaysMentHlthBad\n2466\n0.75\n4.13\n7.83\n0.00\n0.00\n0.00\n4.00\n30.00\n▇▁▁▁▁\n\n\nnPregnancies\n7396\n0.26\n3.03\n1.80\n1.00\n2.00\n3.00\n4.00\n32.00\n▇▁▁▁▁\n\n\nnBabies\n7584\n0.24\n2.46\n1.32\n0.00\n2.00\n2.00\n3.00\n12.00\n▇▅▁▁▁\n\n\nAge1stBaby\n8116\n0.19\n22.65\n4.77\n14.00\n19.00\n22.00\n26.00\n39.00\n▆▇▅▂▁\n\n\nSleepHrsNight\n2245\n0.78\n6.93\n1.35\n2.00\n6.00\n7.00\n8.00\n12.00\n▁▅▇▁▁\n\n\nPhysActiveDays\n5337\n0.47\n3.74\n1.84\n1.00\n2.00\n3.00\n5.00\n7.00\n▇▇▃▅▅\n\n\nTVHrsDayChild\n9347\n0.07\n1.94\n1.43\n0.00\n1.00\n2.00\n3.00\n6.00\n▇▆▂▂▂\n\n\nCompHrsDayChild\n9347\n0.07\n2.20\n2.52\n0.00\n0.00\n1.00\n6.00\n6.00\n▇▁▁▁▃\n\n\nAlcoholDay\n5086\n0.49\n2.91\n3.18\n1.00\n1.00\n2.00\n3.00\n82.00\n▇▁▁▁▁\n\n\nAlcoholYear\n4078\n0.59\n75.10\n103.03\n0.00\n3.00\n24.00\n104.00\n364.00\n▇▁▁▁▁\n\n\nSmokeAge\n6920\n0.31\n17.83\n5.33\n6.00\n15.00\n17.00\n19.00\n72.00\n▇▂▁▁▁\n\n\nAgeFirstMarij\n7109\n0.29\n17.02\n3.90\n1.00\n15.00\n16.00\n19.00\n48.00\n▁▇▂▁▁\n\n\nAgeRegMarij\n8634\n0.14\n17.69\n4.81\n5.00\n15.00\n17.00\n19.00\n52.00\n▂▇▁▁▁\n\n\nSexAge\n4460\n0.55\n17.43\n3.72\n9.00\n15.00\n17.00\n19.00\n50.00\n▇▅▁▁▁\n\n\nSexNumPartnLife\n4275\n0.57\n15.09\n57.85\n0.00\n2.00\n5.00\n12.00\n2000.00\n▇▁▁▁▁\n\n\nSexNumPartYear\n5072\n0.49\n1.34\n2.78\n0.00\n1.00\n1.00\n1.00\n69.00\n▇▁▁▁▁\n\n\n\n\n\nAgain, lots of data from skim, about the Quant and Qual variables. Spend a little time looking through this output.\n\nWhich variables could have been data that was given/stated by each respondent?\nAnd which ones could have been measured dependent data variables? Why do you think so?\nWhy is there so much missing data? Which variable are the most affected by this?\n\n\n Counts, and Charts with Counts\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.1 What are the Education levels and the counts of people with those levels?\n\n\n\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarise(total = n())\n\n\n  \n\n\n# This also works\n# tally(~Education, data = NHANES) %&gt;% as_tibble()\n\nInsight: The count goes up as we go from lower Education levels to higher. Need to keep that in mind. How do we understand the large number of NA entries?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.2 How do counts of Education vs Work-status look like?\n\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n())\nNHANES %&gt;%\n  group_by(Work, Education) %&gt;%\n  summarise(count = n()) %&gt;%\n  e_charts(Education, height = 300) %&gt;%\n  e_bar(count) %&gt;%\n  e_y_axis(max = 1750) %&gt;%\n  e_x_axis(type = \"category\") %&gt;%\n  e_tooltip()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Clear increase in the number of Working people as Education goes from 8th Grade to College. No surprise. Are the NotWorking counts a surprise?\n\n {{}} Stat Summaries, Histograms, and Densities\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3. What is the distribution of Physical Activity Days, across Gender? Across Education?\n\n\n# NHANES %&gt;% gf_histogram( ~ PhysActiveDays | Education, fill = ~ Education)\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(cols = 2, rows = 1) %&gt;%\n  e_tooltip()\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(PhysActiveDays, height = 350) %&gt;%\n  e_histogram(PhysActiveDays) %&gt;%\n  e_x_axis(max = 8) %&gt;%\n  e_facet(rows = 1, cols = 3) %&gt;%\n  e_tooltip()\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Can we conclude anything here? The populations in each category are different, as indicated by the different y-axis scales, so what do we need to do? Take percentages or ratios of course, per-capita! How would one do that?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.3a. What is the distribution of Physical Activity Days, across Education and Sex, per capita?\n\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\nNHANES %&gt;%\n  group_by(Education) %&gt;%\n  summarize(mean_active = mean(PhysActiveDays, na.rm = TRUE))\n\n\n\n\n  \n\n\n\n\n  \n\n\n\n\nInsight: Hmm..no great differences in per-capita physical activity. Females are marginally more active than males. No need to even plot this.\n::: {.callout-note title=“Question”} Q.4. How are people Ages distributed across levels of Education?\n# Recall there are missing data\n# gf_boxplot(Age ~ Education,\n#            fill = ~ Education, # Always a good idea to fill boxes\n#            data = NHANES) %&gt;%\n#   gf_theme(theme_classic()) %&gt;% plotly::ggplotly()\n\nNHANES %&gt;%\n  mutate(Education = as.factor(Education)) %&gt;%\n  group_by(Education) %&gt;%\n  e_charts(height = 300) %&gt;% # Should not mention x-variable!!!\n  e_boxplot(Age,\n    colorBy = \"data\",\n    itemStyle = list(borderWidth = 3)\n  ) %&gt;%\n  e_y_axis(name = \"Age\", nameLocation = \"middle\", max = 100, min = 0, nameGap = 25) %&gt;%\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;% # ensures all tick labels on x-axis\n  e_tooltip()\n\n\n\n\n\n\n\n\nInsight: Older age groups are somewhat more heavily represented in groups with lower educational status. But College Graduates also have slightly older age distributions…So do College Educated people live longer? That is a nice Question for some Inferential Modelling. And how to interpret the NA group?\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.5. How is Education distributed over Race?\n\n\nNHANES_by_Race1 &lt;- NHANES %&gt;%\n  group_by(Race1) %&gt;%\n  summarize(population = n())\nNHANES_by_Race1\nNHANES %&gt;%\n  group_by(Education, Race1) %&gt;%\n  summarize(n = n()) %&gt;%\n  left_join(NHANES_by_Race1, by = c(\"Race1\" = \"Race1\")) %&gt;%\n  mutate(percapita_educated = (n / population) * 100) %&gt;%\n  ungroup() %&gt;%\n  group_by(Race1) %&gt;% # Aesthetic 1\n  e_charts(Education, height = 350) %&gt;% # Aesthetic #2\n  e_bar(percapita_educated) %&gt;% # Aesthetic #3\n\n  e_x_axis(\n    type = \"category\", axisTick = list(alignWithLabel = TRUE),\n    axisLabel = list(interval = 0)\n  ) %&gt;%\n  e_y_axis(max = 35) %&gt;%\n  e_facet(rows = 2, cols = 3) %&gt;%\n  e_flip_coords()\n\n\n\n\n  \n\n\n\n\n\n\n\n\nInsight: Blacks, Hispanics, and Mexicans tend to have fewer people with college degrees, as a percentage of their population. Asians and other immigrants have a significant tendency towards higher education!\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.6. What is the distribution of people’s BMI, split by Gender? By Race1?\n\n\n# One can also plot both histograms and densities in an overlay fashion,\n\nNHANES %&gt;%\n  group_by(Gender) %&gt;%\n  e_charts(height = 300) %&gt;%\n  e_density(BMI)\nNHANES %&gt;%\n  group_by(Race1) %&gt;%\n  e_charts(height = 350) %&gt;%\n  e_density(BMI) %&gt;%\n  e_facet(rows = 2, cols = 3)\n\n\n\n\n\n\n\n\n\n\n\n\nInsight: Non-white races tend to have larger portions of their populations with larger BMI. So these races perhaps tend to obesity. By and large BMI distributions are normal.\n\n\n\n\n\n\nNoteQuestion\n\n\n\nQ.7. What is the distribution of people’s Testosterone level vs BMI? Split By Race1?\n\n\n\nNHANES %&gt;%\n  gf_density2d(Testosterone ~ BMI | Race1) %&gt;%\n  gf_theme(theme_classic()) %&gt;%\n  plotly::ggplotly()\n\n\n\n\n\nInsight: Low testosterone levels exist across all BMI values, but healthy levels of T exists only over a smaller range of BMI.\nNote: echarts4r does not seem to provide a 2D-density plot…yet!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#case-study-3-a-complete-example-with-banned-books",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Case Study #3: A complete example with Banned Books",
    "text": "Case Study #3: A complete example with Banned Books\nHere is a dataset from Jeremy Singer-Vine’s blog, Data Is Plural. This is a list of all books banned in schools across the US.\n Download the data \n\n Look at the Data\n\nbanned &lt;- readxl::read_xlsx(\n  path = \"../data/banned.xlsx\",\n  sheet = \"Sorted by Author & Title\"\n)\nskim(banned)\n\n\nData summary\n\n\nName\nbanned\n\n\nNumber of rows\n1586\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\nAuthor\n0\n1.00\n7\n29\n0\n797\n0\n\n\nTitle\n0\n1.00\n2\n155\n0\n1145\n0\n\n\nType of Ban\n0\n1.00\n21\n36\n0\n4\n0\n\n\nSecondary Author(s)\n1488\n0.06\n9\n187\n0\n61\n0\n\n\nIllustrator(s)\n1222\n0.23\n8\n35\n0\n192\n0\n\n\nTranslator(s)\n1576\n0.01\n14\n25\n0\n9\n0\n\n\nState\n0\n1.00\n4\n14\n0\n26\n0\n\n\nDistrict\n0\n1.00\n4\n40\n0\n86\n0\n\n\nDate of Challenge/Removal\n0\n1.00\n5\n15\n0\n15\n0\n\n\nOrigin of Challenge\n0\n1.00\n13\n16\n0\n2\n0\n\n\n\n\n\nInsight: Clearly the variables are all Qualitative, except perhaps for Date of Challenge/Removal, (which in this case has been badly mangled by Excel) So we need to make counts based on the* levels* of the Qual variables and plot Bar/Column charts. We will not find a use for histograms or densities.\nLet us try to answer this question, about counts:\n\n\n\n\n\n\nNoteQuestion\n\n\n\nWhat is the count of banned books by type and by US state?\n\n\n\nbanned_by_state &lt;-\n  banned %&gt;%\n  group_by(State) %&gt;%\n  summarise(total = n()) %&gt;%\n  ungroup()\nbanned_by_state\n\n\n  \n\n\nbanned %&gt;%\n  group_by(State, `Type of Ban`) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  left_join(., banned_by_state, by = c(\"State\" = \"State\")) %&gt;%\n  #  pivot_wider(.,id_cols = State,\n  #              names_from = `Type of Ban`,\n  #              values_from = count) %&gt;% janitor::clean_names() %&gt;%\n  #  replace_na(list(banned_from_libraries_and_classrooms = 0,\n  #                  banned_from_libraries = 0,\n  #                  banned_pending_investigation = 0,\n  #                  banned_from_classrooms = 0)) %&gt;%\n  # mutate(total = sum(across(where(is.integer)))) %&gt;%\n  gf_col(count ~ reorder(State, total),\n    fill = ~`Type of Ban`\n  ) %&gt;%\n  gf_labs(\n    x = \"Count of Banned Books\",\n    y = \"State\"\n  ) %&gt;%\n  gf_refine(coord_flip()) %&gt;%\n  gf_theme(theme = theme_minimal())\n\n\n\n\n\n\n\nInsight: Do you want to live in Texas? If you are both illiterate and interested in horses, perhaps."
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#conclusion",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#conclusion",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n Conclusion",
    "text": "Conclusion\nAnd that is a wrap!! Try to work with this procedure:\n\nInspect the data using skim or inspect\n\nIdentify Qualitative and Quantitative variables\n\nNotice variables that have missing data\n\nDevelop Counts of Observations for combinations of Qualitative variables (factors)\n\nDevelop Histograms and Densities, and slice them by Qualitative variables to develop facetted plots as needed\nAt each step record the insight and additional questions!!\n\nContinue with other Descriptive Graphs as needed\n\nAnd then on the inference and modelling!!"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#references",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#references",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "\n References",
    "text": "References\n\nSharon Machlis, Plot in R with echarts4r, InfoWorld https://www.infoworld.com/article/3607068/plot-in-r-with-echarts4r.html\n\nA detailed analysis of the NHANES dataset, https://awagaman.people.amherst.edu/stat230/Stat230CodeCompilationExampleCodeUsingNHANES.pdf"
  },
  {
    "objectID": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#footnotes",
    "href": "content/courses/Analytics/Descriptive/Modules/26-Densities/files/distributions-interactive.html#footnotes",
    "title": "EDA: Exploring Interactive Graphs for Data Distributions in R",
    "section": "Footnotes",
    "text": "Footnotes\n\nFundamentals of Data Visualization (clauswilke.com)↩︎"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "The Mad Hatter's Guide to Data Viz and Stats in R",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  },
  {
    "objectID": "readme.html#get-started-with-quarto",
    "href": "readme.html#get-started-with-quarto",
    "title": "The Mad Hatter's Guide to Data Viz and Stats in R",
    "section": "",
    "text": "Preparatory Work to moving my full website to Quarto!"
  }
]